#!/usr/bin/env python3
#===============================================================================
#      +===+ +--+ +--+ +=+ +===+ +===+
#      |   | |  | |  | | | |     |    
#      |===| |  +-+  | | | |     |=== 
#      |   |  |     |  | | |     |    
#      |   |   +---+   +=+ +===+ +===+                                 
#            ~ Alon Vice Tools ~
# Copyright (c) 2025 Alon Vice (avice)
# All rights reserved.
# This script is the intellectual property of Alon Vice.
# For permissions and licensing, contact: avice@nvidia.com
#===============================================================================

import sys
import io
from contextlib import contextmanager

class QuietMode:
    """Context manager for suppressing stdout while allowing selective printing"""
    def __init__(self, enabled=False):
        self.enabled = enabled
        self.original_stdout = None
        self.null_stream = None
    
    def __enter__(self):
        if self.enabled:
            self.original_stdout = sys.stdout
            self.null_stream = io.StringIO()
            sys.stdout = self.null_stream
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.enabled and self.original_stdout:
            sys.stdout = self.original_stdout
        return False
    
    def print_always(self, *args, **kwargs):
        """Print to original stdout even in quiet mode"""
        if self.enabled and self.original_stdout:
            print(*args, **kwargs, file=self.original_stdout)
        else:
            print(*args, **kwargs)

"""
Script Name: avice_wa_review.py
Version: 2.0.0
Purpose: Comprehensive ASIC/SoC design workarea analysis and review tool

Description:
    This script provides a comprehensive analysis of ASIC/SoC design workareas,
    covering the complete ASIC design flow from synthesis through signoff.
    statistics, verification results, and generates interactive HTML reports.

===============================================================================
                           KEY FEATURES
===============================================================================
  - Multi-IPO support with automatic IPO detection
  - Selective section analysis for faster debugging (-s flag)
  - Professional HTML reports with absolute paths for portability
  - Runtime analysis with timeline tracking and flow detection
  - Dual-scenario timing analysis (setup/hold) with DSR skew tracking
  - PT waiver gap detection against central pt.csv
  - ECO analysis with dont_use cell validation
  - Formal verification timestamp tracking
  - Physical verification (LVS/DRC/Antenna) flow analysis
  - GL Check error categorization and analysis
  - Cross-directory execution support

===============================================================================
                         ANALYSIS SECTIONS
===============================================================================
  Section         | Description
  --------------- | --------------------------------------------------------
  setup           | Environment, BeFlow config, PRC configuration
  runtime         | Flow runtimes (DC, PnR, Star, PT, Formal, PV, GL Check)
  synthesis       | QoR reports, floorplan dimensions, timing groups
  pnr             | Step sequence, routing data, timing histograms
  clock           | Clock tree analysis, DSR latency, clock gating
  formal          | Formal verification status, multibit mapping files, timestamps
  star            | Parasitic extraction (SPEF) runtime and status
  pt              | Signoff timing, dual-scenario WNS/TNS/NVP, DSR skew, waiver gaps
  pv              | Physical verification (LVS/DRC/Antenna) analysis
  gl-check        | Gate-level check error analysis and categorization
  eco             | PT-ECO and NV Gate ECO analysis
  nv-gate-eco     | NVIDIA Gate ECO command analysis and validation
  block-release   | Block release information and umake commands

===============================================================================
                            BASIC USAGE
===============================================================================
  RECOMMENDED (C-shell launcher - handles Python path automatically):
    /home/scratch.avice_vlsi/cursor/avice_wa_review_launcher.csh <workarea_path> [ipo_name]

  DIRECT PYTHON (requires correct Python version):
    /home/utils/Python/builds/3.11.9-20250715/bin/python3 avice_wa_review.py <workarea_path>

  HELP:
    /home/scratch.avice_vlsi/cursor/avice_wa_review_launcher.csh --help

===============================================================================
                            ARGUMENTS
===============================================================================
  POSITIONAL:
    workarea_path         Path to workarea directory (required unless -u/--unit used)
    ipo_name              IPO name to analyze (optional, auto-detected if omitted)

===============================================================================
                             OPTIONS
===============================================================================
  WORKAREA SELECTION:
    -u, --unit UNIT       Use unit name from AGUR release table (e.g., prt, pmux)
                          Automatically looks up workarea path
                          Example: -u prt, -u pmux, -u fdb

  SECTION SELECTION:
    -s, --sections SECTION [SECTION ...]
                          Run only specific sections (faster than full analysis)
                          Available: setup, runtime, synthesis, pnr, clock, formal,
                                     star, pt, pv, gl-check, eco, nv-gate-eco, block-release
                          Aliases: syn/dc = synthesis, star = parasitic, pt = timing
                          Example: -s runtime pt formal

  DISPLAY OPTIONS:
    --no-logo             Disable ASCII logo (useful for scripts/automation)
    -v, --verbose         Enable detailed verbose output
    -q, --quiet           Suppress all output except HTML file messages

  ADVANCED:
    --skip-validation     Skip workarea validation (use with caution)
    --version             Show version and exit

  DOCUMENTATION:
    --help-docs           Display formatted documentation in terminal
    --open-docs           Generate HTML docs and open in browser
    --generate-pdf        Generate PDF documentation
    --docs-section SEC    Show specific section: usage, examples, troubleshooting

===============================================================================
                          PREREQUISITES
===============================================================================
  - Python 3.6+ (recommended: Python 3.11.9)
  - Read permissions to workarea directory
  - Unix/Linux environment with standard tools (grep, zcat, du)
  - Design flow output files (DC, Innovus, PrimeTime, Star, etc.)
  - Firefox 118+ for viewing HTML reports (/home/utils/firefox-118.0.1/firefox)

===============================================================================
                             OUTPUT
===============================================================================
  TERMINAL OUTPUT:
    - Color-coded results with ASCII characters only (Unix-safe)
    - Compact tables for quick scanning
    - Status indicators: [OK], [ERROR], [WARN], [SKIP]
    - Minimal line count for readability

  HTML REPORTS (saved in current working directory):
    - avice_MASTER_dashboard_<design>_<timestamp>.html
    - avice_runtime_report_<design>_<timestamp>.html
    - avice_PnR_comprehensive_<design>_<ipo>_<timestamp>.html
    - avice_DC_comprehensive_<design>_<timestamp>.html
    - avice_PT_timing_summary_<design>_<timestamp>.html
    - avice_gl_check_<design>_<ipo>_<timestamp>.html

  HTML FEATURES:
    - Comprehensive data beyond terminal output
    - Clickable log links with absolute paths (portable!)
    - Interactive tables with sorting/filtering
    - Expandable/collapsible sections
    - Timeline visualizations
    - Professional CSS styling
    - Mobile-responsive layout
    - Works from any directory (absolute paths)

===============================================================================
                             EXAMPLES
===============================================================================
  BASIC ANALYSIS:
    # Full analysis (all 13 sections)
    /home/scratch.avice_vlsi/cursor/avice_wa_review_launcher.csh /path/to/workarea

    # Analyze specific IPO
    /home/scratch.avice_vlsi/cursor/avice_wa_review_launcher.csh /path/to/workarea ipo1000

  AGUR UNIT ANALYSIS (automatic workarea lookup):
    /home/scratch.avice_vlsi/cursor/avice_wa_review_launcher.csh -u prt
    /home/scratch.avice_vlsi/cursor/avice_wa_review_launcher.csh -u pmux
    /home/scratch.avice_vlsi/cursor/avice_wa_review_launcher.csh -u fdb

  SELECTIVE SECTION ANALYSIS (faster):
    # Quick timing check
    /home/scratch.avice_vlsi/cursor/avice_wa_review_launcher.csh /path/to/workarea -s runtime pt

    # Pre-release check
    /home/scratch.avice_vlsi/cursor/avice_wa_review_launcher.csh /path/to/workarea -s formal pt pv gl-check

    # Synthesis and PnR only
    /home/scratch.avice_vlsi/cursor/avice_wa_review_launcher.csh /path/to/workarea -s setup synthesis pnr

  AUTOMATION:
    # No logo for scripts
    /home/scratch.avice_vlsi/cursor/avice_wa_review_launcher.csh /path/to/workarea --no-logo

    # Quiet mode (suppress output, only show HTML files)
    /home/scratch.avice_vlsi/cursor/avice_wa_review_launcher.csh /path/to/workarea --quiet
    /home/scratch.avice_vlsi/cursor/avice_wa_review_launcher.csh /path/to/workarea -q

    # Batch processing example
    /home/scratch.avice_vlsi/cursor/avice_wa_review_launcher.csh /path/to/workarea -q --no-logo

  DOCUMENTATION:
    /home/scratch.avice_vlsi/cursor/avice_wa_review_launcher.csh --help-docs
    /home/scratch.avice_vlsi/cursor/avice_wa_review_launcher.csh --open-docs

===============================================================================
                          IMPORTANT NOTES
===============================================================================
  [!] IPO Directory Handling:
      - Tool auto-resolves if IPO from .prc file doesn't exist
      - Users sometimes delete IPO dirs to save disk space
      - Tool will scan for available IPOs and use first found

  [!] Output Compatibility:
      - Terminal output uses ASCII-only characters (Unix-safe)
      - No Unicode symbols (no arrows, checkmarks, bullets)
      - Use: ->, [OK], [ERROR], [WARN] instead

  [!] HTML Reports:
      - Generated in CURRENT working directory (not workarea)
      - All file links use absolute paths (portable across dirs)
      - Can copy HTML anywhere and links still work

  [!] Performance:
      - Use -s flag for selective analysis (much faster)
      - Use --quiet flag for batch processing (suppresses output)
      - Full analysis takes 30-60 seconds

  [!] Known Issues:
      - Formal status may show "SUCCEEDED" when actually "INCONCLUSIVE"
      - Always manually verify formal logs for critical releases

===============================================================================
                            CONTACT
===============================================================================
  For questions, bug reports, or feature requests:
    Email: avice@nvidia.com
    Project: Avice Workarea Review Tool v2.0
===============================================================================
"""

import os
import sys
import glob
import re
import subprocess
import argparse
import shutil
import json
import signal
import csv
import html
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple
from enum import Enum
import base64
from datetime import datetime
import gzip
import time

# Try to import openpyxl for Excel generation
try:
    import openpyxl
    from openpyxl.styles import PatternFill, Font, Alignment, Border, Side
    from openpyxl.utils import get_column_letter
    OPENPYXL_AVAILABLE = True
except ImportError:
    OPENPYXL_AVAILABLE = False

# Handle dataclasses for Python < 3.7
try:
    from dataclasses import dataclass
except ImportError:
    # Fallback for older Python versions
    def dataclass(cls):
        def __init__(self, **kwargs):
            for key, value in kwargs.items():
                setattr(self, key, value)
        cls.__init__ = __init__
        return cls


# ============================================================================
# FEATURE TOGGLE: Tablog Web Server Integration
# ============================================================================
# Enable/disable tablog server mode for opening log files
# When True: Log links use HTTP requests to localhost:8888 (requires server running)
# When False: Log links use direct file:// URLs (original behavior)
# Set to False to completely revert to original behavior
USE_TABLOG_SERVER = True  # Toggle this to enable/disable server mode
TABLOG_SERVER_URL = 'http://localhost:8888'  # Server endpoint
# ============================================================================


class LVSViolationParser:
    """Parser for LVS error files to extract detailed violation information"""
    
    def parse_lvs_errors(self, file_path: str) -> Dict[str, Any]:
        """Parse LVS error file and extract violation details"""
        violations = {
            'status': 'UNKNOWN',
            'failed_equivalence_points': 0,
            'first_priority_errors': 0,
            'second_priority_errors': 0,
            'unmatched_schematic_instances': 0,
            'unmatched_schematic_nets': 0,
            'unmatched_layout_instances': 0,
            'unmatched_layout_nets': 0,
            'unmatched_schematic_ports': 0,
            'unmatched_layout_ports': 0,
            'matched_instances': 0,
            'matched_nets': 0,
            'matched_ports': 0,
            'successful_equivalence_points': 0
        }
        
        try:
            with open(file_path, 'r') as f:
                content = f.read()
            
            # Extract final comparison result
            status_match = re.search(r'Final comparison result:\s*(\w+)', content)
            if status_match:
                violations['status'] = status_match.group(1)
            
            # Extract comparison summary
            summary_match = re.search(
                r'(\d+)\s+Successful equivalence points\s*\*\s*(\d+)\s+Failed equivalence points\s*(\d+)\s+First priority errors\s*(\d+)\s+Second priority errors',
                content, re.MULTILINE
            )
            if summary_match:
                violations['successful_equivalence_points'] = int(summary_match.group(1))
                violations['failed_equivalence_points'] = int(summary_match.group(2))
                violations['first_priority_errors'] = int(summary_match.group(3))
                violations['second_priority_errors'] = int(summary_match.group(4))
            
            # Extract error summary details with more flexible pattern
            error_summary_pattern = r'Error summary:\s*(\d+)\s+Unmatched schematic instance[s]?\s*(\d+)\s+Unmatched schematic nets?\s*(\d+)\s+Unmatched layout instance[s]?\s*(\d+)\s+Unmatched layout nets?'
            error_summary_match = re.search(error_summary_pattern, content, re.MULTILINE | re.DOTALL)
            if error_summary_match:
                violations['unmatched_schematic_instances'] = int(error_summary_match.group(1))
                violations['unmatched_schematic_nets'] = int(error_summary_match.group(2))
                violations['unmatched_layout_instances'] = int(error_summary_match.group(3))
                violations['unmatched_layout_nets'] = int(error_summary_match.group(4))
            
            # Extract matched counts
            matched_match = re.search(
                r'(\d+)\s+Matched instances\s*(\d+)\s+Matched nets',
                content, re.MULTILINE
            )
            if matched_match:
                violations['matched_instances'] = int(matched_match.group(1))
                violations['matched_nets'] = int(matched_match.group(2))
            
            # Extract port summary with more flexible pattern
            port_pattern = r'Port summary:\s*(\d+)\s+Unmatched schematic ports?\s*(\d+)\s+Unmatched layout ports?\s*(\d+)\s+Matched ports'
            port_match = re.search(port_pattern, content, re.MULTILINE | re.DOTALL)
            if port_match:
                violations['unmatched_schematic_ports'] = int(port_match.group(1))
                violations['unmatched_layout_ports'] = int(port_match.group(2))
                violations['matched_ports'] = int(port_match.group(3))
            
        except Exception as e:
            print(f"Error parsing LVS file {file_path}: {e}")
        
        return violations


class Color:
    """ANSI color codes for terminal output"""
    RED = '\033[31m'
    GREEN = '\033[32m'
    YELLOW = '\033[33m'
    BLUE = '\033[34m'
    MAGENTA = '\033[35m'
    CYAN = '\033[36m'
    WHITE = '\033[37m'
    BOLD = '\033[1m'
    RESET = '\033[0m'


class FlowStage(Enum):
    """Design flow stages"""
    SETUP = "Setup"
    SYNTHESIS = "Synthesis (DC)"
    PLACE_ROUTE = "Place & Route (PnR)"
    PNR_ANALYSIS = "PnR Analysis"
    CLOCK_ANALYSIS = "Clock Analysis"
    FORMAL_VERIFICATION = "Formal Verification"
    PARASITIC_EXTRACTION = "Parasitic Extraction (Star)"
    SIGNOFF_TIMING = "Signoff Timing (PT)"
    PHYSICAL_VERIFICATION = "Physical Verification (PV)"
    GL_CHECK = "GL Checks"
    ECO_ANALYSIS = "ECO Analysis"
    NV_GATE_ECO = "NV Gate ECO"
    BLOCK_RELEASE = "Block Release"
    RUNTIME = "Runtime"
    COMMON = "COMMON"


# Stage index mapping for display (corresponds to INDEX.md)
STAGE_INDEX = {
    FlowStage.SETUP: 1,
    FlowStage.RUNTIME: 2,
    FlowStage.SYNTHESIS: 3,
    FlowStage.PNR_ANALYSIS: 4,
    FlowStage.CLOCK_ANALYSIS: 5,
    FlowStage.FORMAL_VERIFICATION: 6,
    FlowStage.PARASITIC_EXTRACTION: 7,
    FlowStage.SIGNOFF_TIMING: 8,
    FlowStage.PHYSICAL_VERIFICATION: 9,
    FlowStage.GL_CHECK: 10,
    FlowStage.ECO_ANALYSIS: 11,
    FlowStage.NV_GATE_ECO: 12,
    FlowStage.BLOCK_RELEASE: 13,
    FlowStage.COMMON: 14,
    FlowStage.PLACE_ROUTE: 15,  # Placeholder, not commonly used
}


@dataclass
class DesignInfo:
    """Design information extracted from workarea"""
    workarea: str
    top_hier: str
    tag: str
    ipo: str
    all_ipos: List[str]


@dataclass
class SectionSummary:
    """Summary information for a single analysis section"""
    section_name: str           # e.g., "Timing Analysis (PT)"
    section_id: str             # e.g., "timing"
    stage: FlowStage            # FlowStage enum
    status: str                 # PASS, WARN, FAIL, NOT_RUN, SKIP
    key_metrics: Dict[str, str] # e.g., {"Setup WNS": "-0.052", "Hold WNS": "+0.150"}
    html_file: str              # Path to detailed section HTML (relative path for cross-user compatibility)
    priority: int               # 1=Critical, 2=High, 3=Medium, 4=Low
    issues: List[str]           # List of notable issues/warnings
    timestamp: str              # When this section was analyzed
    icon: str                   # ASCII emoji/icon for display
    
    def get_status_color(self) -> str:
        """Get HTML color for status badge"""
        colors = {
            'PASS': '#27ae60',
            'WARN': '#f39c12',
            'FAIL': '#e74c3c',
            'NOT_RUN': '#95a5a6',
            'SKIP': '#bdc3c7'
        }
        return colors.get(self.status, '#95a5a6')
    
    def get_status_icon(self) -> str:
        """Get ASCII icon for status"""
        icons = {
            'PASS': '[OK]',
            'WARN': '[WARN]',
            'FAIL': '[ERROR]',
            'NOT_RUN': '[SKIP]',
            'SKIP': '[SKIP]'
        }
        return icons.get(self.status, '[?]')


class MasterDashboard:
    """Generate master HTML dashboard integrating all section HTMLs"""
    
    def __init__(self, design_info: DesignInfo):
        self.design_info = design_info
        self.sections: List[SectionSummary] = []
        self.output_dir = os.path.dirname(design_info.workarea)
        self.timestamp = datetime.now().strftime("%m.%d.%y_%H:%M")
        self.date_str = datetime.now().strftime("%Y%m%d")
        
    def add_section(self, summary: SectionSummary):
        """Add a section summary to the dashboard"""
        self.sections.append(summary)
    
    def get_overall_status(self) -> str:
        """Determine overall health status"""
        if not self.sections:
            return 'NOT_RUN'
        
        statuses = [s.status for s in self.sections if s.status != 'SKIP']
        if not statuses:
            return 'NOT_RUN'
        
        if 'FAIL' in statuses:
            return 'FAIL'
        elif 'WARN' in statuses:
            return 'WARN'
        else:
            return 'PASS'
    
    def count_by_status(self, status: str) -> int:
        """Count sections with given status"""
        return sum(1 for s in self.sections if s.status == status)
    
    def get_sections_needing_attention(self) -> List[SectionSummary]:
        """Get sections with FAIL or WARN status"""
        return [s for s in self.sections if s.status in ['FAIL', 'WARN']]
    
    def generate_html(self, output_path: str = None) -> str:
        """Generate master dashboard HTML file"""
        if output_path is None:
            # Create default output path using design name (not tag)
            output_path = os.path.join(
                os.getcwd(),
                f"{self.design_info.top_hier}_{os.environ.get('USER', 'avice')}_MASTER_dashboard_{self.date_str}.html"
            )
        
        # Ensure output path is absolute
        output_path = os.path.abspath(output_path)
        
        # Create sections directory if it doesn't exist
        sections_dir = os.path.join(os.path.dirname(output_path), "sections")
        os.makedirs(sections_dir, exist_ok=True)
        
        html_content = self._generate_html_content(output_path)
        
        with open(output_path, 'w') as f:
            f.write(html_content)
        
        return output_path
    
    def _generate_html_content(self, output_path: str) -> str:
        """Generate the HTML content for master dashboard"""
        
        # Read and encode logo as base64
        logo_data = ""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        logo_path = os.path.join(script_dir, "assets/images/avice_logo.png")
        if os.path.exists(logo_path):
            with open(logo_path, "rb") as logo_file:
                logo_data = base64.b64encode(logo_file.read()).decode('utf-8')
        
        # Calculate statistics
        total_sections = len(self.sections)
        pass_count = self.count_by_status('PASS')
        warn_count = self.count_by_status('WARN')
        fail_count = self.count_by_status('FAIL')
        not_run_count = self.count_by_status('NOT_RUN')
        overall_status = self.get_overall_status()
        attention_sections = self.get_sections_needing_attention()
        
        # Sort sections by index number
        sorted_sections = sorted(self.sections, key=lambda s: STAGE_INDEX.get(s.stage, 99))
        
        html = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AVICE Workarea Review - Master Dashboard</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            color: #2c3e50;
            padding: 20px;
            line-height: 1.6;
        }}
        
        .container {{
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        
        /* Header Styles */
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
            position: relative;
        }}
        
        .header h1 {{
            font-size: 2.5em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }}
        
        .logo-container {{
            margin: 20px 0;
        }}
        
        .logo-container img {{
            max-width: 200px;
            height: auto;
            cursor: pointer;
            transition: transform 0.3s ease;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.2);
        }}
        
        .logo-container img:hover {{
            transform: scale(1.05);
        }}
        
        .header-info {{
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 30px;
            margin-top: 20px;
            flex-wrap: wrap;
        }}
        
        .header-info-item {{
            display: flex;
            align-items: center;
            gap: 8px;
            font-size: 1.1em;
        }}
        
        /* Status Banner */
        .status-banner {{
            padding: 30px;
            text-align: center;
            border-bottom: 3px solid #ecf0f1;
        }}
        
        .status-banner.PASS {{
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            color: white;
        }}
        
        .status-banner.WARN {{
            background: linear-gradient(135deg, #f39c12 0%, #f1c40f 100%);
            color: white;
        }}
        
        .status-banner.FAIL {{
            background: linear-gradient(135deg, #eb3349 0%, #f45c43 100%);
            color: white;
        }}
        
        .status-banner h2 {{
            font-size: 2em;
            margin-bottom: 15px;
        }}
        
        /* Enhanced Grid Layout for Status Stats */
        .status-stats {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 30px;
            margin-top: 20px;
            max-width: 900px;
            margin-left: auto;
            margin-right: auto;
        }}
        
        .status-stat {{
            font-size: 1.2em;
            text-align: center;
            padding: 15px;
            background: rgba(255, 255, 255, 0.2);
            border-radius: 10px;
            backdrop-filter: blur(10px);
            transition: transform 0.2s ease;
        }}
        
        .status-stat:hover {{
            transform: scale(1.05);
        }}
        
        .status-stat strong {{
            font-size: 2em;
            display: block;
            margin-bottom: 5px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
        }}
        
        /* Quick Actions */
        .quick-actions {{
            padding: 20px 40px;
            background: #f8f9fa;
            border-bottom: 1px solid #dee2e6;
            display: flex;
            justify-content: center;
            gap: 20px;
            flex-wrap: wrap;
        }}
        
        .action-btn {{
            padding: 12px 24px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            border-radius: 25px;
            cursor: pointer;
            font-size: 1em;
            font-weight: bold;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: transform 0.2s ease, box-shadow 0.2s ease;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }}
        
        .action-btn:hover {{
            transform: translateY(-2px);
            box-shadow: 0 6px 12px rgba(0,0,0,0.2);
        }}
        
        .action-btn.secondary {{
            background: linear-gradient(135deg, #3498db 0%, #2980b9 100%);
        }}
        
        /* Attention Required Section */
        .attention-section {{
            padding: 30px 40px;
            background: #fff3cd;
            border-left: 5px solid #f39c12;
            margin: 20px;
            border-radius: 8px;
        }}
        
        .attention-section h3 {{
            color: #856404;
            margin-bottom: 15px;
        }}
        
        .attention-list {{
            list-style: none;
            padding-left: 0;
        }}
        
        .attention-list li {{
            padding: 8px 0;
            border-bottom: 1px solid #f39c12;
        }}
        
        .attention-list li:last-child {{
            border-bottom: none;
        }}
        
        .attention-list a {{
            color: #856404;
            text-decoration: none;
            font-weight: bold;
        }}
        
        .attention-list a:hover {{
            text-decoration: underline;
        }}
        
        /* Section Cards Grid */
        .sections-container {{
            padding: 40px;
        }}
        
        .sections-container h2 {{
            text-align: center;
            margin-bottom: 30px;
            font-size: 2em;
            color: #2c3e50;
        }}
        
        /* Enhanced Grid Layout for Section Cards */
        .sections-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 20px;
            align-items: start;
            max-width: 100%;
        }}
        
        @media (min-width: 1400px) {{
            .sections-grid {{
                grid-template-columns: repeat(3, 1fr);
                max-width: 1400px;
                margin: 0 auto;
            }}
        }}
        
        @media (min-width: 1000px) and (max-width: 1399px) {{
            .sections-grid {{
                grid-template-columns: repeat(2, 1fr);
            }}
        }}
        
        @media (max-width: 999px) {{
            .sections-grid {{
                grid-template-columns: 1fr;
            }}
        }}
        
        /* Section Card */
        .section-card {{
            background: white;
            border-radius: 12px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            padding: 20px;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            border-left: 5px solid #667eea;
            position: relative;
            min-width: 0;
            max-width: 100%;
            overflow: visible;
        }}
        
        .section-card:hover {{
            transform: translateY(-5px);
            box-shadow: 0 8px 15px rgba(0,0,0,0.2);
        }}
        
        .section-card.PASS {{
            border-left-color: #27ae60;
        }}
        
        .section-card.WARN {{
            border-left-color: #f39c12;
        }}
        
        .section-card.FAIL {{
            border-left-color: #e74c3c;
        }}
        
        .section-card.NOT_RUN {{
            border-left-color: #95a5a6;
        }}
        
        .section-header {{
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 15px;
            cursor: pointer;
            user-select: none;
            gap: 10px;
            min-width: 0;
        }}
        
        .section-header:hover {{
            opacity: 0.8;
        }}
        
        .section-title {{
            font-size: 1.3em;
            font-weight: bold;
            color: #2c3e50;
            display: flex;
            align-items: center;
            gap: 10px;
            flex: 1;
            min-width: 0;
            word-wrap: break-word;
            overflow-wrap: break-word;
        }}
        
        .section-title span {{
            word-wrap: break-word;
            overflow-wrap: break-word;
        }}
        
        .card-toggle-icon {{
            font-size: 1.5em;
            transition: transform 0.3s ease;
            color: #667eea;
        }}
        
        .card-toggle-icon.expanded {{
            transform: rotate(180deg);
        }}
        
        .card-content {{
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease;
        }}
        
        .card-content.expanded {{
            max-height: 2000px;
            overflow: visible;
        }}
        
        .section-index {{
            display: inline-block;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            width: 30px;
            height: 30px;
            border-radius: 50%;
            text-align: center;
            line-height: 30px;
            font-size: 0.9em;
            font-weight: bold;
        }}
        
        .status-badge {{
            padding: 6px 12px;
            border-radius: 20px;
            font-size: 0.85em;
            font-weight: bold;
            color: white;
        }}
        
        .status-badge.PASS {{
            background: #27ae60;
        }}
        
        .status-badge.WARN {{
            background: #f39c12;
        }}
        
        .status-badge.FAIL {{
            background: #e74c3c;
        }}
        
        .status-badge.NOT_RUN {{
            background: #95a5a6;
        }}
        
        /* Enhanced Grid Layout for Section Metrics */
        .section-metrics {{
            margin: 15px 0;
            padding: 10px;
            background: #f8f9fa;
            border-radius: 8px;
            display: grid;
            gap: 6px;
            overflow: visible;
            width: 100%;
            max-width: 100%;
            box-sizing: border-box;
        }}
        
        .metric-row {{
            display: grid;
            grid-template-columns: auto 1fr;
            gap: 12px;
            align-items: center;
            padding: 8px 0;
            border-bottom: 1px solid #dee2e6;
        }}
        
        .metric-row:last-child {{
            border-bottom: none;
        }}
        
        .metric-label {{
            color: #7f8c8d;
            font-weight: 600;
            text-align: left;
            min-width: 0;
        }}
        
        .metric-value {{
            color: #2c3e50;
            font-weight: bold;
            word-break: break-word;
            overflow-wrap: break-word;
            max-width: 100%;
            text-align: right;
        }}
        
        /* Special styling for formal flow rows */
        .formal-flow-row {{
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 6px 2px;
            border-bottom: 1px solid #dee2e6;
            gap: 6px;
            width: 100%;
            box-sizing: border-box;
        }}
        
        .formal-flow-row:last-child {{
            border-bottom: none;
        }}
        
        .formal-flow-row .metric-label {{
            flex: 1 1 auto;
            font-size: 0.85em;
            color: #2c3e50;
            min-width: 0;
            overflow: hidden;
            text-overflow: ellipsis;
            white-space: nowrap;
        }}
        
        .formal-flow-status {{
            flex: 0 0 auto;
            text-align: center;
            font-weight: bold;
            transition: transform 0.2s ease;
            white-space: nowrap;
            line-height: 1.2;
        }}
        
        .formal-flow-status:hover {{
            transform: scale(1.05);
        }}
        
        /* Special styling for block release metric rows */
        .release-metric-row {{
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 8px 4px;
            border-bottom: 1px solid #dee2e6;
            gap: 10px;
            width: 100%;
            box-sizing: border-box;
        }}
        
        .release-metric-row:last-child {{
            border-bottom: none;
        }}
        
        .release-metric-row .metric-label {{
            flex: 0 0 auto;
            font-size: 0.9em;
            color: #2c3e50;
            white-space: nowrap;
        }}
        
        .release-metric-row .metric-value {{
            flex: 1 1 auto;
            font-size: 0.9em;
            color: #34495e;
            text-align: right;
            word-break: break-word;
            overflow-wrap: break-word;
        }}
        
        .section-issues {{
            margin: 15px 0;
        }}
        
        .issue-item {{
            padding: 8px;
            background: #fff3cd;
            border-left: 3px solid #f39c12;
            margin: 5px 0;
            border-radius: 4px;
            font-size: 0.9em;
        }}
        
        .section-footer {{
            margin-top: 20px;
            text-align: center;
        }}
        
        .view-details-btn {{
            display: inline-block;
            padding: 10px 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            text-decoration: none;
            border-radius: 20px;
            font-weight: bold;
            transition: transform 0.2s ease;
        }}
        
        .view-details-btn:hover {{
            transform: scale(1.05);
        }}
        
        .no-report-msg {{
            display: inline-block;
            padding: 10px 20px;
            background: #ecf0f1;
            color: #7f8c8d;
            border-radius: 20px;
            font-style: italic;
            font-size: 0.9em;
            border: 2px dashed #bdc3c7;
        }}
        
        .section-timestamp {{
            font-size: 0.85em;
            color: #95a5a6;
            margin-top: 10px;
            text-align: center;
        }}
        
        /* Footer */
        .footer {{
            text-align: center;
            padding: 30px;
            background: #2c3e50;
            color: white;
        }}
        
        .footer p {{
            margin: 5px 0;
        }}
        
        /* Responsive Design */
        @media (max-width: 768px) {{
            .sections-grid {{
                grid-template-columns: 1fr;
            }}
            
            .header h1 {{
                font-size: 1.8em;
            }}
            
            .header-info {{
                flex-direction: column;
                gap: 10px;
            }}
        }}
        
        /* Image Expansion */
        .expanded-image {{
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0,0,0,0.9);
            z-index: 10000;
            justify-content: center;
            align-items: center;
            cursor: pointer;
        }}
        
        .expanded-image img {{
            max-width: 90%;
            max-height: 90%;
            box-shadow: 0 0 50px rgba(255,255,255,0.3);
        }}
    </style>
</head>
<body>
    <div class="container">
        <!-- Header -->
        <div class="header">
            <h1>AVICE Workarea Review</h1>
            <h2>Master Dashboard</h2>
            <div class="logo-container">
                <img src="data:image/png;base64,{logo_data}" alt="AVICE Logo" onclick="expandImage(this)">
            </div>
            <div class="header-info">
                <div class="header-info-item">
                    <strong>Workarea:</strong> {self.design_info.workarea}
                </div>
                <div class="header-info-item">
                    <strong>Design:</strong> {self.design_info.top_hier}
                </div>
                <div class="header-info-item">
                    <strong>Tag:</strong> {self.design_info.tag}
                </div>
                <div class="header-info-item">
                    <strong>IPO:</strong> {self.design_info.ipo}
                </div>
                <div class="header-info-item">
                    <strong>Generated:</strong> {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
                </div>
            </div>
        </div>
        
        <!-- Overall Status Banner -->
        <div class="status-banner {overall_status}">
            <h2>Overall Health: {overall_status} {self._get_status_icon(overall_status)}</h2>
            <div class="status-stats">
                <div class="status-stat">
                    <strong>{pass_count}</strong>
                    <span>Passed</span>
                </div>
                <div class="status-stat">
                    <strong>{warn_count}</strong>
                    <span>Warnings</span>
                </div>
                <div class="status-stat">
                    <strong>{fail_count}</strong>
                    <span>Failed</span>
                </div>
                <div class="status-stat">
                    <strong>{not_run_count}</strong>
                    <span>Not Run</span>
                </div>
                <div class="status-stat">
                    <strong>{total_sections}</strong>
                    <span>Total Sections</span>
                </div>
            </div>
        </div>
        
        <!-- Quick Actions -->
        <div class="quick-actions">
            <button class="action-btn">Open All Failed/Warning Sections</button>
            <button class="action-btn secondary">Open All Sections</button>
            <button class="action-btn secondary" onclick="window.print()">Print Dashboard</button>
        </div>
"""
        
        # Attention Required Section
        if attention_sections:
            html += f"""
        <!-- Attention Required -->
        <div class="attention-section">
            <h3>Attention Required - {len(attention_sections)} Section(s) Need Review:</h3>
            <ul class="attention-list">
"""
            for section in attention_sections:
                if section.html_file:
                    html += f"""
                <li>
                    <a href="{section.html_file}" target="_blank">
                        [{section.status}] {section.section_name}
                    </a>
                    {f' - {section.issues[0]}' if section.issues else ''}
                </li>
"""
                else:
                    html += f"""
                <li>
                    <span style="color: #856404;">
                        [{section.status}] {section.section_name}
                    </span>
                    {f' - {section.issues[0]}' if section.issues else ''}
                    <em style="color: #95a5a6; font-size: 0.9em;"> (No detailed report)</em>
                </li>
"""
            html += """
            </ul>
        </div>
"""
        
        # Section Cards
        html += """
        <!-- Section Cards -->
        <div class="sections-container">
            <h2>Analysis Sections</h2>
            <div class="sections-grid">
"""
        
        for section in sorted_sections:
            section_index = STAGE_INDEX.get(section.stage, "?")
            html += self._generate_section_card(section, section_index)
        
        html += """
            </div>
        </div>
        
        <!-- Footer -->
        <div class="footer">
            <p><strong>AVICE Workarea Review Tool</strong></p>
            <p>Copyright (c) 2025 Alon Vice (avice)</p>
            <p>Contact: avice@nvidia.com</p>
        </div>
    </div>
    
    <!-- Expanded Image Overlay -->
    <div class="expanded-image" id="expandedImage" onclick="closeImage()">
        <img id="expandedImageContent" src="" alt="Expanded">
    </div>
    
    <script>
        function expandImage(img) {{
            var overlay = document.getElementById('expandedImage');
            var expandedImg = document.getElementById('expandedImageContent');
            expandedImg.src = img.src;
            overlay.style.display = 'flex';
        }}
        
        function closeImage() {{
            document.getElementById('expandedImage').style.display = 'none';
        }}
        
        function toggleCard(cardId) {{
            var content = document.getElementById(cardId);
            var icon = document.getElementById('icon-' + cardId);
            
            if (content.classList.contains('expanded')) {{
                content.classList.remove('expanded');
                icon.classList.remove('expanded');
            }} else {{
                content.classList.add('expanded');
                icon.classList.add('expanded');
            }}
        }}
        
        function openAllSections() {{
            var sections = document.querySelectorAll('.section-card.FAIL a, .section-card.WARN a');
            console.log('Opening ' + sections.length + ' failed/warning sections');
            var delay = 0;
            // Convert NodeList to Array for compatibility
            var sectionsArray = Array.prototype.slice.call(sections);
            sectionsArray.forEach(function(link) {{
                if (link.href && link.href !== '') {{
                    console.log('Opening: ' + link.href);
                    setTimeout(function() {{
                        window.open(link.href, '_blank');
                    }}, delay);
                    delay += 300; // 300ms delay between each window to avoid popup blocker
                }}
            }});
        }}
        
        function openAllSectionsComplete() {{
            var sections = document.querySelectorAll('.section-card a');
            console.log('Opening all ' + sections.length + ' sections');
            if (sections.length === 0) {{
                alert('No section links found. This might indicate a problem with the dashboard generation.');
                return;
            }}
            var delay = 0;
            // Convert NodeList to Array for compatibility
            var sectionsArray = Array.prototype.slice.call(sections);
            sectionsArray.forEach(function(link) {{
                if (link.href && link.href !== '') {{
                    console.log('Opening: ' + link.href);
                    setTimeout(function() {{
                        window.open(link.href, '_blank');
                    }}, delay);
                    delay += 300; // 300ms delay between each window to avoid popup blocker
                }} else {{
                    console.log('Skipping link with no href');
                }}
            }});
        }}
        
        // Back to top button functionality - wait for DOM to load
        document.addEventListener('DOMContentLoaded', function() {{
            var backToTopBtn = document.getElementById('backToTopBtn');
            if (backToTopBtn) {{
                window.addEventListener('scroll', function() {{
                    if (window.pageYOffset > 300) {{
                        backToTopBtn.style.display = 'block';
                    }} else {{
                        backToTopBtn.style.display = 'none';
                    }}
                }});
                
                backToTopBtn.addEventListener('click', function() {{
                    window.scrollTo(0, 0);
                }});
            }}
        }});
        
        // Add event listeners to buttons when DOM is ready (more reliable than inline onclick)
        document.addEventListener('DOMContentLoaded', function() {{
            console.log('DOM loaded, setting up button listeners');
            
            // Open Failed/Warning Sections button
            var actionBtns = document.querySelectorAll('.action-btn');
            console.log('Found action buttons:', actionBtns.length);
            
            if (actionBtns.length > 0) {{
                // First button (Open Failed/Warning)
                actionBtns[0].addEventListener('click', function(e) {{
                    console.log('Button clicked: Open Failed/Warning Sections');
                    e.preventDefault();
                    openAllSections();
                }});
                console.log('Attached listener to button 1');
            }}
            
            if (actionBtns.length > 1) {{
                // Second button (Open All Sections)
                actionBtns[1].addEventListener('click', function(e) {{
                    console.log('Button clicked: Open All Sections');
                    e.preventDefault();
                    openAllSectionsComplete();
                }});
                console.log('Attached listener to button 2');
            }}
        }});
    </script>
    
    <button id="backToTopBtn" style="display: none; position: fixed; bottom: 30px; right: 30px; 
            z-index: 99; border: none; outline: none; background-color: #667eea; color: white; 
            cursor: pointer; padding: 15px 20px; border-radius: 50px; font-size: 16px; 
            font-weight: bold; box-shadow: 0 4px 6px rgba(0,0,0,0.3); transition: all 0.3s ease;"
            onmouseover="this.style.backgroundColor='#5568d3'; this.style.transform='scale(1.1)';"
            onmouseout="this.style.backgroundColor='#667eea'; this.style.transform='scale(1)';">
        â†‘ Top
    </button>
</body>
</html>
"""
        
        return html
    
    def _generate_section_card(self, section: SectionSummary, index: int) -> str:
        """Generate HTML for a single section card"""
        
        # Generate metrics HTML with special handling for formal verification and block release
        metrics_html = ""
        if section.key_metrics:
            # Check if this is formal verification or block release section
            is_formal = section.section_id == "formal"
            is_block_release = section.section_id == "block-release"
            
            for label, value in section.key_metrics.items():
                # Special rendering for block release (skip Design for normal rendering)
                if is_block_release and label != "Design":
                    # Determine icon based on label
                    icon = ""
                    if label == "Total Attempts" or label == "Successful" or label == "Failed":
                        icon = "ðŸ“Š"
                    elif label == "Custom Links":
                        icon = "ðŸ”—"
                    elif label == "Latest Success":
                        icon = "ðŸ“…"
                    elif label == "Release Name":
                        icon = "ðŸ“¦"
                    
                    # Create a styled row for block release metrics with icons
                    metrics_html += f"""
                <div class="metric-row release-metric-row">
                    <span class="metric-label"><strong>{icon} {label}</strong></span>
                    <span class="metric-value">{value}</span>
                </div>
"""
                # Special rendering for formal flows (skip Design and RTL Tag for special rendering)
                elif is_formal and label not in ["Design", "RTL Tag"]:
                    # Parse status from value string (e.g., "SUCCEEDED (0.5h)" or "UNRESOLVED (1.2h)")
                    status_badge = ""
                    status_color = "#95a5a6"  # default gray
                    display_value = value
                    
                    # Shorten "hours" to "h" for more compact display
                    display_value = display_value.replace(" hours", "h").replace(" hour", "h")
                    
                    if "SUCCEEDED" in value or "PASS" in value:
                        status_badge = "PASS"
                        status_color = "#27ae60"  # green
                        # Make even more compact: show as checkmark + time
                        display_value = f"[OK] " + display_value.replace("SUCCEEDED ", "")
                    elif "FAILED" in value or "CRASHED" in value:
                        status_badge = "FAIL"
                        status_color = "#e74c3c"  # red
                        display_value = f"[X] " + display_value.replace("FAILED ", "").replace("CRASHED ", "")
                    elif "UNRESOLVED" in value:
                        status_badge = "UNRESOLVED"
                        status_color = "#f39c12"  # orange
                        display_value = f"[WARN] " + display_value.replace("UNRESOLVED ", "")
                    elif "RUNNING" in value:
                        status_badge = "RUNNING"
                        status_color = "#17a2b8"  # blue
                        display_value = f"[RUN] " + display_value.replace("RUNNING ", "")
                    
                    # Create a styled row for formal flows with status badge
                    metrics_html += f"""
                <div class="metric-row formal-flow-row">
                    <span class="metric-label" title="{label}">{label}</span>
                    <span class="formal-flow-status" style="background-color: {status_color}; color: white; padding: 3px 6px; border-radius: 8px; font-size: 0.75em; font-weight: bold;">{display_value}</span>
                </div>
"""
                else:
                    # Normal rendering for non-formal sections or Design/RTL Tag
                    # Add title tooltip for long values and apply truncation for RTL Tag
                    if is_formal and label == "RTL Tag":
                        # Special handling for long RTL tags - truncate with ellipsis
                        metrics_html += f"""
                <div class="metric-row">
                    <span class="metric-label">{label}:</span>
                    <span class="metric-value" style="max-width: 300px; overflow: hidden; text-overflow: ellipsis; white-space: nowrap;" title="{value}">{value}</span>
                </div>
"""
                    else:
                        metrics_html += f"""
                <div class="metric-row">
                    <span class="metric-label">{label}:</span>
                    <span class="metric-value">{value}</span>
                </div>
"""
        
        # Generate issues HTML
        issues_html = ""
        if section.issues:
            for issue in section.issues[:3]:  # Show max 3 issues
                issues_html += f"""
                <div class="issue-item">{issue}</div>
"""
            if len(section.issues) > 3:
                issues_html += f"""
                <div class="issue-item">... and {len(section.issues) - 3} more</div>
"""
        
        # Generate tooltip text for status badge (show criteria that weren't met)
        tooltip_text = ""
        if section.status in ['FAIL', 'WARN'] and section.issues:
            # Create tooltip showing which criteria failed
            tooltip_text = "Criteria not met:\\n" + "\\n".join(f"- {issue}" for issue in section.issues)
            # Escape HTML special characters for safe tooltip display
            tooltip_text = tooltip_text.replace('"', '&quot;').replace('<', '&lt;').replace('>', '&gt;')
        
        # Default to collapsed for all cards
        default_expanded = False
        expanded_class = 'expanded' if default_expanded else ''
        
        # Add title attribute to status badge if there are issues
        status_badge_html = f'<div class="status-badge {section.status}" title="{tooltip_text}">{section.get_status_icon()}</div>' if tooltip_text else f'<div class="status-badge {section.status}">{section.get_status_icon()}</div>'
        
        card_html = f"""
                <div class="section-card {section.status}">
                    <div class="section-header" onclick="toggleCard('card-{section.section_id}-{index}')">
                        <div class="section-title">
                            <span class="section-index">{index}</span>
                            <span>{section.section_name}</span>
                        </div>
                        <div style="display: flex; align-items: center; gap: 10px;">
                            {status_badge_html}
                            <span class="card-toggle-icon {expanded_class}" id="icon-card-{section.section_id}-{index}">â–¼</span>
                        </div>
                    </div>
                    
                    <div class="card-content {expanded_class}" id="card-{section.section_id}-{index}">
                        {f'<div class="section-metrics">{metrics_html}</div>' if metrics_html else ''}
                        
                        {f'<div class="section-issues">{issues_html}</div>' if issues_html else ''}
                        
                        <div class="section-footer">
                            {f'<a href="{section.html_file}" target="_blank" class="view-details-btn" onclick="event.stopPropagation()">View Detailed Report</a>' if section.html_file else '<span class="no-report-msg">No detailed report available</span>'}
                        </div>
                        
                        <div class="section-timestamp">Analyzed: {section.timestamp}</div>
                    </div>
                </div>
"""
        
        return card_html
    
    def _get_status_icon(self, status: str) -> str:
        """Get ASCII icon for status"""
        icons = {
            'PASS': '[OK]',
            'WARN': '[WARN]',
            'FAIL': '[ERROR]',
            'NOT_RUN': '[SKIP]'
        }
        return icons.get(status, '[?]')


class LogoDisplay:
    """Handle logo display functionality"""
    
    @staticmethod
    def get_logo_path() -> str:
        """Get the path to the Avice logo (for reference only)"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        return os.path.join(script_dir, "assets", "images", "avice_logo.png")
    
    @staticmethod
    def display_logo():
        """Display the Avice logo if available (image popup disabled)"""
        # Logo image popup functionality has been disabled
        # Only ASCII art logo is displayed
        pass
    
    @staticmethod
    def print_ascii_logo():
        """Print ASCII art logo as fallback"""
        ascii_logo = f"""
{Color.CYAN}    +===============================================================+
    |                                                               |
    |                    {Color.BOLD}AVICE WORKAREA REVIEW{Color.RESET}{Color.CYAN}                      |
    |                                                               |
    |              {Color.GREEN}Advanced Verification & Integration{Color.RESET}{Color.CYAN}              |
    |                    {Color.GREEN}Circuit Engineering{Color.RESET}{Color.CYAN}                        |
    |                                                               |
    +===============================================================+{Color.RESET}
"""
        print(ascii_logo)


class FileUtils:
    """Utility functions for file operations"""
    
    @staticmethod
    def realpath(path: str) -> str:
        """Get real path of a file/directory"""
        try:
            return os.path.realpath(path)
        except (OSError, FileNotFoundError):
            return path
    
    @staticmethod
    def file_exists(path: str) -> bool:
        """Check if file exists"""
        return os.path.isfile(path)
    
    @staticmethod
    def dir_exists(path: str) -> bool:
        """Check if directory exists"""
        return os.path.isdir(path)
    
    @staticmethod
    def find_files(pattern: str, base_path: str) -> List[str]:
        """Find files matching pattern"""
        search_path = os.path.join(base_path, pattern)
        return glob.glob(search_path)
    
    @staticmethod
    def grep_file(pattern: str, file_path: str, case_insensitive: bool = True) -> List[str]:
        """Grep pattern in file (handles both regular and compressed files)"""
        try:
            # Check if file is compressed
            if file_path.endswith('.gz'):
                with gzip.open(file_path, 'rt', encoding='utf-8') as f:
                    content = f.read()
            else:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            flags = re.IGNORECASE if case_insensitive else 0
            flags |= re.MULTILINE
            matches = re.findall(pattern, content, flags)
            return matches
        except (OSError, FileNotFoundError, UnicodeDecodeError, gzip.BadGzipFile):
            return []
    
    @staticmethod
    def run_command(cmd: str) -> str:
        """Run shell command and return output"""
        try:
            result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            return result.stdout.decode('utf-8').strip()
        except subprocess.SubprocessError:
            return ""
    
    @staticmethod
    def filter_symlinks(file_paths: List[str]) -> List[str]:
        """Filter out symlink duplicates from file list
        
        When searching directories with symlinks (e.g., umake_log/latest_dir),
        the same file can appear multiple times via different paths (once via 
        symlink, once via real path). This method resolves all paths to their
        real locations and removes duplicates.
        
        Args:
            file_paths: List of file paths that may contain symlink duplicates
            
        Returns:
            List of unique file paths with symlinks resolved
            
        Example:
            files = [
                '/wa/umake_log/latest_dir/umake.log',  # symlink
                '/wa/umake_log/2025_10_22/umake.log'   # real file (same as above)
            ]
            result = FileUtils.filter_symlinks(files)
            # Returns: ['/wa/umake_log/latest_dir/umake.log']  # Only one entry
            
        Note:
            This addresses the .cursor/rules/architecture.mdc requirement (Lines 73-78) to
            always filter out symlinks when searching directories to avoid duplicates.
        """
        seen_real_paths = set()
        filtered_paths = []
        
        for path in file_paths:
            real_path = os.path.realpath(path)
            if real_path not in seen_real_paths:
                seen_real_paths.add(real_path)
                filtered_paths.append(path)  # Keep original path
        
        return filtered_paths


class WorkareaReviewer:
    """Main class for workarea review"""
    
    # Centralized Color Coding Thresholds (used by both per-design and summary sheets)
    # These thresholds define when metrics are colored green/yellow/red
    TIMING_THRESHOLDS = {
        'wns_yellow': -0.050,    # WNS >= -0.050 ns = yellow, < -0.050 = red
        'wns_green': 0.0,        # WNS >= 0 = green
        'tns_yellow': -10.0,     # TNS >= -10.0 ns = yellow, < -10.0 = red
        'tns_green': -0.01,      # TNS >= -0.01 = green (treats tiny negatives as passing)
    }
    
    PV_THRESHOLDS = {
        'errors_yellow': 10,     # Errors < 10 = yellow, >= 10 = red
        'errors_green': 0,       # Errors == 0 = green
    }
    
    def __init__(self, workarea: str, ipo: Optional[str] = None, show_logo: bool = True, skip_validation: bool = False, quiet: bool = False, quiet_mode=None):
        # Resolve workarea path correctly even if user is in subdirectory
        # Also find the main workarea root for relative path display
        self.workarea, self.workarea_root = self._resolve_workarea_path(workarea)
        self.workarea_abs = os.path.abspath(self.workarea)  # Absolute path for display
        self.ipo = ipo
        self.user_specified_ipo = ipo is not None  # Track if IPO was user-specified via -i/--ipo
        self.show_logo = show_logo
        self.quiet = quiet  # Suppress terminal output except HTML generation messages
        self.quiet_mode = quiet_mode  # QuietMode instance for selective printing
        self.file_utils = FileUtils()
        self.lvs_parser = LVSViolationParser()
        
        # Validate workarea before proceeding (unless skipped)
        if not skip_validation and not self._validate_workarea():
            sys.exit(1)
        elif skip_validation:
            print(f"{Color.YELLOW}[WARN] Skipping workarea validation (--skip-validation used){Color.RESET}")
            
        self.design_info = self._extract_design_info()
        
        # Initialize Master Dashboard
        self.master_dashboard = MasterDashboard(self.design_info)
        self.section_summaries = []  # Collect section summaries for master dashboard
        
        # NBU Signoff Support - Initialize detection variables
        self.uses_nbu_signoff = False                # Boolean flag - True if NBU signoff detected
        self.nbu_signoff_paths = {}                  # Dict: {ipo_name: nbu_signoff_path}
        self.nbu_signoff_steps = []                  # List of steps using NBU (e.g., ['auto_pt', 'gl-check'])
        self.current_ipo_nbu_path = None             # Currently active IPO's nbu_signoff path
        self.workarea_owner = None                   # Workarea owner for block release validation
        
        # Discover NBU signoff structure and workarea owner
        self._detect_workarea_owner()
        self._discover_nbu_signoff_paths()
        self._detect_nbu_signoff_from_prc()
        
        # Auto-start tablog server if enabled (for one-click log file opening from HTML)
        self._ensure_tablog_server_running()
    
    def _resolve_workarea_path(self, workarea: str) -> tuple:
        """Resolve workarea path correctly even if user is in a subdirectory
        
        This handles cases where user is located in pnr_flow/nv_flow/ or other
        subdirectories and passes a relative path. The method walks up the directory
        tree to find the workarea root, then resolves the path from there.
        
        Args:
            workarea: User-provided workarea path (could be relative or absolute)
            
        Returns:
            Tuple of (resolved_workarea_path, main_workarea_root):
            - resolved_workarea_path: Target workarea (could be nbu_signoff subdirectory)
            - main_workarea_root: Main workarea root for relative path display
            
        Examples:
            User in: /home/.../pnr_flow/nv_flow/
            Input:   pmux/ipo1015/nbu_signoff
            Output:  (/home/.../pnr_flow/nv_flow/pmux/ipo1015/nbu_signoff,
                      /home/.../pmux_rbv.../)
            
            User in: /home/.../nbu_signoff/
            Input:   .
            Output:  (/home/.../nbu_signoff/, /home/.../pmux_rbv.../)
        """
        # Get absolute path of input
        abs_workarea = os.path.abspath(workarea)
        current_dir = os.getcwd()
        
        # SPECIAL CASE: If input is "." or current directory, user wants to analyze where they are
        # This handles: cd /path/to/nbu_signoff && wa1 . -s formal
        if workarea == "." or abs_workarea == current_dir:
            # Find main workarea root by walking up from current directory
            check_dir = current_dir
            for _ in range(15):
                parent_dir = os.path.dirname(check_dir)
                if parent_dir == check_dir:  # Reached filesystem root
                    break
                if self._is_main_workarea_root(parent_dir):
                    return (current_dir, parent_dir)
                check_dir = parent_dir
            
            # If current directory itself is main root
            if self._is_main_workarea_root(current_dir):
                return (current_dir, current_dir)
            
            # Could not find main root - use current directory as both
            return (current_dir, current_dir)
        
        # Find the main workarea root from current directory
        main_workarea_root = None
        check_dir = current_dir
        
        # Walk up directory tree to find workarea root (max 10 levels)
        for _ in range(10):
            if self._is_main_workarea_root(check_dir):
                main_workarea_root = check_dir
                break
            
            parent_dir = os.path.dirname(check_dir)
            if parent_dir == check_dir:  # Reached filesystem root
                break
            check_dir = parent_dir
        
        # If we found a main workarea root and input is relative, resolve from there
        if main_workarea_root and not os.path.isabs(workarea):
            resolved_workarea = os.path.join(main_workarea_root, workarea)
            # Check if resolved path exists
            if os.path.exists(resolved_workarea):
                return (resolved_workarea, main_workarea_root)
        
        # Check if abs_workarea itself is the main root
        if self._is_main_workarea_root(abs_workarea):
            return (abs_workarea, abs_workarea)
        
        # Check if abs_workarea is inside a main workarea root
        # Walk up from abs_workarea to find main root
        check_dir = abs_workarea
        for _ in range(15):
            parent_dir = os.path.dirname(check_dir)
            if parent_dir == check_dir:  # Reached filesystem root
                break
            if self._is_main_workarea_root(parent_dir):
                return (abs_workarea, parent_dir)
            check_dir = parent_dir
        
        # Could not find main workarea root - return workarea as both values
        # (validation will catch if it's invalid)
        return (abs_workarea, abs_workarea)
    
    def _is_main_workarea_root(self, path: str) -> bool:
        """Check if a directory is the MAIN workarea root (not an nbu_signoff subdirectory)
        
        Args:
            path: Directory path to check
            
        Returns:
            True if path is the main workarea root, False for nbu_signoff subdirectories
        """
        if not os.path.isdir(path):
            return False
        
        # Check for common workarea markers
        unit_scripts = os.path.join(path, 'unit_scripts')
        rbv = os.path.join(path, 'rbv')
        pnr_flow = os.path.join(path, 'pnr_flow')
        syn_flow = os.path.join(path, 'syn_flow')
        
        # Main workarea root must have:
        # 1. unit_scripts directory (can be actual or symlink)
        # 2. rbv (can be actual or symlink)
        # 3. pnr_flow as actual directory (not symlink)
        # 4. syn_flow as actual directory (not symlink)
        
        has_unit_scripts = os.path.isdir(unit_scripts)
        has_rbv = os.path.exists(rbv)
        
        # KEY: pnr_flow and syn_flow must be actual directories, not symlinks
        # In nbu_signoff, these are symlinks pointing to parent workarea
        has_pnr_flow = os.path.isdir(pnr_flow) and not os.path.islink(pnr_flow)
        has_syn_flow = os.path.isdir(syn_flow) and not os.path.islink(syn_flow)
        
        return has_unit_scripts and has_rbv and has_pnr_flow and has_syn_flow
    
    def _print(self, *args, **kwargs) -> None:
        """Print to stdout only if quiet mode is disabled
        
        Args:
            *args: Arguments to pass to print()
            **kwargs: Keyword arguments to pass to print()
        
        In quiet mode, suppress all terminal output to allow focus on HTML generation.
        Use _print_always() for messages that must always be shown (e.g., HTML file names).
        """
        if not self.quiet:
            print(*args, **kwargs)
    
    def _print_always(self, *args, **kwargs) -> None:
        """Print to stdout regardless of quiet mode
        
        Args:
            *args: Arguments to pass to print()
            **kwargs: Keyword arguments to pass to print()
        
        Use this for critical messages that must always be shown:
        - HTML file generation messages
        - Master Dashboard generation messages
        - Critical errors
        """
        print(*args, **kwargs)
    
    def _ensure_tablog_server_running(self) -> None:
        """Automatically start tablog server if not already running
        
        This method checks if the tablog web server is running and starts it
        automatically if needed. The server enables one-click log file opening
        from HTML reports.
        
        Behavior:
        - If USE_TABLOG_SERVER is False: Does nothing
        - If server already running: Does nothing
        - If server not running: Starts it in background
        - On failure: Silently fails (fallback to clipboard mode still works)
        """
        if not USE_TABLOG_SERVER:
            return  # Feature disabled
        
        # Check if server already running
        try:
            import urllib.request
            req = urllib.request.Request(f"{TABLOG_SERVER_URL}/ping")
            urllib.request.urlopen(req, timeout=1)
            # Server is already running, inform user
            self._print(f"  {Color.GREEN}[OK] Tablog server is already running (ready for one-click log opening){Color.RESET}")
            self._print(f"  {Color.CYAN}[INFO] To stop server: pkill -f tablog_server.py{Color.RESET}")
            return
        except:
            pass  # Server not running, need to start it
        
        # Start server in background (detached process)
        try:
            server_script = '/home/scratch.avice_vlsi/tablog/tablog_server.py'
            if os.path.exists(server_script):
                self._print(f"  {Color.CYAN}[INFO] Starting tablog server for one-click log file opening...{Color.RESET}")
                
                # Start server as detached background process
                subprocess.Popen(
                    [sys.executable, server_script],
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL,
                    start_new_session=True,  # Detach from parent process
                    cwd=os.path.dirname(server_script)
                )
                
                # Give server time to start and verify it's running
                time.sleep(0.8)
                
                try:
                    req = urllib.request.Request(f"{TABLOG_SERVER_URL}/ping")
                    urllib.request.urlopen(req, timeout=1)
                    self._print(f"  {Color.GREEN}[OK] Tablog server started successfully{Color.RESET}")
                    self._print(f"  {Color.CYAN}[INFO] Server will persist until logout/reboot (no need to stop manually){Color.RESET}")
                    self._print(f"  {Color.CYAN}[INFO] To check if running: curl http://localhost:8888/ping{Color.RESET}")
                    self._print(f"  {Color.CYAN}[INFO] To stop server: pkill -f tablog_server.py{Color.RESET}")
                except:
                    # Server failed to start, but don't crash - fallback will work
                    self._print(f"  {Color.YELLOW}[WARN] Tablog server may not have started (fallback to clipboard mode){Color.RESET}")
            else:
                # Server script not found - silent failure, fallback will work
                pass
        except Exception as e:
            # Silent failure - fallback to clipboard still works
            # Only show warning in non-quiet mode
            if not self.quiet:
                self._print(f"  {Color.YELLOW}[WARN] Could not start tablog server: {e}{Color.RESET}")
    
    def _cleanup_old_html_files(self) -> None:
        """Remove old HTML files from previous runs to prevent confusion"""
        try:
            # Get current working directory where HTMLs are generated
            cwd = os.getcwd()
            
            # Define patterns for HTML files that should be cleaned up
            # Only clean up files for the current design
            design_pattern = f"*{self.design_info.top_hier}*.html"
            
            # Find all matching HTML files
            matching_files = glob.glob(os.path.join(cwd, design_pattern))
            
            if matching_files:
                # Keep track of what was cleaned
                cleaned_count = 0
                
                for html_file in matching_files:
                    try:
                        os.remove(html_file)
                        cleaned_count += 1
                    except OSError as e:
                        # If we can't delete (permissions, etc.), just skip
                        pass
                
                if cleaned_count > 0:
                    print(f"{Color.CYAN}Cleaned up {cleaned_count} old HTML file(s) from previous runs{Color.RESET}")
        
        except Exception as e:
            # Don't fail the review if cleanup fails
            pass
    
    def _get_html_output_dir(self) -> str:
        """
        Determine HTML output directory based on context:
        - If running from project directory (development/testing) -> test_outputs/html/
        - If running from user directory (normal usage) -> html/
        
        Note: Permissions are set to allow disk owner to delete files (see _set_permissive_permissions)
        """
        cwd = os.getcwd()
        
        # Check if we're in the project directory by looking for project-specific markers
        is_project_dir = (
            os.path.isdir(os.path.join(cwd, "sections")) and
            os.path.isdir(os.path.join(cwd, "utilities")) and
            os.path.exists(os.path.join(cwd, "avice_wa_review.py"))
        )
        
        if is_project_dir:
            # Development/testing mode - organize into test_outputs/html/
            return os.path.join(cwd, "test_outputs", "html")
        else:
            # Normal user mode - organize into html/
            return os.path.join(cwd, "html")
    
    def _set_permissive_permissions(self, path: str) -> None:
        """Set permissive permissions on file/directory to allow disk owner to delete
        
        When analyzing another user's workarea, we want to ensure the disk owner
        can clean up HTML files when deleting the workarea. This sets permissions
        to 0666 for files and 0777 for directories.
        
        Args:
            path: File or directory path to set permissions on
        """
        try:
            if os.path.isfile(path):
                # Files: rw-rw-rw- (0666) - read/write for everyone
                os.chmod(path, 0o666)
            elif os.path.isdir(path):
                # Directories: rwxrwxrwx (0777) - full access for everyone
                os.chmod(path, 0o777)
        except OSError:
            # If we can't set permissions, don't fail - file is still created
            pass
    
    def _organize_html_files(self) -> None:
        """Move generated HTML files to appropriate html/ folder for better organization
        
        Also sets permissive permissions (0666 for files, 0777 for dirs) to allow
        disk owner to delete files when cleaning up workareas.
        """
        try:
            # Get current working directory where HTMLs are generated
            cwd = os.getcwd()
            html_dir = self._get_html_output_dir()
            
            # Determine display path for messages (relative to cwd)
            display_path = os.path.relpath(html_dir, cwd)
            
            # Create html directory if it doesn't exist
            os.makedirs(html_dir, exist_ok=True)
            # Set permissive permissions on html directory
            self._set_permissive_permissions(html_dir)
            
            # Define pattern for HTML files generated during this run
            design_pattern = f"*{self.design_info.top_hier}*.html"
            
            # Find all matching HTML files in current directory
            matching_files = glob.glob(os.path.join(cwd, design_pattern))
            
            # Filter to only include files in root (not already in html/)
            root_html_files = [f for f in matching_files if os.path.dirname(f) == cwd]
            
            if root_html_files:
                moved_count = 0
                for html_file in root_html_files:
                    try:
                        # Get just the filename
                        filename = os.path.basename(html_file)
                        dest_path = os.path.join(html_dir, filename)
                        
                        # Move file to html directory
                        shutil.move(html_file, dest_path)
                        
                        # Set permissive permissions on the moved file
                        # This allows disk owner to delete it when cleaning up workarea
                        self._set_permissive_permissions(dest_path)
                        
                        moved_count += 1
                    except OSError as e:
                        # If we can't move (permissions, etc.), just skip
                        pass
                
                if moved_count > 0:
                    print(f"{Color.CYAN}Organized {moved_count} HTML file(s) into {display_path}/ folder{Color.RESET}")
        
        except Exception as e:
            # Don't fail the review if organization fails
            pass
    
    def _add_section_summary(self, section_name: str, section_id: str, stage: FlowStage, 
                            status: str = "NOT_RUN", key_metrics: Optional[Dict[str, str]] = None,
                            html_file: str = "", priority: int = 3, issues: Optional[List[str]] = None,
                            icon: str = "") -> None:
        """Helper method to add a section summary to the master dashboard
        
        Args:
            section_name: Name of the section
            section_id: Unique section identifier
            stage: Flow stage enum
            status: Status string (PASS/WARN/FAIL/NOT_RUN)
            key_metrics: Optional dictionary of key metrics
            html_file: Optional HTML file path
            priority: Priority level (1=highest, 4=lowest)
            issues: Optional list of issue strings
            icon: Optional icon string
        """
        if key_metrics is None:
            key_metrics = {}
        if issues is None:
            issues = []
        
        # Convert absolute path to relative (just filename) for cross-user compatibility
        # This ensures the master dashboard works when opened by different users
        if html_file:
            html_file = os.path.basename(html_file)
        
        summary = SectionSummary(
            section_name=section_name,
            section_id=section_id,
            stage=stage,
            status=status,
            key_metrics=key_metrics,
            html_file=html_file if html_file else "",
            priority=priority,
            issues=issues,
            timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            icon=icon
        )
        
        self.master_dashboard.add_section(summary)
        self.section_summaries.append(summary)
    
    def _validate_workarea(self) -> bool:
        """Validate that the workarea is a proper ASIC/SoC workarea (PnR, Syn, or both)
        
        Returns:
            True if workarea is valid, False otherwise
        """
        print(f"{Color.CYAN}Validating workarea structure...{Color.RESET}")
        
        validation_errors = []
        validation_warnings = []
        flow_types = []
        additional_stages = []  # Track additional stages like fast_dc, RTL
        
        # Always required directories
        required_dirs = ["unit_scripts", "rbv"]
        
        # Check always required directories
        for dir_name in required_dirs:
            dir_path = os.path.join(self.workarea, dir_name)
            if not os.path.isdir(dir_path):
                validation_errors.append(f"Missing required directory: {dir_name}/")
        
        # Check for PnR flow structure
        pnr_flow_dir = os.path.join(self.workarea, "pnr_flow")
        if os.path.isdir(pnr_flow_dir):
            nv_flow_dir = os.path.join(pnr_flow_dir, "nv_flow")
            if not os.path.isdir(nv_flow_dir):
                # Check if this is ECO/Signoff workarea with imported PnR data
                export_innovus_dir = os.path.join(self.workarea, "export/export_innovus")
                signoff_flow_dir = os.path.join(self.workarea, "signoff_flow")
                
                has_pnr_data = False
                if os.path.isdir(export_innovus_dir):
                    # Check for PnR artifacts (DEF, netlist)
                    def_files = glob.glob(os.path.join(export_innovus_dir, "*.def.gz"))
                    netlist_files = glob.glob(os.path.join(export_innovus_dir, "*.gv.gz"))
                    has_pnr_data = bool(def_files or netlist_files)
                
                if has_pnr_data:
                    if os.path.isdir(signoff_flow_dir):
                        flow_types.append("PnR-ECO")
                        validation_warnings.append("ECO/Signoff workarea detected (PnR results imported from external source)")
                    else:
                        flow_types.append("PnR-Imported")
                        validation_warnings.append("PnR results imported (no local PnR execution)")
                else:
                    validation_warnings.append("Missing pnr_flow/nv_flow/ directory")
            else:
                # Check for PRC files (indicates PnR configuration)
                prc_files = glob.glob(os.path.join(nv_flow_dir, "*.prc"))
                if not prc_files:
                    validation_warnings.append("No .prc files found in pnr_flow/nv_flow/")
                else:
                    flow_types.append("PnR")
        
        # Check for synthesis flow structure
        syn_flow_dir = os.path.join(self.workarea, "syn_flow")
        if os.path.isdir(syn_flow_dir):
            dc_dir = os.path.join(syn_flow_dir, "dc")
            if not os.path.isdir(dc_dir):
                validation_warnings.append("Missing syn_flow/dc/ directory")
            else:
                flow_types.append("Syn")
            
            # Check for fast_dc
            fast_dc_dir = os.path.join(syn_flow_dir, "fast_dc")
            fast_dc_log = os.path.join(fast_dc_dir, "log/fast_dc.log")
            if os.path.isfile(fast_dc_log):
                additional_stages.append("fast_dc")
        
        # Check for RTL formal verification
        rtl_formal_dirs = [
            os.path.join(self.workarea, "formal_flow/rtl_vs_pnr_bbox_fm"),
            os.path.join(self.workarea, "formal_flow/rtl_vs_pnr_fm"),
            os.path.join(self.workarea, "rtl_vs_pnr_bbox_fm"),
            os.path.join(self.workarea, "rtl_vs_pnr_fm")
        ]
        rtl_detected = False
        for rtl_dir in rtl_formal_dirs:
            if os.path.isdir(rtl_dir):
                rtl_detected = True
                break
        if rtl_detected:
            additional_stages.append("RTL")
        
        # Check for design definition
        des_def_path = os.path.join(self.workarea, "unit_scripts/des_def.tcl")
        if not os.path.isfile(des_def_path):
            validation_errors.append("Missing unit_scripts/des_def.tcl file")
        
        # Check for RBV information
        rbv_readme = os.path.join(self.workarea, "rbv/README")
        if not os.path.isfile(rbv_readme):
            validation_warnings.append("Missing rbv/README file")
        
        # Validate that at least one flow type is present
        if not flow_types:
            validation_errors.append("No valid flow detected - must have either pnr_flow/ or syn_flow/")
        
        # Print validation results
        if validation_errors:
            print(f"{Color.RED}[ERROR] WORKAREA VALIDATION FAILED:{Color.RESET}")
            for error in validation_errors:
                print(f"  {Color.RED}ERROR:{Color.RESET} {error}")
            print(f"\n{Color.RED}This does not appear to be a valid ASIC/SoC workarea.{Color.RESET}")
            print(f"{Color.YELLOW}Required structure:{Color.RESET}")
            print(f"  - unit_scripts/des_def.tcl")
            print(f"  - rbv/")
            print(f"  - Either pnr_flow/ OR syn_flow/ (or both)")
            return False
        
        if validation_warnings:
            print(f"{Color.YELLOW}[WARN] WORKAREA VALIDATION WARNINGS:{Color.RESET}")
            for warning in validation_warnings:
                print(f"  {Color.YELLOW}WARNING:{Color.RESET} {warning}")
        
        # Determine flow type for success message
        if "PnR-ECO" in flow_types:
            flow_desc = "ECO/Signoff workarea (imported PnR)"
        elif "PnR-Imported" in flow_types:
            flow_desc = "PnR-imported workarea"
        elif "PnR" in flow_types and "Syn" in flow_types:
            flow_desc = "PnR + Syn workarea"
        elif "PnR" in flow_types:
            flow_desc = "PnR-only workarea"
        elif "Syn" in flow_types:
            flow_desc = "Syn-only workarea"
        else:
            flow_desc = "ASIC/SoC workarea"
        
        # Add additional stages info if detected
        if additional_stages:
            additional_info = ", ".join(additional_stages)
            print(f"{Color.GREEN}[OK] Workarea validation passed - {flow_desc} detected{Color.RESET}")
            print(f"{Color.CYAN}     Additional stages detected: {additional_info}{Color.RESET}")
        else:
            print(f"{Color.GREEN}[OK] Workarea validation passed - {flow_desc} detected{Color.RESET}")
        
        return True
    
    def _extract_design_info(self) -> DesignInfo:
        """Extract design information from workarea"""
        # Extract top hierarchy
        des_def_path = os.path.join(self.workarea, "unit_scripts/des_def.tcl")
        top_hier = ""
        if self.file_utils.file_exists(des_def_path):
            matches = self.file_utils.grep_file(r"bset top_hier\s+(\w+)", des_def_path)
            if matches:
                top_hier = matches[0]
        
        # Extract tag
        readme_path = os.path.join(self.workarea, "rbv/README")
        tag = ""
        if self.file_utils.file_exists(readme_path):
            matches = self.file_utils.grep_file(r"tag:\s*(.+)", readme_path)
            if matches:
                tag = matches[0]
        
        # Extract IPO information
        prc_path = os.path.join(self.workarea, f"pnr_flow/nv_flow/{top_hier}.prc")
        all_ipos = []
        ipo = self.ipo
        
        # Discover IPOs from multiple sources
        discovered_ipos = set()
        
        # Method 1: Parse .prc file
        if self.file_utils.file_exists(prc_path):
            # Handle both YAML and legacy PRC formats - capture full IPO name with suffixes
            matches = self.file_utils.grep_file(r"^\s*(ipo\d+(?:_[\w]+)*)\s*:", prc_path)
            discovered_ipos.update(matches)
            if not ipo and matches:
                ipo = matches[0]
        
        # Method 2: Scan pnr_flow/nv_flow/<design>/ directory for IPO directories
        pnr_ipo_dir = os.path.join(self.workarea, f"pnr_flow/nv_flow/{top_hier}")
        if os.path.exists(pnr_ipo_dir):
            try:
                for item in os.listdir(pnr_ipo_dir):
                    item_path = os.path.join(pnr_ipo_dir, item)
                    if os.path.isdir(item_path) and re.match(r'ipo\d+(?:_[\w]+)*$', item):
                        discovered_ipos.add(item)
            except OSError:
                pass
        
        all_ipos = sorted(list(discovered_ipos))
        
        return DesignInfo(
            workarea=self.workarea,
            top_hier=top_hier,
            tag=tag,
            ipo=ipo or "unknown",
            all_ipos=all_ipos
        )
    
    def _detect_workarea_owner(self) -> None:
        """
        Detect workarea owner for block release validation
        
        Used to determine which block_release is "real" vs "stale copy"
        when multiple block_release locations exist (root + nbu_signoff).
        """
        try:
            import pwd
            stat_info = os.stat(self.workarea)
            owner_uid = stat_info.st_uid
            self.workarea_owner = pwd.getpwuid(owner_uid).pw_name
        except Exception as e:
            # Fallback to environment variable
            self.workarea_owner = os.environ.get('USER', 'unknown')
    
    def _discover_nbu_signoff_paths(self) -> None:
        """
        Discover NBU signoff directories for each IPO
        
        NBU signoff structure: pnr_flow/nv_flow/<design>/<ipo>/nbu_signoff/
        
        Updates:
            self.nbu_signoff_paths: Dict mapping IPO names to nbu_signoff paths
            self.uses_nbu_signoff: Boolean flag if any nbu_signoff found
        """
        # Pattern: pnr_flow/nv_flow/<design>/<ipo>/nbu_signoff
        pnr_nv_flow = os.path.join(self.workarea, 'pnr_flow', 'nv_flow')
        
        if not os.path.exists(pnr_nv_flow):
            return
        
        try:
            # Look for <design> subdirectory
            for design_dir in os.listdir(pnr_nv_flow):
                design_path = os.path.join(pnr_nv_flow, design_dir)
                
                if not os.path.isdir(design_path):
                    continue
                
                # Look for IPO directories
                for ipo_dir in os.listdir(design_path):
                    if not ipo_dir.startswith('ipo'):
                        continue
                    
                    ipo_path = os.path.join(design_path, ipo_dir)
                    if not os.path.isdir(ipo_path):
                        continue
                    
                    # Check if nbu_signoff exists
                    nbu_path = os.path.join(ipo_path, 'nbu_signoff')
                    if os.path.exists(nbu_path) and os.path.isdir(nbu_path):
                        # Extract just the ipo name (e.g., "ipo1000" from "ipo1000_ref")
                        # Store full directory name as key
                        self.nbu_signoff_paths[ipo_dir] = nbu_path
                        self.uses_nbu_signoff = True
        
        except Exception as e:
            # Gracefully handle any filesystem errors
            pass
    
    def _detect_nbu_signoff_from_prc(self) -> None:
        """
        Detect NBU signoff steps from PRC file
        
        Looks for: command: ${FLOW2_UTILS_SITE}/runNbuSignoff.py -step <step_name>
        
        Updates:
            self.nbu_signoff_steps: List of steps that use NBU signoff
        """
        # Check main prc file
        prc_files = []
        
        # Pattern 1: pnr_flow/nv_flow/<design>.prc
        if self.design_info.top_hier:
            main_prc = os.path.join(self.workarea, f"pnr_flow/nv_flow/{self.design_info.top_hier}.prc")
            if os.path.exists(main_prc):
                prc_files.append(main_prc)
        
        # Pattern 2: Check nbu_signoff prc files
        for ipo_dir, nbu_path in self.nbu_signoff_paths.items():
            nbu_prc = os.path.join(nbu_path, 'export', 'nv_star', f"{self.design_info.top_hier}.prc")
            if os.path.exists(nbu_prc):
                prc_files.append(nbu_prc)
        
        # Extract runNbuSignoff.py steps from all prc files
        steps_found = set()
        for prc_file in prc_files:
            try:
                with open(prc_file, 'r') as f:
                    for line in f:
                        # Pattern: command: ${FLOW2_UTILS_SITE}/runNbuSignoff.py -step <step_name>
                        if 'runNbuSignoff.py' in line and '-step' in line:
                            match = re.search(r'-step\s+(\S+)', line)
                            if match:
                                step = match.group(1)
                                steps_found.add(step)
            except Exception:
                continue
        
        self.nbu_signoff_steps = sorted(list(steps_found))
    
    def _get_signoff_base_path(self, ipo_name: Optional[str] = None) -> str:
        """
        Get base path for signoff analysis based on NBU mode
        
        Args:
            ipo_name: IPO to get path for (uses current design_info.ipo if None)
        
        Returns:
            Base path (either workarea root or nbu_signoff path)
            
        Examples:
            Regular WA:  /path/to/workarea/
            NBU WA:      /path/to/workarea/pnr_flow/nv_flow/design/ipo1000/nbu_signoff/
        """
        if not ipo_name:
            ipo_name = self.design_info.ipo
        
        # Check if this IPO uses NBU signoff
        if self.uses_nbu_signoff and ipo_name in self.nbu_signoff_paths:
            return self.nbu_signoff_paths[ipo_name]
        else:
            return self.workarea
    
    def _get_signoff_path(self, subdir: str = 'signoff_flow', ipo_name: Optional[str] = None) -> str:
        """
        Get specific signoff subdirectory path
        
        Args:
            subdir: Subdirectory name ('signoff_flow', 'pv_flow', 'formal_flow', 'export', etc.)
            ipo_name: IPO to get path for (uses current design_info.ipo if None)
        
        Returns:
            Full path to signoff subdirectory
            
        Examples:
            Regular WA:  /path/to/workarea/signoff_flow/
            NBU WA:      /path/to/workarea/pnr_flow/nv_flow/design/ipo1000/nbu_signoff/signoff_flow/
        """
        base = self._get_signoff_base_path(ipo_name)
        return os.path.join(base, subdir)
    
    def _detect_nbu_release_from_target(self, target_path: str) -> tuple:
        """
        Detect if a release link points to NBU signoff location
        
        Args:
            target_path: Full path to release target (e.g., /home/.../nbu_signoff_ccorea_..._ipo1001_...)
        
        Returns:
            (is_nbu, ipo_name) - e.g., (True, "ipo1001") or (False, None)
            
        Example:
            nbu_signoff_ccorea_rbv_2025_09_02_..._ipo1001_2025_10_23 -> (True, "ipo1001")
            ccorea_rbv_2025_09_02_..._2025_10_08 -> (False, None)
        """
        basename = os.path.basename(target_path)
        
        # Check if directory name starts with 'nbu_signoff_'
        if basename.startswith('nbu_signoff_'):
            # Extract IPO from path: nbu_signoff_ccorea_..._ipo1001_2025_...
            # Pattern: nbu_signoff_{design}_..._{ipoXXXX}_{timestamp}
            ipo_match = re.search(r'_(ipo\d+[^_]*?)_\d{4}_\d', basename)
            if ipo_match:
                return (True, ipo_match.group(1))
            # If no specific IPO found but it's clearly NBU
            return (True, "unknown")
        
        return (False, None)
    
    def _discover_block_release_locations(self) -> List[tuple]:
        """
        Find ALL block_release locations (root and nbu_signoff)
        
        Returns:
            List of tuples: [(path, ipo, is_nbu, metadata), ...]
            where:
                path: Full path to block_release directory
                ipo: IPO name (None for root location)
                is_nbu: Boolean - True if in nbu_signoff
                metadata: Dict with USER, timestamp, confidence score
        """
        locations = []
        
        # 1. Check root location
        root_br = os.path.join(self.workarea, 'export', 'block_release')
        if os.path.exists(root_br):
            metadata = self._extract_block_release_metadata(root_br)
            locations.append((root_br, None, False, metadata))
        
        # 2. Check each IPO's nbu_signoff location
        for ipo_dir, nbu_path in self.nbu_signoff_paths.items():
            nbu_br = os.path.join(nbu_path, 'export', 'block_release')
            if os.path.exists(nbu_br):
                metadata = self._extract_block_release_metadata(nbu_br)
                locations.append((nbu_br, ipo_dir, True, metadata))
        
        return locations
    
    def _extract_block_release_metadata(self, br_path: str) -> Dict[str, Any]:
        """
        Extract metadata from block_release directory
        
        Args:
            br_path: Path to block_release directory
        
        Returns:
            dict with USER, timestamp, confidence score
        """
        metadata = {
            'user': 'unknown',
            'timestamp': None,
            'log_exists': False,
            'confidence_score': 0,
            'date_str': 'N/A'
        }
        
        # Extract USER from block_release.log
        log_file = os.path.join(br_path, 'log', 'block_release.log')
        if os.path.exists(log_file):
            metadata['log_exists'] = True
            metadata['timestamp'] = os.path.getmtime(log_file)
            
            # Format timestamp for display
            try:
                import datetime
                dt = datetime.datetime.fromtimestamp(metadata['timestamp'])
                metadata['date_str'] = dt.strftime('%b %d %H:%M')
            except Exception:
                pass
            
            # Extract USER field from log
            try:
                with open(log_file, 'r') as f:
                    for line in f:
                        # Pattern: "-I- [2025/10/23 12:03:41] USER: arcohen"
                        match = re.search(r'USER:\s+(\S+)', line, re.IGNORECASE)
                        if match:
                            metadata['user'] = match.group(1)
                            break
            except Exception:
                pass  # Handle read errors gracefully
        
        # Calculate confidence score
        if metadata['user'] == self.workarea_owner:
            metadata['confidence_score'] += 50  # Owner match = high confidence
        
        if metadata['timestamp']:
            days_old = (time.time() - metadata['timestamp']) / 86400
            if days_old < 30:
                metadata['confidence_score'] += 30  # Recent = high confidence
            elif days_old < 90:
                metadata['confidence_score'] += 10  # Somewhat recent
        
        return metadata
    
    def _select_real_block_release(self, locations: List[tuple]) -> tuple:
        """
        Select the REAL block release from multiple candidates
        
        Priority:
        1. USER matches workarea owner (most important!)
        2. Newest timestamp (tiebreaker)
        3. Prefer nbu_signoff if tied (newer workflow)
        
        Args:
            locations: List of tuples from _discover_block_release_locations()
        
        Returns:
            (path, ipo, is_nbu, metadata, reason) or (None, None, False, None, reason)
        """
        if not locations:
            return (None, None, False, None, "No block_release found")
        
        if len(locations) == 1:
            path, ipo, is_nbu, meta = locations[0]
            return (path, ipo, is_nbu, meta, "Only one location")
        
        # Sort by confidence score (highest first), then by timestamp (newest first) as tiebreaker
        sorted_locs = sorted(locations, 
                            key=lambda x: (x[3]['confidence_score'], x[3]['timestamp'] or 0), 
                            reverse=True)
        
        best = sorted_locs[0]
        second = sorted_locs[1] if len(sorted_locs) > 1 else None
        
        # Determine selection reason
        if best[3]['user'] == self.workarea_owner:
            if second and second[3]['user'] != self.workarea_owner:
                reason = f"USER matches workarea owner ({self.workarea_owner})"
            else:
                reason = "Newest with matching USER"
        else:
            reason = "Newest timestamp (no USER match found)"
        
        return (best[0], best[1], best[2], best[3], reason)
    
    def print_header(self, stage: FlowStage) -> None:
        """Print section header with index number"""
        stage_num = STAGE_INDEX.get(stage, "")
        stage_prefix = f"[{stage_num}] " if stage_num else ""
        print(f"\n{'-' * 35} {Color.GREEN}{stage_prefix}{stage.value}{Color.RESET} {'-' * 35}")
    
    def print_file_info(self, file_path: str, description: str = ""):
        """Print file information"""
        if self.file_utils.file_exists(file_path):
            real_path = self.file_utils.realpath(file_path)
            # Use relative path for brevity (relative to main workarea root, not nbu_signoff subdirectory)
            rel_path = real_path.replace(self.workarea_root + '/', '') if self.workarea_root in real_path else os.path.basename(real_path)
            print(f"{description} {Color.CYAN}[Source: {rel_path}]{Color.RESET}")
            return True
        else:
            print(f"{description}: File not found")
            return False
    
    def _extract_power_summary_table(self, power_file: str) -> Optional[Dict[str, Any]]:
        """Extract the first power summary table from power report
        
        Args:
            power_file: Path to power report file (can be .gz compressed)
            
        Returns:
            Dictionary with power metrics or None if extraction fails:
            - Power values (internal, switching, leakage, total)
            - Percentages for each power component
            Returns None if file not found or parsing fails
        """
        try:
            # Read the file content
            if power_file.endswith('.gz'):
                with gzip.open(power_file, 'rt', encoding='utf-8') as f:
                    content = f.read()
            else:
                with open(power_file, 'r', encoding='utf-8') as f:
                    content = f.read()
            
            # Find the first table (summary table)
            lines = content.split('\n')
            table_start = -1
            table_end = -1
            
            # Look for the table header line with the column names
            for i, line in enumerate(lines):
                if 'group' in line and 'area' in line and 'count' in line and 'power' in line and 'leakage' in line:
                    table_start = i
                    break
            
            if table_start != -1:
                # Find the end of the first table (look for the next major separator)
                for i in range(table_start + 1, len(lines)):
                    # Look for the end of the first table (after the total row)
                    if '=======' in lines[i] and i > table_start + 3:
                        # Check if this is the end of the first table
                        if i + 1 < len(lines) and ('group' in lines[i + 1] or 'type' in lines[i + 1]):
                            table_end = i + 1
                            break
                        elif 'total' in lines[i-1] and '=======' in lines[i]:
                            table_end = i + 1
                            break
                
                if table_end == -1:
                    # If we didn't find a clear end, take the next 8 lines (header + 4 data rows + separators)
                    table_end = min(table_start + 8, len(lines))
                
                # Print the table
                print(f"  {Color.CYAN}Power Summary Table:{Color.RESET}")
                for i in range(table_start, table_end):
                    if lines[i].strip():  # Skip empty lines
                        print(f"    {lines[i]}")
            else:
                print("  Power summary table not found")
                
        except (OSError, UnicodeDecodeError, gzip.BadGzipFile) as e:
            print(f"  Error reading power file: {e}")
    
    def _extract_pnr_timing_histogram(self) -> None:
        """Extract and display filtered PnR timing histogram for SETUP and HOLD"""
        try:
            # Define stage priority order
            pnr_stages = ['postroute', 'route', 'cts', 'place', 'plan']
            
            # Find SETUP timing report
            setup_file = None
            hold_file = None
            found_stage = None
            
            for stage in pnr_stages:
                # Try to find both SETUP and HOLD reports
                setup_pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{self.design_info.ipo}/REPs/SUMMARY/{self.design_info.top_hier}.*.{stage}.timing.setup.rpt.gz"
                hold_pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{self.design_info.ipo}/REPs/SUMMARY/{self.design_info.top_hier}.*.{stage}.timing.hold.rpt.gz"
                
                setup_files = self.file_utils.find_files(setup_pattern, self.workarea)
                hold_files = self.file_utils.find_files(hold_pattern, self.workarea)
                
                if setup_files:
                    setup_file = setup_files[0]
                    found_stage = stage
                    if hold_files:
                        hold_file = hold_files[0]
                    break
            
            if setup_file:
                # Parse SETUP timing histogram
                setup_data = self._parse_timing_histogram_subcategories(setup_file)
                
                # Parse HOLD timing histogram (if available)
                hold_data = {}
                if hold_file:
                    hold_data = self._parse_timing_histogram_subcategories(hold_file)
                
                # Print filtered histograms
                self._print_filtered_timing_histograms(setup_data, hold_data, found_stage.upper(), setup_file, hold_file)
                
            else:
                print(f"\n{Color.YELLOW}PnR Timing Histogram: No timing reports found for any stage{Color.RESET}")
                print(f"  {Color.YELLOW}Tried stages: {', '.join(pnr_stages)}{Color.RESET}")
                print(f"  {Color.YELLOW}This could be due to:{Color.RESET}")
                print(f"    - Flow still running (timing reports not yet generated)")
                print(f"    - Flow failed at this stage)")
                print(f"    - Different file naming convention")
                print(f"    - File permissions issue")
                
        except Exception as e:
            print(f"  {Color.RED}Error extracting PnR timing histogram: {e}{Color.RESET}")
    
    def _parse_timing_histogram_subcategories(self, timing_file: str) -> Dict[str, Dict[str, str]]:
        """Parse timing histogram sub-category table and extract WNS/TNS/FEP
        
        Args:
            timing_file: Path to timing report file (.rpt.gz)
        
        Returns:
            Dictionary mapping subcategory names to {WNS, TNS, FEP} values
        """
        try:
            # Find the table with "| sub_category |" column header (with pipes)
            # This avoids finding comments that just mention "sub_category"
            result = self.file_utils.run_command(f"zcat {timing_file} | grep -n '|   sub_category   |' | head -1")
            if not result.strip():
                return {}
            
            # Get line number of sub_category header
            subcat_line = int(result.strip().split(':')[0])
            
            # Extract approximately 20 lines starting from this header
            table_result = self.file_utils.run_command(f"zcat {timing_file} | sed -n '{subcat_line},{subcat_line+20}p'")
            if not table_result.strip():
                return {}
            
            # Parse the table
            parsed_data = {}
            lines = table_result.strip().split('\n')
            
            # Find where the data rows start (after ===== separator)
            data_start_idx = 0
            for i, line in enumerate(lines):
                if '====' in line:
                    data_start_idx = i + 1
                    break
            
            # Parse data rows until we hit another separator or empty section
            for line in lines[data_start_idx:]:
                line = line.strip()
                if not line or '====' in line or '----' in line:
                    break
                
                # Skip dot lines
                if line.startswith('.') or 'histogram' in line.lower():
                    continue
                
                # Parse table row: | type | category | sub_category | wns | tns | fep | histogram... |
                parts = [p.strip() for p in line.split('|')]
                if len(parts) >= 6:
                    # Column indices after split: 0=type, 1=category, 2=sub_category, 3=wns, 4=tns, 5=fep
                    subcategory = parts[2]
                    wns = parts[3]
                    tns = parts[4]
                    fep = parts[5]
                    
                    if subcategory and subcategory not in ['sub_category', '', '..................']:
                        parsed_data[subcategory] = {
                            'WNS': wns,
                            'TNS': tns,
                            'FEP': fep
                        }
            
            return parsed_data
            
        except Exception as e:
            return {}
    
    def _print_filtered_timing_histograms(self, setup_data: Dict, hold_data: Dict, stage: str, setup_file: str = "", hold_file: str = "") -> None:
        """Print filtered timing histograms with External + Internal categories
        
        Args:
            setup_data: SETUP timing subcategory data
            hold_data: HOLD timing subcategory data
            stage: Stage name (POSTROUTE, ROUTE, etc.)
            setup_file: Path to SETUP timing report file
            hold_file: Path to HOLD timing report file
        """
        # Define category filters
        external_categories = {
            'port_to_flop': 'Inputs (port->flop)',
            'port_to_clock_gate': 'Inputs (port->cg)',
            'flop_to_port': 'Outputs (flop->port)',
            'port_to_port': 'Feedthrough (port->port)'
        }
        
        internal_categories = {
            'flop_to_flop': 'Core Logic (flop->flop)',
            'flop_to_hard_macro': 'To Macros (flop->macro)',
            'hard_macro_to_flop': 'From Macros (macro->flop)',
            'flop_to_clock_gate': 'Clock Gating (flop->cg)',
            'hard_macro_to_clock_gate': 'Macro to Gates (macro->cg)'
        }
        
        def format_timing(data: Dict, category: str) -> tuple:
            """Format timing as 'WNS / TNS / FEP' with ps/ns conversion and color coding
            Convert to ps if value < 1ns, format as integer if no fractional part, add unit suffix
            
            Returns:
                Tuple of (formatted_string, color_code)
            """
            if category in data:
                wns = data[category]['WNS']
                tns = data[category]['TNS']
                fep = data[category]['FEP']
                try:
                    wns_val = float(wns)
                    tns_val = float(tns)
                    
                    # Format WNS with ps/ns conversion (use tolerance for floating point)
                    if abs(wns_val) < 1.0:
                        wns_ps = wns_val * 1000
                        if abs(wns_ps - round(wns_ps)) < 0.001:
                            wns_f = f"{int(round(wns_ps))}ps"
                        else:
                            wns_f = f"{wns_ps:.2f}ps"
                    else:
                        if abs(wns_val - round(wns_val)) < 0.001:
                            wns_f = f"{int(round(wns_val))}ns"
                        else:
                            wns_f = f"{wns_val:.2f}ns"
                    
                    # Format TNS with ps/ns conversion (use tolerance for floating point)
                    if abs(tns_val) < 1.0:
                        tns_ps = tns_val * 1000
                        if abs(tns_ps - round(tns_ps)) < 0.001:
                            tns_f = f"{int(round(tns_ps))}ps"
                        else:
                            tns_f = f"{tns_ps:.2f}ps"
                    else:
                        if abs(tns_val - round(tns_val)) < 0.001:
                            tns_f = f"{int(round(tns_val))}ns"
                        else:
                            tns_f = f"{tns_val:.2f}ns"
                    
                    # Color coding based on WNS value
                    if wns_val >= -0.01:  # -0.00 and above should be green
                        color = Color.GREEN  # Meets timing (includes -0.00)
                    elif wns_val < -0.05:
                        color = Color.RED    # Significant violation
                    else:
                        color = Color.YELLOW # Small violation (-0.05 to -0.01)
                    
                    return (f"{wns_f:>10} / {tns_f:>12} / {fep:>6}", color)
                except:
                    return (f"{wns:>10} / {tns:>12} / {fep:>6}", "")
            else:
                # Show 0 instead of N/A, with green color (no violations)
                return (f"{'0ns':>10} / {'0ns':>12} / {'0':>6}", Color.GREEN)
        
        # Get relative paths for display
        setup_rel = setup_file.replace(self.workarea + '/', '') if setup_file and self.workarea in setup_file else os.path.basename(setup_file) if setup_file else f"{stage}.timing.setup.rpt.gz"
        hold_rel = hold_file.replace(self.workarea + '/', '') if hold_file and self.workarea in hold_file else os.path.basename(hold_file) if hold_file else f"{stage}.timing.hold.rpt.gz"
        
        # Print External Timing Breakdown
        print(f"\n{Color.CYAN}{'='*106}{Color.RESET}")
        print(f"{Color.YELLOW}  TIMING BREAKDOWN BY PATH TYPE - EXTERNAL PATHS ({stage}){Color.RESET}")
        print(f"{Color.CYAN}{'='*106}{Color.RESET}")
        has_hold = bool(hold_data)
        if has_hold:
            print(f"  Source (SETUP): {setup_rel}")
            print(f"  Source (HOLD):  {hold_rel}")
        else:
            print(f"  Source (SETUP): {setup_rel}")
            print(f"  Note: HOLD report not available")
        print(f"  {'-'*106}")
        print(f"  {'Sub-Category':<35} {'SETUP (WNS / TNS / FEP)':<34} {'HOLD (WNS / TNS / FEP)':<34}")
        print(f"  {'-'*106}")
        
        for cat_key, cat_label in external_categories.items():
            setup_str, setup_color = format_timing(setup_data, cat_key)
            hold_str, hold_color = format_timing(hold_data, cat_key)
            # Apply color without affecting spacing (color codes are outside the formatted string)
            setup_display = f"{setup_color}{setup_str}{Color.RESET}" if setup_color else setup_str
            hold_display = f"{hold_color}{hold_str}{Color.RESET}" if hold_color else hold_str
            # Use fixed spacing since color codes don't affect visual width
            print(f"  {cat_label:<35} {setup_display} {hold_display}")
        
        # Calculate and display external paths totals
        def calculate_totals(data: Dict, categories: Dict) -> tuple:
            """Calculate total WNS (worst), TNS (sum), FEP (sum) for given categories
            
            Returns:
                Tuple of (wns_worst, tns_sum, fep_sum) as floats or None if no data
            """
            wns_worst = 0.0  # Start with 0 (no violations)
            tns_sum = 0.0
            fep_sum = 0
            has_data = False
            
            for cat_key in categories.keys():
                if cat_key in data:
                    try:
                        wns_val = float(data[cat_key]['WNS'])
                        tns_val = float(data[cat_key]['TNS'])
                        fep_val = int(data[cat_key]['FEP'])
                        
                        # WNS: take worst (most negative)
                        if wns_val < wns_worst:
                            wns_worst = wns_val
                        
                        # TNS and FEP: sum
                        tns_sum += tns_val
                        fep_sum += fep_val
                        has_data = True
                    except:
                        continue
            
            if has_data:
                return (wns_worst, tns_sum, fep_sum)
            else:
                return None
        
        def format_totals(wns: float, tns: float, fep: int) -> tuple:
            """Format totals with ps/ns conversion and color coding
            
            Returns:
                Tuple of (formatted_string, color_code)
            """
            # Format WNS with ps/ns conversion
            if abs(wns) < 1.0:
                wns_ps = wns * 1000
                if abs(wns_ps - round(wns_ps)) < 0.001:
                    wns_f = f"{int(round(wns_ps))}ps"
                else:
                    wns_f = f"{wns_ps:.2f}ps"
            else:
                if abs(wns - round(wns)) < 0.001:
                    wns_f = f"{int(round(wns))}ns"
                else:
                    wns_f = f"{wns:.2f}ns"
            
            # Format TNS with ps/ns conversion
            if abs(tns) < 1.0:
                tns_ps = tns * 1000
                if abs(tns_ps - round(tns_ps)) < 0.001:
                    tns_f = f"{int(round(tns_ps))}ps"
                else:
                    tns_f = f"{tns_ps:.2f}ps"
            else:
                if abs(tns - round(tns)) < 0.001:
                    tns_f = f"{int(round(tns))}ns"
                else:
                    tns_f = f"{tns:.2f}ns"
            
            # Color coding based on WNS value
            if wns >= -0.01:  # -0.00 and above should be green
                color = Color.GREEN
            elif wns < -0.05:
                color = Color.RED
            else:
                color = Color.YELLOW
            
            return (f"{wns_f:>10} / {tns_f:>12} / {fep:>6}", color)
        
        # Calculate totals for SETUP and HOLD
        setup_totals = calculate_totals(setup_data, external_categories)
        hold_totals = calculate_totals(hold_data, external_categories) if has_hold else None
        
        # Print totals line
        if setup_totals or hold_totals:
            print(f"  {'-'*106}")
            
            if setup_totals:
                setup_total_str, setup_total_color = format_totals(setup_totals[0], setup_totals[1], setup_totals[2])
                setup_total_display = f"{setup_total_color}{setup_total_str}{Color.RESET}"
            else:
                setup_total_display = f"{'N/A':^34}"
            
            if hold_totals:
                hold_total_str, hold_total_color = format_totals(hold_totals[0], hold_totals[1], hold_totals[2])
                hold_total_display = f"{hold_total_color}{hold_total_str}{Color.RESET}"
            else:
                hold_total_display = f"{'N/A':^34}"
            
            print(f"  {Color.CYAN}{'TOTAL':<35}{Color.RESET} {setup_total_display} {hold_total_display}")
        
        print()
        
        # Print Internal Timing Breakdown
        print(f"{Color.CYAN}  TIMING BREAKDOWN BY PATH TYPE - INTERNAL PATHS ({stage}){Color.RESET}")
        print(f"  {'-'*106}")
        print(f"  {'Sub-Category':<35} {'SETUP (WNS / TNS / FEP)':<34} {'HOLD (WNS / TNS / FEP)':<34}")
        print(f"  {'-'*106}")
        
        for cat_key, cat_label in internal_categories.items():
            setup_str, setup_color = format_timing(setup_data, cat_key)
            hold_str, hold_color = format_timing(hold_data, cat_key)
            # Apply color without affecting spacing
            setup_display = f"{setup_color}{setup_str}{Color.RESET}" if setup_color else setup_str
            hold_display = f"{hold_color}{hold_str}{Color.RESET}" if hold_color else hold_str
            # Use fixed spacing since color codes don't affect visual width
            print(f"  {cat_label:<35} {setup_display} {hold_display}")
        
        # Calculate and display internal paths totals
        setup_totals_internal = calculate_totals(setup_data, internal_categories)
        hold_totals_internal = calculate_totals(hold_data, internal_categories) if has_hold else None
        
        # Print totals line for internal paths
        if setup_totals_internal or hold_totals_internal:
            print(f"  {'-'*106}")
            
            if setup_totals_internal:
                setup_total_str, setup_total_color = format_totals(setup_totals_internal[0], setup_totals_internal[1], setup_totals_internal[2])
                setup_total_display = f"{setup_total_color}{setup_total_str}{Color.RESET}"
            else:
                setup_total_display = f"{'N/A':^34}"
            
            if hold_totals_internal:
                hold_total_str, hold_total_color = format_totals(hold_totals_internal[0], hold_totals_internal[1], hold_totals_internal[2])
                hold_total_display = f"{hold_total_color}{hold_total_str}{Color.RESET}"
            else:
                hold_total_display = f"{'N/A':^34}"
            
            print(f"  {Color.CYAN}{'TOTAL':<35}{Color.RESET} {setup_total_display} {hold_total_display}")
        
        print(f"{Color.CYAN}{'='*106}{Color.RESET}")
    
    def _extract_max_transition_violations(self) -> None:
        """Extract and display max transition violations for func.std_tt_0c_0p6v.setup.typical scenario"""
        try:
            # Define stage priority order
            pnr_stages = ['postroute', 'route', 'cts', 'place']
            trans_file = None
            found_stage = None
            
            # Try each stage in priority order
            for stage in pnr_stages:
                trans_pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{self.design_info.ipo}/REPs/SUMMARY/{self.design_info.top_hier}.*.{stage}.drv.max_transition.net.rpt.gz"
                trans_files = self.file_utils.find_files(trans_pattern, self.workarea)
                if trans_files:
                    trans_file = trans_files[0]
                    found_stage = stage
                    break
            
            if not trans_file:
                return  # No transition report found
            
            # Parse the report for func.std_tt_0c_0p6v.setup.typical scenario
            violations_data = self._parse_max_transition_report(trans_file)
            
            if violations_data:
                self._print_max_transition_table(violations_data, found_stage, trans_file)
        
        except Exception as e:
            # Silent failure - this is optional data
            pass
    
    def _parse_max_transition_report(self, report_file: str) -> Optional[Dict]:
        """Parse max transition report for func.std_tt_0c_0p6v.setup.typical scenario
        
        Returns:
            Dictionary with net_type breakdown: {net_type: {required, wns, tns, count}}
        """
        try:
            # Extract the full net_type section with context
            cmd = f"zcat {report_file} | grep -A 100 'net_type.*scenario.*required'"
            result = self.file_utils.run_command(cmd)
            
            if not result.strip():
                return None
            
            violations = {}
            lines = result.strip().split('\n')
            
            current_net_type = 'signal'  # First group is implicitly signal
            seen_header = False
            
            for i, line in enumerate(lines):
                # Skip header lines
                if 'net_type|' in line or '========' in line:
                    seen_header = True
                    continue
                
                if not seen_header:
                    continue
                
                # Stop at next section (routing_rule or other)
                if line.startswith(' =====') and i > 10:
                    break
                
                # Detect explicit net_type labels (signal, clock, preroute)
                if line.strip() and not line.startswith('.'):
                    first_col = line.split('|')[0].strip()
                    if first_col in ['signal', 'clock', 'preroute']:
                        current_net_type = first_col
                    elif '........' in first_col:
                        # Dotted line separator - continue with current net_type
                        pass
                
                # Parse data lines with scenario func.std_tt_0c_0p6v.setup.typical
                if 'func.std_tt_0c_0p6v.setup.typical' in line:
                    parts = [p.strip() for p in line.split('|')]
                    # Handle lines that start with empty column (continuation of current net_type)
                    # Format: net_type | scenario | required | wns | tns | count | histogram...
                    # OR:     '' | scenario | required | wns | tns | count | histogram...
                    
                    # Find the parts indices
                    scenario_idx = None
                    for idx, part in enumerate(parts):
                        if 'func.std_tt_0c_0p6v.setup.typical' in part:
                            scenario_idx = idx
                            break
                    
                    if scenario_idx is not None and len(parts) > scenario_idx + 3:
                        try:
                            required = parts[scenario_idx + 1].strip()  # Required value
                            wns = parts[scenario_idx + 2].strip()       # WNS
                            tns = parts[scenario_idx + 3].strip()       # TNS
                            count_str = parts[scenario_idx + 4].split()[0] if len(parts) > scenario_idx + 4 and parts[scenario_idx + 4].strip() else '0'
                            
                            # Store data grouped by net_type and required value
                            key = f"{current_net_type}_{required}"
                            violations[key] = {
                                'net_type': current_net_type,
                                'required': required,
                                'wns': wns,
                                'tns': tns,
                                'count': count_str
                            }
                        except (ValueError, IndexError):
                            continue
            
            return violations if violations else None
            
        except Exception:
            return None
    
    def _print_max_transition_table(self, violations_data: Dict, stage: str, report_file: str) -> None:
        """Print max transition violations table with color coding
        
        Args:
            violations_data: Dictionary with violation data by net_type
            stage: PnR stage name
            report_file: Path to source report file
        """
        def format_trans_value(value_str: str) -> str:
            """Format transition value with ps/ns units and color coding
            Converts to ps if <1ns, formats as integer if no fractional part
            """
            try:
                value = float(value_str)
                # Convert to ps or ns based on magnitude
                if abs(value) < 1.0:
                    value_ps = value * 1000
                    # Format as integer if no fractional part (use tolerance for floating point)
                    if abs(value_ps - round(value_ps)) < 0.001:
                        formatted = f"{int(round(value_ps))}ps"
                    else:
                        formatted = f"{value_ps:.2f}ps"
                else:
                    # Format as integer if no fractional part (use tolerance for floating point)
                    if abs(value - round(value)) < 0.001:
                        formatted = f"{int(round(value))}ns"
                    else:
                        formatted = f"{value:.2f}ns"
                
                # Color coding for violations
                if value >= 0:
                    color = Color.GREEN  # No violation
                elif value >= -0.01:  # -10ps or better
                    color = Color.YELLOW
                else:
                    color = Color.RED  # Worse than -10ps
                
                return f"{color}{formatted:>10}{Color.RESET}"
            except:
                return f"{value_str:>10}"
        
        # Get relative path for display
        rel_path = report_file.replace(self.workarea + '/', '') if self.workarea in report_file else os.path.basename(report_file)
        
        print(f"\n{Color.CYAN}{'='*98}{Color.RESET}")
        print(f"{Color.YELLOW}  MAX TRANSITION VIOLATIONS ({stage.upper()}) - func.std_tt_0c_0p6v.setup.typical{Color.RESET}")
        print(f"{Color.CYAN}{'='*98}{Color.RESET}")
        print(f"  Source: {rel_path}")
        print(f"  {'-'*98}")
        print(f"  {'Net Type':<15} {'Required':<12} {'WNS':<12} {'TNS':<12} {'Count':<10}")
        print(f"  {'-'*98}")
        
        # Sort violations by net_type (signal, clock, preroute)
        net_type_order = {'signal': 1, 'clock': 2, 'preroute': 3}
        sorted_violations = sorted(violations_data.values(), 
                                  key=lambda x: (net_type_order.get(x['net_type'], 99), x['required']))
        
        # Calculate totals
        total_wns = None
        total_tns = 0.0
        total_count = 0
        
        for viol in sorted_violations:
            net_type = viol['net_type']
            required = viol['required']
            wns_val = viol['wns']
            tns_val = viol['tns']
            count_val = viol['count']
            
            # Format required value (convert to ps if <1ns, format as integer if whole number)
            try:
                req_float = float(required)
                if req_float < 1.0:
                    req_ps = req_float * 1000
                    if req_ps == int(req_ps):
                        required_str = f"{int(req_ps)}ps"
                    else:
                        required_str = f"{req_ps:.2f}ps"
                else:
                    if req_float == int(req_float):
                        required_str = f"{int(req_float)}ns"
                    else:
                        required_str = f"{req_float:.2f}ns"
            except:
                required_str = required
            
            # Format WNS and TNS with color
            wns_formatted = format_trans_value(wns_val)
            tns_formatted = format_trans_value(tns_val)
            
            print(f"  {net_type:<15} {required_str:<12} {wns_formatted} {tns_formatted} {count_val:>10}")
            
            # Calculate totals
            try:
                wns_f = float(wns_val)
                if total_wns is None or wns_f < total_wns:
                    total_wns = wns_f
            except:
                pass
            
            try:
                total_tns += float(tns_val)
                total_count += int(count_val)
            except:
                pass
        
        # Print totals
        if sorted_violations:
            print(f"  {'-'*98}")
            total_wns_formatted = format_trans_value(str(total_wns)) if total_wns is not None else "N/A"
            total_tns_formatted = format_trans_value(str(total_tns))
            print(f"  {'TOTAL':<15} {'':<12} {total_wns_formatted} {total_tns_formatted} {total_count:>10}")
        
        print(f"{Color.CYAN}{'='*98}{Color.RESET}")
    
    def _extract_clock_tree_report(self) -> Optional[Dict]:
        """Extract and parse clock tree cell count report
        
        Returns:
            Dictionary with: {
                'summary': {clock: {tree_cells: {...}, clock_sinks: {...}}},
                'tap_details': [{clock, tap_name, tree_cells, clock_sinks}],
                'tap_counts': {clock: count},
                'stage': str,
                'report_file': str
            }
        """
        try:
            # Define stage priority order
            pnr_stages = ['postroute', 'route', 'cts', 'place']
            clock_tree_file = None
            found_stage = None
            
            # Try each stage in priority order
            for stage in pnr_stages:
                pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{self.design_info.ipo}/REPs/SUMMARY/{self.design_info.top_hier}.{self.design_info.ipo}.{stage}.clock_tree.cell_count.rpt.gz"
                files = self.file_utils.find_files(pattern, self.workarea)
                if files:
                    clock_tree_file = files[0]
                    found_stage = stage
                    break
            
            if not clock_tree_file:
                return None  # No clock tree report found
            
            # Read and parse the report
            cmd = f"zcat {clock_tree_file}"
            content = self.file_utils.run_command(cmd)
            
            if not content.strip():
                return None
            
            # Parse all sections
            summary = self._parse_clock_tree_main_summary(content)
            tap_details = self._parse_clock_tap_details(content)
            tap_counts = self._count_taps_per_clock(tap_details)
            skew_groups = self._parse_clock_skew_groups(content)
            clock_sources = self._parse_clock_sources(content)
            
            return {
                'summary': summary,
                'tap_details': tap_details,
                'tap_counts': tap_counts,
                'skew_groups': skew_groups,
                'clock_sources': clock_sources,
                'stage': found_stage,
                'report_file': clock_tree_file
            }
                
        except Exception as e:
            # Silent failure - this is optional data
            return None
    
    def _parse_clock_tree_main_summary(self, content: str) -> Dict:
        """Parse main clock tree summary table
        
        Returns:
            {clock_name: {'tree_cells': {...}, 'clock_sinks': {...}}}
        """
        summary = {}
        lines = content.split('\n')
        
        # Find main summary section (between first set of === markers)
        in_summary = False
        for line in lines:
            # Start of summary section
            if 'CLOCK TREE CELL COUNT SUMMARY' in line and 'PER' not in line:
                in_summary = True
                continue
            
            # End of summary section
            if in_summary and line.strip().startswith('=====') and len(summary) > 0:
                break
            
            # Parse data lines (format: "clock | buffer : inverter : combo : clock_gate : total | flop : latch : hard_macro : total")
            if in_summary and '|' in line and ':' in line:
                # Skip header lines
                if 'clock' in line and 'buffer' in line:
                    continue
                if '========' in line:
                    continue
                
                parts = line.split('|')
                if len(parts) >= 3:
                    clock = parts[0].strip()
                    if not clock or clock == 'clock':
                        continue
                    
                    # Parse tree cells (buffer : inverter : combo : clock_gate : total)
                    tree_cells_str = parts[1].strip()
                    tree_cells = [x.strip() for x in tree_cells_str.split(':')]
                    
                    # Parse clock sinks (flop : latch : hard_macro : total)
                    clock_sinks_str = parts[2].strip()
                    clock_sinks = [x.strip() for x in clock_sinks_str.split(':')]
                    
                    if len(tree_cells) >= 5 and len(clock_sinks) >= 4:
                        summary[clock] = {
                            'tree_cells': {
                                'buffer': tree_cells[0],
                                'inverter': tree_cells[1],
                                'combo': tree_cells[2],
                                'clock_gate': tree_cells[3],
                                'total': tree_cells[4]
                            },
                            'clock_sinks': {
                                'flop': clock_sinks[0],
                                'latch': clock_sinks[1],
                                'hard_macro': clock_sinks[2],
                                'total': clock_sinks[3]
                            }
                        }
        
        return summary
    
    def _parse_clock_tap_details(self, content: str) -> List[Dict]:
        """Parse per-tap clock tree details
        
        Returns:
            List of dicts: [{clock, tap_name, tree_cells, clock_sinks}, ...]
        """
        tap_details = []
        lines = content.split('\n')
        
        # Find tap details section
        in_tap_section = False
        current_clock = None
        
        for line in lines:
            # Start of tap section
            if 'SPECIAL CLOCK TREE CELL COUNT SUMMARY PER CLOCK TAP' in line:
                in_tap_section = True
                continue
            
            # End of tap section
            if in_tap_section and 'CLOCK TREE CELL COUNT SUMMARY PER SKEW GROUP' in line:
                break
            
            if not in_tap_section:
                continue
            
            # Parse tap data lines
            if '|' in line and ':' in line:
                # Skip header/separator lines
                if 'clock' in line and 'tap' in line and 'buffer' in line:
                    continue
                if '========' in line or '........' in line:
                    continue
                if 'tree_type' in line:
                    continue
                
                parts = [p.strip() for p in line.split('|')]
                
                # Format: clock | tap | buffer : inverter : combo : clock_gate : total | flop : latch : hard_macro : total
                if len(parts) >= 4:
                    clock_col = parts[0].strip()
                    tap_col = parts[1].strip()
                    
                    # Skip if no tap name
                    if not tap_col or tap_col == 'tap':
                        continue
                    
                    # Infer clock from tap name if clock column is empty
                    if clock_col and clock_col not in ['', 'clock']:
                        current_clock = clock_col
                    elif tap_col and current_clock is None:
                        # Extract clock name from tap name (e.g., "FHCTS_tap_i1_clk_..." -> "i1_clk")
                        # Look for common clock patterns: i1_clk, i2_clk, m1_clk, etc.
                        import re
                        clock_match = re.search(r'(i\d+_clk|m\d+_clk|[a-z]+_clk)', tap_col)
                        if clock_match:
                            current_clock = clock_match.group(1)
                    
                    # Parse tree cells
                    tree_str = parts[2].strip()
                    tree_parts = [x.strip() for x in tree_str.split(':')]
                    
                    # Parse sinks
                    sinks_str = parts[3].strip()
                    sinks_parts = [x.strip() for x in sinks_str.split(':')]
                    
                    if len(tree_parts) >= 5 and len(sinks_parts) >= 4 and current_clock:
                        tap_details.append({
                            'clock': current_clock,
                            'tap_name': tap_col,
                            'tree_cells': {
                                'buffer': tree_parts[0],
                                'inverter': tree_parts[1],
                                'combo': tree_parts[2],
                                'clock_gate': tree_parts[3],
                                'total': tree_parts[4]
                            },
                            'clock_sinks': {
                                'flop': sinks_parts[0],
                                'latch': sinks_parts[1],
                                'hard_macro': sinks_parts[2],
                                'total': sinks_parts[3]
                            }
                        })
        
        return tap_details
    
    def _count_taps_per_clock(self, tap_details: List[Dict]) -> Dict[str, int]:
        """Count number of taps per clock
        
        Uses tap names to determine base clock (handles i2_clk_0, i2_clk_1 -> i2_clk)
        
        Returns:
            {clock_name: tap_count}
        """
        import re
        tap_counts = {}
        
        for tap in tap_details:
            tap_name = tap['tap_name']
            # Extract base clock name from tap (i1_clk, i2_clk, m1_clk, etc.)
            # Match pattern: i\d+_clk or m\d+_clk or other_clk
            clock_match = re.search(r'(i\d+_clk|m\d+_clk|[a-z]+_clk)', tap_name)
            if clock_match:
                base_clock = clock_match.group(1)
                tap_counts[base_clock] = tap_counts.get(base_clock, 0) + 1
        
        return tap_counts
    
    def _parse_clock_skew_groups(self, content: str) -> List[Dict]:
        """Parse clock tree skew groups and scenarios
        
        Returns:
            List of dicts: [{skew_group, tree_cells, clock_sinks}, ...]
        """
        skew_groups = []
        lines = content.split('\n')
        
        # Find skew groups section
        in_skew_section = False
        
        for line in lines:
            # Start of skew groups section
            if 'CLOCK TREE CELL COUNT SUMMARY PER SKEW GROUP AND SCENARIO' in line:
                in_skew_section = True
                continue
            
            # End of skew groups section
            if in_skew_section and 'CLOCK TREE CELL COUNT SUMMARY PER CLOCK SOURCE' in line:
                break
            
            if not in_skew_section:
                continue
            
            # Parse skew group data lines
            if '|' in line and ':' in line:
                # Skip header/separator lines
                if 'skew_group_and_scenario' in line or 'buffer' in line:
                    continue
                if '========' in line or '........' in line:
                    continue
                
                parts = [p.strip() for p in line.split('|')]
                
                # Format: skew_group | buffer : inverter : combo : clock_gate : total | flop : latch : hard_macro : total
                if len(parts) >= 3:
                    skew_group = parts[0].strip()
                    
                    # Skip if no skew group name
                    if not skew_group or skew_group in ['', 'skew_group_and_scenario']:
                        continue
                    
                    # Parse tree cells
                    tree_str = parts[1].strip()
                    tree_parts = [x.strip() for x in tree_str.split(':')]
                    
                    # Parse sinks
                    sinks_str = parts[2].strip()
                    sinks_parts = [x.strip() for x in sinks_str.split(':')]
                    
                    if len(tree_parts) >= 5 and len(sinks_parts) >= 4:
                        skew_groups.append({
                            'skew_group': skew_group,
                            'tree_cells': {
                                'buffer': tree_parts[0],
                                'inverter': tree_parts[1],
                                'combo': tree_parts[2],
                                'clock_gate': tree_parts[3],
                                'total': tree_parts[4]
                            },
                            'clock_sinks': {
                                'flop': sinks_parts[0],
                                'latch': sinks_parts[1],
                                'hard_macro': sinks_parts[2],
                                'total': sinks_parts[3]
                            }
                        })
        
        return skew_groups
    
    def _parse_clock_sources(self, content: str) -> List[Dict]:
        """Parse clock source hierarchy
        
        Returns:
            List of dicts: [{clock, source_path, tree_cells, clock_sinks}, ...]
        """
        clock_sources = []
        lines = content.split('\n')
        
        # Find clock sources section
        in_sources_section = False
        current_clock = None
        
        for line in lines:
            # Start of clock sources section
            if 'CLOCK TREE CELL COUNT SUMMARY PER CLOCK SOURCE' in line:
                in_sources_section = True
                continue
            
            # End of report
            if in_sources_section and not line.strip():
                break
            
            if not in_sources_section:
                continue
            
            # Parse clock source data lines
            if '|' in line and ':' in line:
                # Skip header/separator lines
                if 'clock' in line and 'clock_source' in line and 'buffer' in line:
                    continue
                if '========' in line or '........' in line:
                    continue
                
                parts = [p.strip() for p in line.split('|')]
                
                # Format: clock | source_path | buffer : inverter : combo : clock_gate : total | flop : latch : hard_macro : total
                if len(parts) >= 4:
                    clock_col = parts[0].strip()
                    source_col = parts[1].strip()
                    
                    # Update current clock if specified
                    if clock_col and clock_col not in ['', 'clock']:
                        current_clock = clock_col
                    
                    # Skip if no source path
                    if not source_col or source_col == 'clock_source':
                        continue
                    
                    # Parse tree cells
                    tree_str = parts[2].strip()
                    tree_parts = [x.strip() for x in tree_str.split(':')]
                    
                    # Parse sinks
                    sinks_str = parts[3].strip()
                    sinks_parts = [x.strip() for x in sinks_str.split(':')]
                    
                    if len(tree_parts) >= 5 and len(sinks_parts) >= 4 and current_clock:
                        clock_sources.append({
                            'clock': current_clock,
                            'source_path': source_col,
                            'tree_cells': {
                                'buffer': tree_parts[0],
                                'inverter': tree_parts[1],
                                'combo': tree_parts[2],
                                'clock_gate': tree_parts[3],
                                'total': tree_parts[4]
                            },
                            'clock_sinks': {
                                'flop': sinks_parts[0],
                                'latch': sinks_parts[1],
                                'hard_macro': sinks_parts[2],
                                'total': sinks_parts[3]
                            }
                        })
        
        return clock_sources
    
    def _print_clock_tree_summary(self, clock_data: Dict) -> None:
        """Print clock tree summary to terminal
        
        Args:
            clock_data: Dictionary with summary, tap_counts, stage, report_file
        """
        summary = clock_data['summary']
        tap_counts = clock_data['tap_counts']
        stage = clock_data['stage'].upper()
        report_file = clock_data['report_file']
        
        # Get relative path
        rel_path = report_file.replace(self.workarea + '/', '') if self.workarea in report_file else os.path.basename(report_file)
        
        print(f"\n{Color.CYAN}{'='*106}{Color.RESET}")
        print(f"{Color.YELLOW}  CLOCK TREE SUMMARY ({stage}){Color.RESET}")
        print(f"{Color.CYAN}{'='*106}{Color.RESET}")
        print(f"  Source: {rel_path}")
        print(f"  {'-'*106}")
        print(f"  {'Clock':<10} | {'Tree Cells (Buf/Inv/CG/Total)':<35} | {'Sinks (FF/Total)':<20} | {'Taps':<8}")
        print(f"  {'-'*106}")
        
        for clock in sorted(summary.keys()):
            tree = summary[clock]['tree_cells']
            sinks = summary[clock]['clock_sinks']
            taps = tap_counts.get(clock, 0)
            
            # Format numbers with commas
            try:
                buf = f"{int(tree['buffer']):,}" if tree['buffer'].strip() else "0"
                inv = f"{int(tree['inverter']):,}" if tree['inverter'].strip() else "0"
                cg = f"{int(tree['clock_gate']):,}" if tree['clock_gate'].strip() else "0"
                tree_tot = f"{int(tree['total']):,}" if tree['total'].strip() else "0"
                ff = f"{int(sinks['flop']):,}" if sinks['flop'].strip() else "0"
                sink_tot = f"{int(sinks['total']):,}" if sinks['total'].strip() else "0"
            except:
                buf = tree['buffer'] or "0"
                inv = tree['inverter'] or "0"
                cg = tree['clock_gate'] or "0"
                tree_tot = tree['total'] or "0"
                ff = sinks['flop'] or "0"
                sink_tot = sinks['total'] or "0"
            
            tree_str = f"{buf} / {inv} / {cg} / {tree_tot}"
            sinks_str = f"{ff} / {sink_tot}"
            taps_str = str(taps) if taps > 0 else "N/A"
            
            print(f"  {clock:<10} | {tree_str:<35} | {sinks_str:<20} | {taps_str:<8}")
        
        print(f"{Color.CYAN}{'='*106}{Color.RESET}")
    
    def _extract_timing_histogram_for_html(self) -> Optional[Dict[str, Any]]:
        """Extract timing histogram data for HTML report
        
        Returns:
            Dictionary with histogram data or None if not found
        """
        try:
            # Define stage priority order
            pnr_stages = ['postroute', 'route', 'cts', 'place', 'plan']
            timing_file = None
            found_stage = None
            
            # Try each stage in priority order
            for stage in pnr_stages:
                # Use wildcard for IPO in filename since it can differ from directory name (e.g., ipo1000 dir with ipo1400 in filenames)
                timing_pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{self.design_info.ipo}/REPs/SUMMARY/{self.design_info.top_hier}.*.{stage}.timing.setup.rpt.gz"
                timing_files = self.file_utils.find_files(timing_pattern, self.workarea)
                if timing_files:
                    timing_file = timing_files[0]
                    found_stage = stage
                    break
            
            if timing_file:
                # Extract the last three histogram tables (category, sub-category, and sub-category+scenario)
                result = self.file_utils.run_command(f"zcat {timing_file} | grep -n 'histogram' | grep '|'")
                if result.strip():
                    histogram_lines = result.strip().split('\n')
                    if len(histogram_lines) >= 4:
                        # Get the last 4 tables: category, scenario, sub-category, and sub-category + scenario
                        table_category_start = int(histogram_lines[-4].split(':')[0])  # Category breakdown
                        table_scenario_start = int(histogram_lines[-3].split(':')[0])  # Scenario breakdown
                        table_subcat_start = int(histogram_lines[-2].split(':')[0])    # Sub-category breakdown
                        table_subcat_scenario_start = int(histogram_lines[-1].split(':')[0])  # Sub-category + scenario breakdown
                        
                        # Find the end of category table (it ends before scenario table starts)
                        table_category_end = table_scenario_start - 1
                        
                        # Extract category table
                        table_category_result = self.file_utils.run_command(f"zcat {timing_file} | sed -n '{table_category_start},{table_category_end}p'")
                        
                        # Find the end of sub-category table (it ends before sub-category + scenario table starts)
                        table_subcat_end = table_subcat_scenario_start - 1
                        
                        # Extract sub-category table
                        table_subcat_result = self.file_utils.run_command(f"zcat {timing_file} | sed -n '{table_subcat_start},{table_subcat_end}p'")
                        
                        # Extract sub-category + scenario table - get remaining lines
                        table_subcat_scenario_result = self.file_utils.run_command(f"zcat {timing_file} | sed -n '{table_subcat_scenario_start},$p'")
                        
                        if table_category_result.strip() and table_subcat_result.strip() and table_subcat_scenario_result.strip():
                            # Skip first 2 lines (histogram header and dots line) for each table
                            category_lines = table_category_result.strip().split('\n')
                            subcat_lines = table_subcat_result.strip().split('\n')
                            scenario_lines = table_subcat_scenario_result.strip().split('\n')
                            
                            return {
                                'stage': found_stage,
                                'file': timing_file,
                                'category_data': '\n'.join(category_lines[2:]) if len(category_lines) > 2 else table_category_result.strip(),
                                'sub_category_data': '\n'.join(subcat_lines[2:]) if len(subcat_lines) > 2 else table_subcat_result.strip(),
                                'scenario_data': '\n'.join(scenario_lines[2:]) if len(scenario_lines) > 2 else table_subcat_scenario_result.strip()
                            }
            
            return None
        except Exception as e:
            print(f"  Error extracting timing histogram for HTML: {e}")
            return None
    
    def _generate_image_html_report(self) -> Optional[str]:
        """Generate HTML report with all relevant pictures using avice_image_debug_report.py
        
        Returns:
            HTML filename if generated successfully, None otherwise
        """
        try:
            # Import the image debug report functions
            import subprocess
            import os
            from datetime import datetime
            
            # Get the path to the avice_image_debug_report.py script
            script_dir = os.path.dirname(os.path.abspath(__file__))
            image_script = os.path.join(script_dir, "avice_image_debug_report.py")
            
            # Check if the image debug script exists
            if not os.path.exists(image_script):
                print(f"  Image debug script not found: {image_script}")
                return
            
            # Run the image debug report script
            cmd = [sys.executable, image_script, self.workarea]
            if self.design_info.ipo and self.design_info.ipo != "unknown":
                cmd.append(self.design_info.ipo)
            
            print(f"  Generating HTML image report...")
            result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=60)
            
            if result.returncode == 0:
                # Decode output
                stdout = result.stdout.decode('utf-8', errors='replace')
                stderr = result.stderr.decode('utf-8', errors='replace')
                
                # Extract the output file name from the script output
                output_lines = stdout.strip().split('\n')
                html_file = None
                for line in output_lines:
                    if 'Generated HTML report:' in line:
                        html_file = line.split('Generated HTML report:')[1].strip()
                        break
                    elif 'HTML report generated:' in line:
                        html_file = line.split('HTML report generated:')[1].strip()
                        break
                
                if html_file and os.path.exists(html_file):
                    # Determine display path
                    html_output_dir = self._get_html_output_dir()
                    display_path = os.path.relpath(html_output_dir, os.getcwd())
                    print(f"  Open with: /home/utils/firefox-118.0.1/firefox {Color.MAGENTA}{display_path}/{os.path.basename(html_file)}{Color.RESET} &")
                    return os.path.abspath(html_file)
                else:
                    print(f"  HTML report generated successfully")
                    return ""
            else:
                stderr = result.stderr.decode('utf-8', errors='replace')
                print(f"  Error generating HTML report: {stderr}")
                return ""
                
        except subprocess.TimeoutExpired:
            print(f"  Timeout generating HTML report")
            return ""
        except Exception as e:
            print(f"  Error generating HTML report: {e}")
            return ""
    
    def _extract_clock_tree_data(self, clock_file: str, quiet: bool = False) -> Tuple[float, float, Dict[str, Tuple[float, float, float]]]:
        """Extract clock tree data for func.std_tt_0c_0p6v.setup.typical scenario
        
        Args:
            clock_file: Path to clock report file
            quiet: If True, suppress terminal output
            
        Returns:
            Tuple of (max_latency_ps, median_latency_ps, clock_dict) where clock_dict maps clock names to (latency_ps, skew_ps, median_ps)
        """
        try:
            if clock_file.endswith('.gz'):
                with gzip.open(clock_file, 'rt', encoding='utf-8') as f:
                    content = f.read()
            else:
                with open(clock_file, 'r', encoding='utf-8') as f:
                    content = f.read()
            
            lines = content.split('\n')
            target_scenario = "func.std_tt_0c_0p6v.setup.typical"
            
            if not quiet:
                print(f"\n{Color.YELLOW}Clock Tree Analysis for {target_scenario}:{Color.RESET}")
            
            current_clock = None
            found_scenario = False
            clock_data = []
            
            for i, line in enumerate(lines):
                # Check for clock section header
                if line.strip().startswith("Clock :"):
                    current_clock = line.split("Clock :")[1].strip()
                    continue
                
                # Check for scenario data line
                if target_scenario in line and "|" in line:
                    found_scenario = True
                    # Parse the data line
                    parts = [part.strip() for part in line.split("|")]
                    if len(parts) >= 5:
                        scenario = parts[0].strip()
                        period = parts[1].strip()
                        global_skew = parts[2].strip()
                        
                        # Parse insertion delay data (min:max:median:mean:stdev)
                        insertion_delay_parts = parts[3].split(":")
                        if len(insertion_delay_parts) >= 5:
                            min_delay = insertion_delay_parts[0].strip()
                            max_delay = insertion_delay_parts[1].strip()
                            median_delay = insertion_delay_parts[2].strip()
                            mean_delay = insertion_delay_parts[3].strip()
                            stdev_delay = insertion_delay_parts[4].strip()
                            
                            clock_data.append({
                                'clock': current_clock,
                                'period': period,
                                'global_skew': global_skew,
                                'min_delay': min_delay,
                                'max_delay': max_delay,
                                'median_delay': median_delay,
                                'mean_delay': mean_delay,
                                'stdev_delay': stdev_delay
                            })
            
            if found_scenario and clock_data:
                if not quiet:
                    # Print table header
                    print(f"  {'Clock':<10} {'Period':<8} {'Skew':<8} {'Min':<8} {'Max':<8} {'Median':<8} {'Mean':<8} {'StdDev':<8}")
                    print(f"  {'-'*10} {'-'*8} {'-'*8} {'-'*8} {'-'*8} {'-'*8} {'-'*8} {'-'*8}")
                    
                    # Print table data
                    for data in clock_data:
                        # Check if max delay exceeds 550ps (0.55ns) and apply red color
                        try:
                            max_delay_val = float(data['max_delay'])
                            if max_delay_val > 0.55:  # 550ps threshold
                                max_delay_colored = f"{Color.RED}{data['max_delay']}{Color.RESET}"
                            else:
                                max_delay_colored = data['max_delay']
                        except (ValueError, TypeError):
                            max_delay_colored = data['max_delay']
                        
                        # Use fixed-width formatting to maintain alignment
                        clock_str = f"{data['clock']:<10}"
                        period_str = f"{data['period']:<8}"
                        skew_str = f"{data['global_skew']:<8}"
                        min_str = f"{data['min_delay']:<8}"
                        median_str = f"{data['median_delay']:<8}"
                        mean_str = f"{data['mean_delay']:<8}"
                        stdev_str = f"{data['stdev_delay']:<8}"
                        
                        print(f"  {clock_str} {period_str} {skew_str} {min_str} {max_delay_colored:<8} {median_str} {mean_str} {stdev_str}")
                    
                    print(f"\n  All values in nanoseconds (ns)")
                    print(f"  {Color.RED}Note: Max latency values > 550ps (0.55ns) are highlighted in red{Color.RESET}")
                
                # Build clock data dictionary for return (include median values)
                clock_dict = {}
                max_latencies = []
                median_latencies = []
                for data in clock_data:
                    try:
                        max_delay_ps = float(data['max_delay']) * 1000  # Convert ns to ps
                        median_delay_ps = float(data['median_delay']) * 1000  # Convert ns to ps
                        skew_ps = float(data['global_skew']) * 1000  # Convert ns to ps
                        clock_dict[data['clock']] = (max_delay_ps, skew_ps, median_delay_ps)
                        max_latencies.append(max_delay_ps)
                        median_latencies.append(median_delay_ps)
                    except (ValueError, TypeError):
                        pass
                
                if max_latencies:
                    # Return: (max_latency, median_latency, clock_dict)
                    return max(max_latencies), max(median_latencies), clock_dict
            else:
                if not quiet:
                    print(f"  Scenario {target_scenario} not found in clock tree report")
            
            return 0, 0, {}
                
        except (OSError, UnicodeDecodeError, gzip.BadGzipFile) as e:
            if not quiet:
                print(f"  Error reading clock file: {e}")
            return 0, 0, {}
    
    def _detect_root_pt_ipo(self) -> str:
        """Detect which IPO the root-level PT (signoff_flow/auto_pt/) belongs to
        
        Root PT uses database from export/export_innovus/, so we detect the IPO
        by looking for files with IPO names in that directory.
        
        Returns:
            IPO name (e.g., 'ipo2000_ndr_test3') or empty string if cannot determine
        """
        try:
            export_dir = os.path.join(self.workarea, "export/export_innovus")
            if not os.path.exists(export_dir):
                return ""
            
            # Look for files with IPO pattern: design_ipo####*.enc.dat or design.ipo####*.gz
            for filename in os.listdir(export_dir):
                # Match patterns like: ccorec_ipo2000.enc.dat, ccorec.ipo2000.def.gz, etc.
                match = re.search(r'(ipo\d+(?:_[\w]+)*)', filename)
                if match:
                    return match.group(1)
            
            return ""
        except Exception as e:
            return ""
    
    def _extract_pt_clock_latency(self, pt_clock_file: str, quiet: bool = False) -> Tuple[float, Dict[str, Tuple[float, float]]]:
        """Extract min and max total clock latency per clock from PT clock latency report
        
        Args:
            pt_clock_file: Path to PrimeTime clock report file
            quiet: If True, suppress terminal output
            
        Returns:
            Tuple of (max_latency_ps, clock_dict) where clock_dict maps clock names to (max_ps, min_ps)
        """
        try:
            with open(pt_clock_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            lines = content.split('\n')
            
            if not quiet:
                print(f"\n{Color.YELLOW}PT Clock Latency Analysis:{Color.RESET}")
            
            clock_latencies = {}
            current_clock = None
            
            for line in lines:
                # Check for clock section header
                if line.strip().startswith("Clock:"):
                    current_clock = line.split("Clock:")[1].strip()
                    if current_clock not in clock_latencies:
                        clock_latencies[current_clock] = []
                    continue
                
                # Check for total clock latency line
                if "total clock latency" in line:
                    # Extract the latency value (last number in the line)
                    parts = line.split()
                    if parts:
                        try:
                            latency = float(parts[-1])
                            if current_clock:
                                clock_latencies[current_clock].append(latency)
                        except (ValueError, IndexError):
                            continue
            
            if clock_latencies:
                if not quiet:
                    # Print table header
                    print(f"  {'Clock':<10} {'Min (ns)':<10} {'Max (ns)':<10}")
                    print(f"  {'-'*10} {'-'*10} {'-'*10}")
                    
                    # Print table data
                    for clock, latencies in clock_latencies.items():
                        if latencies:
                            min_latency = min(latencies)
                            max_latency = max(latencies)
                            
                            # Check if max latency exceeds 550ps (0.55ns) and apply red color
                            if max_latency > 0.55:  # 550ps threshold
                                max_latency_colored = f"{Color.RED}{max_latency:.3f}{Color.RESET}"
                            else:
                                max_latency_colored = f"{max_latency:.3f}"
                            
                            # Use fixed-width formatting to maintain alignment
                            clock_str = f"{clock:<10}"
                            min_str = f"{min_latency:<10.3f}"
                            
                            print(f"  {clock_str} {min_str} {max_latency_colored:<10}")
                    
                    print(f"\n  All values in nanoseconds (ns)")
                    print(f"  {Color.RED}Note: Max latency values > 550ps (0.55ns) are highlighted in red{Color.RESET}")
                
                # Build clock data dictionary for return
                clock_dict = {}
                max_latencies = []
                for clock, latencies in clock_latencies.items():
                    if latencies:
                        max_latency = max(latencies)
                        min_latency = min(latencies)
                        max_latency_ps = max_latency * 1000  # Convert ns to ps
                        min_latency_ps = min_latency * 1000  # Convert ns to ps
                        clock_dict[clock] = (max_latency_ps, min_latency_ps)  # (max, min)
                        max_latencies.append(max_latency_ps)
                
                if max_latencies:
                    return max(max_latencies), clock_dict
            else:
                if not quiet:
                    print(f"  No clock latency data found in PT report")
            
            return 0, {}
                
        except (OSError, UnicodeDecodeError) as e:
            if not quiet:
                print(f"  Error reading PT clock file: {e}")
            return 0, {}
    
    def _extract_formal_verification_status(self, log_file: str) -> tuple:
        """Extract verification status and runtime from formal verification log
        Returns: (status, runtime, flow_name, passing_points, failing_points, compare_table, failing_points_list) tuple
        """
        try:
            # Check if formal is currently running
            file_mtime = os.path.getmtime(log_file)
            current_time = time.time()
            time_since_update = current_time - file_mtime
            
            with open(log_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            lines = content.split('\n')
            
            # Extract verification status from "Verification Results" section
            # Use the LAST occurrence if there are multiple runs in the log
            status = "UNKNOWN"
            passing_points = 0
            failing_points = 0
            compare_table = {}  # Dictionary to store the matched compare points table
            failing_points_list = []  # List of failing compare point names
            last_results_index = -1
            
            # Find the last "Verification Results" section
            for i, line in enumerate(lines):
                if "Verification Results" in line and "*****" in line:
                    last_results_index = i
            
            # Extract status from the last Verification Results section
            if last_results_index >= 0:
                # Look in the next 20 lines after "Verification Results" header to get status and compare points
                for i in range(last_results_index + 1, min(last_results_index + 21, len(lines))):
                    line = lines[i]
                    if "Verification SUCCEEDED" in line:
                        status = "SUCCEEDED"
                    elif "Verification FAILED" in line:
                        status = "FAILED"
                    elif "Verification UNRESOLVED" in line:
                        status = "UNRESOLVED"
                    elif "Verification INCONCLUSIVE" in line:
                        status = "UNRESOLVED"
                    
                    # Extract passing compare points
                    # Format: "238522 Passing compare points"
                    if "Passing compare points" in line:
                        match = re.search(r'(\d+)\s+Passing compare points', line)
                        if match:
                            passing_points = int(match.group(1))
                    
                    # Extract failing compare points
                    # Format: "8 Failing compare points (8 matched, 0 unmatched)"
                    if "Failing compare points" in line:
                        match = re.search(r'(\d+)\s+Failing compare points', line)
                        if match:
                            failing_points = int(match.group(1))
            
            # Fallback: if no "Verification Results" section found, search entire log
            if status == "UNKNOWN":
                for line in lines:
                    if "Verification SUCCEEDED" in line:
                        status = "SUCCEEDED"
                    elif "Verification FAILED" in line:
                        status = "FAILED"
                    elif "Verification UNRESOLVED" in line or "Verification INCONCLUSIVE" in line:
                        status = "UNRESOLVED"
            
            # Check for tool crash/error before completion
            # If status is still UNKNOWN, check for crash indicators
            if status == "UNKNOWN":
                has_crash_error = False
                crash_error_msg = ""
                
                # Look for common crash patterns in the last 100 lines
                for line in lines[-100:]:
                    # CMD-081: Script stopped due to error
                    if "stopped at line" in line and "due to error" in line:
                        has_crash_error = True
                        crash_error_msg = "Script execution error"
                        break
                    # FM-036: Unknown design name error
                    elif "Error: Unknown name:" in line and "FM-036" in line:
                        has_crash_error = True
                        crash_error_msg = "Design not loaded"
                        break
                    # FM-008: Design not set error
                    elif "Error: The current design is not set" in line and "FM-008" in line:
                        has_crash_error = True
                        crash_error_msg = "Design not set"
                        break
                    # General error patterns that indicate crashes
                    elif line.strip().startswith("-E-") and "errMsg:" in line:
                        has_crash_error = True
                        crash_error_msg = "Tool error encountered"
                        break
                
                if has_crash_error:
                    status = f"CRASHED ({crash_error_msg})"
            
            # Check if running (file updated within last 5 minutes and no final status)
            is_running = False
            if status == "UNKNOWN" and time_since_update < 300:  # 5 minutes
                # Check for running indicators in the log
                for line in reversed(lines[-50:]):  # Check last 50 lines
                    if any(indicator in line for indicator in [
                        "Status:  Building verification models",
                        "Status:  Verifying",
                        "Status:  Checking designs",
                        "Matching in progress"
                    ]):
                        is_running = True
                        status = "RUNNING"
                        break
            
            # Extract elapsed time (in hours only)
            elapsed_time = "Unknown"
            for line in lines:
                if line.strip().startswith("Elapsed time:"):
                    # Extract time from "Elapsed time: 3669 seconds ( 1.02 hours )"
                    time_part = line.split("Elapsed time:")[1].strip()
                    # Extract hours from the format "3669 seconds ( 1.02 hours )"
                    if "(" in time_part and "hours" in time_part:
                        hours_part = time_part.split("(")[1].split("hours")[0].strip()
                        elapsed_time = f"{hours_part} hours"
                    else:
                        # Fallback to original format if hours not found
                        elapsed_time = time_part
                    break
            
            # Print status and runtime
            if status == "RUNNING":
                status_color = Color.CYAN
            elif status == "SUCCEEDED":
                status_color = Color.GREEN
            elif status == "FAILED":
                status_color = Color.RED
            elif "CRASHED" in status:
                status_color = Color.RED
            else:
                status_color = Color.YELLOW
            
            print(f"  Status: {status_color}{status}{Color.RESET}")
            if status != "RUNNING":
                print(f"  Runtime: {elapsed_time}")
            else:
                print(f"  Runtime: In progress...")
            
            # Display compare points if FAILED
            if status == "FAILED" and (passing_points > 0 or failing_points > 0):
                print(f"  {Color.GREEN}Passing compare points: {passing_points}{Color.RESET}")
                print(f"  {Color.RED}Failing compare points: {failing_points}{Color.RESET}")
            
            # Extract Matched Compare Points table
            for i, line in enumerate(lines):
                if "Matched Compare Points" in line and "BBPin" in line:
                    # Found the table header, parse the next lines
                    # Skip the separator line
                    if i + 2 < len(lines):
                        # Parse "Passing (equivalent)" line
                        passing_line = lines[i + 2]
                        if "Passing (equivalent)" in passing_line:
                            parts = passing_line.split()
                            if len(parts) >= 9:
                                compare_table['passing'] = {
                                    'BBPin': int(parts[2]),
                                    'Loop': int(parts[3]),
                                    'BBNet': int(parts[4]),
                                    'Cut': int(parts[5]),
                                    'Port': int(parts[6]),
                                    'DFF': int(parts[7]),
                                    'LAT': int(parts[8]),
                                    'TOTAL': int(parts[9])
                                }
                        
                        # Parse "Failing (not equivalent)" line
                        if i + 3 < len(lines):
                            failing_line = lines[i + 3]
                            if "Failing (not equivalent)" in failing_line:
                                parts = failing_line.split()
                                if len(parts) >= 9:
                                    compare_table['failing'] = {
                                        'BBPin': int(parts[3]),
                                        'Loop': int(parts[4]),
                                        'BBNet': int(parts[5]),
                                        'Cut': int(parts[6]),
                                        'Port': int(parts[7]),
                                        'DFF': int(parts[8]),
                                        'LAT': int(parts[9]),
                                        'TOTAL': int(parts[10])
                                    }
                        
                        # Parse "Not Compared" section
                        compare_table['not_compared'] = {}
                        for j in range(i + 5, min(i + 10, len(lines))):
                            line = lines[j].strip()
                            if not line or line.startswith('*'):
                                break
                            # Parse lines like "  Clock-gate LAT                                                            6659    6659"
                            if any(keyword in line for keyword in ['Clock-gate', 'Constant', 'Unread']):
                                parts = line.split()
                                if len(parts) >= 2:
                                    # The category name is the first 1-2 words, the last number is the total
                                    try:
                                        count = int(parts[-1])
                                        # Category name is usually first 2 words or until we hit numbers
                                        name_parts = []
                                        for part in parts:
                                            if part.isdigit():
                                                break
                                            name_parts.append(part)
                                        name = ' '.join(name_parts)
                                        compare_table['not_compared'][name] = count
                                    except:
                                        pass
                    break
            
            # Extract failing compare point names
            for line in lines:
                if "Compare point" in line and "failed (is not equivalent)" in line:
                    # Extract the compare point name between "Compare point" and "failed"
                    match = re.search(r'Compare point\s+(.+?)\s+failed \(is not equivalent\)', line)
                    if match:
                        failing_points_list.append(match.group(1).strip())
            
            # Extract flow name from log file path (e.g., rtl_vs_pnr_fm)
            flow_name = os.path.basename(os.path.dirname(os.path.dirname(log_file)))
            
            return (status, elapsed_time, flow_name, passing_points, failing_points, compare_table, failing_points_list)
            
        except (OSError, UnicodeDecodeError) as e:
            print(f"  Error reading formal log: {e}")
            return ("UNKNOWN", "Unknown", "Unknown", 0, 0, {}, [])
    
    def _display_formal_timestamps(self, log_file: str) -> None:
        """Extract and display start/end timestamps for formal verification
        
        Args:
            log_file: Path to formal verification log file
        """
        try:
            # Get end time from file modification time
            end_time_epoch = os.path.getmtime(log_file)
            start_time_epoch = end_time_epoch
            
            # Try to extract runtime and calculate start time
            try:
                with open(log_file, 'r') as f:
                    content = f.read()
                
                # Look for elapsed time to calculate start time
                # Format: "Elapsed time: 16191 seconds ( 4.50 hours )"
                elapsed_match = re.search(r'Elapsed time:\s*(\d+)\s*seconds', content)
                if elapsed_match:
                    elapsed_seconds = int(elapsed_match.group(1))
                    start_time_epoch = end_time_epoch - elapsed_seconds
            except:
                pass
            
            # Format timestamps
            start_str = time.strftime("%m/%d %H:%M", time.localtime(start_time_epoch))
            end_str = time.strftime("%m/%d %H:%M", time.localtime(end_time_epoch))
            
            print(f"  Start: {start_str}")
            print(f"  End:   {end_str}")
            
            return end_time_epoch  # Return for comparison with ECO timestamps
            
        except Exception as e:
            return None  # Return None if extraction fails
    
    def _resolve_ipo_directory(self, top_hier: str, expected_ipo: str) -> str:
        """
        Check if IPO directory exists and resolve to an available IPO if needed.
        
        Behavior differs based on IPO source:
        - User-specified IPO (via -i/--ipo flag): ERROR out if doesn't exist
        - Auto-detected IPO (from .prc file): Fallback to available IPO (user may have deleted to save disk)
        
        Args:
            top_hier: Top hierarchy name
            expected_ipo: IPO name (e.g., 'ipo1000')
            
        Returns:
            Resolved IPO name (either expected or fallback)
        """
        pnr_base_path = os.path.join(self.workarea, f"pnr_flow/nv_flow/{top_hier}")
        expected_ipo_path = os.path.join(pnr_base_path, expected_ipo)
        
        # Check if expected IPO directory exists
        if os.path.isdir(expected_ipo_path):
            return expected_ipo
        
        # IPO directory doesn't exist - behavior depends on whether user specified it
        
        # Scan for available IPO directories first (needed for both error and fallback cases)
        available_ipos = []
        try:
            if os.path.isdir(pnr_base_path):
                for item in os.listdir(pnr_base_path):
                    item_path = os.path.join(pnr_base_path, item)
                    if os.path.isdir(item_path) and item.startswith('ipo'):
                        available_ipos.append(item)
                available_ipos.sort()
        except Exception as e:
            print(f"    {Color.RED}[ERROR] Failed to scan for IPO directories: {e}{Color.RESET}")
        
        # If user specified this IPO via -i/--ipo flag, error out
        if self.user_specified_ipo:
            print(f"  {Color.RED}[ERROR] Specified IPO '{expected_ipo}' not found in workarea{Color.RESET}")
            if available_ipos:
                print(f"    {Color.CYAN}[INFO] Available IPO directories: {', '.join(available_ipos)}{Color.RESET}")
            else:
                print(f"    {Color.YELLOW}[INFO] No IPO directories found in {pnr_base_path}{Color.RESET}")
            print(f"    {Color.YELLOW}[HINT] Use -i <ipo> with a valid IPO from the list above{Color.RESET}")
            sys.exit(1)
        
        # Auto-detected IPO from .prc file - fallback to available IPO
        # Users sometimes delete IPO directories to save disk space
        print(f"  {Color.YELLOW}[WARN] IPO directory '{expected_ipo}' from .prc file not found{Color.RESET}")
        print(f"    {Color.YELLOW}Note: Users sometimes delete IPO directories to save disk space{Color.RESET}")
        
        if available_ipos:
            fallback_ipo = available_ipos[0]
            print(f"    {Color.CYAN}[INFO] Available IPO directories: {', '.join(available_ipos)}{Color.RESET}")
            print(f"    {Color.GREEN}[OK] Using fallback IPO: {fallback_ipo}{Color.RESET}")
            return fallback_ipo
        else:
            print(f"    {Color.RED}[ERROR] No IPO directories found in {pnr_base_path}{Color.RESET}")
            return expected_ipo  # Return original even though it doesn't exist
    
    def _check_formal_vs_eco_timestamps(self, formal_end_time: float) -> None:
        """Check if ECO was run after formal verification (potential issue)"""
        try:
            if not formal_end_time:
                return
            
            latest_eco_time = 0
            eco_source = None
            
            # Check 1: Auto PT Fix log (automatic ECO fixes)
            auto_pt_fix_log = os.path.join(self.workarea, "signoff_flow/auto_pt/log/auto_pt_fix.log")
            if os.path.exists(auto_pt_fix_log):
                eco_time = os.path.getmtime(auto_pt_fix_log)
                if eco_time > latest_eco_time:
                    latest_eco_time = eco_time
                    eco_source = "Auto PT Fix"
            
            # Check 2: Latest netlist file (manual ECO or any netlist regeneration)
            # Pattern: $wa/export/export_innovus/$b.ipo*.lvs.gv.gz
            netlist_pattern = f"export/export_innovus/{self.design_info.top_hier}.{self.design_info.ipo}*.lvs.gv.gz"
            netlist_files = self.file_utils.find_files(netlist_pattern, self.workarea)
            
            if netlist_files:
                # Get the latest netlist file
                latest_netlist = max(netlist_files, key=os.path.getmtime)
                netlist_time = os.path.getmtime(latest_netlist)
                if netlist_time > latest_eco_time:
                    latest_eco_time = netlist_time
                    eco_source = "Netlist (Manual ECO or regeneration)"
            
            # If any ECO/netlist update ran after formal, warn the user
            if latest_eco_time > formal_end_time:
                time_diff_hours = (latest_eco_time - formal_end_time) / 3600
                formal_end_str = time.strftime("%m/%d %H:%M", time.localtime(formal_end_time))
                eco_str = time.strftime("%m/%d %H:%M", time.localtime(latest_eco_time))
                
                print(f"\n  {Color.YELLOW}[WARN] Design changes were made AFTER formal verification!{Color.RESET}")
                print(f"    Latest formal ended:  {formal_end_str}")
                print(f"    {eco_source} updated: {eco_str} ({time_diff_hours:.1f} hours later)")
                print(f"    {Color.YELLOW}-> Formal verification should be re-run to verify design changes{Color.RESET}")
        except Exception:
            pass  # Silently skip if check fails
    
    def _check_multibit_mapping_files(self) -> tuple:
        """Check that critical multibit mapping files exist in all required locations
        
        These files are needed to prevent formal verification non-equivalence points.
        
        Returns:
            tuple: (status, issues_list, results_dict)
                   status: "PASS", "WARN", or "FAIL"
                   issues_list: List of issue strings for dashboard
                   results_dict: Dict with detailed results per IPO
        """
        try:
            status = "PASS"
            issues = []
            results = {}
            
            design = self.design_info.top_hier
            
            # Get all IPOs to check - auto-detect if not specified or unknown
            ipos = []
            if self.design_info.ipo and self.design_info.ipo != "unknown":
                ipos = [self.design_info.ipo]
            else:
                # Auto-detect IPOs from multibit mapping files in export_innovus
                export_dir = os.path.join(self.workarea, "export/export_innovus")
                if os.path.isdir(export_dir):
                    # Look for files matching pattern: {design}.ipo*.multibitMapping*.gz
                    pattern = os.path.join(export_dir, f"{design}.ipo*.multibitMapping*.gz")
                    found_files = glob.glob(pattern)
                    
                    # Extract unique IPOs from filenames
                    detected_ipos = set()
                    for filepath in found_files:
                        filename = os.path.basename(filepath)
                        # Pattern: design.ipoXXXX.multibitMapping...
                        match = re.search(rf'{design}\.(ipo\d+)\.multibitMapping', filename)
                        if match:
                            detected_ipos.add(match.group(1))
                    
                    ipos = sorted(list(detected_ipos))
            
            if not ipos:
                print(f"  {Color.YELLOW}[SKIP] No IPO specified and none auto-detected{Color.RESET}")
                return "PASS", [], {}
            
            # Inform user if IPOs were auto-detected
            if (not self.design_info.ipo or self.design_info.ipo == "unknown") and ipos:
                print(f"  Auto-detected IPO(s): {', '.join(ipos)}")
            
            # Define the two multibit mapping files to check
            multibit_files = [
                "multibitMapping.gz",
                "multibitMapping_for_gen_mbff_scandef.gz"
            ]
            
            # Define the three locations where files should exist
            locations = [
                ("PnR Flow", f"pnr_flow/nv_flow/{design}/{{ipo}}/IOs/netlists"),
                ("Export Innovus", "export/export_innovus"),
                ("NV Gate ECO", f"signoff_flow/nv_gate_eco/{design}/{{ipo}}/IOs/netlists")
            ]
            
            for ipo in ipos:
                ipo_results = {}
                ipo_missing = []
                
                for loc_name, loc_path_template in locations:
                    # Replace {ipo} placeholder
                    loc_path = loc_path_template.replace("{ipo}", ipo)
                    
                    loc_status = []
                    for mb_file in multibit_files:
                        # Construct filename with design and ipo
                        filename = f"{design}.{ipo}.{mb_file}"
                        
                        # Construct full path
                        full_path = os.path.join(self.workarea, loc_path, filename)
                        
                        # Check if file exists
                        exists = os.path.isfile(full_path)
                        loc_status.append(exists)
                        
                        if not exists:
                            ipo_missing.append(f"{loc_name}: {filename}")
                    
                    # Store results for this location
                    ipo_results[loc_name] = {
                        "path": loc_path,
                        "files_exist": loc_status,
                        "all_exist": all(loc_status)
                    }
                
                results[ipo] = {
                    "locations": ipo_results,
                    "missing_files": ipo_missing
                }
                
                # Determine status based on missing files
                if ipo_missing:
                    # Count how many locations are missing files
                    missing_locations = [loc for loc, data in ipo_results.items() if not data["all_exist"]]
                    
                    if len(missing_locations) >= 2:
                        # Critical: Missing in 2+ locations
                        status = "FAIL"
                        issues.append(f"{ipo}: Missing multibit mapping files in {len(missing_locations)} locations")
                    else:
                        # Warning: Missing in 1 location
                        if status == "PASS":
                            status = "WARN"
                        issues.append(f"{ipo}: Missing multibit mapping files in {missing_locations[0]}")
            
            # Display results in terminal (compact format)
            self._display_multibit_mapping_results(results, multibit_files)
            
            return status, issues, results
            
        except Exception as e:
            print(f"  {Color.RED}[ERROR] Failed to check multibit mapping files: {e}{Color.RESET}")
            import traceback
            traceback.print_exc()
            return "WARN", [f"Multibit check error: {str(e)}"], {}
    
    def _display_multibit_mapping_results(self, results: Dict[str, Any], multibit_files: List[str]) -> None:
        """Display multibit mapping file check results in terminal
        
        Args:
            results: Dictionary containing multibit mapping results
            multibit_files: List of multibit file paths
        """
        if not results:
            return
        
        print(f"  Checking 6 files across 3 locations per IPO...")
        print(f"  Files: 1) multibitMapping.gz  2) multibitMapping_for_gen_mbff_scandef.gz")
        print()
        
        # Prepare table data
        table_data = []
        for ipo, ipo_data in results.items():
            for loc_name, loc_info in ipo_data["locations"].items():
                files_status = loc_info["files_exist"]
                # Create status string: [OK] [OK] or [MISS] [OK]
                status_str = " ".join(["[OK]" if exists else "[MISS]" for exists in files_status])
                
                # Color code the row
                if all(files_status):
                    row_color = Color.GREEN
                    status_label = "OK"
                else:
                    row_color = Color.RED
                    status_label = "MISSING"
                
                table_data.append((ipo, loc_name, status_str, status_label, row_color))
        
        # Print table
        print(f"  {'IPO':<12} {'Location':<18} {'File Status':<20} {'Result':<10}")
        print(f"  {'-'*12} {'-'*18} {'-'*20} {'-'*10}")
        
        for ipo, loc_name, status_str, status_label, row_color in table_data:
            print(f"  {ipo:<12} {loc_name:<18} {status_str:<20} {row_color}{status_label}{Color.RESET}")
        
        # Print summary
        print()
        total_checks = len(table_data)
        passed_checks = sum(1 for _, _, _, label, _ in table_data if label == "OK")
        
        if passed_checks == total_checks:
            print(f"  {Color.GREEN}[OK] All {total_checks} location checks passed{Color.RESET}")
        else:
            failed_checks = total_checks - passed_checks
            print(f"  {Color.RED}[FAIL] {failed_checks}/{total_checks} location checks failed{Color.RESET}")
            print(f"  {Color.YELLOW}-> Formal verification may encounter non-equivalence points{Color.RESET}")
            print(f"  {Color.YELLOW}-> Files are generated in PnR and copied to export_innovus and nv_gate_eco{Color.RESET}")
    
    def _print_metric_table(self, title: str, sections: List[Dict[str, Any]], border_char: str = '-') -> None:
        """Print formatted metric table with subcategories
        
        Args:
            title: Table title
            sections: List of dicts with 'name' and 'rows' keys
                     rows is list of tuples: (key, value) or (key, value, color)
            border_char: Character for borders (default: '-')
        """
        # Determine column widths
        max_key_width = 30
        max_value_width = 80
        
        # Print table header
        total_width = max_key_width + max_value_width + 7
        print(f"\n  {border_char * total_width}")
        print(f"  {title.center(total_width)}")
        print(f"  {border_char * total_width}")
        
        for section in sections:
            section_name = section.get('name', '')
            rows = section.get('rows', [])
            
            if not rows:
                continue
            
            # Print section header
            if section_name:
                print(f"  {section_name.upper()}")
                print(f"  {border_char * total_width}")
            
            # Print rows
            for row in rows:
                if len(row) == 2:
                    key, value = row
                    color = ""
                    reset = ""
                elif len(row) == 3:
                    key, value, color = row
                    reset = Color.RESET if color else ""
                else:
                    continue
                
                # Truncate long values
                value_str = str(value)
                if len(value_str) > max_value_width:
                    value_str = value_str[:max_value_width-3] + "..."
                
                print(f"  {key:<{max_key_width}} {color}{value_str}{reset}")
        
        # Print table footer
        print(f"  {border_char * total_width}")
    
    def _extract_postroute_data_parameters(
        self, 
        data_file: str, 
        stage: str = 'postroute'
    ) -> Optional[Dict[str, Any]]:
        """Extract PnR parameters and display organized tables with subcategories
        
        Args:
            data_file: Path to postroute data file
            stage: Stage name (e.g., 'postroute', 'route', 'cts')
            
        Returns:
            Dictionary with all extracted parameters
        """
        try:
            with open(data_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Extract all parameters
            all_params = {}
            for line in content.split('\n'):
                line = line.strip()
                if ' = ' in line:
                    param_name, param_value = line.split(' = ', 1)
                    all_params[param_name] = param_value
            
            # Extract floorplan dimensions
            floorplan_dims = self._extract_floorplan_dimensions()
            
            stage_display = stage.upper() if stage != 'postroute' else 'POSTROUTE'
            
            # ========== TABLE 1: DESIGN METRICS ==========
            self._print_design_metrics_table(all_params, floorplan_dims, stage_display, data_file)
            
            # ========== TABLE 2: TIMING PARAMETERS ==========
            self._print_timing_parameters_table(all_params, stage_display, data_file)
            
            return all_params
            
        except (OSError, UnicodeDecodeError) as e:
            print(f"  {Color.RED}Error reading post-route data file: {e}{Color.RESET}")
            print(f"  {Color.YELLOW}This could be due to:{Color.RESET}")
            print(f"    - Flow still running (data file not yet generated)")
            print(f"    - Flow failed at this stage")
            print(f"    - File permissions issue")
            print(f"    - File corrupted or incomplete")
            return {}
    
    def _print_timing_parameters_table(self, params: Dict[str, str], stage: str, data_file: str) -> None:
        """Print timing parameters table with External + Internal timing
        
        Args:
            params: Dictionary of all parameters from .data file
            stage: Stage name for title (POSTROUTE, ROUTE, etc.)
            data_file: Path to source data file
        """
        def format_timing(wns: str, tns: str, viol: str) -> str:
            """Format timing values with .2f precision and color coding
            If WNS or TNS < 1 ns (absolute value), display in ps instead
            """
            try:
                # Format values
                wns_val = float(wns) if wns != 'N/A' else None
                tns_val = float(tns) if tns != 'N/A' else None
                
                # Format WNS: if |value| < 1 ns, show in ps (convert first, then format)
                if wns_val is not None:
                    if abs(wns_val) < 1.0:
                        wns_ps = wns_val * 1000
                        # Format as integer if no fractional part (use tolerance for floating point)
                        if abs(wns_ps - round(wns_ps)) < 0.001:
                            wns_f = f"{int(round(wns_ps))}ps"
                        else:
                            wns_f = f"{wns_ps:.2f}ps"
                    else:
                        # Format as integer if no fractional part (use tolerance for floating point)
                        if abs(wns_val - round(wns_val)) < 0.001:
                            wns_f = f"{int(round(wns_val))}ns"
                        else:
                            wns_f = f"{wns_val:.2f}ns"
                else:
                    wns_f = 'N/A'
                
                # Format TNS: if |value| < 1 ns, show in ps (convert first, then format)
                if tns_val is not None:
                    if abs(tns_val) < 1.0:
                        tns_ps = tns_val * 1000
                        # Format as integer if no fractional part (use tolerance for floating point)
                        if abs(tns_ps - round(tns_ps)) < 0.001:
                            tns_f = f"{int(round(tns_ps))}ps"
                        else:
                            tns_f = f"{tns_ps:.2f}ps"
                    else:
                        # Format as integer if no fractional part (use tolerance for floating point)
                        if abs(tns_val - round(tns_val)) < 0.001:
                            tns_f = f"{int(round(tns_val))}ns"
                        else:
                            tns_f = f"{tns_val:.2f}ns"
                else:
                    tns_f = 'N/A'
                
                viol_f = viol if viol == 'N/A' else viol
                
                # Determine color based on WNS value
                if wns_val is not None:
                    if wns_val >= -0.01:  # Includes -0.00
                        color = Color.GREEN  # Meets timing
                    elif wns_val < -0.05:
                        color = Color.RED    # Significant violation
                    else:
                        color = Color.YELLOW # Small violation
                    
                    # Apply color to entire timing string
                    return f"{color}{wns_f:>10} / {tns_f:>12} / {viol_f:>6}{Color.RESET}"
                else:
                    return f"{wns_f:>10} / {tns_f:>12} / {viol_f:>6}"
            except:
                return f"{wns:>10} / {tns:>12} / {viol:>6}"
        
        # Get relative path for display
        rel_path = data_file.replace(self.workarea + '/', '') if self.workarea in data_file else os.path.basename(data_file)
        
        print(f"\n{Color.CYAN}{'='*98}{Color.RESET}")
        print(f"{Color.YELLOW}  TIMING PARAMETERS ({stage}){Color.RESET}")
        print(f"{Color.CYAN}{'='*98}{Color.RESET}")
        print(f"  Source: {rel_path}")
        print(f"  {'-'*98}")
        
        # External Timing (I/O Interfaces)
        print(f"{Color.CYAN}  EXTERNAL TIMING (I/O Interfaces){Color.RESET}")
        print(f"  {'-'*98}")
        print(f"  {'Path Group':<30} {'SETUP Timing (WNS / TNS / ViolPaths)':<40}")
        print(f"  {'-'*98}")
        
        external_groups = [
            ('FEEDTHROUGH (Passthrough)', 'FEEDTHROUGH'),
            ('REGIN (Inputs)', 'REGIN'),
            ('REGOUT (Outputs)', 'REGOUT')
        ]
        
        # Track totals for external groups
        external_wns_values = []
        external_total_tns = 0.0
        external_total_viol = 0
        
        for label, prefix in external_groups:
            wns = params.get(f'{prefix}_WNS', 'N/A')
            tns = params.get(f'{prefix}_TNS', 'N/A')
            viol = params.get(f'{prefix}_ViolPaths', 'N/A')
            timing_str = format_timing(wns, tns, viol)
            print(f"  {label:<30} {timing_str:<40}")
            
            # Accumulate for external totals
            try:
                if wns != 'N/A':
                    external_wns_values.append(float(wns))
            except:
                pass
            try:
                if tns != 'N/A':
                    external_total_tns += float(tns)
            except:
                pass
            try:
                if viol != 'N/A':
                    external_total_viol += int(viol)
            except:
                pass
        
        # Print EXTERNAL TOTAL row
        print(f"  {'-'*98}")
        external_worst_wns = min(external_wns_values) if external_wns_values else 'N/A'
        external_timing_str = format_timing(external_worst_wns, external_total_tns, external_total_viol)
        print(f"  {Color.BOLD}{'TOTAL (External)':<30}{Color.RESET} {Color.BOLD}{external_timing_str:<40}{Color.RESET}")
        
        print()
        
        # Internal Timing (Core Logic)
        print(f"{Color.CYAN}  INTERNAL TIMING (Core Logic){Color.RESET}")
        print(f"  {'-'*98}")
        print(f"  {'Clock Domain':<30} {'SETUP Timing (WNS / TNS / ViolPaths)':<40}")
        print(f"  {'-'*98}")
        
        # Find all clock domains dynamically (including reg2cgate)
        clock_domains = []
        for key in params.keys():
            if key.endswith('_WNS') and not any(key.startswith(prefix) for prefix in ['FEEDTHROUGH', 'REGIN', 'REGOUT']):
                domain = key[:-4]  # Remove '_WNS'
                clock_domains.append(domain)
        
        # Sort clock domains (m* clocks first, then i* clocks, alphabetically)
        clock_domains_sorted = sorted(clock_domains, key=lambda x: (not x.startswith('m'), x))
        
        # Track totals for internal groups
        internal_wns_values = []
        internal_total_tns = 0.0
        internal_total_viol = 0
        
        for domain in clock_domains_sorted:
            wns = params.get(f'{domain}_WNS', 'N/A')
            tns = params.get(f'{domain}_TNS', 'N/A')
            viol = params.get(f'{domain}_ViolPaths', 'N/A')
            
            timing_str = format_timing(wns, tns, viol)
            print(f"  {domain:<30} {timing_str:<40}")
            
            # Accumulate for internal totals
            try:
                if wns != 'N/A':
                    internal_wns_values.append(float(wns))
            except:
                pass
            try:
                if tns != 'N/A':
                    internal_total_tns += float(tns)
            except:
                pass
            try:
                if viol != 'N/A':
                    internal_total_viol += int(viol)
            except:
                pass
        
        # Print INTERNAL TOTAL row
        print(f"  {'-'*98}")
        internal_worst_wns = min(internal_wns_values) if internal_wns_values else 'N/A'
        internal_timing_str = format_timing(internal_worst_wns, internal_total_tns, internal_total_viol)
        print(f"  {Color.BOLD}{'TOTAL (Internal)':<30}{Color.RESET} {Color.BOLD}{internal_timing_str:<40}{Color.RESET}")
        
        print(f"{Color.CYAN}{'='*98}{Color.RESET}")
        print(f"{Color.YELLOW}  Note: HOLD timing will be extracted from timing histogram reports{Color.RESET}")
    
    def _print_design_metrics_table(self, params: Dict[str, str], floorplan_dims: Optional[Dict], stage: str, data_file: str) -> None:
        """Print design metrics table with 7 subcategories
        
        Args:
            params: Dictionary of all parameters from .data file
            floorplan_dims: Floorplan dimensions dict (or None)
            stage: Stage name for title
            data_file: Path to source data file
        """
        sections = []
        
        # Section 1: Area & Utilization
        area_rows = []
        if floorplan_dims:
            x_dim = floorplan_dims['x_dim_um']
            y_dim = floorplan_dims['y_dim_um']
            source = floorplan_dims.get('source', 'unknown')
            area_rows.append(('Die Dimensions (X x Y)', f"{x_dim:.2f} x {y_dim:.2f} um (from: {source})"))
        
        # Combine DieArea, CellArea, ArraysArea into one line
        area_parts = []
        if 'DieArea' in params:
            try:
                area_val = float(params['DieArea'])
                area_parts.append(f"Die: {area_val:.3f}")
            except:
                area_parts.append(f"Die: {params['DieArea']}")
        elif floorplan_dims:
            x_dim = floorplan_dims['x_dim_um']
            y_dim = floorplan_dims['y_dim_um']
            die_area_mm2 = (x_dim * y_dim) / 1_000_000
            area_parts.append(f"Die: {die_area_mm2:.3f}")
        
        if 'CellArea' in params:
            try:
                val = float(params['CellArea'])
                area_parts.append(f"Cell: {val:.3f}")
            except:
                area_parts.append(f"Cell: {params['CellArea']}")
        
        if 'ArraysArea' in params:
            try:
                val = float(params['ArraysArea'])
                area_parts.append(f"Arrays: {val:.3f}")
            except:
                area_parts.append(f"Arrays: {params['ArraysArea']}")
        
        if area_parts:
            area_rows.append(('Area (mm2)', ' | '.join(area_parts)))
        
        # Combine Utilization and Effective Utilization into one line (format to .2f)
        util_parts = []
        if 'Utilization' in params:
            try:
                util_val = float(params['Utilization'].rstrip('%'))
                util_parts.append(f"Util: {util_val:.2f}%")
            except:
                util_parts.append(f"Util: {params['Utilization']}")
        if 'EffictiveUtilization' in params:
            try:
                eff_val = float(params['EffictiveUtilization'].rstrip('%'))
                util_parts.append(f"Effective: {eff_val:.2f}%")
            except:
                util_parts.append(f"Effective: {params['EffictiveUtilization']}")
        
        if util_parts:
            area_rows.append(('Utilization', ' | '.join(util_parts)))
        
        if area_rows:
            sections.append({'name': 'AREA & UTILIZATION', 'rows': area_rows})
        
        # Section 2: Cell Counts & Distribution
        cell_rows = []
        
        # Combine main cell counts into one line
        main_counts = []
        if 'CellCount' in params:
            try:
                main_counts.append(f"Cell: {int(params['CellCount']):,}")
            except:
                main_counts.append(f"Cell: {params['CellCount']}")
        
        if 'CombinationalCount' in params:
            try:
                main_counts.append(f"Comb: {int(params['CombinationalCount']):,}")
            except:
                main_counts.append(f"Comb: {params['CombinationalCount']}")
        
        if 'SequentialCount' in params:
            try:
                main_counts.append(f"Seq: {int(params['SequentialCount']):,}")
            except:
                main_counts.append(f"Seq: {params['SequentialCount']}")
        
        if 'FFCount' in params:
            try:
                main_counts.append(f"FF: {int(params['FFCount']):,}")
            except:
                main_counts.append(f"FF: {params['FFCount']}")
        
        if 'BufInvCellCount' in params:
            try:
                buf_count = f"{int(params['BufInvCellCount']):,}"
                perc = params.get('BufInvPerc', '')
                if perc:
                    try:
                        perc_val = float(perc.rstrip('%'))
                        main_counts.append(f"Buf/Inv: {buf_count} ({perc_val:.2f}%)")
                    except:
                        main_counts.append(f"Buf/Inv: {buf_count} ({perc})")
                else:
                    main_counts.append(f"Buf/Inv: {buf_count}")
            except:
                main_counts.append(f"Buf/Inv: {params['BufInvCellCount']}")
        
        if 'DelBuffer' in params:
            try:
                main_counts.append(f"DelBuf: {int(params['DelBuffer']):,}")
            except:
                main_counts.append(f"DelBuf: {params['DelBuffer']}")
        
        if main_counts:
            cell_rows.append(('Cell Counts', ' | '.join(main_counts)))
        
        # Add remaining cell metrics (Arrays - Count and Size)
        if 'ArraysCount' in params or 'ArraysSize' in params:
            arrays_parts = []
            if 'ArraysCount' in params:
                try:
                    arrays_parts.append(f"Count: {int(params['ArraysCount']):,}")
                except:
                    arrays_parts.append(f"Count: {params['ArraysCount']}")
            if 'ArraysSize' in params:
                arrays_parts.append(f"Size: {params['ArraysSize']}")
            if arrays_parts:
                cell_rows.append(('Arrays', ' | '.join(arrays_parts)))
        
        # Add FF breakdown by clock domain (combined into one line)
        ff_by_clock = []
        for key in sorted(params.keys()):
            if key.endswith('_FFCount') and not key.startswith('Total'):
                domain = key[:-8]  # Remove '_FFCount'
                try:
                    ff_by_clock.append(f"{domain}: {int(params[key]):,}")
                except:
                    ff_by_clock.append(f"{domain}: {params[key]}")
        
        if ff_by_clock:
            cell_rows.append(('FFs by Clock', ' | '.join(ff_by_clock)))
        
        # Add VT Distribution to cell counts section (combined into one line)
        vt_parts = []
        for vt_type in ['HVT_percentage', 'SVT_percentage', 'LVT_percentage', 'ULVT_percentage']:
            if vt_type in params:
                vt_value = params[vt_type]
                # Skip if value is 0 or 0.0%
                try:
                    val_float = float(vt_value.replace('%', ''))
                    if val_float == 0.0:
                        continue
                    # Format to 2 decimal places
                    vt_name = vt_type.replace('_percentage', '')
                    vt_parts.append(f"{vt_name}: {val_float:.2f}%")
                except:
                    vt_name = vt_type.replace('_percentage', '')
                    vt_parts.append(f"{vt_name}: {vt_value}")
        
        if vt_parts:
            cell_rows.append(('VT Distribution', ' | '.join(vt_parts)))
        
        if cell_rows:
            sections.append({'name': 'CELL COUNTS & DISTRIBUTION', 'rows': cell_rows})
        
        # Section 4: Clock Gating & FF Efficiency (format percentages to .2f)
        cg_rows = []
        for key in ['ClkGatesCount', 'GatedFFPerc', 'UngatedFFPerc', 'MBperc', 
                    'PercFFWithOneCg', 'PercFFWithMoreThan2Cg', 'UngatedFF',
                    'CGWithMaxFanout', 'MaxCGFanoutNum', 'NoneScanFlopCount']:
            if key in params:
                value = params[key]
                display_name = key.replace('Perc', '%').replace('Count', '')
                
                if key == 'ClkGatesCount':
                    display_name = 'Clock Gates Count'
                elif key == 'GatedFFPerc':
                    display_name = 'Gated FFs'
                    try:
                        perc_val = float(value.rstrip('%'))
                        value = f"{perc_val:.2f}%"
                    except:
                        pass
                elif key == 'UngatedFFPerc':
                    ungated_count = params.get('UngatedFF', '')
                    display_name = 'Ungated FFs'
                    if ungated_count:
                        try:
                            perc_val = float(value.rstrip('%'))
                            value = f"{perc_val:.2f}% ({int(ungated_count):,} ungated)"
                        except:
                            value = f"{value} ({ungated_count} ungated)"
                        cg_rows.append((display_name, value))
                        continue
                elif key == 'MBperc':
                    display_name = 'Multibit FFs'
                    try:
                        perc_val = float(value.rstrip('%'))
                        value = f"{perc_val:.2f}%"
                    except:
                        pass
                elif key == 'PercFFWithOneCg':
                    display_name = 'FFs with One ClkGate'
                    try:
                        perc_val = float(value)
                        value = f"{perc_val:.2f}%"
                    except:
                        pass
                elif key == 'PercFFWithMoreThan2Cg':
                    display_name = 'FFs with >2 ClkGates'
                    try:
                        perc_val = float(value)
                        value = f"{perc_val:.2f}%"
                    except:
                        pass
                elif key == 'UngatedFF':
                    continue  # Already handled above
                elif key == 'CGWithMaxFanout':
                    display_name = 'Max CG Fanout Instance'
                    # Truncate long instance names
                    if len(value) > 60:
                        value = value[:57] + "..."
                    cg_rows.append((display_name, value))
                    continue
                elif key == 'MaxCGFanoutNum':
                    display_name = 'Max CG Fanout Count'
                elif key == 'NoneScanFlopCount':
                    display_name = 'Non-Scan Flops'
                
                cg_rows.append((display_name, value))
        
        if cg_rows:
            sections.append({'name': 'CLOCK GATING & FF EFFICIENCY', 'rows': cg_rows})
        
        # Section 4.5: Clock Latency Metrics (Max and Avg per clock domain, in ps with color coding)
        latency_rows = []
        # Find all clock domains with latency data
        clock_domains_with_latency = set()
        for key in params.keys():
            if key.endswith('_Latency_Max'):
                domain = key[:-12]  # Remove '_Latency_Max'
                clock_domains_with_latency.add(domain)
        
        # Sort clock domains (main clocks first, then interface clocks)
        sorted_domains = sorted(clock_domains_with_latency, key=lambda x: (not x.startswith('m'), x))
        
        for domain in sorted_domains:
            max_key = f"{domain}_Latency_Max"
            avg_key = f"{domain}_Latency_Avg"
            
            if max_key in params and avg_key in params:
                max_val = params[max_key]
                avg_val = params[avg_key]
                
                # Skip if both are NA
                if max_val.upper() == 'NA' and avg_val.upper() == 'NA':
                    continue
                
                # Convert from ns to ps and format with color coding
                latency_parts = []
                max_color = ""
                
                if max_val.upper() != 'NA':
                    try:
                        max_ns = float(max_val)
                        max_ps = max_ns * 1000  # Convert ns to ps
                        
                        # Color coding based on max latency value
                        if max_ps > 560:
                            max_color = Color.RED  # Critical: >560ps
                        elif max_ps >= 550:
                            max_color = Color.YELLOW  # Warning: 550-560ps
                        else:
                            max_color = Color.GREEN  # Good: <550ps
                        
                        latency_parts.append(f"{max_color}Max: {max_ps:.2f} ps{Color.RESET}")
                    except:
                        latency_parts.append(f"Max: {max_val}")
                else:
                    latency_parts.append(f"Max: NA")
                
                if avg_val.upper() != 'NA':
                    try:
                        avg_ns = float(avg_val)
                        avg_ps = avg_ns * 1000  # Convert ns to ps
                        # Use same color as max for avg
                        if max_color:
                            latency_parts.append(f"{max_color}Avg: {avg_ps:.2f} ps{Color.RESET}")
                        else:
                            latency_parts.append(f"Avg: {avg_ps:.2f} ps")
                    except:
                        latency_parts.append(f"Avg: {avg_val}")
                else:
                    latency_parts.append(f"Avg: NA")
                
                display_name = f"{domain} Latency"
                latency_rows.append((display_name, ' | '.join(latency_parts)))
        
        if latency_rows:
            sections.append({'name': 'CLOCK LATENCY', 'rows': latency_rows})
        
        # Section 5: Routing & Congestion (skip congestion if value = 0)
        routing_rows = []
        if 'TotalWireLength' in params:
            wl = params['TotalWireLength'].replace(' um', '')
            try:
                wl_um = float(wl.replace('e+', 'e'))
                wl_mm = wl_um / 1000
                routing_rows.append(('Total Wire Length', f"{wl_um:,.0f} um ({wl_mm:.2f} mm)"))
            except:
                routing_rows.append(('Total Wire Length', params['TotalWireLength']))
        
        # Add Shorts (always show)
        if 'ShortsAmount' in params:
            routing_rows.append(('Shorts', params['ShortsAmount']))
        
        # Add Congestion metrics (skip if value = 0)
        for key in ['CongestionBothDir', 'CongestionHDir', 'CongestionVDir']:
            if key in params:
                congestion_value = params[key]
                # Skip if congestion is 0
                try:
                    val_float = float(congestion_value.replace('%', ''))
                    if val_float == 0.0:
                        continue
                except:
                    pass
                display_name = key.replace('Dir', ' Dir').replace('Congestion', 'Congestion ')
                routing_rows.append((display_name, congestion_value))
        
        if routing_rows:
            sections.append({'name': 'ROUTING & CONGESTION', 'rows': routing_rows})
        
        # Section 6: Design Violations (combined into one line)
        viol_parts = []
        if 'MaxCapViolations' in params:
            viol_parts.append(f"Cap: {params['MaxCapViolations']}")
        if 'MaxTransViolations' in params:
            viol_parts.append(f"Trans: {params['MaxTransViolations']}")
        if 'MaxFanoutViolations' in params:
            viol_parts.append(f"Fanout: {params['MaxFanoutViolations']}")
        
        if viol_parts:
            viol_rows = [('Max Violations', ' | '.join(viol_parts))]
            sections.append({'name': 'DESIGN VIOLATIONS', 'rows': viol_rows})
        
        # Section 7: DFT Metrics
        dft_rows = []
        for key in ['LongestScanChain', 'NumScanChains', 'NoneScanFlopCount']:
            if key in params:
                display_name = key.replace('Num', 'Number of ').replace('Count', '')
                if key == 'LongestScanChain':
                    display_name = 'Longest Scan Chain'
                elif key == 'NumScanChains':
                    display_name = 'Number of Scan Chains'
                elif key == 'NoneScanFlopCount':
                    continue  # Already in Clock Gating section
                dft_rows.append((display_name, params[key]))
        
        if dft_rows:
            sections.append({'name': 'DFT METRICS', 'rows': dft_rows})
        
        # Print the table
        if sections:
            # Get relative path for display
            rel_path = data_file.replace(self.workarea + '/', '') if self.workarea in data_file else os.path.basename(data_file)
            
            print(f"\n{Color.CYAN}{'='*98}{Color.RESET}")
            print(f"{Color.YELLOW}  DESIGN METRICS ({stage}){Color.RESET}")
            print(f"{Color.CYAN}{'='*98}{Color.RESET}")
            print(f"  Source: {rel_path}")
            print(f"  {'-'*98}")
            
            for section in sections:
                print(f"{Color.CYAN}  {section['name']}{Color.RESET}")
                print(f"  {'-'*98}")
                for key, value in section['rows']:
                    print(f"  {key:<35} {value}")
                print()
            
            print(f"{Color.CYAN}{'='*98}{Color.RESET}")
    
    def _generate_postroute_html_table(self, all_params: dict) -> str:
        """Generate HTML table with full post-route data for all stages
        
        Args:
            all_params: Dictionary of postroute parameters by stage
            
        Returns:
            HTML string containing the formatted table
        """
        try:
            # Get all data files for different stages (reversed order: most important first)
            stages = ['postroute', 'route', 'cts', 'place', 'plan']
            stage_data = {}
            missing_stages = []
            error_stages = []
            
            for stage in stages:
                # Use wildcard for IPO in filename since it can differ from directory name (e.g., ipo1000 dir with ipo1400 in filenames)
                stage_pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{self.design_info.ipo}/reports/{self.design_info.top_hier}_{self.design_info.top_hier}_*_report_{self.design_info.top_hier}_*_{stage}.func.std_tt_0c_0p6v.setup.typical.data"
                stage_files = self.file_utils.find_files(stage_pattern, self.workarea)
                
                if stage_files:
                    stage_file = stage_files[0]
                    try:
                        with open(stage_file, 'r', encoding='utf-8') as f:
                            content = f.read()
                        
                        stage_params = {}
                        for line in content.split('\n'):
                            line = line.strip()
                            if ' = ' in line:
                                param_name, param_value = line.split(' = ', 1)
                                stage_params[param_name] = param_value
                        
                        stage_data[stage] = stage_params
                    except Exception as e:
                        error_stages.append(stage)
                        print(f"    [ERROR] Error reading {stage} data: {e}")
                else:
                    missing_stages.append(stage)
            
            # Report summary
            if missing_stages:
                print(f"    {Color.YELLOW}Missing stages: {', '.join(missing_stages)}{Color.RESET}")
            if error_stages:
                print(f"    {Color.RED}Error stages: {', '.join(error_stages)}{Color.RESET}")
            
            if not stage_data:
                print(f"    {Color.RED}No stage data available - cannot generate HTML table{Color.RESET}")
                return
            
            # Extract timing histogram data
            timing_histogram_data = self._extract_timing_histogram_for_html()
            
            # Generate HTML table
            html_content = self._create_postroute_html_table(stage_data, missing_stages, error_stages, timing_histogram_data)
            
            # Save HTML file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            html_filename = f"{self.design_info.top_hier}_{os.environ.get('USER', 'avice')}_pnr_data_{self.design_info.ipo}_{timestamp}.html"
            html_path = os.path.join(os.getcwd(), html_filename)
            
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            # Determine display path (will be moved to html/ or test_outputs/html/ by _organize_html_files)
            html_output_dir = self._get_html_output_dir()
            display_path = os.path.relpath(html_output_dir, os.getcwd())
            
            print(f"\n  {Color.CYAN}Full PnR Data Table:{Color.RESET}")
            print(f"  Open with: /home/utils/firefox-118.0.1/firefox {Color.MAGENTA}{display_path}/{html_filename}{Color.RESET} &")
            
        except Exception as e:
            print(f"  Error generating HTML table: {e}")
    
    def _create_postroute_html_table(self, stage_data: dict, missing_stages: list = None, error_stages: list = None, timing_histogram_data: dict = None) -> str:
        """Create HTML table with post-route data for all stages"""
        # Get all unique parameters across all stages
        all_params = set()
        for stage_params in stage_data.values():
            all_params.update(stage_params.keys())
        
        all_params = sorted(list(all_params))
        
        html = f"""
<!DOCTYPE html>
<html>
<head>
            <title>Place & Route Data Analysis - {self.design_info.top_hier}</title>
    <style>
        .header {{
            text-align: center;
            border-bottom: 2px solid #2c3e50;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }}
        .logo {{
            text-align: center;
            margin-bottom: 10px;
        }}
        .logo img {{
            max-height: 80px;
            max-width: 300px;
            height: auto;
            width: auto;
            cursor: pointer;
            transition: transform 0.2s ease;
            border-radius: 6px;
        }}
        .logo img:hover {{
            transform: scale(1.05);
            box-shadow: 0 0 15px rgba(0, 0, 0, 0.3);
        }}
        .image-expanded {{
            position: fixed;
            top: 0;
            left: 0;
            width: 100vw;
            height: 100vh;
            background-color: rgba(0, 0, 0, 0.9);
            z-index: 1000;
            display: flex;
            justify-content: center;
            align-items: center;
            cursor: pointer;
        }}
        .image-expanded img {{
            max-width: 90%;
            max-height: 90%;
            border: 3px solid #2c3e50;
        }}
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        h1 {{ color: #2c3e50; }}
        h2 {{ color: #34495e; margin-top: 30px; }}
        table {{ border-collapse: collapse; width: 100%; margin-bottom: 20px; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; font-weight: bold; }}
        tr:nth-child(even) {{ background-color: #f9f9f9; }}
        .stage-header {{ background-color: #2c3e50; color: black; font-weight: bold; }}
        .param-name {{ font-weight: bold; background-color: #ecf0f1; }}
        .timing {{ color: #e74c3c; }}
        .area {{ color: #27ae60; }}
        .cell {{ color: #8e44ad; }}
        .power {{ color: #f39c12; }}
        .clock {{ color: #16a085; }}
        .parameter-groups {{ margin-top: 20px; }}
        .category-header {{ 
            background-color: #34495e; 
            color: white; 
            padding: 10px 15px; 
            margin: 5px 0; 
            cursor: pointer; 
            border-radius: 5px;
            font-weight: bold;
            user-select: none;
        }}
        .category-header:hover {{ background-color: #2c3e50; }}
        .category-icon {{
            width: 24px;
            height: 24px;
            margin-right: 10px;
            vertical-align: middle;
        }}
        .category-content {{ 
            display: none; 
            margin-bottom: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
            overflow: hidden;
        }}
        .category-content.expanded {{ display: block; }}
        .category-table {{ 
            width: 100%; 
            border-collapse: collapse; 
            margin: 0;
        }}
        .category-table th, .category-table td {{ 
            border: 1px solid #ddd; 
            padding: 8px; 
            text-align: left; 
        }}
        .category-table th {{ 
            background-color: #34495e; 
            color: white; 
            font-weight: bold; 
        }}
        .category-table tr:nth-child(even) {{ background-color: #f9f9f9; }}
        .expand-icon {{ 
            float: right; 
            transition: transform 0.3s ease;
        }}
        .expand-icon.expanded {{ transform: rotate(90deg); }}
        .timing-histogram-section {{
            margin-top: 30px;
            padding: 20px;
            background-color: #f8f9fa;
            border-radius: 8px;
            border-left: 4px solid #3498db;
        }}
        .histogram-info {{
            margin-bottom: 20px;
        }}
        .histogram-info h3 {{
            color: #2c3e50;
            margin-bottom: 10px;
        }}
        .histogram-info p {{
            margin: 5px 0;
            color: #7f8c8d;
        }}
        .histogram-tables {{
            margin-top: 20px;
        }}
        .histogram-tables h4 {{
            color: #34495e;
            margin-bottom: 15px;
        }}
        .histogram-table-container {{
            background-color: white;
            border: 1px solid #bdc3c7;
            border-radius: 5px;
            padding: 15px;
            overflow-x: auto;
        }}
        .histogram-table {{
            font-family: 'Courier New', monospace;
            font-size: 12px;
            line-height: 1.4;
            margin: 0;
            white-space: pre;
            color: #2c3e50;
        }}
        
        /* Copyright Footer */
        .footer {{
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            text-align: center;
            padding: 20px;
            margin-top: 40px;
            border-radius: 10px;
            font-size: 14px;
        }}
        
        .footer p {{
            margin: 5px 0;
        }}
        
        .footer strong {{
            color: #00ff00;
        }}
    </style>
</head>
<body>
    <div class="header">
        <div class="logo">
            <img src="/home/scratch.avice_vlsi/cursor/avice_wa_review/assets/images/avice_logo.png" alt="AVICE Logo" onclick="expandImage(this)">
        </div>
    <h1>Place & Route Data Analysis</h1>
    <h2>Design: {self.design_info.top_hier} | IPO: {self.design_info.ipo} | Tag: {self.design_info.tag}</h2>
        <h3>Workarea: {self.workarea_abs}</h3>
    </div>
    
    <h2>Legend</h2>
    <ul>
        <li><span class="timing">Timing Parameters</span> - WNS, TNS, Violation Paths, Skew, Latency, Cycle Time</li>
        <li><span class="area">Area Parameters</span> - Area, Utilization, Die, Core</li>
        <li><span class="cell">Cell Parameters</span> - Cell Count, Combinational, Sequential, FF Count, Buffer/Inverter</li>
        <li><span class="power">Power Parameters</span> - Leakage, Power</li>
        <li><span class="clock">Clock Parameters</span> - Clock-related parameters</li>
    </ul>
    
    <div class="parameter-groups">
"""
        
        # Group parameters by category
        categories = {
            'timing': [],
            'area': [],
            'cell': [],
            'power': [],
            'clock': [],
            'other': []
        }
        
        for param in all_params:
            # Determine parameter category
            if any(keyword in param.upper() for keyword in ['WNS', 'TNS', 'VIOLPATHS', 'SKEW', 'LATENCY', 'CYCLE_TIME']):
                categories['timing'].append(param)
            elif any(keyword in param.upper() for keyword in ['AREA', 'UTILIZATION', 'DIE', 'CORE']):
                categories['area'].append(param)
            elif any(keyword in param.upper() for keyword in ['CELLCOUNT', 'COMBINATIONAL', 'SEQUENTIAL', 'FFCOUNT', 'BUFINV', 'GATED', 'UNGATED']):
                categories['cell'].append(param)
            elif any(keyword in param.upper() for keyword in ['LEAKAGE', 'POWER']):
                categories['power'].append(param)
            elif any(keyword in param.upper() for keyword in ['CLK', 'CLOCK']):
                categories['clock'].append(param)
            else:
                categories['other'].append(param)
        
        # Generate expandable category sections with embedded base64 icons
        # Read base64 data for each icon (relative to script location)
        script_dir = os.path.dirname(os.path.abspath(__file__))
        icons_dir = os.path.join(script_dir, 'icons')
        
        try:
            with open(os.path.join(icons_dir, 'timing.b64'), 'r') as f:
                timing_b64 = f.read().strip()
            with open(os.path.join(icons_dir, 'area.b64'), 'r') as f:
                area_b64 = f.read().strip()
            with open(os.path.join(icons_dir, 'cell.b64'), 'r') as f:
                cell_b64 = f.read().strip()
            with open(os.path.join(icons_dir, 'power.b64'), 'r') as f:
                power_b64 = f.read().strip()
            with open(os.path.join(icons_dir, 'clock.b64'), 'r') as f:
                clock_b64 = f.read().strip()
            with open(os.path.join(icons_dir, 'other.b64'), 'r') as f:
                other_b64 = f.read().strip()
        except FileNotFoundError:
            # Fallback to text icons if base64 files not found
            timing_b64 = area_b64 = cell_b64 = power_b64 = clock_b64 = other_b64 = ""
        
        category_info = {
            'timing': {'name': 'Timing Parameters', 'icon': f'data:image/png;base64,{timing_b64}' if timing_b64 else '[T]'},
            'area': {'name': 'Area Parameters', 'icon': f'data:image/png;base64,{area_b64}' if area_b64 else '[A]'},
            'cell': {'name': 'Cell Parameters', 'icon': f'data:image/png;base64,{cell_b64}' if cell_b64 else '[C]'},
            'power': {'name': 'Power Parameters', 'icon': f'data:image/png;base64,{power_b64}' if power_b64 else '[P]'},
            'clock': {'name': 'Clock Parameters', 'icon': f'data:image/png;base64,{clock_b64}' if clock_b64 else '[K]'},
            'other': {'name': 'Other Parameters', 'icon': f'data:image/png;base64,{other_b64}' if other_b64 else '[O]'},
            'histogram': {'name': 'Timing Histogram Analysis', 'icon': f'data:image/png;base64,{timing_b64}' if timing_b64 else '[H]'}
        }
        
        for category, params in categories.items():
            if not params:
                continue
                
            category_name = category_info[category]['name']
            category_icon = category_info[category]['icon']
            
            # Check if icon is a data URI or text
            if category_icon.startswith('data:'):
                icon_html = f'<img src="{category_icon}" alt="{category_name}" style="width: 16px; height: 16px; margin-right: 8px; vertical-align: middle;">'
            else:
                icon_html = f'<span style="display: inline-block; width: 16px; height: 16px; margin-right: 8px; text-align: center; font-weight: bold; color: #2c3e50;">{category_icon}</span>'
            
            html += f"""
        <div class="category-section">
            <div class="category-header" onclick="toggleCategory('{category}')">
                {icon_html} {category_name} ({len(params)} parameters)
                <span class="expand-icon" id="icon-{category}">â–¶</span>
            </div>
            <div class="category-content" id="content-{category}">
                <table class="category-table">
                    <tr>
                        <th>Parameter</th>
"""
            
            # Add stage headers for this category
            for stage in ['postroute', 'route', 'cts', 'place', 'plan']:
                if stage in stage_data:
                    html += f"                        <th>{stage.upper()}</th>\n"
                elif missing_stages and stage in missing_stages:
                    html += f"                        <th style=\"background-color: #f39c12; color: white;\">{stage.upper()} (MISSING)</th>\n"
                elif error_stages and stage in error_stages:
                    html += f"                        <th style=\"background-color: #e74c3c; color: white;\">{stage.upper()} (ERROR)</th>\n"
                else:
                    html += f"                        <th style=\"background-color: #95a5a6; color: white;\">{stage.upper()} (N/A)</th>\n"
            
            html += "                    </tr>\n"
            
            # Add parameter rows for this category
            for param in sorted(params):
                param_class = category
                html += f"                    <tr>\n"
                html += f"                        <td class=\"param-name {param_class}\">{param}</td>\n"
                
                for stage in ['postroute', 'route', 'cts', 'place', 'plan']:
                    if stage in stage_data:
                        value = stage_data[stage].get(param, 'N/A')
                        html += f"                        <td class=\"{param_class}\">{value}</td>\n"
                    elif missing_stages and stage in missing_stages:
                        html += f"                        <td style=\"background-color: #fdf2e9; color: #d68910; text-align: center;\">FILE MISSING</td>\n"
                    elif error_stages and stage in error_stages:
                        html += f"                        <td style=\"background-color: #fadbd8; color: #c0392b; text-align: center;\">READ ERROR</td>\n"
                    else:
                        html += f"                        <td style=\"background-color: #f8f9fa; color: #7f8c8d; text-align: center;\">N/A</td>\n"
                
                html += "                    </tr>\n"
            
            html += "                </table>\n"
            html += "            </div>\n"
            html += "        </div>\n"
        
        html += """
    </div>
    
    <h2>Timing Histogram Analysis</h2>
    <div class="timing-histogram-section">
"""
        
        # Add timing histogram data if available
        if timing_histogram_data and (timing_histogram_data.get('category_data') or timing_histogram_data.get('data')):
            # Combine all histogram tables into one expandable section
            histogram_content = ""
            
            if timing_histogram_data.get('category_data'):
                histogram_content += f"<h4>Table 1 - Category Breakdown</h4>\n"
                histogram_content += f"<div class='histogram-table-container'>\n"
                histogram_content += f"<pre class='histogram-table'>{timing_histogram_data['category_data']}</pre>\n"
                histogram_content += f"</div>\n\n"
            
            if timing_histogram_data.get('sub_category_data'):
                histogram_content += f"<h4>Table 2 - Sub-Category Breakdown</h4>\n"
                histogram_content += f"<div class='histogram-table-container'>\n"
                histogram_content += f"<pre class='histogram-table'>{timing_histogram_data['sub_category_data']}</pre>\n"
                histogram_content += f"</div>\n\n"
            
            if timing_histogram_data.get('scenario_data'):
                histogram_content += f"<h4>Table 3 - Sub-Category + Scenario Breakdown</h4>\n"
                histogram_content += f"<div class='histogram-table-container'>\n"
                histogram_content += f"<pre class='histogram-table'>{timing_histogram_data['scenario_data']}</pre>\n"
                histogram_content += f"</div>\n\n"
            
            # Fallback for old data format
            elif timing_histogram_data.get('data'):
                histogram_content += f"<h4>Category + Scenario Breakdown</h4>\n"
                histogram_content += f"<div class='histogram-table-container'>\n"
                histogram_content += f"<pre class='histogram-table'>{timing_histogram_data['data']}</pre>\n"
                histogram_content += f"</div>\n\n"
            
            # Get histogram icon
            histogram_icon = category_info['histogram']['icon']
            if histogram_icon.startswith('data:'):
                histogram_icon_html = f'<img src="{histogram_icon}" alt="Timing Histogram" style="width: 16px; height: 16px; margin-right: 8px; vertical-align: middle;">'
            else:
                histogram_icon_html = f'<span style="display: inline-block; width: 16px; height: 16px; margin-right: 8px; text-align: center; font-weight: bold; color: #2c3e50;">{histogram_icon}</span>'
            
            html += f"""
        <div class="category-section">
            <div class="category-header" onclick="toggleCategory('histogram')">
                {histogram_icon_html} Timing Histogram Analysis - {timing_histogram_data['stage'].upper()} Stage
                <span class="expand-icon" id="icon-histogram">â–¶</span>
            </div>
            <div class="category-content" id="content-histogram">
                <div class="histogram-info">
                    <p><strong>Source:</strong> {self._make_source_clickable(timing_histogram_data['file'])}</p>
                </div>
                <div class="histogram-tables">
                    {histogram_content}
                </div>
            </div>
        </div>
"""
        else:
            # Get histogram icon for fallback case
            histogram_icon = category_info['histogram']['icon']
            if histogram_icon.startswith('data:'):
                histogram_icon_html = f'<img src="{histogram_icon}" alt="Timing Histogram" style="width: 16px; height: 16px; margin-right: 8px; vertical-align: middle;">'
            else:
                histogram_icon_html = f'<span style="display: inline-block; width: 16px; height: 16px; margin-right: 8px; text-align: center; font-weight: bold; color: #2c3e50;">{histogram_icon}</span>'
            
            html += f"""
        <div class="category-section">
            <div class="category-header" onclick="toggleCategory('histogram')">
                {histogram_icon_html} Timing Histogram Analysis
                <span class="expand-icon" id="icon-histogram">â–¶</span>
            </div>
            <div class="category-content" id="content-histogram">
                <p><em>No timing histogram data available</em></p>
            </div>
        </div>
"""
        
        html += """
    </div>
    
    <script>
        function toggleCategory(category) {
            const content = document.getElementById('content-' + category);
            const icon = document.getElementById('icon-' + category);
            
            if (content.classList.contains('expanded')) {
                content.classList.remove('expanded');
                icon.classList.remove('expanded');
                icon.textContent = 'â–¶';
            } else {
                content.classList.add('expanded');
                icon.classList.add('expanded');
                icon.textContent = 'â–¼';
            }
        }
        
        
        // All categories start collapsed by default
        document.addEventListener('DOMContentLoaded', function() {
            const categories = ['timing', 'area', 'cell', 'power', 'clock', 'other', 'histogram'];
            categories.forEach(category => {
                const content = document.getElementById('content-' + category);
                const icon = document.getElementById('icon-' + category);
                if (content) {
                    // Categories start collapsed (no expanded class)
                    icon.textContent = 'â–¶';
                }
            });
        });
    </script>
    
    <p><em>Generated by avice_wa_review.py</em></p>
    
    <script>
        function expandImage(imgElement) {{
            // Create overlay
            var overlay = document.createElement('div');
            overlay.className = 'image-expanded';
            
            // Create expanded image
            var expandedImg = document.createElement('img');
            expandedImg.src = imgElement.src;
            expandedImg.alt = imgElement.alt;
            
            overlay.appendChild(expandedImg);
            document.body.appendChild(overlay);
            
            // Close on click
            overlay.onclick = function() {{
                if (document.body.contains(overlay)) {{
                    document.body.removeChild(overlay);
                }}
            }};
            
            // Close on escape key
            function escapeHandler(e) {{
                e = e || window.event;
                if ((e.keyCode || e.which) === 27) {{
                    if (document.body.contains(overlay)) {{
                        document.body.removeChild(overlay);
                        if (document.removeEventListener) {{
                            document.removeEventListener('keydown', escapeHandler);
                        }} else if (document.detachEvent) {{
                            document.detachEvent('onkeydown', escapeHandler);
                        }}
                    }}
                }}
            }}
            
            if (document.addEventListener) {{
                document.addEventListener('keydown', escapeHandler);
            }} else if (document.attachEvent) {{
                document.attachEvent('onkeydown', escapeHandler);
            }}
        }}
        // Back to top button functionality
        var backToTopBtn = document.getElementById('backToTopBtn');
        if (backToTopBtn) {{
            window.addEventListener('scroll', function() {{
                if (window.pageYOffset > 300) {{
                    backToTopBtn.style.display = 'block';
                }} else {{
                    backToTopBtn.style.display = 'none';
                }}
            }});
            
            backToTopBtn.addEventListener('click', function() {{
                window.scrollTo(0, 0);
            }});
        }}
    </script>
    
    <button id="backToTopBtn" style="display: none; position: fixed; bottom: 30px; right: 30px; 
            z-index: 99; border: none; outline: none; background-color: #667eea; color: white; 
            cursor: pointer; padding: 15px 20px; border-radius: 50px; font-size: 16px; 
            font-weight: bold; box-shadow: 0 4px 6px rgba(0,0,0,0.3); transition: all 0.3s ease;"
            onmouseover="this.style.backgroundColor='#5568d3'; this.style.transform='scale(1.1)';"
            onmouseout="this.style.backgroundColor='#667eea'; this.style.transform='scale(1)';">
        â†‘ Top
    </button>
    
    <!-- Copyright Footer -->
    <div class="footer">
        <p><strong>AVICE P&R Data Analysis Report</strong></p>
        <p>Copyright (c) 2025 Alon Vice (avice)</p>
        <p>Contact: avice@nvidia.com</p>
    </div>
</body>
</html>
"""
        
        return html
    
    def run_synthesis_analysis(self) -> None:
        """Run synthesis (DC) analysis"""
        self.print_header(FlowStage.SYNTHESIS)
        
        # Design Definition
        des_def = os.path.join(self.workarea, "unit_scripts/des_def.tcl")
        if self.print_file_info(des_def, "Design Definition"):
            matches = self.file_utils.grep_file(r"trans_factor|dont_use", des_def)
            for match in matches:
                print(f"  {match}")
        
        # DC Log
        dc_log = os.path.join(self.workarea, "syn_flow/dc/log/dc.log")
        if self.file_utils.file_exists(dc_log):
            self.print_file_info(dc_log, "DC Log")
            # Extract and print DC version
            self._extract_dc_version(dc_log)
            # Check for errors and warnings
            self._check_dc_errors(dc_log)
        
        # QoR Report (Quality of Results)
        qor_report = os.path.join(self.workarea, f"syn_flow/dc/reports/{self.design_info.top_hier}_rtl2gate.qor.rpt")
        qor_metrics = None
        if self.print_file_info(qor_report, "QoR Report (Quality of Results)"):
            qor_metrics = self._analyze_qor_report(qor_report)
        else:
            # Fallback: Try to extract data from qor.csv if .qor.rpt is missing
            qor_csv = os.path.join(self.workarea, "syn_flow/dc/qor.csv")
            cell_count_rep = os.path.join(self.workarea, f"syn_flow/dc/reports/{self.design_info.top_hier}.cell_count.rep")
            if self.file_utils.file_exists(qor_csv):
                print(f"  {Color.YELLOW}[WARN] QoR report (.qor.rpt) not found - using fallback sources{Color.RESET}")
                print(f"         Expected location: {qor_report}")
                print(f"  {Color.CYAN}Fallback QoR CSV:{Color.RESET} {qor_csv}")
                if self.file_utils.file_exists(cell_count_rep):
                    print(f"  {Color.CYAN}Cell Count Source:{Color.RESET} {cell_count_rep}")
                qor_metrics = self._analyze_qor_csv(qor_csv)
            else:
                print(f"  {Color.YELLOW}[WARN] QoR report not found - synthesis may not have completed successfully{Color.RESET}")
                print(f"         Expected location: {qor_report}")
                print(f"         Fallback CSV: {qor_csv} (also not found)")
        
        # BeFlow Configuration
        beflow_config = os.path.join(self.workarea, "syn_flow/dc/beflow_config.yaml")
        if self.print_file_info(beflow_config, "BeFlow Configuration"):
            self._analyze_beflow_config(beflow_config)
        
        # Check for additional synthesis reports (skip if already handled)
        report_map = {
            "syn_flow/dc/reports/be4rtl/internal_high_width_logical_cones.rpt": "High Width Logical Cones Report",
            "syn_flow/dc/reports/debug/*.rtl2gate.removed_cgates.rep": "Removed Clock Gates Report",
            f"syn_flow/dc/reports/debug/{self.design_info.top_hier}.rtl2gate.removed_registers.rep": "Removed Registers Report"
        }
        
        for report_pattern, report_name in report_map.items():
            if "*" in report_pattern:
                files = self.file_utils.find_files(report_pattern, self.workarea)
                if files:
                    self.print_file_info(files[0], report_name)
            else:
                report_path = os.path.join(self.workarea, report_pattern)
                self.print_file_info(report_path, report_name)
        
        # Clock gates removed
        files = self.file_utils.find_files("syn_flow/dc/reports/debug/*.rtl2gate.removed_cgates.rep", self.workarea)
        if files:
            matches = self.file_utils.grep_file(r"Total clock gates removed:\s*(\d+)", files[0])
            if matches:
                count = matches[0]
                print(f"Clock gates removed: {count}")
            else:
                print("Clock gates removed: Unable to extract count")
        
        # Registers removed
        reg_rep = os.path.join(self.workarea, f"syn_flow/dc/reports/debug/{self.design_info.top_hier}.rtl2gate.removed_registers.rep")
        if self.file_utils.file_exists(reg_rep):
            try:
                if reg_rep.endswith('.gz'):
                    with gzip.open(reg_rep, 'rt', encoding='utf-8') as f:
                        lines = f.readlines()
                else:
                    with open(reg_rep, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                print(f"Removed registers: {len(lines)}")
            except (OSError, UnicodeDecodeError, gzip.BadGzipFile):
                print("Removed registers: Unable to read file")
        
        # Don't use cells
        dont_use_cells_rpt = os.path.join(self.workarea, f"syn_flow/dc/reports/{self.design_info.top_hier}.dont_use_cells.rpt")
        if self.file_utils.file_exists(dont_use_cells_rpt):
            try:
                if dont_use_cells_rpt.endswith('.gz'):
                    with gzip.open(dont_use_cells_rpt, 'rt', encoding='utf-8') as f:
                        lines = f.readlines()
                else:
                    with open(dont_use_cells_rpt, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                
                # Count non-empty lines (actual cells)
                cell_count = len([line for line in lines if line.strip()])
                if cell_count > 0:
                    print(f"{Color.YELLOW}Don't use cells found: {cell_count}{Color.RESET}")
                    print(f"  Report: {dont_use_cells_rpt}")
                    # Show first few cells as examples
                    if cell_count <= 10:
                        print(f"  Cells: {', '.join([line.strip() for line in lines if line.strip()])}")
                    else:
                        sample_cells = [line.strip() for line in lines if line.strip()][:5]
                        print(f"  Sample cells: {', '.join(sample_cells)} ... (+{cell_count-5} more)")
                else:
                    print(f"{Color.GREEN}No don't use cells{Color.RESET}")
            except (OSError, UnicodeDecodeError, gzip.BadGzipFile) as e:
                print(f"{Color.YELLOW}Don't use cells report found but unable to read: {e}{Color.RESET}")
        
        # Generate unified DC HTML report
        dc_html = self._generate_unified_dc_html()
        
        # Add section summary for master dashboard
        self._add_section_summary(
            section_name="Synthesis (DC)",
            section_id="synthesis",
            stage=FlowStage.SYNTHESIS,
            status="PASS",
            html_file=dc_html if dc_html else "",
            key_metrics={
                "Design": self.design_info.top_hier,
                "IPO": self.design_info.ipo
            },
            priority=3,
            issues=[],
            icon="[DC]"
        )
    
    def _extract_floorplan_dimensions(self) -> Optional[Dict[str, str]]:
        """Extract floorplan dimensions from DEF file
        
        Priority:
        1. Initial floorplan from flp/ directory (ALWAYS use this if available)
        2. Fallback to PnR stage DBs only if flp/ not found
        
        Returns:
            Dictionary with dimension data or None if not found
        """
        flp_file = None
        source = None
        
        try:
            # PRIORITY 1: ALWAYS use initial floorplan from flp/ directory if available
            flp_pattern = os.path.join(self.workarea, "flp", f"{self.design_info.top_hier}_fp.def.gz")
            
            if os.path.exists(flp_pattern):
                flp_file = flp_pattern
                source = "flp"
            else:
                # Try finding any *_fp.def.gz file in flp/ directory
                flp_files = glob.glob(os.path.join(self.workarea, "flp", "*_fp.def.gz"))
                if flp_files:
                    flp_file = flp_files[0]
                    source = "flp"
            
            # PRIORITY 2: Fallback to PnR stage DBs ONLY if flp/ not found
            # Path: pnr_flow/nv_flow/{design}/{ipo}/DBs/{design}_{ipo}_{stage}.enc.dat/{design}.fp.gz
            if not flp_file:
                pnr_stages = ['postroute', 'route', 'cts', 'place', 'plan']
                
                if hasattr(self.design_info, 'ipo') and self.design_info.ipo:
                    for stage in pnr_stages:
                        stage_db_pattern = os.path.join(
                            self.workarea, 
                            "pnr_flow/nv_flow", 
                            self.design_info.top_hier,
                            self.design_info.ipo,
                            "DBs",
                            f"{self.design_info.top_hier}_{self.design_info.ipo}_{stage}.enc.dat",
                            f"{self.design_info.top_hier}.fp.gz"
                        )
                        if os.path.exists(stage_db_pattern):
                            flp_file = stage_db_pattern
                            source = f"{stage} DB"
                            break
                
                # If no specific IPO, try to find any PnR stage DB
                if not flp_file:
                    for stage in pnr_stages:
                        stage_db_search = os.path.join(
                            self.workarea,
                            "pnr_flow/nv_flow",
                            self.design_info.top_hier,
                            "ipo*",
                            "DBs",
                            f"*_{stage}.enc.dat",
                            f"{self.design_info.top_hier}.fp.gz"
                        )
                        stage_matches = glob.glob(stage_db_search)
                        if stage_matches:
                            # Use the most recent stage DB
                            flp_file = max(stage_matches, key=os.path.getmtime)
                            source = f"{stage} DB"
                            break
            
            # If no file found, return None
            if not flp_file:
                return None
            
            # Parse DEF/fp file
            if flp_file.endswith('.gz'):
                with gzip.open(flp_file, 'rt', encoding='utf-8') as f:
                    content = f.read()
            else:
                with open(flp_file, 'r', encoding='utf-8') as f:
                    content = f.read()
            
            # Try parsing Innovus .fp format first (postroute DB format)
            # Format: Head Box: x1 y1 x2 y2 (already in micrometers)
            head_box_match = re.search(r'Head Box:\s+([\d.]+)\s+([\d.]+)\s+([\d.]+)\s+([\d.]+)', content)
            if head_box_match:
                x1, y1, x2, y2 = map(float, head_box_match.groups())
                
                # Dimensions already in micrometers
                x_dim_um = x2 - x1
                y_dim_um = y2 - y1
                design_area_um2 = x_dim_um * y_dim_um
                
                return {
                    'x_dim_um': x_dim_um,
                    'y_dim_um': y_dim_um,
                    'design_area': design_area_um2,
                    'source': source,  # Track which source was used (description)
                    'source_file': flp_file  # Track actual file path
                }
            
            # Fallback to DEF format (initial floorplan format)
            # Format: DIEAREA ( x1 y1 ) ( x2 y2 ) (in DEF units, need to divide by 2000)
            units_match = re.search(r'UNITS\s+DISTANCE\s+MICRONS\s+(\d+)', content)
            units = int(units_match.group(1)) if units_match else 2000  # Default to 2000
            
            diearea_match = re.search(r'DIEAREA\s+\(\s*(\d+)\s+(\d+)\s*\)\s+\(\s*(\d+)\s+(\d+)\s*\)', content)
            if diearea_match:
                x1, y1, x2, y2 = map(int, diearea_match.groups())
                
                # Calculate X, Y dimensions in um: top-right coordinates / units
                x_dim_um = x2 / units
                y_dim_um = y2 / units
                design_area_um2 = x_dim_um * y_dim_um
                
                return {
                    'x_dim_um': x_dim_um,
                    'y_dim_um': y_dim_um,
                    'design_area': design_area_um2,
                    'source': source,  # Track which source was used (description)
                    'source_file': flp_file  # Track actual file path
                }
        except Exception as e:
            # Silently fail if floorplan not found or parsing fails
            return None
        
        return None
    
    def _analyze_qor_csv(self, qor_csv: str) -> Optional[Dict]:
        """Analyze QoR CSV file (fallback when .qor.rpt is missing)
        
        Args:
            qor_csv: Path to qor.csv file
            
        Returns:
            Dictionary with extracted metrics for comparison (Phase 3)
        """
        try:
            print(f"  {Color.CYAN}QoR Analysis (from CSV):{Color.RESET}")
            
            # Read the CSV file
            with open(qor_csv, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # Initialize metrics dictionary
            metrics_dict = {
                'area': {},
                'cells': {},
                'timing_by_pathgroup': {},
                'timing_summary': {},
                'clock_gates_removed': None,
                'registers_removed': None,
                'sources': {
                    'qor_report': qor_csv,
                    'clock_gates_file': None,
                    'registers_file': None,
                    'cell_count_file': None
                }
            }
            
            # Parse path group timing data
            for line in lines[1:]:  # Skip header
                line = line.strip()
                if not line or line.startswith('Summary') or line.startswith('CAP,'):
                    continue
                
                # Parse CSV fields
                parts = [p.strip() for p in line.split(',')]
                if len(parts) >= 8:
                    pg_name = parts[0].split('(')[0].strip()  # Extract path group name
                    try:
                        wns_setup = float(parts[1])
                        tns_setup = float(parts[2])
                        nvp_setup = int(float(parts[3]))
                        wns_hold = float(parts[5])
                        tns_hold = float(parts[6])
                        nvp_hold = int(float(parts[7]))
                        
                        metrics_dict['timing_by_pathgroup'][pg_name] = {
                            'wns_setup': wns_setup,
                            'tns_setup': tns_setup,
                            'nvp_setup': nvp_setup,
                            'wns_hold': wns_hold,
                            'tns_hold': tns_hold,
                            'nvp_hold': nvp_hold
                        }
                    except (ValueError, IndexError):
                        continue
            
            # Parse cell statistics from last line (qor.csv)
            if lines:
                last_line = lines[-1].strip()
                parts = [p.strip() for p in last_line.split(',')]
                if len(parts) >= 10:
                    try:
                        metrics_dict['cells']['leaf_cells'] = int(parts[6].replace('K', '000'))  # LEAFS
                        metrics_dict['cells']['seq_cells'] = int(parts[9])  # REGS
                        metrics_dict['cells']['buf_inv_cells'] = int(parts[5].replace('K', '000'))  # BUFS
                        metrics_dict['cells']['total_nets'] = int(parts[7].replace('K', '000'))  # TNETS
                    except (ValueError, IndexError):
                        pass
            
            # Extract die dimensions from floorplan (PRIORITY: flp/ directory for DC section)
            flp_dims = self._extract_floorplan_dimensions()
            if flp_dims:
                # Die dimensions ALWAYS from flp/
                metrics_dict['area']['die_x'] = flp_dims.get('x_dim_um')
                metrics_dict['area']['die_y'] = flp_dims.get('y_dim_um')
                metrics_dict['area']['die_source'] = flp_dims.get('source')
                # Track the source file for die dimensions
                if flp_dims.get('source_file'):
                    metrics_dict['sources']['die_dimensions_file'] = flp_dims['source_file']
                # CRITICAL: Design area for DC section ALWAYS comes from flp/ directory (die_x * die_y)
                if flp_dims.get('design_area'):
                    metrics_dict['area']['design_area'] = flp_dims.get('design_area')
            
            # Extract cell count from cell_count.rep (fallback for missing .qor.rpt)
            cell_count_rep = os.path.join(self.workarea, f"syn_flow/dc/reports/{self.design_info.top_hier}.cell_count.rep")
            if self.file_utils.file_exists(cell_count_rep):
                try:
                    with open(cell_count_rep, 'r', encoding='utf-8', errors='ignore') as f:
                        lines_cc = f.readlines()
                        # Find the first data line (after header with dashes)
                        for i, line in enumerate(lines_cc):
                            if '------------' in line:
                                # Next line is the first data line (top-level hierarchy)
                                if i + 1 < len(lines_cc):
                                    data_line = lines_cc[i + 1].strip()
                                    parts = data_line.split()
                                    if len(parts) >= 3:
                                        # NumOfCells is first column
                                        total_cells = int(parts[0])
                                        if 'leaf_cells' not in metrics_dict['cells']:
                                            metrics_dict['cells']['leaf_cells'] = total_cells
                                        # NOTE: Design area from cell_count.rep is NOT used
                                        # Design area MUST come from flp/ directory (die dimensions)
                                        metrics_dict['sources']['cell_count_file'] = cell_count_rep
                                break
                except Exception:
                    pass
            
            # Extract clock gates removed and registers removed (same as .qor.rpt path)
            cgate_file = self.file_utils.find_files("syn_flow/dc/reports/debug/*.rtl2gate.removed_cgates.rep", self.workarea)
            if cgate_file:
                metrics_dict['sources']['clock_gates_file'] = cgate_file[0]
                matches = self.file_utils.grep_file(r"Total clock gates removed:\s*(\d+)", cgate_file[0])
                if matches:
                    metrics_dict['clock_gates_removed'] = int(matches[0])
            
            reg_file = os.path.join(self.workarea, f"syn_flow/dc/reports/debug/{self.design_info.top_hier}.rtl2gate.removed_registers.rep")
            if self.file_utils.file_exists(reg_file):
                metrics_dict['sources']['registers_file'] = reg_file
                try:
                    with open(reg_file, 'r', encoding='utf-8', errors='ignore') as f:
                        lines = f.readlines()
                        count = sum(1 for line in lines if line.strip() and not line.strip().startswith('#'))
                        if count > 0:
                            count -= 1
                        metrics_dict['registers_removed'] = count
                except Exception:
                    pass
            
            # Display extracted metrics in compact format
            self._display_qor_metrics_compact(metrics_dict, "")
            
            return metrics_dict
            
        except Exception as e:
            print(f"    {Color.RED}Error analyzing QoR CSV: {e}{Color.RESET}")
            return None
    
    def _analyze_qor_report(self, qor_file: str) -> Optional[Dict]:
        """Analyze QoR report and extract key metrics
        
        Args:
            qor_file: Path to QoR report file
            
        Returns:
            Dictionary with extracted metrics for comparison (Phase 3)
            Includes: area, cells, timing_by_pathgroup, timing_summary, clock_gates_removed, 
                     registers_removed, sources (file paths)
        """
        try:
            print(f"  {Color.CYAN}QoR Analysis:{Color.RESET}")
            
            # Read the QoR report
            if qor_file.endswith('.gz'):
                with gzip.open(qor_file, 'rt', encoding='utf-8') as f:
                    content = f.read()
            else:
                with open(qor_file, 'r', encoding='utf-8') as f:
                    content = f.read()
            
            # Initialize metrics dictionary (for Phase 3 comparison)
            metrics_dict = {
                'area': {},
                'cells': {},
                'timing_by_pathgroup': {},  # Per path group
                'timing_summary': {},        # Overall
                'clock_gates_removed': None,
                'registers_removed': None,
                'sources': {
                    'qor_report': qor_file,
                    'clock_gates_file': None,
                    'registers_file': None
                }
            }
            
            # Extract floorplan dimensions (PRIORITY: flp/ directory for DC section)
            flp_dims = self._extract_floorplan_dimensions()
            if flp_dims:
                metrics_dict['area']['die_x'] = flp_dims['x_dim_um']
                metrics_dict['area']['die_y'] = flp_dims['y_dim_um']
                metrics_dict['area']['die_source'] = flp_dims.get('source', 'unknown')
                # Track the source file for die dimensions
                if flp_dims.get('source_file'):
                    metrics_dict['sources']['die_dimensions_file'] = flp_dims['source_file']
                # CRITICAL: Design area for DC section comes from flp/ directory (die_x * die_y)
                if flp_dims.get('design_area'):
                    metrics_dict['area']['design_area'] = flp_dims.get('design_area')
            
            # Area metrics from QoR report (FALLBACK only if flp not available)
            if 'design_area' not in metrics_dict['area']:
                design_area_match = re.search(r'Design Area:\s+([\d,]+\.?\d*)', content)
                if design_area_match:
                    metrics_dict['area']['design_area'] = design_area_match.group(1).replace(',', '')
            
            # Cell count metrics
            leaf_cell_match = re.search(r'Leaf Cell Count:\s+([\d,]+)', content)
            if leaf_cell_match:
                metrics_dict['cells']['leaf_cells'] = int(leaf_cell_match.group(1).replace(',', ''))
            
            comb_cell_match = re.search(r'Combinational Cell Count:\s+([\d,]+)', content)
            if comb_cell_match:
                metrics_dict['cells']['comb_cells'] = int(comb_cell_match.group(1).replace(',', ''))
            
            seq_cell_match = re.search(r'Sequential Cell Count:\s+([\d,]+)', content)
            if seq_cell_match:
                metrics_dict['cells']['seq_cells'] = int(seq_cell_match.group(1).replace(',', ''))
            
            buf_match = re.search(r'Buf/Inv Cell Count:\s+([\d,]+)', content)
            if buf_match:
                metrics_dict['cells']['buf_inv_cells'] = int(buf_match.group(1).replace(',', ''))
            
            # Net count
            net_count_match = re.search(r'Total Number of Nets:\s+([\d,]+)', content)
            if net_count_match:
                metrics_dict['cells']['total_nets'] = int(net_count_match.group(1).replace(',', ''))
            
            # Macro count
            macro_count_match = re.search(r'Macro Count:\s+([\d,]+)', content)
            if macro_count_match:
                metrics_dict['cells']['macro_count'] = int(macro_count_match.group(1).replace(',', ''))
            
            # Extract timing per path group from the report
            # Look for "Timing Path Group" sections with both setup and hold timing
            # Format: Timing Path Group 'NAME' ... 
            #         Critical Path Slack: ... Total Negative Slack: ... No. of Violating Paths: ...
            #         Worst Hold Violation: ... Total Hold Violation: ... No. of Hold Violations: ...
            path_group_pattern = r"Timing Path Group '([^']+)'.*?Critical Path Slack:\s+([-\d.]+).*?Total Negative Slack:\s+([-\d.]+).*?No\. of Violating Paths:\s+([\d.]+).*?Worst Hold Violation:\s+([-\d.]+).*?Total Hold Violation:\s+([-\d.]+).*?No\. of Hold Violations:\s+([\d.]+)"
            path_group_matches = re.findall(path_group_pattern, content, re.DOTALL)
            
            for pg_name, wns_setup, tns_setup, nvp_setup, wns_hold, tns_hold, nvp_hold in path_group_matches:
                # Convert to appropriate types (stored as floats like 24.0000)
                metrics_dict['timing_by_pathgroup'][pg_name] = {
                    'wns_setup': float(wns_setup),
                    'tns_setup': float(tns_setup),
                    'nvp_setup': int(float(nvp_setup)),
                    'wns_hold': float(wns_hold),  # Worst Negative Slack (Hold)
                    'tns_hold': float(tns_hold),  # Total Negative Slack (Hold)
                    'nvp_hold': int(float(nvp_hold))  # Number of Violating Paths (Hold)
                }
            
            # Extract overall timing summary (if no per-path-group data)
            if not metrics_dict['timing_by_pathgroup']:
                wns_match = re.search(r'Critical Path Slack:\s+([-\d,]+\.?\d*)', content)
                if wns_match:
                    metrics_dict['timing_summary']['wns'] = float(wns_match.group(1).replace(',', ''))
                
                tns_match = re.search(r'Total Negative Slack:\s+([-\d,]+\.?\d*)', content)
                if tns_match:
                    metrics_dict['timing_summary']['tns'] = float(tns_match.group(1).replace(',', ''))
                
                viol_paths_match = re.search(r'No\. of Violating Paths:\s+([\d,]+)', content)
                if viol_paths_match:
                    metrics_dict['timing_summary']['nvp'] = int(viol_paths_match.group(1).replace(',', ''))
            
            # Extract clock gates removed
            cgate_file = self.file_utils.find_files("syn_flow/dc/reports/debug/*.rtl2gate.removed_cgates.rep", self.workarea)
            if cgate_file:
                metrics_dict['sources']['clock_gates_file'] = cgate_file[0]
                matches = self.file_utils.grep_file(r"Total clock gates removed:\s*(\d+)", cgate_file[0])
                if matches:
                    metrics_dict['clock_gates_removed'] = int(matches[0])
            
            # Extract removed registers
            reg_file = os.path.join(self.workarea, f"syn_flow/dc/reports/debug/{self.design_info.top_hier}.rtl2gate.removed_registers.rep")
            if self.file_utils.file_exists(reg_file):
                metrics_dict['sources']['registers_file'] = reg_file
                try:
                    with open(reg_file, 'r', encoding='utf-8', errors='ignore') as f:
                        lines = f.readlines()
                        # Count non-comment lines (excluding header)
                        count = sum(1 for line in lines if line.strip() and not line.strip().startswith('#'))
                        # Subtract header line if present
                        if count > 0:
                            count -= 1
                        metrics_dict['registers_removed'] = count
                except Exception:
                    pass
            
            # Display extracted metrics in compact format
            self._display_qor_metrics_compact(metrics_dict, content)
            
            return metrics_dict
            
        except Exception as e:
            print(f"    {Color.RED}Error analyzing QoR report: {e}{Color.RESET}")
            return None
    
    def _display_qor_metrics_compact(self, metrics: Dict, content: str) -> None:
        """Display QoR metrics in compact table format
        
        Args:
            metrics: Dictionary of extracted metrics
            content: Full QoR report content for scenario extraction
        """
        print(f"    {Color.GREEN}Key QoR Metrics:{Color.RESET}")
        
        # Die dimensions & Design Area (single line) - source displayed
        area_parts = []
        if metrics['area'].get('die_x') and metrics['area'].get('die_y'):
            die_x = metrics['area']['die_x']
            die_y = metrics['area']['die_y']
            source = metrics['area'].get('die_source', 'unknown')
            area_parts.append(f"Die: {die_x:.2f} x {die_y:.2f} um (from: {source})")
        
        if metrics['area'].get('design_area'):
            # Convert from umÂ² to mmÂ² and show 3 decimal places
            design_area_um2 = metrics['area']['design_area']
            design_area_mm2 = design_area_um2 / 1_000_000
            area_parts.append(f"Design Area: {design_area_mm2:.3f} mm2")
        
        if area_parts:
            print(f"      {' | '.join(area_parts)}")
        
        # Cell counts (single line, compact)
        cell_parts = []
        cells = metrics['cells']
        if cells.get('leaf_cells'):
            cell_parts.append(f"Leaf: {cells['leaf_cells']:,}")
        if cells.get('comb_cells'):
            cell_parts.append(f"Comb: {cells['comb_cells']:,}")
        if cells.get('seq_cells'):
            cell_parts.append(f"Seq: {cells['seq_cells']:,}")
        if cells.get('buf_inv_cells'):
            buf_inv = cells['buf_inv_cells']
            leaf = cells.get('leaf_cells', 1)
            pct = (buf_inv / leaf * 100) if leaf > 0 else 0
            cell_parts.append(f"Buf/Inv: {buf_inv:,} ({pct:.1f}%)")
        if cells.get('macro_count'):
            cell_parts.append(f"Macro: {cells['macro_count']:,}")
        if cells.get('total_nets'):
            cell_parts.append(f"Nets: {cells['total_nets']:,}")
        
        if cell_parts:
            print(f"      {' | '.join(cell_parts)}")
        
        # Timing per path group (table format with setup and hold)
        if metrics['timing_by_pathgroup']:
            print(f"\n    {Color.CYAN}Timing by Path Group:{Color.RESET} (sorted by Setup TNS, worst first)")
            # Fixed width columns: Path(30), WNS(13), TNS(13), NVP(11 for commas)
            print(f"      {'Path Group':<30} {'Setup WNS':>13} {'Setup TNS':>13} {'Setup NVP':>11} {'Hold WNS':>13} {'Hold TNS':>13} {'Hold NVP':>11}")
            print(f"      {'-'*30} {'-'*13} {'-'*13} {'-'*11} {'-'*13} {'-'*13} {'-'*11}")
            
            # Sort by setup TNS (worst first)
            sorted_pgs = sorted(metrics['timing_by_pathgroup'].items(), 
                              key=lambda x: x[1]['tns_setup'])
            
            for pg_name, timing in sorted_pgs:
                # Setup timing
                wns_setup = timing['wns_setup']
                tns_setup = timing['tns_setup']
                nvp_setup = timing['nvp_setup']
                
                # Hold timing
                wns_hold = timing.get('wns_hold', 0.0)
                tns_hold = timing.get('tns_hold', 0.0)
                nvp_hold = timing.get('nvp_hold', 0)
                
                # Format with units (convert to ps if < 1.0ns) - ALWAYS 13 chars
                def format_timing(val):
                    # Check if value is effectively zero or an integer
                    is_integer = abs(val - round(val)) < 0.0001
                    
                    if abs(val) < 1.0:
                        # Convert to ps
                        val_ps = val * 1000
                        is_integer_ps = abs(val_ps - round(val_ps)) < 0.1
                        
                        if abs(val_ps) < 0.1:  # Effectively zero
                            return f"{'0':>13}"
                        elif is_integer_ps:
                            return f"{int(round(val_ps)):>10}ps"
                        else:
                            return f"{val_ps:>9.1f}ps"
                    else:
                        # Display in ns
                        if abs(val) < 0.0001:  # Effectively zero
                            return f"{'0':>13}"
                        elif is_integer:
                            return f"{int(round(val)):>10}ns"
                        else:
                            return f"{val:>10.3f}ns"
                
                # Color code setup based on violations
                wns_setup_fmt = format_timing(wns_setup)
                tns_setup_fmt = format_timing(tns_setup)
                if nvp_setup > 0:
                    wns_setup_str = f"{Color.RED}{wns_setup_fmt}{Color.RESET}"
                    tns_setup_str = f"{Color.RED}{tns_setup_fmt}{Color.RESET}"
                    nvp_setup_str = f"{Color.RED}{nvp_setup:>11,}{Color.RESET}"
                else:
                    wns_setup_str = f"{Color.GREEN}{wns_setup_fmt}{Color.RESET}"
                    tns_setup_str = f"{Color.GREEN}{tns_setup_fmt}{Color.RESET}"
                    nvp_setup_str = f"{Color.GREEN}{nvp_setup:>11}{Color.RESET}"
                
                # Color code hold based on violations
                wns_hold_fmt = format_timing(wns_hold)
                tns_hold_fmt = format_timing(tns_hold)
                if nvp_hold > 0:
                    wns_hold_str = f"{Color.RED}{wns_hold_fmt}{Color.RESET}"
                    tns_hold_str = f"{Color.RED}{tns_hold_fmt}{Color.RESET}"
                    nvp_hold_str = f"{Color.RED}{nvp_hold:>11,}{Color.RESET}"
                else:
                    wns_hold_str = f"{Color.GREEN}{wns_hold_fmt}{Color.RESET}"
                    tns_hold_str = f"{Color.GREEN}{tns_hold_fmt}{Color.RESET}"
                    nvp_hold_str = f"{Color.GREEN}{nvp_hold:>11}{Color.RESET}"
                
                print(f"      {pg_name:<30} {wns_setup_str} {tns_setup_str} {nvp_setup_str} {wns_hold_str} {tns_hold_str} {nvp_hold_str}")
        
        # Overall timing summary (if no per-path-group data)
        elif metrics['timing_summary']:
            timing = metrics['timing_summary']
            timing_parts = []
            if 'wns' in timing:
                wns_color = Color.RED if timing['wns'] < 0 else Color.GREEN
                timing_parts.append(f"WNS: {wns_color}{timing['wns']:.3f}ns{Color.RESET}")
            if 'tns' in timing:
                tns_color = Color.RED if timing['tns'] < 0 else Color.GREEN
                timing_parts.append(f"TNS: {tns_color}{timing['tns']:.3f}ns{Color.RESET}")
            if 'nvp' in timing:
                nvp_color = Color.RED if timing['nvp'] > 0 else Color.GREEN
                timing_parts.append(f"NVP: {nvp_color}{timing['nvp']:,}{Color.RESET}")
            
            if timing_parts:
                print(f"\n    {Color.CYAN}Timing Summary:{Color.RESET}")
                print(f"      {' | '.join(timing_parts)}")
        
        # Extract and display scenario lines from the end of the file
        lines = content.split('\n')
        scenario_lines = [line for line in lines if line.strip().startswith('Scenario')]
        if scenario_lines and not metrics['timing_by_pathgroup']:  # Only show if no path group detail
            print(f"\n    {Color.CYAN}Scenario Summary:{Color.RESET}")
            print(f"      {'Scenario':<50} {'WNS':<10} {'TNS':<10} {'NVP':<8}")
            print(f"      {'-'*50} {'-'*10} {'-'*10} {'-'*8}")
            
            for line in scenario_lines[-2:]:  # Last 2 scenario lines
                # Parse the scenario line to extract components
                scenario_match = re.search(r'Scenario:\s+([^\s]+)', line)
                hold_match = re.search(r'\(Hold\)', line)
                wns_match = re.search(r'WNS:\s+([-\d.]+)', line)
                tns_match = re.search(r'TNS:\s+([-\d.]+)', line)
                viol_paths_match = re.search(r'Number of Violating Paths:\s+([\d.]+)', line)
                
                scenario = scenario_match.group(1) if scenario_match else "N/A"
                if hold_match:
                    scenario += " (Hold)"
                wns = wns_match.group(1) if wns_match else "N/A"
                tns = tns_match.group(1) if tns_match else "N/A"
                viol_paths = viol_paths_match.group(1) if viol_paths_match else "N/A"
                
                print(f"      {scenario:<50} {wns:<10} {tns:<10} {viol_paths:<8}")
        
        # Display clock gates removed and removed registers (single line)
        opt_parts = []
        if metrics.get('clock_gates_removed') is not None:
            opt_parts.append(f"Clock gates removed: {metrics['clock_gates_removed']:,}")
        if metrics.get('registers_removed') is not None:
            opt_parts.append(f"Removed registers: {metrics['registers_removed']:,}")
        
        if opt_parts:
            print(f"\n    {Color.CYAN}Optimization Summary:{Color.RESET}")
            print(f"      {' | '.join(opt_parts)}")
        
        # Display sources (file paths)
        if metrics.get('sources'):
            print(f"\n    {Color.CYAN}Data Sources:{Color.RESET}")
            sources = metrics['sources']
            if sources.get('qor_report'):
                print(f"      QoR Report: {sources['qor_report']}")
            if sources.get('cell_count_file'):
                print(f"      Cell Count: {sources['cell_count_file']}")
            if sources.get('die_dimensions_file'):
                print(f"      Die Dimensions: {sources['die_dimensions_file']}")
            if sources.get('clock_gates_file'):
                print(f"      Clock Gates: {sources['clock_gates_file']}")
            if sources.get('registers_file'):
                print(f"      Registers: {sources['registers_file']}")
    
    def _extract_dc_version(self, dc_log: str) -> None:
        """Extract and print DC (Design Compiler) version from dc.log
        
        Args:
            dc_log: Path to dc.log file
        """
        try:
            # Search for DC version in the log file
            # Typical format: "Version <version_string>"
            with open(dc_log, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    # Look for version line (e.g., "Version Q-2019.12-SP5" or similar)
                    if 'Design Compiler' in line and 'Version' in line:
                        # Extract the version
                        version_match = re.search(r'Version\s+([\w\-\.]+)', line)
                        if version_match:
                            dc_version = version_match.group(1)
                            print(f"  {Color.GREEN}DC Version: {dc_version}{Color.RESET}")
                            return
                    # Alternative format: just "Version X-YYYY.MM"
                    elif line.strip().startswith('Version '):
                        version_match = re.search(r'Version\s+([\w\-\.]+)', line)
                        if version_match:
                            dc_version = version_match.group(1)
                            print(f"  {Color.GREEN}DC Version: {dc_version}{Color.RESET}")
                            return
        except Exception as e:
            # Silently fail if version not found
            pass
    
    def _extract_dc_version_for_html(self, dc_log: str) -> str:
        """Extract DC version for HTML report (returns string, doesn't print)
        
        Args:
            dc_log: Path to dc.log file
            
        Returns:
            DC version string or "Unknown" if not found
        """
        try:
            with open(dc_log, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    if 'Design Compiler' in line and 'Version' in line:
                        version_match = re.search(r'Version\s+([\w\-\.]+)', line)
                        if version_match:
                            return version_match.group(1)
                    elif line.strip().startswith('Version '):
                        version_match = re.search(r'Version\s+([\w\-\.]+)', line)
                        if version_match:
                            return version_match.group(1)
        except Exception:
            pass
        return "Unknown"
    
    def _extract_dc_errors_for_html(self, dc_log: str) -> tuple:
        """Extract DC error and warning counts for HTML report
        
        Args:
            dc_log: Path to dc.log file
            
        Returns:
            Tuple of (error_count, warning_count)
        """
        try:
            error_count = 0
            warning_count = 0
            with open(dc_log, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    if line.startswith('Error:'):
                        error_count += 1
                    elif line.startswith('Warning:'):
                        warning_count += 1
            return (error_count, warning_count)
        except Exception:
            return (0, 0)
    
    def _check_dc_errors(self, dc_log: str) -> None:
        """Check for errors and warnings in dc.log
        
        Args:
            dc_log: Path to dc.log file
        """
        try:
            error_count = 0
            warning_count = 0
            errors_list = []
            
            # Count errors and warnings
            with open(dc_log, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    if line.startswith('Error:'):
                        error_count += 1
                        # Store first 5 errors for display
                        if len(errors_list) < 5:
                            errors_list.append(line.strip())
                    elif line.startswith('Warning:'):
                        warning_count += 1
            
            # Display results
            if error_count > 0:
                print(f"  {Color.RED}[X] DC Errors: {error_count}{Color.RESET}")
                if errors_list:
                    print(f"    {Color.RED}Sample errors:{Color.RESET}")
                    for error in errors_list:
                        print(f"      {error[:120]}")  # Truncate long errors
            else:
                print(f"  {Color.GREEN}[OK] DC Errors: 0{Color.RESET}")
            
            # Display warnings (informational only)
            if warning_count > 0:
                print(f"  {Color.CYAN}[INFO] DC Warnings: {warning_count:,}{Color.RESET}")
            
        except Exception as e:
            # Silently fail if unable to parse
            pass
    
    def _analyze_beflow_config(self, beflow_file: str) -> None:
        """Analyze BeFlow configuration and extract useful variables
        
        Args:
            beflow_file: Path to BeFlow configuration file
        """
        try:
            print(f"  {Color.CYAN}BeFlow Configuration Analysis:{Color.RESET}")
            
            # Read the BeFlow config file
            with open(beflow_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Extract useful variables
            config_vars = {}
            
            # Environment variables
            lib_snap_match = re.search(r'LIB_SNAP_REV:\s+(\d+)', content)
            if lib_snap_match:
                config_vars['Library Snapshot'] = lib_snap_match.group(1)
            
            # Process and technology
            nv_process_match = re.search(r'nv_process:\s+\[[\'\"]([^\'\"]+)[\'\"]\]', content)
            if nv_process_match:
                config_vars['NV Process'] = nv_process_match.group(1)
            
            tracks_match = re.search(r'TracksNum:\s+\[[\'\"]([^\'\"]+)[\'\"]\]', content)
            if tracks_match:
                config_vars['Tracks Number'] = tracks_match.group(1)
            
            # Design information
            project_match = re.search(r'project:\s+\[[\'\"]([^\'\"]+)[\'\"]\]', content)
            if project_match:
                config_vars['Project'] = project_match.group(1)
            
            
            # Timing scenario
            scenario_match = re.search(r'default_scenario\(dc\):\s+\[[\'\"]([^\'\"]+)[\'\"]\]', content)
            if scenario_match:
                config_vars['Default Scenario'] = scenario_match.group(1)
            
            # Library paths
            stdcell_lib_match = re.search(r'stdcell_lib_path:\s+\[[\'\"]([^\'\"]+)[\'\"]\]', content)
            if stdcell_lib_match:
                config_vars['StdCell Library Path'] = stdcell_lib_match.group(1)
            
            memories_path_match = re.search(r'MemoriesPath:\s+\[[\'\"]([^\'\"]+)[\'\"]\]', content)
            if memories_path_match:
                config_vars['Memories Path'] = memories_path_match.group(1)
            
            
            # VT types
            vt_types_match = re.search(r'vt_type_list:\s+\[([^\]]+)\]', content)
            if vt_types_match:
                vt_types = vt_types_match.group(1).replace("'", "").replace('"', '').replace('[', '').replace(']', '')
                config_vars['VT Types'] = vt_types.strip()
            
            # BeFlow environment
            beflow_root_match = re.search(r'BEFLOW_ROOT:\s+([^\n]+)', content)
            if beflow_root_match:
                config_vars['BEFLOW_ROOT'] = beflow_root_match.group(1).strip()
            
            beflow_config_site_match = re.search(r'BEFLOW_CONFIG_SITE:\s+([^\n]+)', content)
            if beflow_config_site_match:
                config_vars['BEFLOW_CONFIG_SITE'] = beflow_config_site_match.group(1).strip()
            
            # Array names list (full list, no truncation)
            array_names_match = re.search(r'arrayNames:\s+\[([^\]]+)\]', content)
            if array_names_match:
                array_names = array_names_match.group(1)
                # Clean up the array names and join them
                clean_names = [name.strip().strip("'\"") for name in array_names.split(',')]
                config_vars['Array Names'] = ', '.join(clean_names)  # Full list, no truncation
            
            # agur_unit_be_ip for the design
            agur_unit_match = re.search(rf'agur_unit_be_ip\({self.design_info.top_hier}\):\s+\[([^\]]+)\]', content)
            if agur_unit_match:
                agur_units = agur_unit_match.group(1)
                # Clean up the unit names and join them
                clean_units = [unit.strip().strip("'\"") for unit in agur_units.split(',')]
                config_vars['Agur Unit BE IP'] = ', '.join(clean_units)
            
            # Display extracted variables in compact format
            if config_vars:
                print(f"    {Color.GREEN}Key Configuration Variables:{Color.RESET}")
                
                # Line 1: Library Snapshot, NV Process, Tracks, Project (compact)
                line1_parts = []
                if 'Library Snapshot' in config_vars:
                    line1_parts.append(f"Lib Snap: {config_vars['Library Snapshot']}")
                if 'NV Process' in config_vars:
                    line1_parts.append(f"Process: {config_vars['NV Process']}")
                if 'Tracks Number' in config_vars:
                    line1_parts.append(f"Tracks: {config_vars['Tracks Number']}")
                if 'Project' in config_vars:
                    line1_parts.append(f"Project: {config_vars['Project']}")
                if line1_parts:
                    print(f"      {' | '.join(line1_parts)}")
                
                # Line 2: Default Scenario + VT Types
                line2_parts = []
                if 'Default Scenario' in config_vars:
                    line2_parts.append(f"Scenario: {config_vars['Default Scenario']}")
                if 'VT Types' in config_vars:
                    line2_parts.append(f"VT: {config_vars['VT Types']}")
                if line2_parts:
                    print(f"      {' | '.join(line2_parts)}")
                
                # Line 3: BeFlow Environment (compact)
                line3_parts = []
                if 'BEFLOW_ROOT' in config_vars:
                    line3_parts.append(f"BEFLOW_ROOT: {config_vars['BEFLOW_ROOT']}")
                if 'BEFLOW_CONFIG_SITE' in config_vars:
                    line3_parts.append(f"BEFLOW_CONFIG_SITE: {config_vars['BEFLOW_CONFIG_SITE']}")
                if line3_parts:
                    print(f"      {' | '.join(line3_parts)}")
                
                # Line 4+: Paths (StdCell Lib, Memories) - keep full paths for reference
                if 'StdCell Library Path' in config_vars:
                    print(f"      StdCell Lib: {config_vars['StdCell Library Path']}")
                if 'Memories Path' in config_vars:
                    print(f"      Memories: {config_vars['Memories Path']}")
                
                # Array Names (full list, no truncation)
                if 'Array Names' in config_vars:
                    print(f"      Arrays: {config_vars['Array Names']}")
                
                # Agur Unit BE IP (if present)
                if 'Agur Unit BE IP' in config_vars:
                    print(f"      BE IP: {config_vars['Agur Unit BE IP']}")
                
                # Source
                print(f"\n    {Color.CYAN}Config Source:{Color.RESET}")
                print(f"      {beflow_file}")
            else:
                print(f"    {Color.YELLOW}No configuration variables found{Color.RESET}")
                
        except (OSError, UnicodeDecodeError) as e:
            print(f"    {Color.RED}Error reading BeFlow config: {e}{Color.RESET}")
    
    def _send_critical_disk_alert(self, usage_pct: int, filesystem: str, mount: str, 
                                    size: str, used: str, avail: str) -> None:
        """Send email alert when disk usage reaches critical levels (>=90%)
        
        Uses SMTP (smtp.nvidia.com) to send formatted email alerts.
        
        Args:
            usage_pct: Disk usage percentage
            filesystem: Filesystem name
            mount: Mount point
            size: Total size
            used: Used space
            avail: Available space
        """
        try:
            import smtplib
            from email.mime.multipart import MIMEMultipart
            from email.mime.text import MIMEText
            import base64
            
            # Extract user from workarea path (e.g., /home/scratch.username_vlsi/... -> username)
            workarea_owner = None
            if 'scratch.' in self.workarea:
                # Pattern: /home/scratch.username_vlsi_1/... or /home/scratch.username_vlsi/... or /home/scratch.username/...
                match = re.search(r'/scratch\.([^/]+?)(?:_vlsi(?:_\d+)?)?/', self.workarea)
                if match:
                    workarea_owner = match.group(1)
            
            # Fallback: use USER environment variable
            if not workarea_owner:
                workarea_owner = os.environ.get('USER', 'unknown')
            
            # Format username to display name (capitalize, replace underscores with spaces)
            display_name = workarea_owner.replace('_', ' ').title()
            
            # Construct email addresses
            from_email = f"{os.environ.get('USER', 'avice')}@nvidia.com"
            to_email = f"{workarea_owner}@nvidia.com"
            cc_email = "avice@nvidia.com"
            
            # Email subject
            subject = f"[CRITICAL] Disk Usage Alert: {usage_pct}% on {mount}"
            
            # Read and encode logo
            logo_data = ""
            logo_path = os.path.join(os.path.dirname(__file__), "assets/images/avice_logo.png")
            if os.path.exists(logo_path):
                with open(logo_path, "rb") as logo_file:
                    logo_data = base64.b64encode(logo_file.read()).decode('utf-8')
            
            # Calculate progress bar percentage
            bar_length = 30
            filled = int((usage_pct / 100) * bar_length)
            empty = bar_length - filled
            progress_bar = 'â–ˆ' * filled + 'â–‘' * empty
            
            # Plain text version (fallback)
            text_body = f"""CRITICAL DISK USAGE ALERT - {usage_pct}% USED
{'='*80}

Hello {display_name},

Your workarea storage is critically low and may cause flow failures!

Workarea: {self.workarea}
Design: {self.design_info.top_hier}

DISK USAGE DETAILS:
  Filesystem:  {filesystem}
  Mount Point: {mount}
  Size:        {size}
  Used:        {used}
  Available:   {avail}
  Usage:       {usage_pct}% [CRITICAL]

RECOMMENDED ACTIONS:
  1. Remove old PnR work directories (pnr_flow/nv_flow/*/ipo*/work_*)
  2. Delete old PT timing sessions (signoff_flow/auto_pt/work_*)
  3. Remove unused IPO snapshots (pnr_flow/nv_flow/*/ipo* - keep latest only)
  4. Archive old workareas to backup storage
  5. Check for large temporary files (find . -type f -size +1G)

RISK:
  - Synthesis/PnR flows may fail due to insufficient disk space
  - Log files may be truncated
  - Database writes may fail
  - System performance degradation

This alert was generated by avice_wa_review tool.
For assistance, contact: avice@nvidia.com

{'='*80}
Generated by: avice_wa_review.py
Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
            
            # HTML version (professional, styled with Alon Vice branding)
            html_body = f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.4;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 15px;
            background-color: #f5f5f5;
        }}
        .container {{
            background: white;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #e74c3c 0%, #c0392b 100%);
            color: white;
            padding: 0 15px 10px 15px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0 0 5px 0;
            font-size: 24px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }}
        .header-table {{
            width: auto;
            margin: 8px auto 0 auto;
            border-collapse: collapse;
            border-spacing: 0;
        }}
        .logo-cell {{
            padding: 0 12px 0 0;
            vertical-align: middle;
            text-align: right;
        }}
        .logo-cell img {{
            height: 60px;
            width: 60px;
            border-radius: 8px;
            display: inline-block;
            box-shadow: 0 2px 6px rgba(0,0,0,0.4);
        }}
        .critical-cell {{
            padding: 0 0 0 12px;
            vertical-align: middle;
            text-align: left;
        }}
        .critical-box {{
            background: #c0392b;
            border: 2px solid #fff;
            padding: 8px 18px;
            border-radius: 6px;
            text-align: center;
            display: inline-block;
            box-shadow: 0 2px 6px rgba(0,0,0,0.3);
        }}
        .critical-box .percent {{
            font-size: 36px;
            font-weight: bold;
            color: #fff;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);
            margin: 0;
            line-height: 1;
        }}
        .critical-box .label {{
            color: #fff;
            font-size: 11px;
            margin: 4px 0 0 0;
            font-weight: 500;
            letter-spacing: 0.5px;
        }}
        .greeting {{
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 12px 15px;
            margin: 15px;
            border-radius: 5px;
        }}
        .greeting p {{
            margin: 0;
            line-height: 1.3;
        }}
        .greeting strong {{
            color: #667eea;
            font-size: 16px;
        }}
        .content {{
            padding: 10px 15px;
        }}
        .section {{
            margin: 10px 0;
            padding: 12px;
            background: #f8f9fa;
            border-radius: 6px;
            border-left: 4px solid #667eea;
        }}
        .section h2 {{
            margin: 0 0 8px 0;
            color: #667eea;
            font-size: 18px;
            border-bottom: 2px solid #e0e0e0;
            padding-bottom: 6px;
        }}
        .metrics-table {{
            width: 100%;
            border-collapse: collapse;
            margin: 8px 0;
        }}
        .metrics-table td {{
            padding: 6px 8px;
            border-bottom: 1px solid #e0e0e0;
            font-size: 14px;
        }}
        .metrics-table td:first-child {{
            font-weight: bold;
            color: #555;
            width: 30%;
        }}
        .progress-bar {{
            background: #e0e0e0;
            border-radius: 8px;
            height: 25px;
            overflow: hidden;
            margin: 10px 0;
            box-shadow: inset 0 2px 4px rgba(0,0,0,0.1);
        }}
        .progress-fill {{
            background: linear-gradient(90deg, #e74c3c 0%, #c0392b 100%);
            height: 100%;
            width: {usage_pct}%;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: bold;
            font-size: 14px;
        }}
        .actions {{
            background: #e8f5e9;
            border-left: 4px solid #4caf50;
            padding: 12px;
            border-radius: 6px;
            margin: 10px 0;
        }}
        .actions h2 {{
            color: #2e7d32;
            margin: 0 0 8px 0;
            font-size: 18px;
        }}
        .action-item {{
            padding: 8px 10px;
            margin: 6px 0;
            background: white;
            border-radius: 4px;
            border-left: 3px solid #4caf50;
            box-shadow: 0 1px 3px rgba(0,0,0,0.05);
        }}
        .action-item strong {{
            color: #2e7d32;
            font-size: 14px;
        }}
        .action-item code {{
            background: #f5f5f5;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 12px;
            color: #d32f2f;
            display: block;
            margin-top: 3px;
        }}
        .risks {{
            background: #ffebee;
            border-left: 4px solid #f44336;
            padding: 12px;
            border-radius: 6px;
            margin: 10px 0;
        }}
        .risks h2 {{
            color: #c62828;
            margin: 0 0 8px 0;
            font-size: 18px;
        }}
        .risk-item {{
            padding: 4px 0;
            color: #d32f2f;
            font-weight: 500;
            font-size: 14px;
        }}
        .risk-item::before {{
            content: "âš ï¸ ";
            margin-right: 6px;
        }}
        .footer {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 15px 20px;
            text-align: center;
            font-size: 13px;
        }}
        .footer a {{
            color: #a8ff78;
            text-decoration: none;
            font-weight: bold;
        }}
        .timestamp {{
            color: rgba(255,255,255,0.8);
            font-size: 11px;
            margin-top: 8px;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>CRITICAL DISK USAGE ALERT</h1>
            <p style="margin: 3px 0 3px 0; font-size: 14px;">Immediate Action Required</p>
            <table width="100%" cellpadding="0" cellspacing="0" border="0" style="margin: 0;">
                <tr>
                    <td width="54" style="padding-right: 10px; vertical-align: middle;">
                        <img src="data:image/png;base64,{logo_data}" alt="Avice Logo" width="54" height="54" style="display: block; border-radius: 6px;">
                    </td>
                    <td style="padding-left: 10px; vertical-align: middle;">
                        <table width="100%" cellpadding="0" cellspacing="0" border="0" style="background-color: #c0392b; border: 2px solid #fff; border-radius: 6px; height: 50px;">
                            <tr>
                                <td style="text-align: center; vertical-align: middle;">
                                    <div style="font-size: 28px; font-weight: bold; color: #fff; line-height: 1; margin: 0;">{usage_pct}%</div>
                                    <div style="font-size: 9px; color: #fff; margin-top: 2px; letter-spacing: 0.5px;">DISK SPACE USED</div>
                                </td>
                            </tr>
                        </table>
                    </td>
                </tr>
            </table>
        </div>
        
        <div class="greeting">
            <p>Hello <strong>{display_name}</strong>,</p>
            <p style="margin-top: 3px;">Your workarea storage is critically low and may cause flow failures!</p>
        </div>
        
        <div class="content">
            <div class="section">
                <h2>ðŸ“ Workarea Information</h2>
                <table class="metrics-table">
                    <tr>
                        <td>Path:</td>
                        <td style="font-family: monospace; font-size: 12px;">{self.workarea}</td>
                    </tr>
                    <tr>
                        <td>Design:</td>
                        <td><strong>{self.design_info.top_hier}</strong></td>
                    </tr>
                </table>
            </div>
            
            <div class="section">
                <h2>ðŸ’¾ Disk Usage Metrics</h2>
                <table class="metrics-table">
                    <tr>
                        <td>Filesystem:</td>
                        <td style="font-family: monospace; font-size: 11px;">{filesystem}</td>
                    </tr>
                    <tr>
                        <td>Mount Point:</td>
                        <td style="font-family: monospace;">{mount}</td>
                    </tr>
                    <tr>
                        <td>Total Size:</td>
                        <td><strong>{size}</strong></td>
                    </tr>
                    <tr>
                        <td>Used Space:</td>
                        <td><strong>{used}</strong></td>
                    </tr>
                    <tr>
                        <td>Available:</td>
                        <td style="color: #e74c3c; font-weight: bold;">{avail}</td>
                    </tr>
                </table>
                
                <div class="progress-bar">
                    <div class="progress-fill">{usage_pct}%</div>
                </div>
            </div>
            
            <div class="actions">
                <h2>ðŸ”§ Recommended Cleanup Actions</h2>
                
                <div class="action-item">
                    <strong>1. Remove old PnR work directories</strong>
                    <code>pnr_flow/nv_flow/*/ipo*/work_*</code>
                </div>
                
                <div class="action-item">
                    <strong>2. Delete old PT timing sessions</strong>
                    <code>signoff_flow/auto_pt/work_*</code>
                </div>
                
                <div class="action-item">
                    <strong>3. Remove unused IPO snapshots (keep latest only)</strong>
                    <code>pnr_flow/nv_flow/*/ipo*</code>
                </div>
                
                <div class="action-item">
                    <strong>4. Archive old workareas to backup storage</strong>
                </div>
                
                <div class="action-item">
                    <strong>5. Check for large temporary files</strong>
                    <code>find . -type f -size +1G</code>
                </div>
            </div>
            
            <div class="risks">
                <h2>âš ï¸ Potential Risks</h2>
                <div class="risk-item">Synthesis/PnR flows may FAIL due to insufficient disk space</div>
                <div class="risk-item">Log files may be TRUNCATED</div>
                <div class="risk-item">Database writes may FAIL</div>
                <div class="risk-item">System performance DEGRADATION</div>
            </div>
        </div>
        
        <div class="footer">
            <p style="margin: 0;">Generated by <strong>avice_wa_review.py</strong></p>
            <p style="margin: 5px 0;">For assistance, contact: <a href="mailto:avice@nvidia.com">avice@nvidia.com</a></p>
            <p class="timestamp">â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
    </div>
</body>
</html>
"""
            
            # Create message with alternative content (HTML + plain text fallback)
            msg = MIMEMultipart('alternative')
            msg['From'] = from_email
            msg['To'] = to_email
            msg['Cc'] = cc_email
            msg['Subject'] = subject
            
            # Attach both versions (plain text first, then HTML)
            msg.attach(MIMEText(text_body, 'plain'))
            msg.attach(MIMEText(html_body, 'html'))
            
            # All recipients for SMTP
            all_recipients = [to_email, cc_email]
            
            # Send email via SMTP
            smtp_server = "smtp.nvidia.com"
            smtp_port = 25
            
            with smtplib.SMTP(smtp_server, smtp_port, timeout=30) as server:
                server.sendmail(from_email, all_recipients, msg.as_string())
            
            print(f"  {Color.GREEN}[EMAIL] Critical disk alert sent to {to_email} (CC: {cc_email}){Color.RESET}")
            
        except ImportError as e:
            print(f"  {Color.YELLOW}[INFO] Missing required module for email: {e}{Color.RESET}")
        except Exception as e:
            print(f"  {Color.YELLOW}[INFO] Could not send email alert: {e}{Color.RESET}")
            import traceback
            traceback.print_exc()
    
    def _check_disk_utilization(self) -> Optional[float]:
        """Check disk utilization for the workarea path and warn if >80%
        
        Sends email alert to workarea owner (CC: avice@nvidia.com) if usage >= 90%
        
        Returns:
            Float percentage (0-100) of disk usage, or None if check fails
        """
        try:
            # Run df -h on the workarea path
            result = self.file_utils.run_command(f"df -h {self.workarea}")
            
            if result:
                lines = result.strip().split('\n')
                if len(lines) >= 2:  # Header + data line
                    # Parse the output
                    # Typical format: Filesystem Size Used Avail Use% Mounted on
                    parts = lines[1].split()
                    if len(parts) >= 5:
                        usage_str = parts[4]  # Use% column (e.g., "85%")
                        usage_pct = int(usage_str.rstrip('%'))
                        filesystem = parts[0]
                        size = parts[1]
                        used = parts[2]
                        avail = parts[3]
                        mount = parts[5] if len(parts) > 5 else parts[0]
                        
                        # Determine status based on threshold
                        if usage_pct >= 80:
                            status_color = Color.RED
                            status_msg = "[WARN]"
                        elif usage_pct >= 70:
                            status_color = Color.YELLOW
                            status_msg = "[INFO]"
                        else:
                            status_color = Color.GREEN
                            status_msg = "[OK]"
                        
                        # Print disk usage information
                        print(f"\n{Color.CYAN}Disk Utilization:{Color.RESET}")
                        print(f"  Filesystem: {filesystem}")
                        print(f"  Mount Point: {mount}")
                        print(f"  Size: {size} | Used: {used} | Available: {avail}")
                        print(f"  {status_color}Usage: {usage_pct}% {status_msg}{Color.RESET}")
                        
                        # Additional warning if critical
                        if usage_pct >= 80:
                            print(f"  {Color.RED}[WARN] Disk usage is high! Consider cleaning up old files.{Color.RESET}")
                            if usage_pct >= 90:
                                print(f"  {Color.RED}[CRITICAL] Disk almost full! This may cause flow failures.{Color.RESET}")
                                # Send email alert to user (CC: avice@nvidia.com) if flag is enabled
                                if hasattr(self.args, 'send_disk_alert') and self.args.send_disk_alert:
                                    self._send_critical_disk_alert(usage_pct, filesystem, mount, size, used, avail)
                                    # Extract owner for confirmation message
                                    workarea_owner = 'unknown'
                                    if 'scratch.' in self.workarea:
                                        match = re.search(r'/scratch\.([^/]+?)(?:_vlsi(?:_\d+)?)?/', self.workarea)
                                        if match:
                                            workarea_owner = match.group(1)
                                    print(f"  {Color.CYAN}[EMAIL]{Color.RESET} Critical disk alert sent to {workarea_owner}@nvidia.com (CC: avice@nvidia.com)")
                                else:
                                    print(f"  {Color.YELLOW}[INFO]{Color.RESET} Email alerts disabled (use --send-disk-alert to enable)")
                        
                        return usage_pct
        except Exception as e:
            print(f"  {Color.YELLOW}[INFO] Could not check disk utilization: {e}{Color.RESET}")
            return None
    
    def run_setup_analysis(self) -> None:
        """Run setup analysis including environment and runtime information"""
        self.print_header(FlowStage.SETUP)
        
        # Check disk utilization (at the beginning as requested)
        disk_usage = self._check_disk_utilization()
        
        # Check actual IPO directories (may differ from .prc file)
        pnr_base_path = os.path.join(self.workarea, f"pnr_flow/nv_flow/{self.design_info.top_hier}")
        actual_ipos = []
        if os.path.isdir(pnr_base_path):
            try:
                for item in os.listdir(pnr_base_path):
                    item_path = os.path.join(pnr_base_path, item)
                    if os.path.isdir(item_path) and item.startswith('ipo'):
                        actual_ipos.append(item)
                actual_ipos.sort()
            except Exception:
                pass
        
        # Display design information
        print(f"UNIT: {self.design_info.top_hier}")
        print(f"TAG: {self.design_info.tag}")
        
        # Show IPO information with directory status
        prc_ipo = self.design_info.ipo
        ipo_dir_exists = prc_ipo in actual_ipos
        
        if ipo_dir_exists:
            print(f"IPO: {prc_ipo} (Available IPOs: {', '.join(actual_ipos) if actual_ipos else 'None'})")
        else:
            # IPO from .prc doesn't exist as directory
            print(f"IPO: {Color.YELLOW}{prc_ipo}{Color.RESET} (from .prc file)")
            if actual_ipos:
                print(f"  {Color.YELLOW}[WARN] IPO directory '{prc_ipo}' not found{Color.RESET}")
                print(f"  {Color.CYAN}[INFO] Available IPO directories: {', '.join(actual_ipos)}{Color.RESET}")
                print(f"  {Color.YELLOW}Note: Users sometimes delete IPO directories to save disk space{Color.RESET}")
            else:
                print(f"  {Color.RED}[ERROR] No IPO directories found{Color.RESET}")
        
        # Display NBU Signoff information if detected
        if self.uses_nbu_signoff:
            print(f"\n{Color.CYAN}NBU Signoff Mode: {Color.GREEN}DETECTED{Color.RESET}")
            print(f"  Workarea Owner: {Color.CYAN}{self.workarea_owner}{Color.RESET}")
            
            # Show NBU signoff paths for each IPO
            for ipo_dir, nbu_path in sorted(self.nbu_signoff_paths.items()):
                # Get relative path for display
                rel_path = os.path.relpath(nbu_path, self.workarea)
                print(f"  -> {Color.YELLOW}{ipo_dir}{Color.RESET}: {rel_path}")
            
            # Show detected NBU steps
            if self.nbu_signoff_steps:
                steps_str = ', '.join(self.nbu_signoff_steps)
                print(f"  Steps using NBU: {Color.MAGENTA}{steps_str}{Color.RESET}")
            
            print(f"\n  {Color.YELLOW}[!] Signoff tools (PT, Star, Formal, PV, GL Check) run in nbu_signoff directories{Color.RESET}")
            print(f"  {Color.YELLOW}[!] Block release may run from ROOT or NBU (auto-detected){Color.RESET}")
        
        # Extract environment information
        self._extract_environment_info()
        
        # Use resolved IPO for file access (first available if .prc IPO doesn't exist)
        access_ipo = actual_ipos[0] if (not ipo_dir_exists and actual_ipos) else prc_ipo
        
        # Design Definition
        design_def_pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{access_ipo}/design_definition.tcl"
        design_def_file = os.path.join(self.workarea, design_def_pattern)
        if self.file_utils.file_exists(design_def_file):
            self.print_file_info(design_def_file, "Design Definition")
        
        # PnR Configuration
        pnr_config_pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{access_ipo}/pnr_config.tcl"
        pnr_config_file = os.path.join(self.workarea, pnr_config_pattern)
        if self.file_utils.file_exists(pnr_config_file):
            self.print_file_info(pnr_config_file, "PnR Configuration")
        
        # Add section summary for master dashboard
        key_metrics = {
            "Design": self.design_info.top_hier,
            "Tag": self.design_info.tag,
            "IPO": self.design_info.ipo
        }
        
        # Add disk usage if available
        if disk_usage is not None:
            key_metrics["Disk Usage"] = f"{disk_usage}%"
        
        self._add_section_summary(
            section_name="Setup",
            section_id="setup",
            stage=FlowStage.SETUP,
            status="PASS",
            key_metrics=key_metrics,
            html_file="",
            priority=4,
            issues=[],
            icon="[Setup]"
        )
    
    def _extract_environment_info(self) -> Dict[str, str]:
        """Extract BeFlow, Tech Data, and Tool Override environment information
        
        Returns:
            Dictionary containing environment variables and paths
        """
        print(f"  {Color.CYAN}Environment Information:{Color.RESET}")
        
        # Check formal flow environment files for BeFlow and Tech Data info
        # Note: LIB_SNAP_REV is already shown in DC/PnR sections
        env_locations = [
            "formal_flow/rtl_vs_pnr_fm/FM_INFO/env",
            "formal_flow/rtl_vs_syn_fm/FM_INFO/env",
            "formal_flow/rtl_vs_pnr_bbox_fm/FM_INFO/env",
            "formal_flow/rtl_vs_syn_bbox_fm/FM_INFO/env"
        ]
        
        env_vars_found = {
            'BEFLOW_REV': False,
            'BEFLOW_CONFIG_REV': False,
            'TECH_DATA_T6_REV': False,
            'BE_OVERRIDE_TOOLVERS': False
        }
        
        for env_path in env_locations:
            full_path = os.path.join(self.workarea, env_path)
            if os.path.exists(full_path):
                try:
                    with open(full_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    lines = content.split('\n')
                    for line in lines:
                        line = line.strip()
                        for var_name in env_vars_found.keys():
                            if line.startswith(f"{var_name}=") and not env_vars_found[var_name]:
                                print(f"    {line}")
                                env_vars_found[var_name] = True
                    
                    # If we found all variables, no need to check more files
                    if all(env_vars_found.values()):
                        break
                            
                except (OSError, UnicodeDecodeError):
                    continue
        
        # Also check PnR debug files for BE_OVERRIDE_TOOLVERS
        if not env_vars_found['BE_OVERRIDE_TOOLVERS']:
            debug_pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{self.design_info.ipo}/LOGs/PRIME/STEP__BEGIN__*.debug"
            debug_files = self.file_utils.find_files(debug_pattern, self.workarea)
            
            for debug_file in debug_files[:1]:  # Check first BEGIN debug file
                try:
                    result = self.file_utils.run_command(f"grep '^BE_OVERRIDE_TOOLVERS' {debug_file}")
                    if result.strip():
                        print(f"    {result.strip()}")
                        env_vars_found['BE_OVERRIDE_TOOLVERS'] = True
                        break
                except Exception:
                    continue
        
        # If no environment info found at all
        if not any(env_vars_found.values()):
            print(f"    No environment information found")
    
    def _analyze_pnr_status(self, prc_status_file: str) -> None:
        """Analyze PnR status to show flow progress, running stages, and errors
        
        Args:
            prc_status_file: Path to prc.status file
        """
        try:
            with open(prc_status_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # Parse status data
            status_data = {}
            current_ipo = None
            
            for line in lines:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                
                # Parse status line: block ipo step status duration logfile
                parts = line.split()
                if len(parts) >= 6:
                    block, ipo, step, status, duration, logfile = parts[0], parts[1], parts[2], parts[3], parts[4], ' '.join(parts[5:])
                    
                    if ipo not in status_data:
                        status_data[ipo] = []
                    
                    status_data[ipo].append({
                        'step': step,
                        'status': status,
                        'duration': duration,
                        'logfile': logfile
                    })
            
            if not status_data:
                print(f"  {Color.YELLOW}No PnR status data found{Color.RESET}")
                return
            
            # Analyze and display status for each IPO
            print(f"\n{Color.CYAN}PnR Flow Status Analysis:{Color.RESET}")
            
            for ipo in sorted(status_data.keys()):
                steps = status_data[ipo]
                print(f"\n  {Color.CYAN}IPO {ipo}:{Color.RESET}")
                
                # Categorize steps
                pnr_steps = []
                report_steps = []
                other_steps = []
                
                for step_info in steps:
                    step = step_info['step']
                    if step.startswith('report_'):
                        report_steps.append(step_info)
                    elif step in ['BEGIN', 'END']:
                        other_steps.append(step_info)
                    else:
                        pnr_steps.append(step_info)
                
                # Analyze PnR flow status
                pnr_status = self._analyze_step_sequence(pnr_steps, "PnR Flow")
                if pnr_status:
                    print(f"    {pnr_status}")
                
                # Analyze reporting status
                report_status = self._analyze_step_sequence(report_steps, "Reporting")
                if report_status:
                    print(f"    {report_status}")
                
                # Show current running stage
                running_steps = [s for s in steps if s['status'] == 'RUN']
                if running_steps:
                    print(f"    {Color.YELLOW}Currently Running: {', '.join([s['step'] for s in running_steps])}{Color.RESET}")
                
                # Show errors
                error_steps = [s for s in steps if s['status'] == 'ERR']
                if error_steps:
                    print(f"    {Color.RED}Errors in: {', '.join([s['step'] for s in error_steps])}{Color.RESET}")
                
                # Overall completion status
                total_steps = len(steps)
                done_steps = len([s for s in steps if s['status'] == 'DONE'])
                error_steps_count = len(error_steps)
                running_steps_count = len(running_steps)
                
                if done_steps == total_steps:
                    print(f"    {Color.GREEN}Status: COMPLETED ({done_steps}/{total_steps} steps){Color.RESET}")
                elif error_steps_count > 0:
                    print(f"    {Color.RED}Status: FAILED ({done_steps}/{total_steps} done, {error_steps_count} errors){Color.RESET}")
                elif running_steps_count > 0:
                    print(f"    {Color.YELLOW}Status: RUNNING ({done_steps}/{total_steps} done, {running_steps_count} active){Color.RESET}")
                else:
                    print(f"    {Color.YELLOW}Status: UNKNOWN ({done_steps}/{total_steps} done){Color.RESET}")
        
        except Exception as e:
            print(f"  Error analyzing PnR status: {e}")
    
    def _analyze_step_sequence(self, steps: List[Dict[str, Any]], category_name: str) -> str:
        """Analyze a sequence of steps and return status summary
        
        Args:
            steps: List of step dictionaries
            category_name: Name of the category being analyzed
            
        Returns:
            Summary string of progress
        """
        if not steps:
            return None
        
        # Define expected PnR flow order
        pnr_flow_order = ['setup', 'edi_plan', 'place', 'cts', 'route', 'postroute', 'export', 'extraction', 'nbu_auto_pt']
        report_flow_order = ['report_edi_plan', 'report_place', 'report_cts', 'report_route', 'report_postroute']
        
        if category_name == "PnR Flow":
            expected_order = pnr_flow_order
        else:
            expected_order = report_flow_order
        
        # Find the last completed step
        last_completed = None
        first_error = None
        first_running = None
        
        for step_info in steps:
            step = step_info['step']
            status = step_info['status']
            
            if status == 'DONE':
                last_completed = step
            elif status == 'ERR' and first_error is None:
                first_error = step
            elif status == 'RUN' and first_running is None:
                first_running = step
        
        # Determine progress
        if last_completed:
            try:
                completed_index = expected_order.index(last_completed)
                progress = f"Completed through: {last_completed} ({completed_index + 1}/{len(expected_order)})"
            except ValueError:
                progress = f"Completed: {last_completed}"
        else:
            progress = "No steps completed"
        
        # Add error/running info
        if first_error:
            progress += f", Error at: {first_error}"
        if first_running:
            progress += f", Running: {first_running}"
        
        return f"{category_name}: {progress}"

    def _extract_prc_configuration(self, prc_file: str) -> Optional[Dict[str, Any]]:
        """Extract key configuration information from PRC file (YAML or legacy format)
        
        Args:
            prc_file: Path to .prc configuration file
            
        Returns:
            Dictionary with configuration data or None if parsing fails
        """
        try:
            with open(prc_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Check if it's YAML format (starts with top_hier:)
            if f"{self.design_info.top_hier}:" in content:
                self._extract_yaml_prc_configuration(content)
            else:
                # Legacy format - search for useful/MULTIBIT keywords
                matches = self.file_utils.grep_file(r"useful|MULTIBIT", prc_file)
                for match in matches:
                    print(f"  {match}")
                    
        except Exception as e:
            print(f"  Error reading PRC configuration: {e}")
            # Fallback to simple keyword search
            try:
                matches = self.file_utils.grep_file(r"useful|MULTIBIT", prc_file)
                for match in matches:
                    print(f"  {match}")
            except:
                pass
    
    def _print_flow_sequence_with_hooks(self, ipo: str, flow_sequence: List[str], hooks: Dict[str, List[str]]) -> None:
        """Print flow sequence with TCL hooks inline
        
        Args:
            ipo: IPO name
            flow_sequence: List of flow stages
            hooks: Dictionary mapping hook types to hook scripts
        """
        # Build the sequence list with hooks and stages combined
        sequence_parts = []
        
        # Stage name mapping (flow_sequence name -> hook stage name)
        stage_mapping = {
            'edi_plan': 'plan',
            'plan': 'plan',
            # Add more mappings as needed
        }
        
        for step in flow_sequence:
            # Try direct match first, then try mapped name
            matched_hooks = None
            if step in hooks:
                matched_hooks = hooks[step]
            elif step in stage_mapping and stage_mapping[step] in hooks:
                matched_hooks = hooks[stage_mapping[step]]
            
            # Separate hooks by type (begin vs end)
            begin_hooks = []
            end_hooks = []
            if matched_hooks:
                for script, hook_type in matched_hooks:
                    if hook_type == 'begin':
                        begin_hooks.append(script)
                    elif hook_type == 'end':
                        end_hooks.append(script)
            
            # Build stage string with hooks attached (no arrows between them)
            stage_str = ""
            if begin_hooks:
                stage_str += f"[+{',+'.join(begin_hooks)}] "
            stage_str += f"{Color.YELLOW}{step}{Color.RESET}"
            if end_hooks:
                stage_str += f" [+{',+'.join(end_hooks)}]"
            
            sequence_parts.append(stage_str)
        
        # Print with arrows only between stages (not between hooks and stages)
        print(f"  {Color.CYAN}Flow Sequence ({ipo}):{Color.RESET} {' -> '.join(sequence_parts)}")
    
    def _extract_yaml_prc_configuration(self, content: str) -> Dict[str, Any]:
        """Extract configuration from YAML-based PRC file
        
        Args:
            content: YAML content string
            
        Returns:
            Dictionary with parsed YAML configuration
        """
        lines = content.split('\n')
        
        # Extract IPO information
        ipos = []
        for line in lines:
            if re.match(r'^\s*ipo(\d+)\s*:', line):
                ipo_match = re.search(r'ipo(\d+)', line)
                if ipo_match:
                    ipos.append(ipo_match.group(1))
        
        if ipos:
            print(f"  {Color.CYAN}Available IPOs:{Color.RESET} {', '.join(ipos)}")
        
        # Extract tool information
        tools = set()
        for line in lines:
            if 'tool:' in line:
                tool_match = re.search(r'tool:\s*(\w+)', line)
                if tool_match:
                    tools.add(tool_match.group(1))
        if tools:
            print(f"  {Color.CYAN}Tools:{Color.RESET} {', '.join(tools)}")
        
        # Extract TCL hook scripts per IPO
        hook_scripts = {}  # {ipo: {stage: [(script, type)]}}
        current_ipo = None
        in_scripts = False
        current_script = None
        current_stage = None
        current_type = None
        
        for i, line in enumerate(lines):
            # Check for IPO start
            ipo_match = re.match(r'^\s*(ipo\d+)\s*:', line)
            if ipo_match:
                current_ipo = ipo_match.group(1)
                if current_ipo not in hook_scripts:
                    hook_scripts[current_ipo] = {}
                in_scripts = False
                continue
            
            # Check for scripts section
            if current_ipo and re.match(r'^\s{4}scripts:', line):
                in_scripts = True
                continue
            
            # Exit scripts section when we hit another key at same level
            if in_scripts and re.match(r'^\s{4}[a-z_]+:', line) and 'scripts:' not in line:
                in_scripts = False
                continue
            
            # Parse script entries
            if in_scripts and current_ipo:
                # Script name line: "     - script_name.tcl:" (flexible whitespace before dash)
                script_match = re.match(r'^\s+-\s+([\w.]+):', line)
                if script_match:
                    current_script = script_match.group(1)
                    current_stage = None
                    current_type = None
                    continue
                
                # Stage line: "          stage: place" (flexible whitespace)
                if current_script and 'stage:' in line:
                    stage_match = re.search(r'stage:\s*(\w+)', line)
                    if stage_match:
                        current_stage = stage_match.group(1)
                        continue
                
                # Type line: "          type: begin" (flexible whitespace)
                if current_script and 'type:' in line:
                    type_match = re.search(r'type:\s*(\w+)', line)
                    if type_match:
                        current_type = type_match.group(1)
                        
                        # Store the complete hook info
                        if current_stage:
                            if current_stage not in hook_scripts[current_ipo]:
                                hook_scripts[current_ipo][current_stage] = []
                            hook_scripts[current_ipo][current_stage].append((current_script, current_type))
                        
                        # Reset for next script
                        current_script = None
                        current_stage = None
                        current_type = None
        
        # Process each IPO individually for flow sequence
        current_ipo = None
        in_flow_sequence = False
        flow_sequence = []
        
        for i, line in enumerate(lines):
            # Check for IPO start
            ipo_match = re.match(r'^\s*(ipo\d+)\s*:', line)
            if ipo_match:
                # Print previous IPO's flow sequence if we have one
                if current_ipo and flow_sequence:
                    self._print_flow_sequence_with_hooks(current_ipo, flow_sequence, hook_scripts.get(current_ipo, {}))
                
                current_ipo = ipo_match.group(1)
                in_flow_sequence = False
                flow_sequence = []
                continue
            
            # Extract flow sequence for current IPO
            if current_ipo and 'flow_sequence:' in line:
                in_flow_sequence = True
                continue
            elif in_flow_sequence and current_ipo:
                # Exit flow_sequence section when we hit another YAML key at same indentation
                # (e.g., ipo_number:, handoffs:, recipes:, scripts:)
                if re.match(r'^\s{4}[a-z_]+:', line):  # 4-space indent = same level as flow_sequence
                    in_flow_sequence = False
                    continue
                    
                if line.strip().startswith('- '):
                    step = line.strip()[2:].strip()
                    if ':' in step:
                        step = step.split(':')[0]
                    # Only include main flow steps, not handoffs/recipes
                    if step not in ['prime.SELF_BLOCK.mk', 'prime.beflow.mk', 'CUSTOM_MAKE', 'CUSTOM_YAML', 'SELF_BLOCK.yaml?']:
                        flow_sequence.append(step)
                elif line.strip() and not line.startswith(' '):
                    in_flow_sequence = False
        
        # Print last IPO's flow sequence
        if current_ipo and flow_sequence:
            self._print_flow_sequence_with_hooks(current_ipo, flow_sequence, hook_scripts.get(current_ipo, {}))
        
        # Extract useful skew settings per IPO
        current_ipo = None
        useful_skew_settings = {}
        
        for i, line in enumerate(lines):
            # Track current IPO
            ipo_match = re.match(r'^\s*(ipo\d+)\s*:', line)
            if ipo_match:
                current_ipo = ipo_match.group(1)
                continue
            
            # Extract useful skew for current IPO
            if current_ipo and 'USEFUL_SKEW:' in line:
                # Look for ENABLE in the next few lines
                for j in range(i, min(i+10, len(lines))):
                    if 'ENABLE:' in lines[j] and 'value:' in lines[j+1]:
                        value_line = lines[j+1]
                        value_match = re.search(r'value:\s*(\d+)', value_line)
                        if value_match:
                            value = "Useful Skew Enabled" if value_match.group(1) == "1" else "Useful Skew Disabled"
                            # Determine context based on previous lines
                            context = "Unknown"
                            for k in range(max(0, i-20), i):
                                if 'PLACE:' in lines[k]:
                                    context = "PLACE"
                                    break
                                elif 'POSTROUTE:' in lines[k]:
                                    context = "POSTROUTE"
                                    break
                            
                            if current_ipo not in useful_skew_settings:
                                useful_skew_settings[current_ipo] = {}
                            useful_skew_settings[current_ipo][context] = value
                            break
        
        # Print useful skew settings per IPO
        if useful_skew_settings:
            print(f"  {Color.CYAN}Useful Skew Settings:{Color.RESET}")
            for ipo, settings in useful_skew_settings.items():
                print(f"    {ipo}:")
                for context, value in settings.items():
                    print(f"      {context}: {value}")
        
        # Extract power optimization settings
        power_ratios = set()
        for i, line in enumerate(lines):
            if 'LEAKAGE_DYNAMIC_RATIO:' in line:
                # Look for value in next line
                if i + 1 < len(lines):
                    value_line = lines[i + 1]
                    ratio_match = re.search(r'value:\s*([\d.]+)', value_line)
                    if ratio_match:
                        power_ratios.add(ratio_match.group(1))
        
        if power_ratios:
            print(f"  {Color.CYAN}Power Optimization Ratios:{Color.RESET} {', '.join(sorted(power_ratios))}")
        
        # Extract clock tree settings per IPO
        current_ipo = None
        clock_names = {}
        
        for i, line in enumerate(lines):
            # Track current IPO
            ipo_match = re.match(r'^\s*(ipo\d+)\s*:', line)
            if ipo_match:
                current_ipo = ipo_match.group(1)
                continue
            
            # Extract clock names for current IPO
            if current_ipo and 'SDC_CLOCK_NAMES:' in line:
                # Look for value in next line
                if i + 1 < len(lines):
                    value_line = lines[i + 1]
                    if 'value:' in value_line:
                        clock_line = value_line.split('value:')[1].strip()
                        clock_names[current_ipo] = clock_line
                        break
        
        # Print clock names per IPO
        if clock_names:
            print(f"  {Color.CYAN}Clock Names:{Color.RESET}")
            for ipo, clocks in clock_names.items():
                print(f"    {ipo}: {clocks}")
    
    def _verify_tcl_usage_in_prc(self, prc_file: str, common_dir: str) -> None:
        """Verify that Common TCL files are actually used in the PnR configuration
        
        Args:
            prc_file: Path to PRC file
            common_dir: Path to common directory
        """
        try:
            # Read the PnR configuration file
            with open(prc_file, 'r', encoding='utf-8') as f:
                prc_content = f.read()
            
            # Get all TCL files in COMMON directory
            tcl_files = self.file_utils.find_files("pnr_flow/nv_flow/COMMON/*.tcl", self.workarea)
            
            if not tcl_files:
                return
            
            print(f"  {Color.CYAN}TCL Usage Verification:{Color.RESET}")
            
            used_tcl_files = []
            unused_tcl_files = []
            
            # Check if it's YAML format
            if f"{self.design_info.top_hier}:" in prc_content:
                # For YAML format, look in CUSTOM_MAKE sections
                for tcl_file in tcl_files:
                    tcl_filename = os.path.basename(tcl_file)
                    # Check if TCL filename appears in CUSTOM_MAKE sections
                    if f"CUSTOM_MAKE:" in prc_content and tcl_filename in prc_content:
                        used_tcl_files.append(tcl_filename)
                    else:
                        unused_tcl_files.append(tcl_filename)
            else:
                # Legacy format - simple filename matching
                for tcl_file in tcl_files:
                    tcl_filename = os.path.basename(tcl_file)
                if tcl_filename in prc_content:
                    used_tcl_files.append(tcl_filename)
                else:
                    unused_tcl_files.append(tcl_filename)
            
            # Report results
            if used_tcl_files:
                print(f"    {Color.GREEN}[OK] Used TCL files ({len(used_tcl_files)}):{Color.RESET}")
                for tcl_file in sorted(used_tcl_files):
                    print(f"      - {tcl_file}")
            
            if unused_tcl_files:
                print(f"    {Color.YELLOW}[WARN] Unused TCL files ({len(unused_tcl_files)}):{Color.RESET}")
                for tcl_file in sorted(unused_tcl_files):
                    print(f"      - {tcl_file}")
            
            if not used_tcl_files and not unused_tcl_files:
                print(f"    {Color.YELLOW}No TCL files found to verify{Color.RESET}")
                
        except Exception as e:
            print(f"    {Color.RED}Error verifying TCL usage: {e}{Color.RESET}")
    
    def _extract_unified_flow_configuration(self, runset_file: str, beflow_file: str) -> None:
        """Extract and display unified flow configuration from runset.tcl and beflow_config.yaml
        
        Args:
            runset_file: Path to runset.tcl file
            beflow_file: Path to beflow_config.yaml file
        """
        config_rows = []
        
        # FIRST: Extract BEFLOW_ROOT, BEFLOW_CONFIG_SITE, FLOW2_CONFIG_SITE, SCAN_INSERTION_SITE and GLCHECK_SITE from logs
        beflow_root_version = None
        beflow_root_log_source = None
        beflow_config_site = None
        beflow_config_log_source = None
        flow2_config_site = None
        flow2_config_log_source = None
        scan_insertion_site = None
        scan_insertion_log_source = None
        glcheck_site = None
        glcheck_site_log_source = None
        
        # Look for BEFLOW_ROOT, BEFLOW_CONFIG_SITE, FLOW2_CONFIG_SITE and SCAN_INSERTION_SITE in pnr_flow/nv_flow/latest/log/*.log
        # runset_file is at: pnr_flow/nv_flow/<design>/<ipo>/runset.tcl
        # latest is at: pnr_flow/nv_flow/latest
        # So go up 2 levels from runset_file directory
        log_pattern = os.path.join(self.workarea, "pnr_flow/nv_flow/latest/log/*.log")
        try:
            log_files = glob.glob(log_pattern)
            for log_file in log_files:
                try:
                    # Search for "Using BEFLOW_ROOT" line
                    if not beflow_root_version:
                        result = self.file_utils.run_command(f"grep 'Using BEFLOW_ROOT' {log_file}")
                        if result.strip():
                            match = re.search(r'Using BEFLOW_ROOT\s+(.+)', result)
                            if match:
                                beflow_root_version = match.group(1).strip()
                                beflow_root_log_source = log_file
                    
                    # Search for "BEFLOW_CONFIG_SITE" line
                    if not beflow_config_site:
                        result = self.file_utils.run_command(f"grep 'BEFLOW_CONFIG_SITE' {log_file}")
                        if result.strip():
                            # Pattern: "unix environment var BEFLOW_CONFIG_SITE: bf project_beflow = /path/to/config"
                            match = re.search(r'BEFLOW_CONFIG_SITE:.*=\s*(.+)', result)
                            if match:
                                beflow_config_site = match.group(1).strip()
                                beflow_config_log_source = log_file
                    
                    # Search for "FLOW2_CONFIG_SITE" line
                    if not flow2_config_site:
                        result = self.file_utils.run_command(f"grep 'FLOW2_CONFIG_SITE' {log_file}")
                        if result.strip():
                            # Pattern: "unix environment var FLOW2_CONFIG_SITE: bf project_flow2_config = /path/to/config"
                            match = re.search(r'FLOW2_CONFIG_SITE:.*=\s*(.+)', result)
                            if match:
                                flow2_config_site = match.group(1).strip()
                                flow2_config_log_source = log_file
                    
                    # Search for "SCAN_INSERTION_SITE" line
                    if not scan_insertion_site:
                        result = self.file_utils.run_command(f"grep 'SCAN_INSERTION_SITE' {log_file}")
                        if result.strip():
                            # Pattern: "unix environment var SCAN_INSERTION_SITE: bf scan_insertion = /path/to/scan"
                            match = re.search(r'SCAN_INSERTION_SITE:.*=\s*(.+)', result)
                            if match:
                                scan_insertion_site = match.group(1).strip()
                                scan_insertion_log_source = log_file
                    
                    # Break if all found
                    if beflow_root_version and beflow_config_site and flow2_config_site and scan_insertion_site:
                        break
                except Exception:
                    continue
        except Exception:
            pass
        
        # Extract GLCHECK_SITE from signoff umake logs (it's not in the main setup logs)
        # Pattern: pnr_flow/nv_flow/<design>/<ipo>/nbu_signoff/umake_log/<timestamp>/*.umake.log
        glcheck_source_type = None
        try:
            umake_log_pattern = os.path.join(self.workarea, "pnr_flow/nv_flow/*/ipo*/nbu_signoff/umake_log/*/*.umake.log")
            umake_log_files = glob.glob(umake_log_pattern)
            
            # Sort by modification time to get the most recent
            if umake_log_files:
                umake_log_files.sort(key=os.path.getmtime, reverse=True)
                
                for umake_log_file in umake_log_files:
                    try:
                        result = self.file_utils.run_command(f"grep 'GLCHECK_SITE' {umake_log_file}")
                        if result.strip():
                            # Pattern: "unix environment var GLCHECK_SITE: bf glcheck_site = /path/to/glcheck"
                            match = re.search(r'GLCHECK_SITE:.*=\s*(.+)', result)
                            if not match:
                                # Alternative pattern: "-I- GLCHECK_SITE = /path/to/glcheck"
                                match = re.search(r'GLCHECK_SITE\s*=\s*(.+)', result)
                            if match:
                                glcheck_site = match.group(1).strip()
                                glcheck_site_log_source = umake_log_file
                                glcheck_source_type = "signoff/umake.log"
                                break
                    except Exception:
                        continue
        except Exception:
            pass
        
        # If GLCHECK_SITE not found in umake logs, try debug files under <path to ipo>/LOGs/PRIME/*.debug
        if not glcheck_site:
            try:
                debug_pattern = os.path.join(self.workarea, "pnr_flow/nv_flow/*/ipo*/LOGs/PRIME/*.debug")
                debug_files = glob.glob(debug_pattern)
                
                # Sort by modification time to get the most recent
                if debug_files:
                    debug_files.sort(key=os.path.getmtime, reverse=True)
                    
                    for debug_file in debug_files:
                        try:
                            result = self.file_utils.run_command(f"grep 'GLCHECK_SITE' {debug_file}")
                            if result.strip():
                                # Pattern: "unix environment var GLCHECK_SITE: bf glcheck_site = /path/to/glcheck"
                                match = re.search(r'GLCHECK_SITE:.*=\s*(.+)', result)
                                if not match:
                                    # Alternative pattern: "-I- GLCHECK_SITE = /path/to/glcheck"
                                    match = re.search(r'GLCHECK_SITE\s*=\s*(.+)', result)
                                if not match:
                                    # Alternative pattern in debug files: "GLCHECK_SITE: /path/to/glcheck"
                                    match = re.search(r'GLCHECK_SITE:\s*(.+)', result)
                                if match:
                                    glcheck_site = match.group(1).strip()
                                    glcheck_site_log_source = debug_file
                                    glcheck_source_type = "LOGs/PRIME/debug"
                                    break
                        except Exception:
                            continue
            except Exception:
                pass
        
        # SECOND: Extract BE_OVERRIDE_TOOLVERS (priority extraction)
        be_toolvers_value = None
        be_toolvers_source = None
        
        # Try formal flow env files first
        formal_env_locations = [
            "formal_flow/rtl_vs_pnr_fm/FM_INFO/env",
            "formal_flow/rtl_vs_syn_fm/FM_INFO/env",
            "formal_flow/rtl_vs_pnr_bbox_fm/FM_INFO/env",
            "formal_flow/rtl_vs_syn_bbox_fm/FM_INFO/env"
        ]
        
        for env_path in formal_env_locations:
            full_path = os.path.join(self.workarea, env_path)
            if os.path.exists(full_path):
                try:
                    result = self.file_utils.run_command(f"grep '^BE_OVERRIDE_TOOLVERS' {full_path}")
                    if result.strip():
                        match = re.search(r'BE_OVERRIDE_TOOLVERS[=:]\s*(.+)', result)
                        if match:
                            be_toolvers_value = match.group(1).strip()
                            be_toolvers_source = env_path
                            break
                except Exception:
                    continue
        
        # Fallback: Try PnR debug files
        if not be_toolvers_value:
            debug_pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{self.design_info.ipo}/LOGs/PRIME/STEP__BEGIN__*.debug"
            debug_files = self.file_utils.find_files(debug_pattern, self.workarea)
            
            for debug_file in debug_files[:1]:  # Check first BEGIN debug file
                try:
                    result = self.file_utils.run_command(f"grep '^BE_OVERRIDE_TOOLVERS' {debug_file}")
                    if result.strip():
                        match = re.search(r'BE_OVERRIDE_TOOLVERS[:\s]\s*(.+)', result)
                        if match:
                            be_toolvers_value = match.group(1).strip()
                            be_toolvers_source = debug_file
                            break
                except Exception:
                    continue
        
        # Extract from runset.tcl
        runset_exists = os.path.exists(runset_file)
        if runset_exists:
            try:
                with open(runset_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Variables to extract with their actual patterns in runset.tcl
                variable_patterns = [
                    (r'set\s+FLOW\(PATH\)\s+([^\s\n]+)', 'FLOW_PATH'),
                    (r'set\s+PROJECT\(CUSTOM_SCRIPTS_DIR\)\s+([^\s\n]+)', 'CUSTOM_SCRIPTS_DIR'),
                    (r'set\s+BLOCK\(FLOORPLAN,FILE\)\s+([^\s\n]+)', 'FLOORPLAN'),
                    (r'set\s+BLOCK\(FLOORPLAN,PIN_PLACEMENT_FILE\)\s+([^\s\n]+)', 'PIN_PLACEMENT_FILE'),
                    (r'set\s+PROJECT\(TOP_PLANNER_YAML\)\s+([^\s\n]+)', 'TOP_PLANNER_YAML'),
                    (r'set\s+OPT\(MULTIBIT_FLOP,ENABLE\)\s+([^\s\n]+)', 'MULTIBIT_FLOP')
                ]
                
                for pattern, var_name in variable_patterns:
                    matches = re.findall(pattern, content, re.IGNORECASE | re.MULTILINE)
                    if matches:
                        value = matches[0].strip()
                        if value and value != '""' and value != "''" and value != "None":
                            # Highlight MULTIBIT_FLOP setting with color
                            if var_name == 'MULTIBIT_FLOP':
                                if value.lower() in ['1', 'true', 'yes', 'enabled']:
                                    config_rows.append((var_name, 'runset.tcl', f"{value} (ENABLED)", Color.GREEN, False))
                                else:
                                    config_rows.append((var_name, 'runset.tcl', f"{value} (DISABLED)", Color.YELLOW, False))
                            # Don't truncate file path values (FLOORPLAN, PIN_PLACEMENT_FILE, TOP_PLANNER_YAML)
                            elif var_name in ['FLOORPLAN', 'PIN_PLACEMENT_FILE', 'TOP_PLANNER_YAML']:
                                config_rows.append((var_name, 'runset.tcl', value, "", True))  # no_truncate=True
                            else:
                                config_rows.append((var_name, 'runset.tcl', value, "", False))
            except Exception as e:
                pass
        
        # Extract from beflow_config.yaml
        beflow_exists = os.path.exists(beflow_file)
        if beflow_exists:
            try:
                with open(beflow_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Extract useful variables
                patterns = [
                    (r'LIB_SNAP_REV:\s+(\d+)', 'Library Snapshot'),
                    (r'nv_process:\s+\[[\'\"]([^\'\"]+)[\'\"]\]', 'NV Process'),
                    (r'TracksNum:\s+\[[\'\"]([^\'\"]+)[\'\"]\]', 'Tracks Number'),
                    (r'project:\s+\[[\'\"]([^\'\"]+)[\'\"]\]', 'Project'),
                    (r'default_scenario\(dc\):\s+\[[\'\"]([^\'\"]+)[\'\"]\]', 'Default Scenario')
                ]
                
                for pattern, var_name in patterns:
                    matches = re.findall(pattern, content, re.IGNORECASE | re.MULTILINE)
                    if matches:
                        config_rows.append((var_name, 'beflow_config', matches[0].strip(), "", False))
                
                # VT types
                vt_types_match = re.search(r'vt_type_list:\s+\[([^\]]+)\]', content)
                if vt_types_match:
                    vt_types = vt_types_match.group(1).replace("'", "").replace('"', '').replace('[', '').replace(']', '').strip()
                    config_rows.append(('VT Types', 'beflow_config', vt_types, "", False))
                
                # Array names
                array_names_match = re.search(r'arrayNames:\s+\[([^\]]+)\]', content)
                if array_names_match:
                    array_names = array_names_match.group(1)
                    clean_names = [name.strip().strip("'\"") for name in array_names.split(',')]
                    arrays_str = ', '.join(clean_names) if clean_names else '(none)'
                    config_rows.append(('Array Names', 'beflow_config', arrays_str, "", False))
                
                # Agur unit BE IP
                agur_unit_match = re.search(rf'agur_unit_be_ip\({self.design_info.top_hier}\):\s+\[([^\]]+)\]', content)
                if agur_unit_match:
                    agur_units = agur_unit_match.group(1)
                    clean_units = [unit.strip().strip("'\"") for unit in agur_units.split(',')]
                    config_rows.append(('Agur Unit BE IP', 'beflow_config', ', '.join(clean_units), "", False))
            except Exception as e:
                pass
        
        # Add parameters from logs at the very beginning (if found)
        insert_position = 0
        if beflow_root_version:
            # Display full path with cyan color for visibility
            config_rows.insert(insert_position, ('BEFLOW_ROOT', 'latest/log', beflow_root_version, Color.CYAN, True))
            insert_position += 1
        
        if beflow_config_site:
            # Display full path with cyan color for visibility
            config_rows.insert(insert_position, ('BEFLOW_CONFIG_SITE', 'latest/log', beflow_config_site, Color.CYAN, True))
            insert_position += 1
        
        if flow2_config_site:
            # Display full path with cyan color for visibility
            config_rows.insert(insert_position, ('FLOW2_CONFIG_SITE', 'latest/log', flow2_config_site, Color.CYAN, True))
            insert_position += 1
        
        if scan_insertion_site:
            # Display full path with cyan color for visibility
            config_rows.insert(insert_position, ('SCAN_INSERTION_SITE', 'latest/log', scan_insertion_site, Color.CYAN, True))
            insert_position += 1
        
        if glcheck_site:
            # Display full path with cyan color for visibility
            source_label = glcheck_source_type if glcheck_source_type else 'signoff/umake.log'
            config_rows.insert(insert_position, ('GLCHECK_SITE', source_label, glcheck_site, Color.CYAN, True))
            insert_position += 1
        
        # Add BE_OVERRIDE_TOOLVERS at the beginning (if found)
        if be_toolvers_value:
            # Extract just filename from path if it's a file path
            display_value = os.path.basename(be_toolvers_value) if '/' in be_toolvers_value else be_toolvers_value
            # Insert at current position with magenta color and no truncation
            config_rows.insert(insert_position, ('BE_OVERRIDE_TOOLVERS', 'env/debug', display_value, Color.MAGENTA, True))
        
        # Print unified table
        if config_rows or be_toolvers_value:
            # Get relative paths for display
            runset_rel = runset_file.replace(self.workarea + '/', '') if self.workarea in runset_file else os.path.basename(runset_file)
            beflow_rel = beflow_file.replace(self.workarea + '/', '') if self.workarea in beflow_file else os.path.basename(beflow_file)
            
            print(f"\n{Color.CYAN}{'='*120}{Color.RESET}")
            print(f"{Color.YELLOW}  FLOW CONFIGURATION{Color.RESET}")
            print(f"{Color.CYAN}{'='*120}{Color.RESET}")
            
            # Show BEFLOW_ROOT log source if found
            if beflow_root_log_source:
                beflow_log_rel = beflow_root_log_source.replace(self.workarea + '/', '') if self.workarea in beflow_root_log_source else beflow_root_log_source
                print(f"  Source (BEFLOW_ROOT): {beflow_log_rel}")
            
            # Show BEFLOW_CONFIG_SITE log source if found
            if beflow_config_log_source:
                beflow_cfg_log_rel = beflow_config_log_source.replace(self.workarea + '/', '') if self.workarea in beflow_config_log_source else beflow_config_log_source
                print(f"  Source (BEFLOW_CONFIG_SITE): {beflow_cfg_log_rel}")
            
            # Show FLOW2_CONFIG_SITE log source if found
            if flow2_config_log_source:
                flow2_log_rel = flow2_config_log_source.replace(self.workarea + '/', '') if self.workarea in flow2_config_log_source else flow2_config_log_source
                print(f"  Source (FLOW2_CONFIG_SITE): {flow2_log_rel}")
            
            # Show SCAN_INSERTION_SITE log source if found
            if scan_insertion_log_source:
                scan_log_rel = scan_insertion_log_source.replace(self.workarea + '/', '') if self.workarea in scan_insertion_log_source else scan_insertion_log_source
                print(f"  Source (SCAN_INSERTION_SITE): {scan_log_rel}")
            
            # Show GLCHECK_SITE log source if found
            if glcheck_site_log_source:
                glcheck_log_rel = glcheck_site_log_source.replace(self.workarea + '/', '') if self.workarea in glcheck_site_log_source else glcheck_site_log_source
                print(f"  Source (GLCHECK_SITE): {glcheck_log_rel}")
            
            # Show BE_OVERRIDE_TOOLVERS source if found
            if be_toolvers_source:
                be_toolvers_rel = be_toolvers_source.replace(self.workarea + '/', '') if self.workarea in be_toolvers_source else be_toolvers_source
                print(f"  Source (BE_OVERRIDE_TOOLVERS): {be_toolvers_rel}")
            
            if runset_exists:
                print(f"  Source (runset): {runset_rel}")
            if beflow_exists:
                print(f"  Source (beflow): {beflow_rel}")
            print(f"  {'-'*120}")
            print(f"  {'Parameter':<30} {'Source':<18} {'Value':<70}")
            print(f"  {'-'*120}")
            
            for row in config_rows:
                if len(row) == 5:
                    param, source, value, color, no_truncate = row
                    # Truncate long values unless no_truncate is True
                    if not no_truncate and len(value) > 70:
                        value = value[:67] + "..."
                    reset = Color.RESET if color else ""
                    print(f"  {param:<30} {source:<18} {color}{value}{reset}")
                elif len(row) == 4:
                    # Legacy format support
                    param, source, value, color = row
                    if len(value) > 70:
                        value = value[:67] + "..."
                    reset = Color.RESET if color else ""
                    print(f"  {param:<30} {source:<18} {color}{value}{reset}")
                else:
                    # Fallback format
                    param, source, value = row[:3]
                    if len(value) > 70:
                        value = value[:67] + "..."
                    print(f"  {param:<30} {source:<18} {value}")
            
            print(f"{Color.CYAN}{'='*120}{Color.RESET}")
        else:
            if not runset_exists and not beflow_exists:
                print(f"\n{Color.YELLOW}Flow Configuration: Both runset.tcl and beflow_config.yaml not found{Color.RESET}")
            elif not runset_exists:
                print(f"\n{Color.YELLOW}Flow Configuration: runset.tcl not found (beflow_config.yaml processed){Color.RESET}")
            elif not beflow_exists:
                print(f"\n{Color.YELLOW}Flow Configuration: beflow_config.yaml not found (runset.tcl processed){Color.RESET}")
            else:
                print(f"\n{Color.YELLOW}Flow Configuration: No configuration variables extracted{Color.RESET}")
    
    def _extract_pnr_flow_variables(self, runset_file: str) -> Dict[str, str]:
        """Extract important PnR flow variables from runset.tcl file
        
        Args:
            runset_file: Path to runset.tcl file
            
        Returns:
            Dictionary mapping variable names to values
        """
        try:
            with open(runset_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            print(f"  {Color.CYAN}PnR Flow Variables:{Color.RESET}")
            
            # Variables to extract with their actual patterns in runset.tcl
            variable_patterns = [
                (r'set\s+FLOW\(PATH\)\s+([^\s\n]+)', 'FLOW(PATH)'),
                (r'set\s+PROJECT\(CUSTOM_SCRIPTS_DIR\)\s+([^\s\n]+)', 'PROJECT(CUSTOM_SCRIPTS_DIR)'),
                (r'set\s+BLOCK\(FLOORPLAN,FILE\)\s+([^\s\n]+)', 'FLOORPLAN'),
                (r'set\s+BLOCK\(FLOORPLAN,PIN_PLACEMENT_FILE\)\s+([^\s\n]+)', 'PIN_PLACEMENT_FILE'),
                (r'set\s+PROJECT\(TOP_PLANNER_YAML\)\s+([^\s\n]+)', 'TOP_PLANNER_YAML'),
                (r'set\s+VERSION\(DC_SHELL\)\s+([^\s\n]+)', 'DC_SHELL_VERSION'),
                (r'set\s+LIB\(PHYSICAL,PATH,RAM\)\s+([^\s\n]+)', 'RAM_LIB_PATH'),
                (r'set\s+LIB\(CENTRAL,PATH\)\s+([^\s\n]+)', 'CENTRAL_LIB_PATH'),
                (r'set\s+NETWORK_FLOW\(PATH\)\s+([^\s\n]+)', 'NETWORK_FLOW_PATH'),
                (r'set\s+NETWORK_FLOW\(UTILS_DIR\)\s+([^\s\n]+)', 'NETWORK_FLOW_UTILS_DIR'),
                (r'set\s+OPT\(MULTIBIT_FLOP,ENABLE\)\s+([^\s\n]+)', 'MULTIBIT_FLOP')
            ]
            
            found_variables = []
            
            for pattern, var_name in variable_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE | re.MULTILINE)
                if matches:
                    value = matches[0].strip()
                    if value and value != '""' and value != "''" and value != "None":
                        found_variables.append((var_name, value))
            
            if found_variables:
                for var, value in found_variables:
                    # Display full values without truncation
                    # Highlight MULTIBIT_FLOP setting with color
                    if var == 'MULTIBIT_FLOP':
                        if value.lower() in ['1', 'true', 'yes', 'enabled']:
                            print(f"    {Color.GREEN}{var}: {value} (ENABLED){Color.RESET}")
                        else:
                            print(f"    {Color.YELLOW}{var}: {value} (DISABLED){Color.RESET}")
                    else:
                        print(f"    {var}: {value}")
                
                # Return dictionary for use in HTML reports
                return dict(found_variables)
            else:
                print(f"    {Color.YELLOW}No PnR flow variables found{Color.RESET}")
                return {}
                
        except Exception as e:
            print(f"    {Color.RED}Error extracting PnR flow variables: {e}{Color.RESET}")
            return {}
    
    def _get_status_indicator(self, status: str) -> tuple:
        """Map PnR stage status to icon and CSS class
        
        Args:
            status: Status string
            
        Returns:
            Tuple of (icon, css_class)
        """
        status_map = {
            'DONE': ('âœ“', 'status-pass'),
            'COMPLETED': ('âœ“', 'status-pass'),
            'PASS': ('âœ“', 'status-pass'),
            'FAILED': ('âœ—', 'status-fail'),
            'FAIL': ('âœ—', 'status-fail'),
            'ERROR': ('âœ—', 'status-fail'),
            'RUN': ('âŸ³', 'status-running'),
            'RUNNING': ('âŸ³', 'status-running'),
            'UNKNOWN': ('?', 'status-unknown')
        }
        return status_map.get(status.upper(), ('?', 'status-unknown'))
    
    def _extract_status_from_log(self, log_file: str) -> str:
        """Extract status from individual PRIME log file
        
        Args:
            log_file: Path to log file
            
        Returns:
            Status string ('DONE'/'FAIL'/'ERROR'/'RUNNING'/'UNKNOWN')
        """
        if not os.path.exists(log_file):
            return 'UNKNOWN'
        
        try:
            # Read last 50 lines of log file for status
            result = self.file_utils.run_command(f"tail -50 {log_file}")
            
            # Check for explicit STATUS markers
            if 'STATUS: DONE' in result:
                return 'DONE'
            elif 'STATUS: FAIL' in result:
                return 'FAIL'
            elif 'STATUS: ERROR' in result:
                return 'ERROR'
            elif 'STATUS: RUN' in result:
                return 'RUNNING'
            
            # Check for completion markers
            if 'Successfully completed' in result:
                return 'DONE'
            
            # Check for failure markers
            if 'Exited with exit code' in result:
                # Extract exit code
                match = re.search(r'Exited with exit code (\d+)', result)
                if match and match.group(1) != '0':
                    return 'FAIL'
                elif match and match.group(1) == '0':
                    return 'DONE'
            
            # If log file exists but no clear status, assume it's still running or incomplete
            return 'UNKNOWN'
            
        except Exception:
            return 'UNKNOWN'
    
    def _extract_be_override_toolvers_data(self) -> Optional[Dict[str, Any]]:
        """Extract BE_OVERRIDE_TOOLVERS data from all PnR stages for analysis and visualization
        
        Returns:
            Dictionary with structured data including stage executions, configs, timeline, and status, or None if not available
        """
        try:
            debug_pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{self.design_info.ipo}/LOGs/PRIME/*.debug"
            debug_files = self.file_utils.find_files(debug_pattern, self.workarea)
            
            if not debug_files:
                return None
            
            # Extract BE_OVERRIDE_TOOLVERS from each debug file
            toolvers_data = []
            
            for debug_file in sorted(debug_files):
                try:
                    # Get file modification time
                    file_stat = os.stat(debug_file)
                    timestamp = file_stat.st_mtime
                    timestamp_str = datetime.fromtimestamp(timestamp).strftime('%m/%d %H:%M')
                    timestamp_full = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')
                    
                    # Extract stage name from filename
                    basename = os.path.basename(debug_file)
                    stage_full = basename.replace('.debug', '')
                    stage_name = re.sub(r'__\d{14}_[A-Z0-9]+$', '', stage_full)
                    
                    # Clean stage name
                    stage_clean = re.sub(r'^STEP_update__', '', stage_name)
                    stage_clean = re.sub(r'^STEP__', '', stage_clean)
                    
                    # Get status from corresponding log file
                    log_file = debug_file.replace('.debug', '.log')
                    status = self._extract_status_from_log(log_file)
                    
                    # Search for BE_OVERRIDE_TOOLVERS in the file
                    result = self.file_utils.run_command(f"grep '^BE_OVERRIDE_TOOLVERS:' {debug_file}")
                    if result.strip():
                        match = re.search(r'BE_OVERRIDE_TOOLVERS:\s*(.+)', result)
                        if match:
                            config_path = match.group(1).strip()
                            config_file = os.path.basename(config_path)
                            toolvers_data.append({
                                'stage': stage_name,
                                'stage_clean': stage_clean,
                                'timestamp': timestamp,
                                'timestamp_str': timestamp_str,
                                'timestamp_full': timestamp_full,
                                'config_file': config_file,
                                'config_path': config_path,
                                'status': status
                            })
                except Exception:
                    continue
            
            if not toolvers_data:
                return None
            
            # Sort by timestamp
            toolvers_data.sort(key=lambda x: x['timestamp'])
            
            # Check for inconsistencies
            unique_configs = set(item['config_path'] for item in toolvers_data)
            is_consistent = len(unique_configs) == 1
            
            # Group by config
            config_groups = {}
            for item in toolvers_data:
                config = item['config_path']
                if config not in config_groups:
                    config_groups[config] = []
                config_groups[config].append(item)
            
            # Find config change point
            config_change_index = None
            if len(unique_configs) > 1:
                prev_config = toolvers_data[0]['config_path']
                for idx, item in enumerate(toolvers_data[1:], 1):
                    if item['config_path'] != prev_config:
                        config_change_index = idx
                        break
            
            return {
                'is_consistent': is_consistent,
                'unique_configs': list(unique_configs),
                'toolvers_data': toolvers_data,
                'config_groups': config_groups,
                'config_change_index': config_change_index,
                'total_executions': len(toolvers_data)
            }
            
        except Exception as e:
            return None
    
    def _check_be_override_toolvers_consistency(self) -> Dict[str, Any]:
        """Check BE_OVERRIDE_TOOLVERS consistency across all PnR stages
        
        Detects if the user changed tool version configuration mid-flow.
        
        Returns:
            Dictionary with consistency check results
        """
        try:
            data = self._extract_be_override_toolvers_data()
            if not data:
                return
            
            toolvers_data = data['toolvers_data']
            is_consistent = data['is_consistent']
            config_groups = data['config_groups']
            
            if not is_consistent:
                # Inconsistency detected!
                print(f"\n  {Color.RED}[ERROR] BE_OVERRIDE_TOOLVERS Inconsistency Detected:{Color.RESET}")
                print(f"    {Color.YELLOW}Tool version configuration changed mid-flow!{Color.RESET}")
                print(f"    {Color.YELLOW}This can cause inconsistent results across PnR stages.{Color.RESET}\n")
                
                # Group by config file
                config_groups = {}
                for item in toolvers_data:
                    config = item['config_path']
                    if config not in config_groups:
                        config_groups[config] = []
                    config_groups[config].append(item)
                
                # Display each configuration with its stages (sorted by first occurrence)
                sorted_groups = sorted(config_groups.items(), key=lambda x: x[1][0]['timestamp'])
                for idx, (config_path, items) in enumerate(sorted_groups, 1):
                    config_file = os.path.basename(config_path)
                    # Sort items by timestamp to get proper first/last
                    items_sorted = sorted(items, key=lambda x: x['timestamp'])
                    first_time = items_sorted[0]['timestamp_str']
                    last_time = items_sorted[-1]['timestamp_str']
                    stage_count = len(items)
                    
                    print(f"    Config {idx}: {config_file}")
                    print(f"      Path: {config_path}")
                    print(f"      Used: {first_time} -> {last_time} ({stage_count} executions)")
                    
                    # Clean stage names and group by stage with timestamps
                    # Format: stage (timestamp) or stage (Nx: first, last) for re-runs
                    stage_groups = {}
                    stage_order = []  # Preserve chronological order
                    
                    for item in items_sorted:
                        stage = item['stage']
                        # Remove STEP_update__ and STEP__ prefixes
                        stage = re.sub(r'^STEP_update__', '', stage)
                        stage = re.sub(r'^STEP__', '', stage)
                        
                        if stage not in stage_groups:
                            stage_groups[stage] = []
                            stage_order.append(stage)
                        stage_groups[stage].append(item['timestamp_str'])
                    
                    # Build compact display with timestamps
                    stage_entries = []
                    for stage in stage_order:
                        timestamps = stage_groups[stage]
                        if len(timestamps) == 1:
                            stage_entries.append(f"{stage} ({timestamps[0]})")
                        else:
                            # Multiple executions - show count and first/last
                            stage_entries.append(f"{stage} ({len(timestamps)}x: {timestamps[0]}..{timestamps[-1]})")
                    
                    # Print stages across multiple lines with proper indentation
                    stages_display = ', '.join(stage_entries)
                    print(f"      Stages: {stages_display}")
                    print()
                
            else:
                # All consistent
                unique_configs = data['unique_configs']
                config_path = unique_configs[0]
                config_file = os.path.basename(config_path)
                print(f"  {Color.CYAN}BE_OVERRIDE_TOOLVERS:{Color.RESET} {config_file}")
                print(f"    Path: {config_path}")
                print(f"    [OK] Consistent across all {len(toolvers_data)} PnR stages")
                
        except Exception as e:
            # Silent failure - this is an optional check
            pass
    
    def _format_flow_runtime(self, seconds: int) -> str:
        """Format runtime from seconds to readable format
        
        Args:
            seconds: Runtime in seconds
        
        Returns:
            Formatted string (e.g., '3h 25m', '45m', '30s')
        """
        if seconds < 60:
            return f"{seconds}s"
        elif seconds < 3600:
            minutes = seconds // 60
            return f"{minutes}m"
        else:
            hours = seconds // 3600
            minutes = (seconds % 3600) // 60
            if minutes > 0:
                return f"{hours}h {minutes}m"
            return f"{hours}h"
    
    def _parse_prc_status_json(self) -> Optional[Dict[str, Any]]:
        """Parse .prc.status.json file and extract all IPO execution data
        
        Returns:
            Dictionary with IPO execution data or None if not found
        """
        try:
            # Find .prc.status.json file
            prc_status_file = os.path.join(self.workarea, "pnr_flow/nv_flow", f"{self.design_info.top_hier}.prc.status.json")
            
            if not os.path.exists(prc_status_file):
                return None
            
            # Parse JSON
            with open(prc_status_file, 'r') as f:
                data = json.load(f)
            
            # Extract unit data (should be self.design_info.top_hier)
            unit_name = self.design_info.top_hier
            if unit_name not in data:
                return None
            
            unit_data = data[unit_name]
            
            # Build IPO execution history
            ipo_data = {}
            
            for ipo_key, ipo_info in unit_data.items():
                if not ipo_key.startswith('ipo'):
                    continue  # Skip non-IPO keys
                
                ipo_num = ipo_key
                
                # Extract run_step_list
                if 'run_step_list' not in ipo_info:
                    continue
                
                stages = {}
                
                for step_entry in ipo_info['run_step_list']:
                    # Each entry is a dict with one key (step ID) -> step data
                    for step_id, step_data in step_entry.items():
                        stage_name = step_data.get('step_name', '')
                        
                        # Skip hidden steps like BEGIN/END
                        if step_data.get('is_hidden') == '1':
                            continue
                        
                        # Extract key information
                        status = step_data.get('full_status', {}).get('status', 'UNKNOWN')
                        start_date = step_data.get('start_date', '')
                        duration = step_data.get('duration', 0)
                        logfile = step_data.get('logfile', '')
                        
                        # Format timestamp
                        if start_date:
                            try:
                                # Format: YYYYMMDDHHMMSS -> MM/DD HH:MM
                                dt_str = start_date
                                month = dt_str[4:6]
                                day = dt_str[6:8]
                                hour = dt_str[8:10]
                                minute = dt_str[10:12]
                                timestamp_formatted = f"{month}/{day} {hour}:{minute}"
                            except:
                                timestamp_formatted = start_date
                        else:
                            timestamp_formatted = 'N/A'
                        
                        # Store execution
                        if stage_name not in stages:
                            stages[stage_name] = []
                        
                        stages[stage_name].append({
                            'status': status,
                            'timestamp': timestamp_formatted,
                            'start_date_raw': start_date,
                            'duration': duration,
                            'logfile': logfile,
                            'step_id': step_id
                        })
                
                ipo_data[ipo_num] = {
                    'stages': stages,
                    'ipo_info': ipo_info
                }
            
            return {
                'unit_name': unit_name,
                'ipo_data': ipo_data,
                'prc_file': prc_status_file
            }
            
        except Exception as e:
            print(f"  Error parsing .prc.status.json: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _get_flow_status_badge(self, status: str) -> tuple:
        """Get HTML status badge and color for flow execution
        
        Args:
            status: Status string (DONE, FAIL, RUN, etc.)
            
        Returns:
            Tuple of (icon, color_class)
        """
        status_upper = status.upper()
        
        if status_upper in ['DONE', 'PASS', 'SUCCESS']:
            return ('âœ“', 'status-pass')
        elif status_upper in ['FAIL', 'FAILED', 'ERROR']:
            return ('âœ—', 'status-fail')
        elif status_upper in ['RUN', 'RUNNING', 'PRELAUNCH']:
            return ('âŸ³', 'status-running')
        else:
            return ('â—¯', 'status-notrun')
    
    def _get_toolvers_config_html_content(self) -> str:
        """Generate HTML content for Flow Execution tracker tab
        
        Parses .prc.status.json to show complete execution history across all IPOs
        Shows status, timestamps, durations, and failures for each stage
        
        Returns:
            HTML string for flow execution section
        """
        try:
            flow_data = self._parse_prc_status_json()
            
            if not flow_data:
                return """
                <div class="no-data">
                    <p>No PnR flow execution data available</p>
                    <p>.prc.status.json file not found or could not be parsed.</p>
                    </div>
                """
            
            ipo_data = flow_data['ipo_data']
            unit_name = flow_data['unit_name']
            
            # Get all unique stage names across all IPOs
            all_stages = set()
            for ipo_num, ipo_info in ipo_data.items():
                all_stages.update(ipo_info['stages'].keys())
            
            # Sort stages in typical flow order
            flow_order = ['setup', 'edi_plan', 'place', 'cts', 'route', 'postroute', 
                         'report_edi_plan', 'report_place', 'report_cts', 'report_route', 'report_postroute',
                         'export', 'extraction', 'auto_pt']
            sorted_stages = []
            for stage in flow_order:
                if stage in all_stages:
                    sorted_stages.append(stage)
                    all_stages.remove(stage)
            # Add any remaining stages not in flow_order
            sorted_stages.extend(sorted(all_stages))
            
            # Sort IPOs numerically (handle suffixes like ipo1000_ref, ipo1001_eco)
            def extract_ipo_number(ipo_str):
                """Extract numeric part from IPO string (e.g., ipo1000_ref -> 1000)"""
                import re
                match = re.search(r'ipo(\d+)', ipo_str)
                return int(match.group(1)) if match else 0
            
            ipo_numbers = sorted(ipo_data.keys(), key=extract_ipo_number)
            
            # Build summary statistics
            summary_cards = []
            for ipo_num in ipo_numbers:
                ipo_info = ipo_data[ipo_num]
                stages = ipo_info['stages']
                
                total_stages = len(stages)
                completed_stages = 0
                failed_stages = 0
                running_stages = 0
                total_runtime = 0
                retry_count = 0
                
                for stage_name, executions in stages.items():
                    # Count retries (multiple executions of same stage)
                    if len(executions) > 1:
                        retry_count += len(executions) - 1
                    
                    # Get latest execution
                    latest = executions[-1]
                    status = latest['status'].upper()
                    
                    if status in ['DONE', 'PASS', 'SUCCESS']:
                        completed_stages += 1
                    elif status in ['FAIL', 'FAILED', 'ERROR']:
                        failed_stages += 1
                    elif status in ['RUN', 'RUNNING', 'PRELAUNCH']:
                        running_stages += 1
                    
                    # Sum up durations
                    for exec_data in executions:
                        total_runtime += exec_data.get('duration', 0)
                
                not_run = total_stages - completed_stages - failed_stages - running_stages
                
                # Status determination
                if failed_stages > 0:
                    status_badge = '<span class="ipo-status-badge status-fail">FAIL</span>'
                elif running_stages > 0:
                    status_badge = '<span class="ipo-status-badge status-running">RUNNING</span>'
                elif completed_stages == total_stages:
                    status_badge = '<span class="ipo-status-badge status-pass">COMPLETE</span>'
                else:
                    status_badge = '<span class="ipo-status-badge status-notrun">INCOMPLETE</span>'
                
                summary_cards.append(f"""
                <div class="ipo-summary-card">
                    <h3>{ipo_num.upper()} {status_badge}</h3>
                    <div class="ipo-stats">
                        <div class="ipo-stat">
                            <span class="stat-label">Completed:</span>
                            <span class="stat-value" style="color: #27ae60;">{completed_stages}/{total_stages}</span>
                </div>
                        <div class="ipo-stat">
                            <span class="stat-label">Failed:</span>
                            <span class="stat-value" style="color: #e74c3c;">{failed_stages}</span>
                    </div>
                        <div class="ipo-stat">
                            <span class="stat-label">Running:</span>
                            <span class="stat-value" style="color: #f39c12;">{running_stages}</span>
                    </div>
                        <div class="ipo-stat">
                            <span class="stat-label">Retries:</span>
                            <span class="stat-value">{retry_count}</span>
                    </div>
                        <div class="ipo-stat">
                            <span class="stat-label">Total Runtime:</span>
                            <span class="stat-value">{self._format_flow_runtime(total_runtime)}</span>
                    </div>
                    </div>
                    </div>
                """)
            
            summary_html = f"""
            <div class="flow-summary-container">
                {"".join(summary_cards)}
            </div>
            """
            
            # Build comparison table
            table_header = '<tr><th class="stage-col">Stage</th>'
            for ipo_num in ipo_numbers:
                table_header += f'<th class="ipo-col">{ipo_num.upper()}</th>'
            table_header += '</tr>'
            
            table_rows = []
            for stage_name in sorted_stages:
                row_html = f'<tr><td class="stage-name">{stage_name}</td>'
                
                for ipo_num in ipo_numbers:
                    ipo_info = ipo_data[ipo_num]
                    stages = ipo_info['stages']
                    
                    if stage_name in stages:
                        executions = stages[stage_name]
                        cell_html = '<td class="execution-cell">'
                        
                        for exec_data in executions:
                            status = exec_data['status']
                            timestamp = exec_data['timestamp']
                            duration = exec_data['duration']
                            logfile = exec_data['logfile']
                            
                            icon, status_class = self._get_flow_status_badge(status)
                            runtime_str = self._format_flow_runtime(duration)
                            
                            # Make logfile clickable - create prominent log button
                            if logfile and os.path.exists(logfile):
                                if USE_TABLOG_SERVER:
                                    # Server mode: Try server first, fallback to clipboard
                                    timestamp_html = f'<a href="#" onclick="openLogWithServer(\'{logfile}\', event)" class="exec-time-link" title="Click to open in tablog (server mode)">{timestamp}</a>'
                                    log_button = f'<a href="#" onclick="openLogWithServer(\'{logfile}\', event)" class="log-button" title="Open in tablog via server (fallback: copy command)">ðŸ–¥ï¸ tablog</a>'
                                else:
                                    # Direct file mode: Original behavior
                                    timestamp_html = f'<a href="file://{logfile}" class="exec-time-link" title="Click to view log: {logfile}">{timestamp}</a>'
                                    log_button = f'<a href="file://{logfile}" class="log-button" title="View log file: {logfile}">ðŸ“„ Log</a>'
                            else:
                                timestamp_html = f'<span class="exec-time">{timestamp}</span>'
                                log_button = '<span class="log-button-disabled" title="Log file not found">ðŸ“„ N/A</span>'
                            
                            cell_html += f'''
                            <div class="execution-entry {status_class}">
                                <span class="exec-icon">{icon}</span>
                                {timestamp_html}
                                <span class="exec-duration">({runtime_str})</span>
                                {log_button}
                            </div>
                            '''
                        
                        cell_html += '</td>'
                        row_html += cell_html
                    else:
                        # Stage not run in this IPO
                        row_html += '<td class="execution-cell"><div class="execution-entry status-notrun"><span class="exec-icon">â—¯</span> <span class="exec-time">NOT RUN</span> <span class="log-button-disabled">ðŸ“„ N/A</span></div></td>'
                
                row_html += '</tr>'
                table_rows.append(row_html)
            
            table_html = f"""
            <table class="flow-execution-table">
                <thead>
                    {table_header}
                </thead>
                <tbody>
                    {"".join(table_rows)}
                </tbody>
            </table>
            """
            
            # Legend
            legend_html = """
            <div class="flow-legend">
                <div class="legend-title">Status Legend:</div>
                <div class="legend-items">
                    <span class="legend-item"><span class="exec-icon status-pass">âœ“</span> PASS</span>
                    <span class="legend-item"><span class="exec-icon status-fail">âœ—</span> FAIL</span>
                    <span class="legend-item"><span class="exec-icon status-running">âŸ³</span> RUNNING</span>
                    <span class="legend-item"><span class="exec-icon status-notrun">â—¯</span> NOT RUN</span>
                        </div>
                </div>
                """
            
            # Combine all sections
            html_content = f"""
            <div class="section-header">IPO Summary</div>
            {summary_html}
            
            <div class="section-header">Execution Timeline - All IPOs</div>
            {table_html}
            
            {legend_html}
            
            <div class="flow-notes">
                <p><strong>Notes:</strong></p>
                <ul>
                    <li>Multiple entries per cell indicate stage re-runs (failures or manual reruns)</li>
                    <li>Click ðŸ“„ icon to view log file in browser</li>
                    <li>Timestamps show start time in MM/DD HH:MM format</li>
                    <li>Durations extracted from .prc.status.json</li>
                            </ul>
                    </div>
            """
            
            return html_content
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            return f"""
            <div class="no-data">
                <p>Error generating Flow Execution content</p>
                <p>Error: {str(e)}</p>
                <pre>{traceback.format_exc()}</pre>
            </div>
            """
    
    def run_recipe_analysis(self) -> None:
        """Run recipe configuration analysis"""
        pass
    
    def _analyze_imported_pnr_data(self, export_dir: str) -> None:
        """Analyze PnR data from export_innovus (ECO/Signoff workflow)"""
        print(f"\n{Color.CYAN}[INFO] ECO/Signoff workarea detected - PnR results imported from external source{Color.RESET}\n")
        
        # Check for PnR artifacts in export_innovus
        def_files = glob.glob(os.path.join(export_dir, "*.def.gz"))
        netlist_lvs_files = glob.glob(os.path.join(export_dir, "*.lvs.gv.gz"))
        netlist_nopower_files = glob.glob(os.path.join(export_dir, "*.nopower.gv.gz"))
        lef_files = glob.glob(os.path.join(export_dir, "*.lef"))
        oas_files = glob.glob(os.path.join(export_dir, "*_fill.oas"))
        multibit_files = glob.glob(os.path.join(export_dir, "*multibitMapping.gz"))
        
        # Extract IPO from filenames if available
        detected_ipo = self.design_info.ipo
        if def_files:
            # Try to extract IPO from filename like ccoreb.ipo1400.def.gz
            import re
            match = re.search(r'\.ipo(\d+)\.', os.path.basename(def_files[0]))
            if match:
                detected_ipo = f"ipo{match.group(1)}"
        
        # Helper function to format file size
        def format_size(filepath):
            try:
                size = os.path.getsize(filepath)
                for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
                    if size < 1024.0:
                        return f"{size:.1f} {unit}"
                    size /= 1024.0
                return f"{size:.1f} PB"
            except:
                return "Unknown"
        
        # Print imported PnR data
        print(f"{Color.CYAN}Imported PnR Data (export/export_innovus/):{Color.RESET}")
        if detected_ipo != "unknown":
            print(f"  {Color.GREEN}IPO: {detected_ipo}{Color.RESET}")
        
        if def_files:
            size = format_size(def_files[0])
            print(f"  {Color.GREEN}[OK] Physical Design (DEF):{Color.RESET}     {size:<12} ({os.path.basename(def_files[0])})")
        else:
            print(f"  {Color.YELLOW}[X] Physical Design (DEF):{Color.RESET}     Not found")
        
        if oas_files:
            size = format_size(oas_files[0])
            print(f"  {Color.GREEN}[OK] Layout (GDS/OAS):{Color.RESET}          {size:<12} ({os.path.basename(oas_files[0])})")
        else:
            print(f"  {Color.YELLOW}[X] Layout (GDS/OAS):{Color.RESET}          Not found")
        
        if netlist_lvs_files:
            size = format_size(netlist_lvs_files[0])
            print(f"  {Color.GREEN}[OK] Netlist (LVS):{Color.RESET}            {size:<12} ({os.path.basename(netlist_lvs_files[0])})")
        else:
            print(f"  {Color.YELLOW}[X] Netlist (LVS):{Color.RESET}            Not found")
        
        if netlist_nopower_files:
            size = format_size(netlist_nopower_files[0])
            print(f"  {Color.GREEN}[OK] Netlist (no-power):{Color.RESET}       {size:<12} ({os.path.basename(netlist_nopower_files[0])})")
        else:
            print(f"  {Color.YELLOW}[X] Netlist (no-power):{Color.RESET}       Not found")
        
        if lef_files:
            size = format_size(lef_files[0])
            print(f"  {Color.GREEN}[OK] LEF:{Color.RESET}                      {size:<12} ({os.path.basename(lef_files[0])})")
        else:
            print(f"  {Color.YELLOW}[X] LEF:{Color.RESET}                      Not found")
        
        if multibit_files:
            print(f"  {Color.GREEN}[OK] Multibit Mapping:{Color.RESET}         Present")
        else:
            print(f"  {Color.YELLOW}[X] Multibit Mapping:{Color.RESET}         Not found")
        
        # Check for signoff_flow activities
        signoff_flow_dir = os.path.join(self.workarea, "signoff_flow")
        if os.path.isdir(signoff_flow_dir):
            print(f"\n{Color.CYAN}Signoff/ECO Activities (signoff_flow/):{Color.RESET}")
            
            # Check for specific signoff directories
            signoff_activities = [
                ("auto_pt", "PrimeTime Signoff"),
                ("nv_gate_eco", "NV Gate ECO"),
                ("gl-check", "Gate-Level Checks"),
                ("gl-check_eco", "Gate-Level Checks (ECO)"),
                ("eco_vs_pnr_fm", "Formal Verification (ECO vs PnR)"),
                ("gen_eco_netlist_innovus", "ECO Netlist Generation"),
            ]
            
            found_any = False
            for dir_name, description in signoff_activities:
                full_path = os.path.join(signoff_flow_dir, dir_name)
                if os.path.isdir(full_path):
                    print(f"  {Color.GREEN}[OK] {description}:{Color.RESET}".ljust(60) + f"{dir_name}/ (present)")
                    found_any = True
            
            if not found_any:
                print(f"  {Color.YELLOW}No signoff activities detected{Color.RESET}")
        
        # Informative note
        print(f"\n{Color.YELLOW}Note:{Color.RESET} PnR flow configuration files (prc, runset, beflow_config) not present")
        print(f"      This is {Color.GREEN}expected{Color.RESET} for ECO/Signoff workflow - workarea uses imported PnR results")
        print(f"      from external source and focuses on signoff/ECO activities.")
    
    def _get_clock_tree_html_content(self) -> str:
        """Generate HTML content for Clock Tree tab with summary and tap details
        
        Returns:
            HTML string for clock tree section
        """
        # Get clock tree data
        clock_data = self._extract_clock_tree_report()
        
        if not clock_data:
            return """
            <div class="no-data">
                <p>No Clock Tree data available</p>
                <p>Clock tree cell count report not found or could not be parsed.</p>
            </div>
            """
        
        summary = clock_data['summary']
        tap_details = clock_data['tap_details']
        tap_counts = clock_data['tap_counts']
        stage = clock_data['stage'].upper()
        
        # Build summary table
        summary_rows = []
        for clock in sorted(summary.keys()):
            tree = summary[clock]['tree_cells']
            sinks = summary[clock]['clock_sinks']
            taps = tap_counts.get(clock, 0)
            
            # Format numbers with commas
            try:
                buf = f"{int(tree['buffer']):,}" if tree['buffer'].strip() else "0"
                inv = f"{int(tree['inverter']):,}" if tree['inverter'].strip() else "0"
                combo = f"{int(tree['combo']):,}" if tree['combo'].strip() else "0"
                cg = f"{int(tree['clock_gate']):,}" if tree['clock_gate'].strip() else "0"
                tree_tot = f"{int(tree['total']):,}" if tree['total'].strip() else "0"
                ff = f"{int(sinks['flop']):,}" if sinks['flop'].strip() else "0"
                latch = f"{int(sinks['latch']):,}" if sinks['latch'].strip() else "0"
                macro = f"{int(sinks['hard_macro']):,}" if sinks['hard_macro'].strip() else "0"
                sink_tot = f"{int(sinks['total']):,}" if sinks['total'].strip() else "0"
            except:
                buf = tree['buffer'] or "0"
                inv = tree['inverter'] or "0"
                combo = tree['combo'] or "0"
                cg = tree['clock_gate'] or "0"
                tree_tot = tree['total'] or "0"
                ff = sinks['flop'] or "0"
                latch = sinks['latch'] or "0"
                macro = sinks['hard_macro'] or "0"
                sink_tot = sinks['total'] or "0"
            
            taps_display = f"{taps}" if taps > 0 else "N/A"
            taps_class = "has-taps" if taps > 0 else "no-taps"
            
            summary_rows.append(f"""
            <tr>
                <td class="clock-name">{clock}</td>
                <td>{buf}</td>
                <td>{inv}</td>
                <td>{combo}</td>
                <td>{cg}</td>
                <td class="total-col">{tree_tot}</td>
                <td>{ff}</td>
                <td>{latch}</td>
                <td>{macro}</td>
                <td class="total-col">{sink_tot}</td>
                <td class="{taps_class}">{taps_display}</td>
            </tr>
            """)
        
        summary_table_html = f"""
        <div class="section-card">
            <h2>Clock Tree Summary ({stage})</h2>
            <table class="data-table">
                <thead>
                    <tr>
                        <th rowspan="2" class="clock-col">Clock</th>
                        <th colspan="5" class="section-header">Clock Tree Cells</th>
                        <th colspan="4" class="section-header">Clock Sinks</th>
                        <th rowspan="2">Taps</th>
                    </tr>
                    <tr>
                        <th>Buffer</th>
                        <th>Inverter</th>
                        <th>Combo</th>
                        <th>Clock Gate</th>
                        <th class="total-col">Total</th>
                        <th>Flop</th>
                        <th>Latch</th>
                        <th>Hard Macro</th>
                        <th class="total-col">Total</th>
                    </tr>
                </thead>
                <tbody>
                    {''.join(summary_rows)}
                </tbody>
            </table>
        </div>
        """
        
        # Build tap details sections (grouped by clock)
        tap_sections = []
        
        # Group taps by clock
        taps_by_clock = {}
        for tap in tap_details:
            clock = tap['clock']
            if clock not in taps_by_clock:
                taps_by_clock[clock] = []
            taps_by_clock[clock].append(tap)
        
        # Sort taps within each clock by total sinks (descending)
        for clock in taps_by_clock:
            taps_by_clock[clock].sort(key=lambda x: int(x['clock_sinks']['total']) if x['clock_sinks']['total'].strip() else 0, reverse=True)
        
        # Generate HTML for each clock's taps
        for clock in sorted(taps_by_clock.keys()):
            taps = taps_by_clock[clock]
            tap_rows = []
            
            for tap in taps:
                tree = tap['tree_cells']
                sinks = tap['clock_sinks']
                tap_name = tap['tap_name']
                
                # Truncate long tap names for display
                tap_display = tap_name if len(tap_name) <= 60 else tap_name[:57] + "..."
                
                try:
                    buf = f"{int(tree['buffer']):,}" if tree['buffer'].strip() else "0"
                    inv = f"{int(tree['inverter']):,}" if tree['inverter'].strip() else "0"
                    combo = f"{int(tree['combo']):,}" if tree['combo'].strip() else "0"
                    cg = f"{int(tree['clock_gate']):,}" if tree['clock_gate'].strip() else "0"
                    tree_tot = f"{int(tree['total']):,}" if tree['total'].strip() else "0"
                    ff = f"{int(sinks['flop']):,}" if sinks['flop'].strip() else "0"
                    latch = f"{int(sinks['latch']):,}" if sinks['latch'].strip() else "0"
                    macro = f"{int(sinks['hard_macro']):,}" if sinks['hard_macro'].strip() else "0"
                    sink_tot = f"{int(sinks['total']):,}" if sinks['total'].strip() else "0"
                except:
                    buf = tree['buffer'] or "0"
                    inv = tree['inverter'] or "0"
                    combo = tree['combo'] or "0"
                    cg = tree['clock_gate'] or "0"
                    tree_tot = tree['total'] or "0"
                    ff = sinks['flop'] or "0"
                    latch = sinks['latch'] or "0"
                    macro = sinks['hard_macro'] or "0"
                    sink_tot = sinks['total'] or "0"
                
                tap_rows.append(f"""
                <tr title="{tap_name}">
                    <td class="tap-name">{tap_display}</td>
                    <td>{buf}</td>
                    <td>{inv}</td>
                    <td>{combo}</td>
                    <td>{cg}</td>
                    <td class="total-col">{tree_tot}</td>
                    <td>{ff}</td>
                    <td>{latch}</td>
                    <td>{macro}</td>
                    <td class="total-col">{sink_tot}</td>
                </tr>
                """)
            
            tap_sections.append(f"""
            <div class="section-card collapsible">
                <div class="section-header-clickable" onclick="toggleSection(this)">
                    <span class="arrow">â–¶</span>
                    <h3>{clock} - Tap Details ({len(taps)} taps)</h3>
                </div>
                <div class="section-content" style="display: none;">
                    <table class="data-table tap-table">
                        <thead>
                            <tr>
                                <th class="tap-col">Tap Name</th>
                                <th>Buffer</th>
                                <th>Inverter</th>
                                <th>Combo</th>
                                <th>Clock Gate</th>
                                <th class="total-col">Total</th>
                                <th>Flop</th>
                                <th>Latch</th>
                                <th>Hard Macro</th>
                                <th class="total-col">Total</th>
                            </tr>
                        </thead>
                        <tbody>
                            {''.join(tap_rows)}
                        </tbody>
                    </table>
                </div>
            </div>
            """)
        
        # CSS styles for clock tree section
        styles = """
        <style>
            .section-card {
                background: white;
                border-radius: 8px;
                padding: 20px;
                margin-bottom: 20px;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            }
            
            .section-card h2 {
                margin-top: 0;
                color: #1e3c72;
                font-size: 24px;
                margin-bottom: 20px;
            }
            
            .section-card h3 {
                margin: 0;
                color: #1e3c72;
                font-size: 18px;
                display: inline-block;
            }
            
            .collapsible .section-header-clickable {
                cursor: pointer;
                user-select: none;
                padding: 10px;
                background: #f0f4f8;
                border-radius: 6px;
                transition: background 0.3s;
            }
            
            .collapsible .section-header-clickable:hover {
                background: #e0e8f0;
            }
            
            .collapsible .arrow {
                display: inline-block;
                transition: transform 0.3s;
                margin-right: 10px;
                color: #667eea;
                font-size: 14px;
            }
            
            .collapsible .arrow.expanded {
                transform: rotate(90deg);
            }
            
            .section-content {
                margin-top: 15px;
            }
            
            .data-table {
                width: 100%;
                border-collapse: collapse;
                margin-top: 10px;
            }
            
            .data-table th {
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: white;
                padding: 12px 8px;
                text-align: center;
                font-weight: 600;
                font-size: 13px;
            }
            
            .data-table th.section-header {
                background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            }
            
            .data-table td {
                padding: 10px 8px;
                text-align: center;
                border-bottom: 1px solid #e0e0e0;
                font-size: 13px;
            }
            
            .data-table tr:hover {
                background-color: #f5f9ff;
            }
            
            .clock-name {
                font-weight: 600;
                color: #1e3c72;
                text-align: left !important;
            }
            
            .clock-col {
                text-align: left !important;
            }
            
            .tap-name {
                text-align: left !important;
                font-family: 'Courier New', monospace;
                font-size: 12px;
                max-width: 400px;
                overflow: hidden;
                text-overflow: ellipsis;
                white-space: nowrap;
            }
            
            .tap-col {
                text-align: left !important;
                min-width: 300px;
            }
            
            .total-col {
                font-weight: 600;
                background-color: #f0f4f8;
            }
            
            .has-taps {
                color: #27ae60;
                font-weight: 600;
            }
            
            .no-taps {
                color: #95a5a6;
            }
            
            .no-data {
                text-align: center;
                padding: 60px 20px;
                color: #7f8c8d;
            }
            
            .no-data p:first-child {
                font-size: 20px;
                font-weight: 600;
                margin-bottom: 10px;
            }
        </style>
        """
        
        # JavaScript for collapsible sections
        scripts = """
        <script>
            function toggleSection(header) {
                const arrow = header.querySelector('.arrow');
                const content = header.nextElementSibling;
                
                if (content.style.display === 'none') {
                    content.style.display = 'block';
                    arrow.classList.add('expanded');
                } else {
                    content.style.display = 'none';
                    arrow.classList.remove('expanded');
                }
            }
        </script>
        """
        
        # Build Skew Groups section
        skew_groups = clock_data.get('skew_groups', [])
        skew_groups_html = ""
        
        if skew_groups:
            skew_rows = []
            for group in skew_groups:
                tree = group['tree_cells']
                sinks = group['clock_sinks']
                skew_group_name = group['skew_group']
                
                try:
                    buf = f"{int(tree['buffer']):,}" if tree['buffer'].strip() else "0"
                    inv = f"{int(tree['inverter']):,}" if tree['inverter'].strip() else "0"
                    combo = f"{int(tree['combo']):,}" if tree['combo'].strip() else "0"
                    cg = f"{int(tree['clock_gate']):,}" if tree['clock_gate'].strip() else "0"
                    tree_tot = f"{int(tree['total']):,}" if tree['total'].strip() else "0"
                    ff = f"{int(sinks['flop']):,}" if sinks['flop'].strip() else "0"
                    latch = f"{int(sinks['latch']):,}" if sinks['latch'].strip() else "0"
                    macro = f"{int(sinks['hard_macro']):,}" if sinks['hard_macro'].strip() else "0"
                    sink_tot = f"{int(sinks['total']):,}" if sinks['total'].strip() else "0"
                except:
                    buf = tree['buffer'] or "0"
                    inv = tree['inverter'] or "0"
                    combo = tree['combo'] or "0"
                    cg = tree['clock_gate'] or "0"
                    tree_tot = tree['total'] or "0"
                    ff = sinks['flop'] or "0"
                    latch = sinks['latch'] or "0"
                    macro = sinks['hard_macro'] or "0"
                    sink_tot = sinks['total'] or "0"
                
                skew_rows.append(f"""
                <tr>
                    <td class="skew-group-name">{skew_group_name}</td>
                    <td>{buf}</td>
                    <td>{inv}</td>
                    <td>{combo}</td>
                    <td>{cg}</td>
                    <td class="total-col">{tree_tot}</td>
                    <td>{ff}</td>
                    <td>{latch}</td>
                    <td>{macro}</td>
                    <td class="total-col">{sink_tot}</td>
                </tr>
                """)
            
            skew_groups_html = f"""
            <div class="section-card collapsible">
                <div class="section-header-clickable" onclick="toggleSection(this)">
                    <span class="arrow">â–¶</span>
                    <h3>Skew Groups & Scenarios ({len(skew_groups)} groups)</h3>
                </div>
                <div class="section-content" style="display: none;">
                    <table class="data-table">
                        <thead>
                            <tr>
                                <th class="skew-group-col">Skew Group / Scenario</th>
                                <th>Buffer</th>
                                <th>Inverter</th>
                                <th>Combo</th>
                                <th>Clock Gate</th>
                                <th class="total-col">Total</th>
                                <th>Flop</th>
                                <th>Latch</th>
                                <th>Hard Macro</th>
                                <th class="total-col">Total</th>
                            </tr>
                        </thead>
                        <tbody>
                            {''.join(skew_rows)}
                        </tbody>
                    </table>
                </div>
            </div>
            """
        
        # Build Clock Sources section
        clock_sources = clock_data.get('clock_sources', [])
        clock_sources_html = ""
        
        if clock_sources:
            source_rows = []
            for source in clock_sources:
                tree = source['tree_cells']
                sinks = source['clock_sinks']
                clock_name = source['clock']
                source_path = source['source_path']
                
                # Truncate very long source paths
                source_display = source_path if len(source_path) <= 80 else source_path[:77] + "..."
                
                try:
                    buf = f"{int(tree['buffer']):,}" if tree['buffer'].strip() else "0"
                    inv = f"{int(tree['inverter']):,}" if tree['inverter'].strip() else "0"
                    combo = f"{int(tree['combo']):,}" if tree['combo'].strip() else "0"
                    cg = f"{int(tree['clock_gate']):,}" if tree['clock_gate'].strip() else "0"
                    tree_tot = f"{int(tree['total']):,}" if tree['total'].strip() else "0"
                    ff = f"{int(sinks['flop']):,}" if sinks['flop'].strip() else "0"
                    latch = f"{int(sinks['latch']):,}" if sinks['latch'].strip() else "0"
                    macro = f"{int(sinks['hard_macro']):,}" if sinks['hard_macro'].strip() else "0"
                    sink_tot = f"{int(sinks['total']):,}" if sinks['total'].strip() else "0"
                except:
                    buf = tree['buffer'] or "0"
                    inv = tree['inverter'] or "0"
                    combo = tree['combo'] or "0"
                    cg = tree['clock_gate'] or "0"
                    tree_tot = tree['total'] or "0"
                    ff = sinks['flop'] or "0"
                    latch = sinks['latch'] or "0"
                    macro = sinks['hard_macro'] or "0"
                    sink_tot = sinks['total'] or "0"
                
                source_rows.append(f"""
                <tr title="{source_path}">
                    <td class="clock-name">{clock_name}</td>
                    <td class="source-path">{source_display}</td>
                    <td>{buf}</td>
                    <td>{inv}</td>
                    <td>{combo}</td>
                    <td>{cg}</td>
                    <td class="total-col">{tree_tot}</td>
                    <td>{ff}</td>
                    <td>{latch}</td>
                    <td>{macro}</td>
                    <td class="total-col">{sink_tot}</td>
                </tr>
                """)
            
            clock_sources_html = f"""
            <div class="section-card collapsible">
                <div class="section-header-clickable" onclick="toggleSection(this)">
                    <span class="arrow">â–¶</span>
                    <h3>Clock Sources ({len(clock_sources)} sources)</h3>
                </div>
                <div class="section-content" style="display: none;">
                    <table class="data-table">
                        <thead>
                            <tr>
                                <th class="clock-col">Clock</th>
                                <th class="source-path-col">Source Path</th>
                                <th>Buffer</th>
                                <th>Inverter</th>
                                <th>Combo</th>
                                <th>Clock Gate</th>
                                <th class="total-col">Total</th>
                                <th>Flop</th>
                                <th>Latch</th>
                                <th>Hard Macro</th>
                                <th class="total-col">Total</th>
                            </tr>
                        </thead>
                        <tbody>
                            {''.join(source_rows)}
                        </tbody>
                    </table>
                </div>
            </div>
            """
        
        # Find and embed clock tree images
        images_html = ""
        
        # Find topology spine image (prioritize postroute, then route, cts, place)
        topology_files = []
        for img_stage in ['postroute', 'route', 'cts', 'place']:
            topology_pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{self.design_info.ipo}/REPs/IMAGES/*.{img_stage}.custom.clock_tree.topology_spine.all_clocks.png"
            files = self.file_utils.find_files(topology_pattern, self.workarea)
            if files:
                topology_files = files
                break
        
        # Find tap endpoint images (multiple files, prioritize same stage)
        tap_endpoint_files = []
        for img_stage in ['postroute', 'route', 'cts', 'place']:
            tap_endpoint_pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{self.design_info.ipo}/REPs/IMAGES/*.{img_stage}.custom.clock_tree.common_tap_endpoint_groups.*.png"
            files = self.file_utils.find_files(tap_endpoint_pattern, self.workarea)
            if files:
                tap_endpoint_files = files
                break
        
        # Find endpoint manhattan distance from tap images (multiple files)
        manhattan_tap_files = []
        manhattan_tap_pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{self.design_info.ipo}/REPs/IMAGES/*.postroute.custom.clock_tree.endpoint_manhattan_distance_from_clock_tap.*.png"
        manhattan_tap_files = self.file_utils.find_files(manhattan_tap_pattern, self.workarea)
        
        # Find endpoint manhattan distance from source images (multiple files)
        manhattan_source_files = []
        manhattan_source_pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{self.design_info.ipo}/REPs/IMAGES/*.postroute.custom.clock_tree.endpoint_manhattan_distance_from_clock_source.*.png"
        manhattan_source_files = self.file_utils.find_files(manhattan_source_pattern, self.workarea)
        
        if topology_files or tap_endpoint_files or manhattan_tap_files or manhattan_source_files:
            image_cards = []
            
            # Add topology spine image
            if topology_files:
                topology_file = topology_files[0]
                try:
                    with open(topology_file, 'rb') as img_file:
                        img_data = base64.b64encode(img_file.read()).decode('utf-8')
                        img_src = f"data:image/png;base64,{img_data}"
                        image_cards.append(f"""
                        <div class="image-card">
                            <h4>Clock Tree Topology - All Clocks</h4>
                            <img src="{img_src}" alt="Clock Tree Topology" onclick="showClockTreeModal(this.src)" style="cursor: zoom-in;">
                            <p class="image-source">{topology_file.replace(self.workarea + '/', '') if self.workarea in topology_file else os.path.basename(topology_file)}</p>
                        </div>
                        """)
                except Exception as e:
                    pass
            
            # Add tap endpoint images
            for tap_file in tap_endpoint_files:
                try:
                    with open(tap_file, 'rb') as img_file:
                        img_data = base64.b64encode(img_file.read()).decode('utf-8')
                        # Extract clock name from filename if possible
                        filename = os.path.basename(tap_file)
                        clock_match = re.search(r'common_tap_endpoint_groups\.(.+?)\.png', filename)
                        clock_label = clock_match.group(1) if clock_match else "Unknown"
                        
                        img_src = f"data:image/png;base64,{img_data}"
                        image_cards.append(f"""
                        <div class="image-card">
                            <h4>Tap Endpoint Groups - {clock_label}</h4>
                            <img src="{img_src}" alt="Tap Endpoint Groups - {clock_label}" onclick="showClockTreeModal(this.src)" style="cursor: zoom-in;">
                            <p class="image-source">{tap_file.replace(self.workarea + '/', '') if self.workarea in tap_file else os.path.basename(tap_file)}</p>
                        </div>
                        """)
                except Exception as e:
                    pass
            
            # Add endpoint manhattan distance from tap images
            for manhattan_file in manhattan_tap_files:
                try:
                    with open(manhattan_file, 'rb') as img_file:
                        img_data = base64.b64encode(img_file.read()).decode('utf-8')
                        # Extract clock name from filename
                        filename = os.path.basename(manhattan_file)
                        clock_match = re.search(r'endpoint_manhattan_distance_from_clock_tap\.(.+?)\.png', filename)
                        clock_label = clock_match.group(1) if clock_match else "Unknown"
                        
                        img_src = f"data:image/png;base64,{img_data}"
                        image_cards.append(f"""
                        <div class="image-card">
                            <h4>Manhattan Distance from Tap - {clock_label}</h4>
                            <img src="{img_src}" alt="Manhattan Distance from Tap - {clock_label}" onclick="showClockTreeModal(this.src)" style="cursor: zoom-in;">
                            <p class="image-source">{manhattan_file.replace(self.workarea + '/', '') if self.workarea in manhattan_file else os.path.basename(manhattan_file)}</p>
                        </div>
                        """)
                except Exception as e:
                    pass
            
            # Add endpoint manhattan distance from source images
            for manhattan_file in manhattan_source_files:
                try:
                    with open(manhattan_file, 'rb') as img_file:
                        img_data = base64.b64encode(img_file.read()).decode('utf-8')
                        # Extract clock name from filename
                        filename = os.path.basename(manhattan_file)
                        clock_match = re.search(r'endpoint_manhattan_distance_from_clock_source\.(.+?)\.png', filename)
                        clock_label = clock_match.group(1) if clock_match else "Unknown"
                        
                        img_src = f"data:image/png;base64,{img_data}"
                        image_cards.append(f"""
                        <div class="image-card">
                            <h4>Manhattan Distance from Source - {clock_label}</h4>
                            <img src="{img_src}" alt="Manhattan Distance from Source - {clock_label}" onclick="showClockTreeModal(this.src)" style="cursor: zoom-in;">
                            <p class="image-source">{manhattan_file.replace(self.workarea + '/', '') if self.workarea in manhattan_file else os.path.basename(manhattan_file)}</p>
                        </div>
                        """)
                except Exception as e:
                    pass
            
            if image_cards:
                images_html = f"""
                <div class="section-card collapsible">
                    <div class="section-header-clickable" onclick="toggleSection(this)">
                        <span class="arrow">â–¶</span>
                        <h3>Clock Tree Visualization ({len(image_cards)} images)</h3>
                    </div>
                    <div class="section-content" style="display: none;">
                        <div class="images-grid">
                            {''.join(image_cards)}
                        </div>
                    </div>
                </div>
                
                <!-- Image Modal -->
                <div id="clockTreeImageModal" class="clock-tree-image-modal" onclick="hideClockTreeImageModal()">
                    <span class="clock-tree-image-modal-close">&times;</span>
                    <img id="clockTreeModalImage" src="">
                </div>
                
                <style>
                    .images-grid {{
                        display: grid;
                        grid-template-columns: repeat(auto-fit, minmax(500px, 1fr));
                        gap: 20px;
                        margin-top: 15px;
                    }}
                    
                    .image-card {{
                        background: #f8f9fa;
                        border-radius: 8px;
                        padding: 15px;
                        text-align: center;
                    }}
                    
                    .image-card h4 {{
                        margin-top: 0;
                        color: #1e3c72;
                        font-size: 16px;
                    }}
                    
                    .image-card img {{
                        max-width: 100%;
                        height: auto;
                        border-radius: 4px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                        transition: transform 0.2s;
                    }}
                    
                    .image-card img:hover {{
                        transform: scale(1.02);
                    }}
                    
                    .image-source {{
                        font-size: 11px;
                        color: #7f8c8d;
                        margin-top: 8px;
                        font-family: monospace;
                    }}
                    
                    .clock-tree-image-modal {{
                        display: none;
                        position: fixed;
                        z-index: 10000;
                        left: 0;
                        top: 0;
                        width: 100%;
                        height: 100%;
                        background-color: rgba(0,0,0,0.9);
                        justify-content: center;
                        align-items: center;
                    }}
                    
                    .clock-tree-image-modal.active {{
                        display: flex;
                    }}
                    
                    .clock-tree-image-modal img {{
                        max-width: 90%;
                        max-height: 90%;
                        border-radius: 10px;
                    }}
                    
                    .clock-tree-image-modal-close {{
                        position: absolute;
                        top: 20px;
                        right: 35px;
                        color: #f1f1f1;
                        font-size: 40px;
                        font-weight: bold;
                        cursor: pointer;
                    }}
                    
                    .clock-tree-image-modal-close:hover {{
                        color: #bbb;
                    }}
                    
                    .skew-group-name {{
                        text-align: left !important;
                        font-family: 'Courier New', monospace;
                        font-size: 12px;
                    }}
                    
                    .skew-group-col {{
                        text-align: left !important;
                        min-width: 300px;
                    }}
                    
                    .source-path {{
                        text-align: left !important;
                        font-family: 'Courier New', monospace;
                        font-size: 11px;
                        max-width: 500px;
                        overflow: hidden;
                        text-overflow: ellipsis;
                        white-space: nowrap;
                    }}
                    
                    .source-path-col {{
                        text-align: left !important;
                        min-width: 400px;
                    }}
                </style>
                
                <script>
                    function showClockTreeModal(src) {{
                        document.getElementById('clockTreeModalImage').src = src;
                        document.getElementById('clockTreeImageModal').classList.add('active');
                    }}
                    
                    function hideClockTreeImageModal() {{
                        document.getElementById('clockTreeImageModal').classList.remove('active');
                    }}
                </script>
                """
        
        # Combine all sections
        return styles + summary_table_html + ''.join(tap_sections) + skew_groups_html + clock_sources_html + images_html + scripts
    
    def run_pnr_analysis(self) -> None:
        """Run comprehensive PnR (Place & Route) analysis"""
        self.print_header(FlowStage.PNR_ANALYSIS)
        
        # Check if this is ECO/Signoff workarea with imported PnR data
        export_innovus_dir = os.path.join(self.workarea, "export/export_innovus")
        nv_flow_dir = os.path.join(self.workarea, "pnr_flow/nv_flow")
        is_eco_workarea = (not os.path.isdir(nv_flow_dir) and os.path.isdir(export_innovus_dir))
        
        if is_eco_workarea:
            # Check for PnR artifacts to confirm this is ECO/Signoff workarea
            def_files = glob.glob(os.path.join(export_innovus_dir, "*.def.gz"))
            netlist_files = glob.glob(os.path.join(export_innovus_dir, "*.gv.gz"))
            
            if def_files or netlist_files:
                print(f"  {Color.CYAN}[WARN] IPO directory '{self.design_info.ipo}' from .prc file not found{Color.RESET}")
                print(f"    {Color.YELLOW}Note: Users sometimes delete IPO directories to save disk space{Color.RESET}")
                self._analyze_imported_pnr_data(export_innovus_dir)
                return  # Exit PnR analysis - ECO workarea handles differently
        
        # Resolve IPO directory (may have been deleted to save disk space)
        resolved_ipo = self._resolve_ipo_directory(self.design_info.top_hier, self.design_info.ipo)
        if resolved_ipo != self.design_info.ipo:
            # Update design_info with resolved IPO for this analysis
            original_ipo = self.design_info.ipo
            self.design_info.ipo = resolved_ipo
            print(f"  {Color.CYAN}[INFO] IPO resolved: {original_ipo} -> {resolved_ipo}{Color.RESET}\n")
        
        # PnR Status
        prc_status = os.path.join(self.workarea, f"pnr_flow/nv_flow/{self.design_info.top_hier}.prc.status")
        if self.file_utils.file_exists(prc_status):
            self.print_file_info(prc_status, "PnR Status")
            self._analyze_pnr_status(prc_status)
        
        # PnR Configuration
        prc_file = os.path.join(self.workarea, f"pnr_flow/nv_flow/{self.design_info.top_hier}.prc")
        if self.print_file_info(prc_file, "PnR Configuration"):
            self._extract_prc_configuration(prc_file)
        
        # Verify TCL files are used in PnR configuration
        common_dir = os.path.join(self.workarea, "pnr_flow/nv_flow/COMMON")
        if os.path.exists(prc_file) and os.path.exists(common_dir):
            self._verify_tcl_usage_in_prc(prc_file, common_dir)
        
        # Extract unified flow configuration (runset.tcl + beflow_config.yaml)
        runset_file = os.path.join(self.workarea, f"pnr_flow/nv_flow/{self.design_info.top_hier}/{self.design_info.ipo}/runset.tcl")
        beflow_config = os.path.join(self.workarea, f"pnr_flow/nv_flow/{self.design_info.top_hier}/{self.design_info.ipo}/beflow_config.yaml")
        
        # Check if either file exists
        if os.path.exists(runset_file) or os.path.exists(beflow_config):
            if os.path.exists(runset_file):
                self.print_file_info(runset_file, "PnR Runset")
            if os.path.exists(beflow_config):
                self.print_file_info(beflow_config, f"PnR BeFlow Configuration ({self.design_info.ipo})")
            self._extract_unified_flow_configuration(runset_file, beflow_config)
        
        # Check BE_OVERRIDE_TOOLVERS consistency across PnR stages
        self._check_be_override_toolvers_consistency()
        
        # Summary reports
        summary_pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/ipo*/REPs/SUMMARY/*route.nbu_summary.rpt*"
        summary_files = self.file_utils.find_files(summary_pattern, self.workarea)
        
        if summary_files:
            self.print_file_info(summary_files[0], "Route Summary")
            matches = self.file_utils.grep_file(r"diearea|CellCount|SequentialCount|VT|Shorts", summary_files[0])
            for match in matches:
                print(f"  {match}")
        
        # Data reports - try postroute first, then fallback to earlier stages
        temperature_corners = ['0c_0p6v', '125c_0p6v', '25c_0p6v', '85c_0p6v']
        pnr_stages = ['postroute', 'route', 'cts', 'place', 'plan']
        data_files = []
        found_stage = None
        
        # Try each stage in order
        for stage in pnr_stages:
            for temp_corner in temperature_corners:
                # Use wildcard for IPO in filename since it can differ from directory name (e.g., ipo1000 dir with ipo1400 in filenames)
                data_pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{self.design_info.ipo}/reports/{self.design_info.top_hier}_{self.design_info.top_hier}_*_report_{self.design_info.top_hier}_*_{stage}.func.std_tt_{temp_corner}.setup.typical.data"
                found_files = self.file_utils.find_files(data_pattern, self.workarea)
                if found_files:
                    data_files = found_files
                    found_stage = stage
                    break
            if data_files:
                break
        
        all_params = {}
        if data_files:
            if found_stage == 'postroute':
                self.print_file_info(data_files[0], "Post-Route Data")
            else:
                self.print_file_info(data_files[0], f"PnR Data ({found_stage.upper()} stage)")
                print(f"    {Color.YELLOW}Note: Post-route data not available, showing {found_stage.upper()} stage data instead{Color.RESET}")
            
            all_params = self._extract_postroute_data_parameters(data_files[0], found_stage)
        else:
            print(f"  {Color.YELLOW}PnR Data: No data files found for any stage or temperature corner{Color.RESET}")
            print(f"    {Color.YELLOW}Tried stages: {', '.join(pnr_stages)}{Color.RESET}")
            print(f"    {Color.YELLOW}Tried corners: {', '.join(temperature_corners)}{Color.RESET}")
            print(f"    {Color.YELLOW}This could be due to:{Color.RESET}")
            print(f"      - Flow still running (data files not yet generated)")
            print(f"      - Flow failed at this stage")
            print(f"      - Different temperature corner used")
            print(f"      - File permissions issue")
        
        # Power summary
        power_pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/ipo*/REPs/SUMMARY/*.postroute.power.rpt*"
        power_files = self.file_utils.find_files(power_pattern, self.workarea)
        
        if power_files:
            self.print_file_info(power_files[0], "Power Summary")
            self._extract_power_summary_table(power_files[0])
        
        # PnR Timing Histogram
        self._extract_pnr_timing_histogram()
        
        # Max Transition Violations
        self._extract_max_transition_violations()
        
        # Clock Tree Cell Count Summary
        clock_tree_data = self._extract_clock_tree_report()
        if clock_tree_data:
            self._print_clock_tree_summary(clock_tree_data)
        
        # Generate unified PnR HTML report combining all sections
        print(f"\n{Color.CYAN}Generating Unified PnR HTML Report...{Color.RESET}")
        unified_pnr_html = self._generate_unified_pnr_html()
        
        # Collect metrics for dashboard summary
        status = "NOT_RUN"
        key_metrics = {"Design": self.design_info.top_hier, "IPO": self.design_info.ipo}
        issues = []
        priority = 2
        
        # Check if we have PnR data
        if data_files and all_params:
            status = "PASS"
            
            # Extract key metrics
            if 'EffictiveUtilization' in all_params:
                util = all_params['EffictiveUtilization']
                key_metrics["Utilization"] = util
                # WARN if utilization > 85%, FAIL if > 95%
                try:
                    util_float = float(util.strip('%'))
                    if util_float > 95:
                        status = "FAIL"
                        issues.append(f"Utilization extremely high: {util}")
                    elif util_float > 85:
                        status = "WARN"
                        issues.append(f"Utilization high: {util}")
                except ValueError:
                    pass
            
            if 'CellCount' in all_params:
                key_metrics["Cells"] = all_params['CellCount']
            
            # Check for timing violations in PnR data
            for param in ['w1_clk_WNS', 'i1_clk_WNS', 'i2_clk_WNS']:
                if param in all_params:
                    try:
                        wns = float(all_params[param])
                        if wns < 0:
                            if status == "PASS":
                                status = "WARN"
                            clock_name = param.replace('_WNS', '')
                            issues.append(f"{clock_name} WNS negative: {wns:.3f} ns")
                    except ValueError:
                        pass
        elif prc_status and self.file_utils.file_exists(prc_status):
            # PnR exists but no data files yet - possibly still running
            status = "WARN"
            issues.append("PnR data files not found - flow may still be running")
        
        # Add section summary for master dashboard
        self._add_section_summary(
            section_name="Place & Route (PnR)",
            section_id="pnr",
            stage=FlowStage.PNR_ANALYSIS,
            status=status,
            key_metrics=key_metrics,
            html_file=unified_pnr_html if unified_pnr_html else "",
            priority=priority,
            issues=issues,
            icon="[PnR]"
        )
    
    def _generate_unified_pnr_html(self) -> Optional[str]:
        """Generate unified PnR HTML report combining PnR Data, Timing Histogram, and Images
        
        Returns:
            HTML filename if generated successfully, None otherwise
        """
        try:
            # Generate timestamp for filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            html_filename = f"{self.design_info.top_hier}_{os.environ.get('USER', 'avice')}_PnR_comprehensive_{self.design_info.ipo}_{timestamp}.html"
            html_path = os.path.join(os.getcwd(), html_filename)
            
            # Read and encode logo
            logo_data = ""
            logo_path = os.path.join(os.path.dirname(__file__), "assets/images/avice_logo.png")
            if os.path.exists(logo_path):
                with open(logo_path, "rb") as logo_file:
                    logo_data = base64.b64encode(logo_file.read()).decode('utf-8')
            
            # Collect PnR Data content
            pnr_data_content = self._get_pnr_data_html_content()
            
            # Collect Timing Histogram content
            timing_histogram_content = self._get_timing_histogram_html_content()
            
            # Collect Images content  
            images_content = self._get_images_html_content()
            
            # Collect Tool Version Configuration content
            toolvers_config_content = self._get_toolvers_config_html_content()
            
            # Collect Clock Tree content
            clock_tree_content = self._get_clock_tree_html_content()
            
            # Generate unified HTML
            html_content = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PnR Comprehensive Report - {self.design_info.top_hier}</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f5f5f5;
        }}
        
        .header {{
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            padding: 30px;
            display: grid;
            grid-template-columns: auto 1fr;
            gap: 20px;
            align-items: center;
        }}
        
        .logo {{
            width: 80px;
            height: 80px;
            border-radius: 10px;
            background: white;
            padding: 10px;
            cursor: pointer;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }}
        
        .logo:hover {{
            transform: scale(1.05);
            box-shadow: 0 8px 16px rgba(0,0,0,0.3);
        }}
        
        .header-text h1 {{
            font-size: 32px;
            margin: 0 0 8px 0;
        }}
        
        .header-text p {{
            opacity: 0.9;
            font-size: 14px;
            margin: 4px 0;
        }}
        
        .logo-modal {{
            display: none;
            position: fixed;
            z-index: 9999;
            left: 0;
            top: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0,0,0,0.9);
            justify-content: center;
            align-items: center;
        }}
        
        .logo-modal.active {{
            display: flex;
        }}
        
        .logo-modal-content {{
            max-width: 90%;
            max-height: 90%;
            border-radius: 10px;
        }}
        
        .logo-modal-close {{
            position: absolute;
            top: 20px;
            right: 35px;
            color: #f1f1f1;
            font-size: 40px;
            font-weight: bold;
            cursor: pointer;
        }}
        
        .logo-modal-close:hover {{
            color: #bbb;
        }}
        
        .container {{
            max-width: 98%;
            margin: 20px auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        
        /* Tab Navigation */
        .tab-navigation {{
            display: flex;
            background: #34495e;
            border-bottom: 3px solid #2c3e50;
        }}
        
        .tab-btn {{
            flex: 1;
            padding: 18px 20px;
            background: #34495e;
            color: white;
            border: none;
            cursor: pointer;
            font-size: 16px;
            font-weight: 600;
            transition: all 0.3s ease;
            border-right: 1px solid #2c3e50;
        }}
        
        .tab-btn:last-child {{
            border-right: none;
        }}
        
        .tab-btn:hover {{
            background: #2c3e50;
        }}
        
        .tab-btn.active {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
        }}
        
        .tab-content {{
            display: none;
            padding: 30px;
            animation: fadeIn 0.3s ease;
        }}
        
        .tab-content.active {{
            display: block;
        }}
        
        @keyframes fadeIn {{
            from {{ opacity: 0; transform: translateY(-10px); }}
            to {{ opacity: 1; transform: translateY(0); }}
        }}
        
        /* Section Headers */
        .section-header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 15px 25px;
            border-radius: 8px;
            margin: 20px 0;
            font-size: 20px;
            font-weight: bold;
        }}
        
        .no-data {{
            text-align: center;
            padding: 60px 20px;
            color: #999;
            font-size: 18px;
            font-style: italic;
        }}
        
        /* Override nested styles to work within tabs */
        .tab-content table {{
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }}
        
        .tab-content th,
        .tab-content td {{
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }}
        
        .tab-content th {{
            background-color: #34495e;
            color: white;
        }}
        
        .tab-content tr:nth-child(even) {{
            background-color: #f9f9f9;
        }}
        
        /* Collapsible Category Styles */
        .category-section {{
            margin: 20px 0;
        }}
        
        .category-header {{
            background-color: #34495e;
            color: white;
            padding: 15px 20px;
            margin: 5px 0;
            cursor: pointer;
            border-radius: 5px;
            font-weight: bold;
            user-select: none;
            transition: background-color 0.3s ease;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }}
        
        .category-header:hover {{
            background-color: #2c3e50;
        }}
        
        .category-content {{
            display: none;
            margin-bottom: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
            overflow: hidden;
            animation: slideDown 0.3s ease;
        }}
        
        @keyframes slideDown {{
            from {{ opacity: 0; max-height: 0; }}
            to {{ opacity: 1; max-height: 2000px; }}
        }}
        
        .category-table {{
            width: 100%;
            border-collapse: collapse;
            margin: 0;
        }}
        
        .category-table th,
        .category-table td {{
            border: 1px solid #ddd;
            padding: 10px;
            text-align: left;
        }}
        
        .category-table th {{
            background-color: #34495e;
            color: white;
            font-weight: bold;
        }}
        
        .category-table tr:nth-child(even) {{
            background-color: #f9f9f9;
        }}
        
        .expand-icon {{
            transition: transform 0.3s ease;
            font-size: 14px;
        }}
        
        /* Environment Details Table - Special handling for long text */
        #content-metadata td {{
            max-width: 300px;
            word-wrap: break-word;
            word-break: break-all;
            overflow-wrap: break-word;
            white-space: normal;
        }}
        
        #content-metadata td:first-child {{
            max-width: 120px;
            font-weight: bold;
        }}
        
        /* Copyright Footer */
        .footer {{
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            text-align: center;
            padding: 20px;
            margin-top: 40px;
            border-radius: 10px;
            font-size: 14px;
        }}
        
        .footer p {{
            margin: 5px 0;
        }}
        
        .footer strong {{
            color: #00ff00;
        }}
        
        /* Flow Execution Tab Styles */
        .flow-summary-container {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }}
        
        .ipo-summary-card {{
            background: white;
            border-radius: 10px;
            padding: 20px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
            border-left: 5px solid #667eea;
        }}
        
        .ipo-summary-card h3 {{
            margin: 0 0 15px 0;
            font-size: 20px;
            color: #333;
            display: flex;
            align-items: center;
            gap: 10px;
        }}
        
        .ipo-status-badge {{
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 12px;
            font-weight: bold;
        }}
        
        .ipo-status-badge.status-pass {{
            background: #27ae60;
            color: white;
        }}
        
        .ipo-status-badge.status-fail {{
            background: #e74c3c;
            color: white;
        }}
        
        .ipo-status-badge.status-running {{
            background: #f39c12;
            color: white;
        }}
        
        .ipo-status-badge.status-notrun {{
            background: #95a5a6;
            color: white;
        }}
        
        .ipo-stats {{
            display: flex;
            flex-direction: column;
            gap: 10px;
        }}
        
        .ipo-stat {{
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 8px 0;
            border-bottom: 1px solid #eee;
        }}
        
        .ipo-stat:last-child {{
            border-bottom: none;
        }}
        
        .ipo-stat .stat-label {{
            color: #666;
            font-size: 14px;
        }}
        
        .ipo-stat .stat-value {{
            font-weight: bold;
            font-size: 16px;
            color: #333;
        }}
        
        .flow-execution-table {{
            width: 100%;
            border-collapse: collapse;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
            margin: 20px 0;
        }}
        
        .flow-execution-table thead {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
        }}
        
        .flow-execution-table th {{
            padding: 15px 12px;
            text-align: left;
            font-weight: 600;
            font-size: 14px;
            border-bottom: 2px solid #ddd;
        }}
        
        .flow-execution-table th.stage-col {{
            width: 150px;
            position: sticky;
            left: 0;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            z-index: 10;
        }}
        
        .flow-execution-table th.ipo-col {{
            text-align: center;
            min-width: 200px;
        }}
        
        .flow-execution-table td {{
            padding: 12px;
            border-bottom: 1px solid #eee;
            vertical-align: top;
        }}
        
        .flow-execution-table td.stage-name {{
            font-weight: 600;
            font-family: 'Courier New', Courier, monospace;
            background: #f8f9fa;
            position: sticky;
            left: 0;
            z-index: 5;
            border-right: 2px solid #ddd;
        }}
        
        .flow-execution-table tr:hover td {{
            background: #f5f7fa;
        }}
        
        .flow-execution-table tr:hover td.stage-name {{
            background: #e9ecef;
        }}
        
        .execution-cell {{
            text-align: center;
        }}
        
        .execution-entry {{
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 8px;
            padding: 8px;
            margin: 4px 0;
            border-radius: 6px;
            font-size: 13px;
            background: #f8f9fa;
        }}
        
        .execution-entry.status-pass {{
            background: #d4edda;
            border-left: 4px solid #27ae60;
        }}
        
        .execution-entry.status-fail {{
            background: #f8d7da;
            border-left: 4px solid #e74c3c;
        }}
        
        .execution-entry.status-running {{
            background: #fff3cd;
            border-left: 4px solid #f39c12;
        }}
        
        .execution-entry.status-notrun {{
            background: #e9ecef;
            border-left: 4px solid #95a5a6;
            opacity: 0.6;
        }}
        
        .exec-icon {{
            font-size: 16px;
            font-weight: bold;
        }}
        
        .exec-icon.status-pass {{
            color: #27ae60;
        }}
        
        .exec-icon.status-fail {{
            color: #e74c3c;
        }}
        
        .exec-icon.status-running {{
            color: #f39c12;
        }}
        
        .exec-icon.status-notrun {{
            color: #95a5a6;
        }}
        
        .exec-time {{
            font-family: 'Courier New', Courier, monospace;
            font-size: 12px;
            font-weight: 600;
        }}
        
        .exec-time-link {{
            font-family: 'Courier New', Courier, monospace;
            font-size: 12px;
            font-weight: 600;
            color: #667eea;
            text-decoration: none;
            border-bottom: 1px dashed #667eea;
            transition: all 0.2s;
            cursor: pointer;
        }}
        
        .exec-time-link:hover {{
            color: #764ba2;
            border-bottom: 1px solid #764ba2;
            background: rgba(102, 126, 234, 0.1);
            padding: 2px 4px;
            border-radius: 3px;
        }}
        
        .exec-duration {{
            font-size: 11px;
            color: #666;
            font-style: italic;
        }}
        
        .log-button {{
            display: inline-flex;
            align-items: center;
            gap: 4px;
            padding: 4px 10px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            text-decoration: none;
            border-radius: 12px;
            font-size: 11px;
            font-weight: 600;
            transition: all 0.2s;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        
        .log-button:hover {{
            transform: translateY(-1px);
            box-shadow: 0 4px 8px rgba(102, 126, 234, 0.3);
            background: linear-gradient(135deg, #764ba2 0%, #667eea 100%);
        }}
        
        .log-button-disabled {{
            display: inline-flex;
            align-items: center;
            gap: 4px;
            padding: 4px 10px;
            background: #e9ecef;
            color: #6c757d;
            border-radius: 12px;
            font-size: 11px;
            font-weight: 600;
            opacity: 0.6;
        }}
        
        .log-link {{
            text-decoration: none;
            font-size: 14px;
            transition: transform 0.2s;
            display: inline-block;
        }}
        
        .log-link:hover {{
            transform: scale(1.2);
        }}
        
        .status-badge.status-pass {{
            background: #27ae60;
            color: white;
        }}
        
        .status-badge.status-fail {{
            background: #e74c3c;
            color: white;
        }}
        
        .status-badge.status-running {{
            background: #f39c12;
            color: white;
            animation: spin 2s linear infinite;
        }}
        
        .status-badge.status-unknown {{
            background: #95a5a6;
            color: white;
        }}
        
        @keyframes spin {{
            0% {{ transform: rotate(0deg); }}
            100% {{ transform: rotate(360deg); }}
        }}
        
        .connector {{
            width: 3px;
            height: 25px;
            background: linear-gradient(180deg, #4A90E2 0%, #357ABD 100%);
            margin: 0;
            position: relative;
        }}
        
        .connector::after {{
            content: '';
            position: absolute;
            bottom: -8px;
            left: 50%;
            transform: translateX(-50%);
            width: 0;
            height: 0;
            border-left: 6px solid transparent;
            border-right: 6px solid transparent;
            border-top: 8px solid #357ABD;
        }}
        
        .connector-change {{
            width: 3px;
            height: 40px;
            background: repeating-linear-gradient(
                180deg,
                #e74c3c 0px,
                #e74c3c 4px,
                transparent 4px,
                transparent 8px
            );
            margin: 5px 0;
            position: relative;
        }}
        
        .connector-change::before {{
            content: 'âš  CONFIG CHANGE';
            position: absolute;
            top: 50%;
            left: 20px;
            transform: translateY(-50%);
            background: #e74c3c;
            color: white;
            padding: 4px 12px;
            border-radius: 4px;
            font-size: 11px;
            font-weight: bold;
            white-space: nowrap;
            box-shadow: 0 2px 4px rgba(0,0,0,0.2);
        }}
        
        .connector-change::after {{
            content: '';
            position: absolute;
            bottom: -8px;
            left: 50%;
            transform: translateX(-50%);
            width: 0;
            height: 0;
            border-left: 6px solid transparent;
            border-right: 6px solid transparent;
            border-top: 8px solid #e74c3c;
        }}
        
        .parallel-container {{
            width: 100%;
            display: flex;
            justify-content: center;
            margin: 5px 0;
        }}
        
        .parallel-group {{
            display: flex;
            gap: 20px;
            flex-wrap: wrap;
            justify-content: center;
            align-items: flex-start;
            position: relative;
        }}
        
        .parallel-group::before {{
            content: 'âš¡ PARALLEL EXECUTION';
            position: absolute;
            top: -25px;
            left: 50%;
            transform: translateX(-50%);
            background: #9b59b6;
            color: white;
            padding: 4px 12px;
            border-radius: 4px;
            font-size: 10px;
            font-weight: bold;
            white-space: nowrap;
        }}
        
        .parallel-group .flow-node {{
            min-width: 200px;
            max-width: 280px;
        }}
        
        .rerun-badge {{
            display: inline-block;
            background: rgba(255, 255, 255, 0.3);
            padding: 2px 8px;
            border-radius: 12px;
            font-size: 11px;
            font-weight: bold;
            margin-left: 6px;
        }}
        
        .legend {{
            margin-top: 30px;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 8px;
            display: flex;
            flex-wrap: wrap;
            gap: 25px;
            justify-content: center;
            border: 2px solid #e9ecef;
        }}
        
        .legend-item {{
            display: flex;
            align-items: center;
            gap: 10px;
            font-size: 13px;
        }}
        
        .legend-box {{
            width: 40px;
            height: 30px;
            border-radius: 4px;
            border: 2px solid #333;
        }}
        
        .legend-box.config1 {{
            background: linear-gradient(135deg, #4A90E2 0%, #357ABD 100%);
        }}
        
        .legend-box.config2 {{
            background: linear-gradient(135deg, #F5A623 0%, #D68910 100%);
        }}
        
        .legend-connector {{
            width: 3px;
            height: 30px;
            background: linear-gradient(180deg, #4A90E2 0%, #357ABD 100%);
            position: relative;
        }}
        
        .legend-connector::after {{
            content: '';
            position: absolute;
            bottom: -6px;
            left: 50%;
            transform: translateX(-50%);
            width: 0;
            height: 0;
            border-left: 5px solid transparent;
            border-right: 5px solid transparent;
            border-top: 6px solid #357ABD;
        }}
        
        .legend-connector-change {{
            width: 3px;
            height: 30px;
            background: repeating-linear-gradient(
                180deg,
                #e74c3c 0px,
                #e74c3c 3px,
                transparent 3px,
                transparent 6px
            );
            position: relative;
        }}
        
        .legend-connector-change::after {{
            content: '';
            position: absolute;
            bottom: -6px;
            left: 50%;
            transform: translateX(-50%);
            width: 0;
            height: 0;
            border-left: 5px solid transparent;
            border-right: 5px solid transparent;
            border-top: 6px solid #e74c3c;
        }}
        
        .execution-table {{
            width: 100%;
            border-collapse: collapse;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        
        .execution-table thead {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
        }}
        
        .execution-table th {{
            padding: 15px;
            text-align: left;
            font-weight: 600;
            cursor: pointer;
            user-select: none;
        }}
        
        .execution-table th:hover {{
            background: rgba(255,255,255,0.1);
        }}
        
        .execution-table td {{
            padding: 12px 15px;
            border-bottom: 1px solid #eee;
        }}
        
        .execution-table tr:hover {{
            background: #f8f9fa;
        }}
        
        .config1-row {{
            background: rgba(74, 144, 226, 0.1);
        }}
        
        .config2-row {{
            background: rgba(245, 166, 35, 0.1);
        }}
        
        .config-change-row {{
            border-top: 3px solid #e74c3c;
            border-bottom: 3px solid #e74c3c;
            font-weight: bold;
        }}
        
        .comparison-container {{
            display: flex;
            gap: 20px;
            margin: 20px 0;
            flex-wrap: wrap;
        }}
        
        .config-card {{
            flex: 1;
            min-width: 300px;
            border-radius: 10px;
            padding: 20px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            color: white;
        }}
        
        .config-card.config1 {{
            background: linear-gradient(135deg, #4A90E2 0%, #357ABD 100%);
        }}
        
        .config-card.config2 {{
            background: linear-gradient(135deg, #F5A623 0%, #D68910 100%);
        }}
        
        .config-card h3 {{
            margin: 0 0 15px 0;
            font-size: 22px;
            border-bottom: 2px solid rgba(255,255,255,0.3);
            padding-bottom: 10px;
        }}
        
        .config-info p {{
            margin: 8px 0;
            line-height: 1.6;
        }}
        
        .config-info code {{
            background: rgba(0,0,0,0.2);
            padding: 2px 6px;
            border-radius: 3px;
            font-size: 13px;
        }}
        
        .stage-list {{
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid rgba(255,255,255,0.3);
        }}
        
        .impact-warning {{
            background: #fff3cd;
            border: 2px solid #ffc107;
            border-radius: 10px;
            padding: 25px;
            margin: 20px 0;
        }}
        
        .impact-warning h3 {{
            color: #856404;
            margin: 0 0 15px 0;
            font-size: 22px;
        }}
        
        .impact-warning p {{
            color: #856404;
            margin: 10px 0;
        }}
        
        .impact-warning ul {{
            margin: 15px 0;
            padding-left: 25px;
        }}
        
        .impact-warning li {{
            margin: 8px 0;
            color: #856404;
        }}
        
        .impact-issues {{
            background: rgba(231, 76, 60, 0.1);
            border-left: 4px solid #e74c3c;
            padding: 15px;
            margin: 15px 0;
            border-radius: 5px;
        }}
        
        .impact-issues h4 {{
            margin: 0 0 10px 0;
            color: #c0392b;
        }}
        
        .recommendation {{
            background: #d4edda;
            border: 1px solid #c3e6cb;
            color: #155724;
            padding: 15px;
            border-radius: 5px;
            margin-top: 15px;
            font-weight: 500;
        }}
        
        .flow-legend {{
            background: #f8f9fa;
            border-radius: 8px;
            padding: 15px 20px;
            margin: 20px 0;
            display: flex;
            align-items: center;
            gap: 20px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }}
        
        .flow-legend .legend-title {{
            font-weight: 600;
            color: #333;
            font-size: 14px;
        }}
        
        .flow-legend .legend-items {{
            display: flex;
            gap: 20px;
            flex-wrap: wrap;
        }}
        
        .flow-legend .legend-item {{
            display: flex;
            align-items: center;
            gap: 6px;
            font-size: 13px;
            color: #666;
        }}
        
        .flow-notes {{
            background: #e7f3ff;
            border-left: 4px solid #667eea;
            border-radius: 6px;
            padding: 15px 20px;
            margin: 20px 0;
        }}
        
        .flow-notes p {{
            margin: 0 0 10px 0;
            font-weight: 600;
            color: #333;
        }}
        
        .flow-notes ul {{
            margin: 0;
            padding-left: 20px;
        }}
        
        .flow-notes li {{
            margin: 6px 0;
            color: #555;
            font-size: 13px;
        }}
    </style>
</head>
<body>
    <div class="header">
        <img class='logo' src='data:image/png;base64,{logo_data}' alt='AVICE Logo' onclick="showLogoModal()" title="Click to enlarge">
        <div class="header-text">
            <h1>PnR Comprehensive Report</h1>
            <p>Design: {self.design_info.top_hier} | IPO: {self.design_info.ipo}</p>
            <p>Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
        </div>
    </div>
    
    <!-- Logo Modal -->
    <div id="logoModal" class="logo-modal" onclick="hideLogoModal()">
        <span class="logo-modal-close">&times;</span>
        <img class="logo-modal-content" src='data:image/png;base64,{logo_data}' alt='AVICE Logo'>
    </div>
    
    <div class="container">
        <!-- Tab Navigation -->
        <div class="tab-navigation">
            <button class="tab-btn active" onclick="openTab(event, 'pnr-data')">PnR Data</button>
            <button class="tab-btn" onclick="openTab(event, 'timing-histogram')">Timing Analysis</button>
            <button class="tab-btn" onclick="openTab(event, 'clock-tree')">Clock Tree</button>
            <button class="tab-btn" onclick="openTab(event, 'images')">Debug Images</button>
            <button class="tab-btn" onclick="openTab(event, 'toolvers-config')">Flow Execution</button>
        </div>
        
        <!-- PnR Data Tab -->
        <div id="pnr-data" class="tab-content active">
            {pnr_data_content}
        </div>
        
        <!-- Timing Histogram Tab -->
        <div id="timing-histogram" class="tab-content">
            {timing_histogram_content}
        </div>
        
        <!-- Clock Tree Tab -->
        <div id="clock-tree" class="tab-content">
            {clock_tree_content}
        </div>
        
        <!-- Images Tab -->
        <div id="images" class="tab-content">
            {images_content}
        </div>
        
        <!-- Flow Execution Tab -->
        <div id="toolvers-config" class="tab-content">
            {toolvers_config_content}
        </div>
    </div>
    
    <script>
        // Tab switching function
        function openTab(evt, tabName) {{
            // Hide all tab contents
            var tabContents = document.getElementsByClassName('tab-content');
            for (var i = 0; i < tabContents.length; i++) {{
                tabContents[i].classList.remove('active');
            }}
            
            // Remove active class from all tab buttons
            var tabBtns = document.getElementsByClassName('tab-btn');
            for (var i = 0; i < tabBtns.length; i++) {{
                tabBtns[i].classList.remove('active');
            }}
            
            // Show the selected tab and mark button as active
            document.getElementById(tabName).classList.add('active');
            evt.currentTarget.classList.add('active');
        }}
        
        // Logo modal functions
        function showLogoModal() {{
            document.getElementById('logoModal').classList.add('active');
        }}
        
        function hideLogoModal() {{
            document.getElementById('logoModal').classList.remove('active');
        }}
        
        // Allow ESC key to close logo modal
        document.addEventListener('keydown', function(event) {{
            if (event.key === 'Escape') {{
                hideLogoModal();
            }}
        }});
        
        // Back to top button functionality - wait for DOM to load
        document.addEventListener('DOMContentLoaded', function() {{
            var backToTopBtn = document.getElementById('backToTopBtn');
            if (backToTopBtn) {{
                window.addEventListener('scroll', function() {{
                    if (window.pageYOffset > 300) {{
                        backToTopBtn.style.display = 'block';
                    }} else {{
                        backToTopBtn.style.display = 'none';
                    }}
                }});
                
                backToTopBtn.addEventListener('click', function() {{
                    window.scrollTo(0, 0);
                }});
            }}
        }});
    </script>
    
    <!-- Tablog Server Integration JavaScript -->
    <script>
        // Show toast notification
        function showToast(message, type) {{
            const toast = document.createElement('div');
            toast.className = 'toast toast-' + type;
            toast.textContent = message;
            toast.style.cssText = `
                position: fixed;
                bottom: 30px;
                right: 30px;
                padding: 15px 25px;
                border-radius: 8px;
                color: white;
                font-weight: 600;
                box-shadow: 0 4px 12px rgba(0,0,0,0.3);
                z-index: 10000;
                opacity: 0;
                transform: translateY(20px);
                transition: all 0.3s ease;
            `;
            
            if (type === 'success') {{
                toast.style.background = 'linear-gradient(135deg, #27ae60 0%, #229954 100%)';
            }} else if (type === 'error') {{
                toast.style.background = 'linear-gradient(135deg, #e74c3c 0%, #c0392b 100%)';
            }} else if (type === 'warning') {{
                toast.style.background = 'linear-gradient(135deg, #f39c12 0%, #d68910 100%)';
            }}
            
            document.body.appendChild(toast);
            
            setTimeout(function() {{
                toast.style.opacity = '1';
                toast.style.transform = 'translateY(0)';
            }}, 100);
            
            setTimeout(function() {{
                toast.style.opacity = '0';
                toast.style.transform = 'translateY(20px)';
                setTimeout(function() {{ toast.remove(); }}, 300);
            }}, 3000);
        }}
        
        // Copy text to clipboard
        function copyToClipboard(text) {{
            navigator.clipboard.writeText(text).then(function() {{
                showToast('âœ“ Command copied to clipboard! Paste in terminal to execute.', 'warning');
            }}).catch(function(err) {{
                showToast('Failed to copy: ' + err, 'error');
            }});
        }}
        
        // Open log with server (with fallback to clipboard)
        function openLogWithServer(logfile, event) {{
            if (event) {{
                event.preventDefault();
            }}
            
            const serverUrl = 'http://localhost:8888/open_log?file=' + encodeURIComponent(logfile);
            
            // Try to open via server
            fetch(serverUrl, {{ method: 'GET', mode: 'cors' }})
                .then(function(response) {{
                    if (response.ok) {{
                        showToast('âœ“ Opening in tablog...', 'success');
                    }} else {{
                        throw new Error('Server returned error');
                    }}
                }})
                .catch(function(error) {{
                    // Server not running - fallback to clipboard
                    const command = '/home/scratch.avice_vlsi/tablog/tablog ' + logfile;
                    copyToClipboard(command);
                }});
            
            return false;
        }}
    </script>
    
    <button id="backToTopBtn" style="display: none; position: fixed; bottom: 30px; right: 30px; 
            z-index: 99; border: none; outline: none; background-color: #667eea; color: white; 
            cursor: pointer; padding: 15px 20px; border-radius: 50px; font-size: 16px; 
            font-weight: bold; box-shadow: 0 4px 6px rgba(0,0,0,0.3); transition: all 0.3s ease;"
            onmouseover="this.style.backgroundColor='#5568d3'; this.style.transform='scale(1.1)';"
            onmouseout="this.style.backgroundColor='#667eea'; this.style.transform='scale(1)';">
        â†‘ Top
    </button>
    
    <!-- Copyright Footer -->
    <div class="footer">
        <p><strong>AVICE P&R Comprehensive Report</strong></p>
        <p>Copyright (c) 2025 Alon Vice (avice)</p>
        <p>Contact: avice@nvidia.com</p>
    </div>
</body>
</html>"""
            
            # Write HTML file
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            # Determine display path (will be moved to html/ or test_outputs/html/ by _organize_html_files)
            html_output_dir = self._get_html_output_dir()
            display_path = os.path.relpath(html_output_dir, os.getcwd())
            
            print(f"  {Color.GREEN}[OK] Unified PnR HTML Report Generated{Color.RESET}")
            print(f"  Open with: /home/utils/firefox-118.0.1/firefox {Color.MAGENTA}{display_path}/{html_filename}{Color.RESET} &")
            
            return os.path.abspath(html_path)
            
        except Exception as e:
            print(f"  {Color.RED}Error generating unified PnR HTML: {e}{Color.RESET}")
            import traceback
            traceback.print_exc()
            return None
    
    def _get_pnr_data_html_content(self) -> str:
        """Extract PnR data content for unified HTML
        
        Returns:
            HTML string for PnR data section
        """
        try:
            # Get postroute data from all stages (reversed order: most important first)
            stages = ['postroute', 'route', 'cts', 'place', 'plan']
            stage_data = {}
            
            for stage in stages:
                stage_pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{self.design_info.ipo}/reports/{self.design_info.top_hier}_{self.design_info.top_hier}_{self.design_info.ipo}_report_{self.design_info.top_hier}_{self.design_info.ipo}_{stage}.func.std_tt_0c_0p6v.setup.typical.data"
                stage_file = os.path.join(self.workarea, stage_pattern)
                
                if os.path.exists(stage_file):
                    try:
                        with open(stage_file, 'r', encoding='utf-8') as f:
                            content = f.read()
                        
                        stage_params = {}
                        for line in content.split('\n'):
                            line = line.strip()
                            if ' = ' in line:
                                param_name, param_value = line.split(' = ', 1)
                                stage_params[param_name] = param_value
                        
                        stage_data[stage] = stage_params
                    except:
                        pass
            
            if not stage_data:
                return '<div class="no-data">No PnR data available</div>'
            
            # Build data table with all parameters
            content = '<div class="section-header">PnR Stage Data - Key Metrics</div>'
            content += '<p>Comparison of key design metrics across all PnR stages.</p>'
            
            # Add filter bar
            content += '''
            <div style="margin: 20px 0;">
                <input type="text" id="paramFilter" placeholder="Filter parameters..." 
                       style="width: 100%; padding: 10px; font-size: 14px; border: 2px solid #667eea; 
                              border-radius: 5px; box-sizing: border-box;">
            </div>
            '''
            
            # Get all unique parameters and sort them
            all_params = set()
            for stage_params in stage_data.values():
                all_params.update(stage_params.keys())
            
            # Define parameter groups for better organization
            timing_params = []
            area_params = []
            power_params = []
            clock_params = []
            dft_params = []
            compute_params = []
            metadata_params = []
            other_params = []
            
            for param in sorted(all_params):
                param_lower = param.lower()
                # Metadata/Run Info - check first for run metadata patterns
                if param in ['User', 'WW', 'Tool', 'RTL_TAG', 'Scenario', 'Stage', 'Date', 'Directory', 'Technology', 'ToolVersion', 'TracksNum', 'Unit', 'Project', 'SiteVersion']:
                    metadata_params.append(param)
                # Compute Metrics - check for cpu/memory/server/runtime patterns
                elif any(x in param_lower for x in ['cpu', 'memory', 'server', 'stepruntime']):
                    compute_params.append(param)
                # Clock Metrics - check for clock-specific patterns
                elif any(x in param_lower for x in ['latency', 'skew', 'cycle_time', 'clk_ffcount', 'parrallel_driver']):
                    clock_params.append(param)
                # DFT Metrics - check for scan/dft patterns
                elif any(x in param_lower for x in ['dft', 'scan']):
                    dft_params.append(param)
                # Timing Metrics (removed cycle_time as it goes to clock)
                elif any(x in param_lower for x in ['wns', 'tns', 'nvp', 'slack', 'timing', 'violpaths', 'maxfanoutviolations', 'maxtransviolations']):
                    timing_params.append(param)
                # Area & Cell Metrics
                elif any(x in param_lower for x in ['area', 'cell', 'instance', 'net', 'utilization', 'array', 'ffcount', 'buffer', 'gate', 'count', 'bufinv']):
                    area_params.append(param)
                # Power Metrics
                elif any(x in param_lower for x in ['power', 'leakage', 'dynamic', 'vt_percentage', 'svt_', 'lvt_', 'hvt_', 'ulvt_', 'mbperc', 
                                                     'ungatedff', 'ffsbigger', 'ffsoverlogic', 'maxclonedcg', 'tapfanout', 'usemultibit', 'percffwith',
                                                     'totalleakage', 'totallogicg']):
                    power_params.append(param)
                else:
                    other_params.append(param)
            
            # Parameter display name mapping for cleaner presentation
            param_display_names = {
                'HVT_percentage': '%HVT',
                'LVT_percentage': '%LVT',
                'SVT_percentage': '%SVT',
                'ULVT_percentage': '%ULVT',
                'Utilization': '%Utilization',
                'EffictiveUtilization': '%EffictiveUtilization'
            }
            
            # Custom sort function to group VT percentages together in Power Metrics
            def sort_power_params(params):
                """Sort power parameters with VT percentages grouped together (high to low threshold)"""
                vt_order = ['HVT_percentage', 'SVT_percentage', 'LVT_percentage', 'ULVT_percentage']
                vt_params = [p for p in vt_order if p in params]
                other_params = [p for p in sorted(params) if p not in vt_order]
                return vt_params + other_params
            
            # Custom sort function to group Utilization parameters together in Area & Cell Metrics
            def sort_area_params(params):
                """Sort area parameters with utilization metrics grouped together"""
                util_order = ['Utilization', 'EffictiveUtilization']
                util_params = [p for p in util_order if p in params]
                other_params = [p for p in sorted(params) if p not in util_order]
                return util_params + other_params
            
            # Custom sort function to order metadata parameters
            def sort_metadata_params(params):
                """Sort metadata parameters in a specific order"""
                meta_order = ['User', 'WW', 'Tool', 'RTL_TAG', 'Scenario', 'Stage', 'Date', 'Directory', 'Technology', 'ToolVersion', 'TracksNum', 'Unit', 'Project', 'SiteVersion']
                ordered_params = [p for p in meta_order if p in params]
                other_params = [p for p in sorted(params) if p not in meta_order]
                return ordered_params + other_params
            
            # Sort power_params with custom ordering
            power_params = sort_power_params(power_params)
            
            # Sort area_params with custom ordering
            area_params = sort_area_params(area_params)
            
            # Sort metadata_params with custom ordering
            metadata_params = sort_metadata_params(metadata_params)
            
            # Build collapsible categories
            param_groups = [
                ('metadata', 'Environment Details', metadata_params),
                ('timing', 'Timing Metrics', timing_params),
                ('clock', 'Clock Metrics', clock_params),
                ('area', 'Area & Cell Metrics', area_params),
                ('power', 'Power Metrics', power_params),
                ('dft', 'DFT Metrics', dft_params),
                ('compute', 'Compute Metrics', compute_params),
                ('other', 'Other Parameters', other_params)
            ]
            
            for category_id, group_name, params in param_groups:
                if params:
                    # Generate category header with expand/collapse (collapsed by default)
                    content += f'''
            <div class="category-section">
                <div class="category-header" onclick="toggleCategory('{category_id}')">
                    <span style="font-size: 16px; margin-right: 8px;">ðŸ“Š</span> {group_name} ({len(params)} parameters)
                    <span class="expand-icon" id="icon-{category_id}">â–¶</span>
                </div>
                <div class="category-content" id="content-{category_id}" style="display: none;">
                    <table class="category-table">
                        <thead>
                            <tr>
                                <th style="background-color: #34495e; color: white; padding: 12px; text-align: left;">Parameter</th>
'''
                    
                    # Add stage headers
                    for stage in stages:
                        if stage in stage_data:
                            content += f'                                <th style="background-color: #34495e; color: white; padding: 12px; text-align: center;">{stage.upper()}</th>\n'
                    
                    content += '                            </tr>\n                        </thead>\n                        <tbody>\n'
                    
                    # Add parameter rows
                    for param in params:
                        # Get display name (use mapping if available, otherwise use original)
                        display_name = param_display_names.get(param, param)
                        content += f'                            <tr class="param-row" data-param-name="{param.lower()}" data-display-name="{display_name.lower()}">\n'
                        content += f'                                <td style="padding: 10px; border: 1px solid #ddd;"><strong>{display_name}</strong></td>\n'
                        
                        for stage in stages:
                            if stage in stage_data:
                                value = stage_data[stage].get(param, '-')
                                # Color code timing violations
                                cell_style = 'padding: 10px; border: 1px solid #ddd; text-align: center;'
                                if 'WNS' in param or 'TNS' in param:
                                    try:
                                        num_val = float(value)
                                        if num_val < 0:
                                            cell_style += ' background-color: #ffebee; color: #c62828;'
                                    except:
                                        pass
                                content += f'                                <td style="{cell_style}">{value}</td>\n'
                        
                        content += '                            </tr>\n'
                    
                    content += '''                        </tbody>
                    </table>
                </div>
            </div>
'''
            
            # Add "Back to Top" button
            content += '''
            <button id="backToTopBtn" style="display: none; position: fixed; bottom: 30px; right: 30px; 
                    z-index: 99; border: none; outline: none; background-color: #667eea; color: white; 
                    cursor: pointer; padding: 15px 20px; border-radius: 50px; font-size: 16px; 
                    font-weight: bold; box-shadow: 0 4px 6px rgba(0,0,0,0.3); transition: all 0.3s ease;"
                    onmouseover="this.style.backgroundColor='#5568d3'; this.style.transform='scale(1.1)';"
                    onmouseout="this.style.backgroundColor='#667eea'; this.style.transform='scale(1)';">
                â†‘ Top
            </button>
            '''
            
            # Add JavaScript for category toggle, filtering, and back-to-top button
            content += '''
            <script>
            // Toggle category expand/collapse
            function toggleCategory(category) {
                const content = document.getElementById('content-' + category);
                const icon = document.getElementById('icon-' + category);
                
                if (content.style.display === 'none' || content.style.display === '') {
                    content.style.display = 'block';
                    icon.textContent = 'â–¼';
                } else {
                    content.style.display = 'none';
                    icon.textContent = 'â–¶';
                }
            }
            
            document.addEventListener('DOMContentLoaded', function() {
                // Filter functionality
                const filterInput = document.getElementById('paramFilter');
                if (filterInput) {
                    filterInput.addEventListener('input', function() {
                        const filterText = this.value.toLowerCase();
                        const rows = document.querySelectorAll('.param-row');
                        
                        rows.forEach(function(row) {
                            const paramName = row.getAttribute('data-param-name') || '';
                            const displayName = row.getAttribute('data-display-name') || '';
                            
                            // Show row if filter text is found in either param name or display name
                            if (paramName.includes(filterText) || displayName.includes(filterText)) {
                                row.style.display = '';
                            } else {
                                row.style.display = 'none';
                            }
                        });
                    });
                }
                
                // Back to top button functionality
                var backToTopBtn = document.getElementById('backToTopBtn');
                if (backToTopBtn) {
                    // Show button when user scrolls down 300px
                    window.addEventListener('scroll', function() {
                        if (window.pageYOffset > 300) {
                            backToTopBtn.style.display = 'block';
                        } else {
                            backToTopBtn.style.display = 'none';
                        }
                    });
                    
                    // Scroll to top when button is clicked
                    backToTopBtn.addEventListener('click', function() {
                        window.scrollTo({
                            top: 0,
                            behavior: 'smooth'
                        });
                    });
                }
            });
            </script>
            '''
            
            return content
            
        except Exception as e:
            import traceback
            return f'<div class="no-data">Error loading PnR data: {e}<br><pre>{traceback.format_exc()}</pre></div>'
    
    def _colorize_histogram_table(self, table_text: str) -> str:
        """Add color coding to histogram table - highlight negative slack columns in red
        
        Args:
            table_text: Raw histogram table text
            
        Returns:
            HTML-formatted table with color highlighting
        """
        try:
            import re
            lines = table_text.split('\n')
            colored_lines = []
            
            for line in lines:
                # Check if this line contains negative slack values
                # Pattern 1: Header row with negative slack ranges (e.g., "-0.010 -0.020 -0.030")
                # Pattern 2: Data rows with negative values in WNS/TNS columns
                
                if '|' in line:
                    # Color any negative numbers (with minus sign and digits)
                    # This will catch: -0.010, -1.234, -0.094, etc.
                    colored_line = re.sub(
                        r'(-\d+\.?\d*)',
                        r'<span style="color: #e74c3c; font-weight: bold;">\1</span>',
                        line
                    )
                    colored_lines.append(colored_line)
                else:
                    colored_lines.append(line)
            
            return '\n'.join(colored_lines)
        except Exception as e:
            # If coloring fails, return original text
            return table_text
    
    def _colorize_timing_summary(self, summary_text: str) -> str:
        """Add color coding to timing summary - highlight negative WNS/TNS values in red
        
        Args:
            summary_text: Raw timing summary text
            
        Returns:
            HTML-formatted summary with color highlighting
        """
        try:
            import re
            lines = summary_text.split('\n')
            colored_lines = []
            
            for line in lines:
                # Look for negative numbers in WNS/TNS lines
                if any(keyword in line for keyword in ['WNS', 'TNS', 'NVP']):
                    # Find negative numbers (e.g., -0.123, -1.5)
                    colored_line = re.sub(
                        r'(-\d+\.?\d*)',
                        r'<span style="color: #e74c3c; font-weight: bold;">\1</span>',
                        line
                    )
                    colored_lines.append(colored_line)
                else:
                    colored_lines.append(line)
            
            return '\n'.join(colored_lines)
        except Exception as e:
            # If coloring fails, return original text
            return summary_text
    
    def _parse_worst_paths_report(self, report_file: str, max_paths: int = 50) -> List[Dict]:
        """Parse worst50 path report and extract detailed path traces
        
        Args:
            report_file: Path to worst50 timing report
            max_paths: Maximum number of paths to extract
            
        Returns:
            List of dictionaries containing path information
        """
        paths = []
        try:
            # Read the report file
            result = self.file_utils.run_command(f"zcat {report_file}")
            if not result.strip():
                return paths
            
            # Find all path sections using the pattern "Path N: VIOLATED" or "Path N: MET"
            # Each path starts with "Path N:" and ends when the next "Path N:" begins
            path_pattern = re.compile(r'Path (\d+):\s*(VIOLATED|MET)', re.MULTILINE)
            path_matches = list(path_pattern.finditer(result))
            
            # Prepare list of (idx, section) tuples
            sections_to_parse = []
            
            if path_matches:
                # Use the new method with proper boundaries
                for i in range(min(len(path_matches), max_paths)):
                    match = path_matches[i]
                    path_num = int(match.group(1))
                    
                    # Extract section from this match to the next match (or end of file)
                    start_pos = match.start()
                    if i + 1 < len(path_matches):
                        end_pos = path_matches[i + 1].start()
                    else:
                        end_pos = len(result)
                    
                    section = result[start_pos:end_pos]
                    sections_to_parse.append((path_num, section))
            else:
                # Fallback to old split method
                path_sections = re.split(r'\nPath \d+:', result)
                for idx, section in enumerate(path_sections[1:max_paths+1], 1):  # Skip first empty section
                    sections_to_parse.append((idx, section))
            
            # Now parse all sections uniformly
            for idx, section in sections_to_parse:
                try:
                    path_data = {
                        'number': idx,
                        'header': {},
                        'timing_path': [],
                        'clock_path': [],
                        'raw_section': section[:5000]  # Limit size for performance
                    }
                    
                    # Extract path header information
                    header_match = re.search(r'(VIOLATED|MET)\s+(Setup|Hold|Clock Gating Setup|Clock Gating Hold)\s+Check.*?with Pin\s+(\S+)', section)
                    if header_match:
                        path_data['header']['status'] = header_match.group(1)
                        path_data['header']['check_type'] = header_match.group(2)
                        path_data['header']['endpoint_pin'] = header_match.group(3)
                    
                    # Extract endpoint and beginpoint
                    endpoint_match = re.search(r'Endpoint:\s+(\S+).*?checked with.*?\'(\w+)\'', section)
                    if endpoint_match:
                        path_data['header']['endpoint'] = endpoint_match.group(1)
                        path_data['header']['clock'] = endpoint_match.group(2)
                    
                    beginpoint_match = re.search(r'Beginpoint:\s+(\S+).*?triggered by.*?\'(\w+)\'', section)
                    if beginpoint_match:
                        path_data['header']['beginpoint'] = beginpoint_match.group(1)
                        path_data['header']['launch_clock'] = beginpoint_match.group(2)
                    
                    # Extract path groups
                    pathgroup_match = re.search(r'Path Groups:\s+\{([^}]+)\}', section)
                    if pathgroup_match:
                        path_data['header']['path_group'] = pathgroup_match.group(1)
                    
                    # Extract analysis view
                    view_match = re.search(r'Analysis View:\s+(\S+)', section)
                    if view_match:
                        path_data['header']['analysis_view'] = view_match.group(1)
                    
                    # Extract slack (handle both "= Slack Time" for setup and "  Slack Time" for hold)
                    slack_match = re.search(r'[=\s]+Slack Time\s+([-\d.]+)', section)
                    if slack_match:
                        path_data['header']['slack'] = float(slack_match.group(1))
                    
                    # Extract arrival and required times (handle both "=" and whitespace formats)
                    required_match = re.search(r'[=\s]+Required Time\s+([-\d.]+)', section)
                    if required_match:
                        path_data['header']['required_time'] = float(required_match.group(1))
                    
                    arrival_match = re.search(r'[-\s]+Arrival Time\s+([-\d.]+)', section)
                    if arrival_match:
                        path_data['header']['arrival_time'] = float(arrival_match.group(1))
                    
                    # UNIVERSAL APPROACH: Display COMPLETE path data between "Path N: VIOLATED" markers
                    # Works for ALL path types:
                    # - External paths: input->reg, input->clock-gate, reg->output (various formats)
                    # - Internal paths: reg->reg with cell-by-cell timing breakdown
                    # Simple, robust, and shows everything without format-specific parsing!
                    
                    # Extract complete section content (everything between path markers)
                    lines = section.split('\n')
                    if lines and re.match(r'^Path \d+:\s*(VIOLATED|MET)', lines[0]):
                        # Skip first line, it's the path marker
                        section_content = '\n'.join(lines[1:]).strip()
                    else:
                        section_content = section.strip()
                    
                    # Remove the trailing "Path N: VIOLATED" marker if present at the end
                    section_content = re.sub(r'\n\s*Path \d+:\s*(VIOLATED|MET)\s*$', '', section_content).strip()
                    
                    # Store complete path data if we have content
                    if len(section_content) > 100:  # Must have some content
                        path_data['timing_path_full'] = section_content
                        
                        # Also store in timing_path for backward compatibility
                        for line in section_content.split('\n'):
                            if line.strip() and len(line.strip()) > 10:
                                path_data['timing_path'].append({'line': line})
                    
                    paths.append(path_data)
                    
                except Exception as e:
                    self.logger.debug(f"Error parsing path {idx}: {e}")
                    continue
            
            return paths
            
        except Exception as e:
            self.logger.error(f"Error parsing worst paths report {report_file}: {e}")
            return paths
    
    def _find_worst_path_reports(self, stage: str) -> Dict[str, Optional[str]]:
        """Find worst50 path reports for given stage
        
        Args:
            stage: PnR stage (postroute, route, cts, etc.)
            
        Returns:
            Dictionary with report file paths
        """
        reports = {
            'setup_external': None,
            'setup_internal': None,
            'hold_external': None,
            'hold_internal': None
        }
        
        try:
            details_dir = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{self.design_info.ipo}/REPs/DETAILS"
            
            # Find setup reports
            setup_ext_pattern = f"{details_dir}/{self.design_info.top_hier}.*.{stage}.timing.gba.setup.external.worst50.rpt.gz"
            setup_int_pattern = f"{details_dir}/{self.design_info.top_hier}.*.{stage}.timing.gba.setup.internal.worst50.rpt.gz"
            
            # Find hold reports
            hold_ext_pattern = f"{details_dir}/{self.design_info.top_hier}.*.{stage}.timing.gba.hold.external.worst50.rpt.gz"
            hold_int_pattern = f"{details_dir}/{self.design_info.top_hier}.*.{stage}.timing.gba.hold.internal.worst50.rpt.gz"
            
            setup_ext_files = self.file_utils.find_files(setup_ext_pattern, self.workarea)
            setup_int_files = self.file_utils.find_files(setup_int_pattern, self.workarea)
            hold_ext_files = self.file_utils.find_files(hold_ext_pattern, self.workarea)
            hold_int_files = self.file_utils.find_files(hold_int_pattern, self.workarea)
            
            reports['setup_external'] = setup_ext_files[0] if setup_ext_files else None
            reports['setup_internal'] = setup_int_files[0] if setup_int_files else None
            reports['hold_external'] = hold_ext_files[0] if hold_ext_files else None
            reports['hold_internal'] = hold_int_files[0] if hold_int_files else None
            
        except Exception as e:
            self.logger.debug(f"Error finding worst path reports: {e}")
        
        return reports

    def _make_source_clickable(self, filepath: str) -> str:
        """Convert a file path to a clickable HTML link using tablog server integration
        
        Args:
            filepath: Absolute or relative file path
            
        Returns:
            HTML string with clickable link to file (uses tablog server if enabled)
        """
        if not filepath or filepath == 'N/A':
            return 'N/A'
        
        # Ensure absolute path
        abs_path = os.path.abspath(filepath) if not os.path.isabs(filepath) else filepath
        
        # Get basename for display
        basename = os.path.basename(abs_path)
        
        if USE_TABLOG_SERVER:
            # Use tablog server integration (with graceful fallback to clipboard)
            return f'<a href="#" onclick="openLogWithServer(\'{abs_path}\', event); return false;" style="color: #3498db; text-decoration: none; font-family: monospace; cursor: pointer;" title="Click to open in tablog: {abs_path}">{abs_path}</a>'
        else:
            # Direct file:// protocol (original behavior)
            return f'<a href="file://{abs_path}" style="color: #3498db; text-decoration: none; font-family: monospace;" title="{abs_path}">{abs_path}</a>'
    
    def _get_timing_histogram_html_content(self) -> str:
        """Extract enhanced timing histogram content with path traces for unified HTML
        
        Returns:
            HTML string for timing histogram section with tabs
        """
        try:
            # Find timing histogram files (both setup and hold)
            pnr_stages = ['postroute', 'route', 'cts', 'place', 'plan']
            setup_file = None
            hold_file = None
            found_stage = None
            
            for stage in pnr_stages:
                # Use wildcard for IPO in filename since it can differ from directory name
                setup_pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{self.design_info.ipo}/REPs/SUMMARY/{self.design_info.top_hier}.*.{stage}.timing.setup.rpt.gz"
                hold_pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{self.design_info.ipo}/REPs/SUMMARY/{self.design_info.top_hier}.*.{stage}.timing.hold.rpt.gz"
                
                setup_files = self.file_utils.find_files(setup_pattern, self.workarea)
                hold_files = self.file_utils.find_files(hold_pattern, self.workarea)
                
                if setup_files or hold_files:
                    setup_file = setup_files[0] if setup_files else None
                    hold_file = hold_files[0] if hold_files else None
                    found_stage = stage
                    break
            
            if not setup_file and not hold_file:
                return '<div class="no-data">No timing histogram data available</div>'
            
            # Find worst path reports
            worst_reports = self._find_worst_path_reports(found_stage)
            
            # Parse worst paths (limit to 10 paths each for performance)
            setup_ext_paths = []
            setup_int_paths = []
            hold_ext_paths = []
            hold_int_paths = []
            
            if worst_reports['setup_external']:
                setup_ext_paths = self._parse_worst_paths_report(worst_reports['setup_external'], max_paths=10)
            if worst_reports['setup_internal']:
                setup_int_paths = self._parse_worst_paths_report(worst_reports['setup_internal'], max_paths=10)
            if worst_reports['hold_external']:
                hold_ext_paths = self._parse_worst_paths_report(worst_reports['hold_external'], max_paths=10)
            if worst_reports['hold_internal']:
                hold_int_paths = self._parse_worst_paths_report(worst_reports['hold_internal'], max_paths=10)
            
            # Count total VIOLATING paths for tab titles (slack < 0 only)
            setup_ext_violating = len([p for p in setup_ext_paths if p['header'].get('slack', 0) < 0])
            setup_int_violating = len([p for p in setup_int_paths if p['header'].get('slack', 0) < 0])
            hold_ext_violating = len([p for p in hold_ext_paths if p['header'].get('slack', 0) < 0])
            hold_int_violating = len([p for p in hold_int_paths if p['header'].get('slack', 0) < 0])
            
            total_setup_paths = setup_ext_violating + setup_int_violating
            total_hold_paths = hold_ext_violating + hold_int_violating
            
            # Extract timing summary metrics from timing histogram tables (separate external/internal)
            setup_ext_metrics = {'wns': 'N/A', 'tns': 'N/A', 'nvp': 'N/A'}
            setup_int_metrics = {'wns': 'N/A', 'tns': 'N/A', 'nvp': 'N/A'}
            hold_ext_metrics = {'wns': 'N/A', 'tns': 'N/A', 'nvp': 'N/A'}
            hold_int_metrics = {'wns': 'N/A', 'tns': 'N/A', 'nvp': 'N/A'}
            
            if setup_file:
                # Extract EXTERNAL timing data
                # Format: " external|-0.080|-32.080|1603|..."
                ext_result = self.file_utils.run_command(f"zcat {setup_file} | grep -E '^ external' | head -1")
                timing_match = re.search(r'\|\s*([-\d.]+)\s*\|\s*([-\d.]+)\s*\|\s*(\d+)', ext_result)
                if timing_match:
                    setup_ext_metrics['wns'] = timing_match.group(1)
                    setup_ext_metrics['tns'] = timing_match.group(2)
                    setup_ext_metrics['nvp'] = timing_match.group(3)
                
                # Extract INTERNAL timing data
                # Format: " internal|-0.015| -2.062| 475|..."
                int_result = self.file_utils.run_command(f"zcat {setup_file} | grep -E '^ internal' | head -1")
                timing_match = re.search(r'\|\s*([-\d.]+)\s*\|\s*([-\d.]+)\s*\|\s*(\d+)', int_result)
                if timing_match:
                    setup_int_metrics['wns'] = timing_match.group(1)
                    setup_int_metrics['tns'] = timing_match.group(2)
                    setup_int_metrics['nvp'] = timing_match.group(3)
            
            if hold_file:
                # Extract EXTERNAL timing data
                ext_result = self.file_utils.run_command(f"zcat {hold_file} | grep -E '^ external' | head -1")
                timing_match = re.search(r'\|\s*([-\d.]+)\s*\|\s*([-\d.]+)\s*\|\s*(\d+)', ext_result)
                if timing_match:
                    hold_ext_metrics['wns'] = timing_match.group(1)
                    hold_ext_metrics['tns'] = timing_match.group(2)
                    hold_ext_metrics['nvp'] = timing_match.group(3)
                
                # Extract INTERNAL timing data
                int_result = self.file_utils.run_command(f"zcat {hold_file} | grep -E '^ internal' | head -1")
                timing_match = re.search(r'\|\s*([-\d.]+)\s*\|\s*([-\d.]+)\s*\|\s*(\d+)', int_result)
                if timing_match:
                    hold_int_metrics['wns'] = timing_match.group(1)
                    hold_int_metrics['tns'] = timing_match.group(2)
                    hold_int_metrics['nvp'] = timing_match.group(3)
            
            # Start building content with tabs
            content = f'''
            <div class="section-header">Timing Analysis - {found_stage.upper()} Stage</div>
            <p style="margin-bottom: 20px;">Comprehensive timing analysis including path distribution and critical path traces</p>
            
            <!-- Tab Navigation -->
            <div class="timing-tabs">
                <button class="timing-tab-btn active" onclick="openTimingTab(event, 'overview-tab')">Overview</button>
                <button class="timing-tab-btn" onclick="openTimingTab(event, 'histogram-tab')">Path Distribution</button>
                <button class="timing-tab-btn" onclick="openTimingTab(event, 'setup-paths-tab')">Critical Paths - Setup ({total_setup_paths})</button>
                <button class="timing-tab-btn" onclick="openTimingTab(event, 'hold-paths-tab')">Critical Paths - Hold ({total_hold_paths})</button>
            </div>
            
            <!-- Tab Content -->
            '''
            
            # TAB 1: Overview (with external/internal breakdown)
            content += self._generate_overview_tab_content(
                setup_ext_metrics, setup_int_metrics, hold_ext_metrics, hold_int_metrics,
                total_setup_paths, total_hold_paths, setup_file, hold_file, found_stage
            )
            
            # TAB 2: Path Distribution (Histograms)
            content += self._generate_histogram_tab_content(setup_file, hold_file, found_stage)
            
            # TAB 3: Critical Paths - Setup (separated by external/internal)
            content += self._generate_critical_paths_tab_content_separated(
                setup_ext_paths, setup_int_paths, 'setup', 
                worst_reports['setup_external'], worst_reports['setup_internal']
            )
            
            # TAB 4: Critical Paths - Hold (separated by external/internal)
            content += self._generate_critical_paths_tab_content_separated(
                hold_ext_paths, hold_int_paths, 'hold', 
                worst_reports['hold_external'], worst_reports['hold_internal']
            )
            
            # Add CSS and JavaScript for tabs
            content += self._generate_timing_tab_styles_and_scripts()
            
            return content
            
        except Exception as e:
            import traceback
            return f'<div class="no-data">Error loading timing histogram: {e}<br><pre>{traceback.format_exc()}</pre></div>'
    
    def _generate_overview_tab_content(self, setup_ext_metrics: Dict, setup_int_metrics: Dict,
                                       hold_ext_metrics: Dict, hold_int_metrics: Dict,
                                       setup_path_count: int, hold_path_count: int,
                                       setup_file: Optional[str], hold_file: Optional[str],
                                       stage: str) -> str:
        """Generate overview tab with setup/hold comparison cards (external/internal breakdown)
        
        Args:
            setup_ext_metrics: Dictionary with setup external WNS/TNS/NVP
            setup_int_metrics: Dictionary with setup internal WNS/TNS/NVP
            hold_ext_metrics: Dictionary with hold external WNS/TNS/NVP
            hold_int_metrics: Dictionary with hold internal WNS/TNS/NVP
            setup_path_count: Number of critical setup paths found
            hold_path_count: Number of critical hold paths found
            setup_file: Path to setup timing file
            hold_file: Path to hold timing file
            stage: PnR stage
            
        Returns:
            HTML string for overview tab
        """
        def get_slack_color(wns_str):
            """Return color based on WNS value"""
            try:
                wns = float(wns_str)
                if wns >= 0:
                    return '#27ae60'  # Green
                elif wns >= -0.050:
                    return '#e67e22'  # Orange (better visibility)
                else:
                    return '#e74c3c'  # Red
            except:
                return '#95a5a6'  # Gray for N/A
        
        # Get worst WNS from external/internal for overall card color
        def get_worst_wns(ext_wns, int_wns):
            try:
                ext_val = float(ext_wns) if ext_wns != 'N/A' else 0
                int_val = float(int_wns) if int_wns != 'N/A' else 0
                return min(ext_val, int_val)
            except:
                return 0
        
        setup_worst_wns = get_worst_wns(setup_ext_metrics['wns'], setup_int_metrics['wns'])
        hold_worst_wns = get_worst_wns(hold_ext_metrics['wns'], hold_int_metrics['wns'])
        setup_color = get_slack_color(str(setup_worst_wns))
        hold_color = get_slack_color(str(hold_worst_wns))
        
        setup_ext_color = get_slack_color(setup_ext_metrics['wns'])
        setup_int_color = get_slack_color(setup_int_metrics['wns'])
        hold_ext_color = get_slack_color(hold_ext_metrics['wns'])
        hold_int_color = get_slack_color(hold_int_metrics['wns'])
        
        content = f'''
        <div id="overview-tab" class="timing-tab-content" style="display: block;">
            <h2 style="color: #2c3e50; margin-bottom: 20px;">Timing Summary - {stage.upper()} Stage</h2>
            
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin-bottom: 30px;">
                
                <!-- Setup Timing Card -->
                <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 12px; padding: 25px; color: white; box-shadow: 0 4px 15px rgba(0,0,0,0.2);">
                    <h3 style="margin: 0 0 20px 0; font-size: 24px; border-bottom: 2px solid rgba(255,255,255,0.3); padding-bottom: 10px;">
                        Setup Timing
                </h3>
                    
                    <!-- External Paths -->
                    <div style="background: rgba(255,255,255,0.15); border-radius: 8px; padding: 15px; margin-bottom: 15px;">
                        <div style="font-size: 13px; opacity: 0.9; margin-bottom: 10px; border-bottom: 1px solid rgba(255,255,255,0.2); padding-bottom: 5px;">
                            <strong>ðŸ“¥ EXTERNAL (I/O Interfaces)</strong>
                        </div>
                        <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; font-size: 12px;">
                    <div>
                                <div style="opacity: 0.8;">WNS</div>
                                <div style="font-size: 18px; font-weight: bold; color: {setup_ext_color}; text-shadow: 1px 1px 3px rgba(0,0,0,0.5);">{setup_ext_metrics['wns']} ns</div>
                    </div>
                    <div>
                                <div style="opacity: 0.8;">TNS</div>
                                <div style="font-size: 18px; font-weight: bold;">{setup_ext_metrics['tns']} ns</div>
                            </div>
                            <div>
                                <div style="opacity: 0.8;">Viol. Paths</div>
                                <div style="font-size: 18px; font-weight: bold;">{setup_ext_metrics['nvp']}</div>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Internal Paths -->
                    <div style="background: rgba(255,255,255,0.15); border-radius: 8px; padding: 15px; margin-bottom: 15px;">
                        <div style="font-size: 13px; opacity: 0.9; margin-bottom: 10px; border-bottom: 1px solid rgba(255,255,255,0.2); padding-bottom: 5px;">
                            <strong>âš™ INTERNAL (Core Logic)</strong>
                        </div>
                        <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; font-size: 12px;">
                            <div>
                                <div style="opacity: 0.8;">WNS</div>
                                <div style="font-size: 18px; font-weight: bold; color: {setup_int_color}; text-shadow: 1px 1px 3px rgba(0,0,0,0.5);">{setup_int_metrics['wns']} ns</div>
                            </div>
                            <div>
                                <div style="opacity: 0.8;">TNS</div>
                                <div style="font-size: 18px; font-weight: bold;">{setup_int_metrics['tns']} ns</div>
                            </div>
                            <div>
                                <div style="opacity: 0.8;">Viol. Paths</div>
                                <div style="font-size: 18px; font-weight: bold;">{setup_int_metrics['nvp']}</div>
                            </div>
                        </div>
                    </div>
                </div>
                
                <!-- Hold Timing Card -->
                <div style="background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); border-radius: 12px; padding: 25px; color: white; box-shadow: 0 4px 15px rgba(0,0,0,0.2);">
                    <h3 style="margin: 0 0 20px 0; font-size: 24px; border-bottom: 2px solid rgba(255,255,255,0.3); padding-bottom: 10px;">
                        Hold Timing
                    </h3>
                    
                    <!-- External Paths -->
                    <div style="background: rgba(255,255,255,0.15); border-radius: 8px; padding: 15px; margin-bottom: 15px;">
                        <div style="font-size: 13px; opacity: 0.9; margin-bottom: 10px; border-bottom: 1px solid rgba(255,255,255,0.2); padding-bottom: 5px;">
                            <strong>ðŸ“¥ EXTERNAL (I/O Interfaces)</strong>
                        </div>
                        <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; font-size: 12px;">
                            <div>
                                <div style="opacity: 0.8;">WNS</div>
                                <div style="font-size: 18px; font-weight: bold; color: {hold_ext_color}; text-shadow: 1px 1px 3px rgba(0,0,0,0.5);">{hold_ext_metrics['wns']} ns</div>
                            </div>
                            <div>
                                <div style="opacity: 0.8;">TNS</div>
                                <div style="font-size: 18px; font-weight: bold;">{hold_ext_metrics['tns']} ns</div>
                            </div>
                            <div>
                                <div style="opacity: 0.8;">Viol. Paths</div>
                                <div style="font-size: 18px; font-weight: bold;">{hold_ext_metrics['nvp']}</div>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Internal Paths -->
                    <div style="background: rgba(255,255,255,0.15); border-radius: 8px; padding: 15px; margin-bottom: 15px;">
                        <div style="font-size: 13px; opacity: 0.9; margin-bottom: 10px; border-bottom: 1px solid rgba(255,255,255,0.2); padding-bottom: 5px;">
                            <strong>âš™ INTERNAL (Core Logic)</strong>
                        </div>
                        <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; font-size: 12px;">
                            <div>
                                <div style="opacity: 0.8;">WNS</div>
                                <div style="font-size: 18px; font-weight: bold; color: {hold_int_color}; text-shadow: 1px 1px 3px rgba(0,0,0,0.5);">{hold_int_metrics['wns']} ns</div>
                            </div>
                            <div>
                                <div style="opacity: 0.8;">TNS</div>
                                <div style="font-size: 18px; font-weight: bold;">{hold_int_metrics['tns']} ns</div>
                            </div>
                            <div>
                                <div style="opacity: 0.8;">Viol. Paths</div>
                                <div style="font-size: 18px; font-weight: bold;">{hold_int_metrics['nvp']}</div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <!-- Legend -->
            <div style="background: #f0f8ff; border: 2px solid #3498db; border-radius: 8px; padding: 20px; margin: 20px 0;">
                <h3 style="color: #2c3e50; margin-top: 0; margin-bottom: 15px;">
                    <span style="font-size: 18px;">&#9432;</span> Understanding Timing Metrics
                </h3>
                <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 20px; font-size: 13px;">
                    <div>
                        <strong style="color: #16a085;">WNS (Worst Negative Slack):</strong>
                        <p style="margin: 5px 0;">Most critical timing violation. Negative values indicate timing failure.</p>
                        <ul style="margin: 5px 0; padding-left: 20px;">
                            <li><span style="color: #27ae60; font-weight: bold;">>= 0 ns:</span> Passing</li>
                            <li><span style="color: #f39c12; font-weight: bold;">-0.05 to 0 ns:</span> Marginal</li>
                            <li><span style="color: #e74c3c; font-weight: bold;">< -0.05 ns:</span> Critical</li>
                        </ul>
                    </div>
                    <div>
                        <strong style="color: #16a085;">TNS (Total Negative Slack):</strong>
                        <p style="margin: 5px 0;">Sum of all negative slacks. Indicates overall timing health.</p>
                    </div>
                    <div>
                        <strong style="color: #16a085;">Violating Paths:</strong>
                        <p style="margin: 5px 0;">Number of paths with negative slack requiring fixes.</p>
                    </div>
                </div>
            </div>
            
            <!-- Source Files -->
            <div style="background: #f9f9f9; border-left: 4px solid #3498db; padding: 15px; margin-top: 20px; font-size: 12px; color: #666;">
                <strong style="color: #2c3e50;">Data Sources:</strong><br>
                Setup: {self._make_source_clickable(setup_file)}<br>
                Hold: {self._make_source_clickable(hold_file)}
                </div>
            </div>
            '''
            
        return content
    
    def _generate_histogram_tab_content(self, setup_file: Optional[str], hold_file: Optional[str], stage: str) -> str:
        """Generate histogram tab with path distribution tables
        
        Args:
            setup_file: Path to setup timing file
            hold_file: Path to hold timing file
            stage: PnR stage
            
        Returns:
            HTML string for histogram tab
        """
        content = f'''
        <div id="histogram-tab" class="timing-tab-content" style="display: none;">
            <h2 style="color: #2c3e50; margin-bottom: 15px;">Path Distribution Analysis</h2>
            <p style="color: #666; margin-bottom: 25px;">Histogram showing timing path distribution across different slack ranges and categories</p>
        '''
        
        # Setup Histogram
        if setup_file:
            try:
                content += '<h3 style="color: #27ae60; background: #ecf9f0; padding: 12px; border-left: 4px solid #27ae60; margin-top: 20px; border-radius: 4px;">Setup Timing Distribution</h3>'
                
                result = self.file_utils.run_command(f"zcat {setup_file} | grep -n 'histogram' | grep '|'")
                if result.strip():
                    histogram_lines = result.strip().split('\n')
                    if len(histogram_lines) >= 2:
                        table_start = int(histogram_lines[-1].split(':')[0])
                        table_result = self.file_utils.run_command(f"zcat {setup_file} | sed -n '{table_start},{table_start+30}p'")
                        
                        if table_result.strip():
                            colored_table = self._colorize_histogram_table(table_result.strip())
                            content += '<pre style="background: #f5f5f5; padding: 15px; border-radius: 5px; overflow-x: auto; font-family: monospace; font-size: 11px; line-height: 1.4;">'
                            content += colored_table
                            content += '</pre>'
                
                content += f'<p style="margin-top: 10px; font-size: 11px; color: #666;"><strong>Source:</strong> {self._make_source_clickable(setup_file)}</p>'
                
            except Exception as e:
                content += f'<p style="color: #e74c3c;">Error parsing setup histogram: {e}</p>'
        
        # Hold Histogram
        if hold_file:
            try:
                content += '<h3 style="color: #e67e22; background: #fef5e7; padding: 12px; border-left: 4px solid #e67e22; margin-top: 30px; border-radius: 4px;">Hold Timing Distribution</h3>'
                
                result = self.file_utils.run_command(f"zcat {hold_file} | grep -n 'histogram' | grep '|'")
                if result.strip():
                    histogram_lines = result.strip().split('\n')
                    if len(histogram_lines) >= 2:
                        table_8_start = int(histogram_lines[-2].split(':')[0])
                        table_9_start = int(histogram_lines[-1].split(':')[0])
                        table_8_end = table_9_start - 2
                        
                        table_result = self.file_utils.run_command(f"zcat {hold_file} | sed -n '{table_8_start},{table_8_end}p'")
                        
                        if table_result.strip():
                            colored_table = self._colorize_histogram_table(table_result.strip())
                            content += '<pre style="background: #f5f5f5; padding: 15px; border-radius: 5px; overflow-x: auto; font-family: monospace; font-size: 10px; line-height: 1.4;">'
                            content += colored_table
                            content += '</pre>'
                
                content += f'<p style="margin-top: 10px; font-size: 11px; color: #666;"><strong>Source:</strong> {self._make_source_clickable(hold_file)}</p>'
            
            except Exception as e:
                content += f'<p style="color: #e74c3c;">Error parsing hold histogram: {e}</p>'
        
        content += '</div>'
        return content
    
    def _generate_critical_paths_tab_content_separated(self, external_paths: List[Dict], internal_paths: List[Dict], 
                                                       timing_type: str, external_file: Optional[str], 
                                                       internal_file: Optional[str]) -> str:
        """Generate critical paths tab with SEPARATED external and internal path traces
        
        Args:
            external_paths: List of external (input/output) path dictionaries
            internal_paths: List of internal (core logic) path dictionaries
            timing_type: 'setup' or 'hold'
            external_file: Path to external paths report
            internal_file: Path to internal paths report
            
        Returns:
            HTML string for critical paths tab with external/internal subsections
        """
        tab_id = f"{timing_type}-paths-tab"
        title_color = '#27ae60' if timing_type == 'setup' else '#e67e22'
        
        # Count only violating paths (slack < 0)
        external_violating = [p for p in external_paths if p['header'].get('slack', 0) < 0]
        internal_violating = [p for p in internal_paths if p['header'].get('slack', 0) < 0]
        total_violating = len(external_violating) + len(internal_violating)
        
        content = f'''
        <div id="{tab_id}" class="timing-tab-content" style="display: none;">
            <h2 style="color: {title_color}; margin-bottom: 15px;">Critical Paths - {timing_type.upper()}</h2>
            <p style="color: #666; margin-bottom: 20px;">
                Detailed timing path traces separated by path type: 
                <strong>{len(external_violating)} External</strong> (input/output ports) + 
                <strong>{len(internal_violating)} Internal</strong> (core logic) = 
                <strong>{total_violating} Total Violations</strong>
            </p>
        '''
        
        if not external_paths and not internal_paths:
            content += f'<div style="background: #f0f8ff; border: 2px solid #3498db; border-radius: 8px; padding: 20px; text-align: center;">'
            content += f'<p style="font-size: 16px; color: #2c3e50; margin: 0;">No critical {timing_type} paths found or reports not available</p>'
            content += '</div></div>'
            return content
            
        # If all paths are passing (no violations)
        if total_violating == 0:
            content += '''
            <div style="background: #d4edda; border: 2px solid #c3e6cb; border-radius: 8px; padding: 30px; text-align: center; margin: 20px 0;">
                <h3 style="color: #155724; margin: 0 0 10px 0; font-size: 24px;">
                    &#x2705; All Paths Meeting Timing
                </h3>
                <p style="color: #155724; margin: 0; font-size: 16px;">
                    No timing violations found - All external and internal paths meet timing requirements
                </p>
            </div>
            </div>
            '''
            return content
        
        # EXTERNAL PATHS SUBSECTION
        if external_violating:
            content += f'''
            <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 15px 20px; border-radius: 8px; margin: 20px 0 15px 0;">
                <h3 style="margin: 0; font-size: 20px;">
                    &#x1F4E5; External Paths ({len(external_violating)} violations)
                    <span style="font-size: 14px; font-weight: normal; opacity: 0.9; margin-left: 15px;">
                        Input/Output interface timing paths
                    </span>
                </h3>
            </div>
            '''
            content += self._generate_path_table(external_paths, timing_type, 'external')
            if external_file:
                content += f'<p style="margin: 10px 0 0 0; font-size: 11px; color: #666;"><strong>Source:</strong> {self._make_source_clickable(external_file)}</p>'
        elif external_paths:  # Has paths but no violations
            content += '''
            <div style="background: #d4edda; border: 1px solid #c3e6cb; border-radius: 8px; padding: 15px; margin: 20px 0;">
                <p style="color: #155724; margin: 0; font-size: 14px;">
                    &#x2705; <strong>External Paths:</strong> All external paths meet timing (no violations)
                </p>
            </div>
            '''
        
        # INTERNAL PATHS SUBSECTION
        if internal_violating:
            content += f'''
            <div style="background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); color: white; padding: 15px 20px; border-radius: 8px; margin: 30px 0 15px 0;">
                <h3 style="margin: 0; font-size: 20px;">
                    &#x2699; Internal Paths ({len(internal_violating)} violations)
                    <span style="font-size: 14px; font-weight: normal; opacity: 0.9; margin-left: 15px;">
                        Core logic timing paths (flop-to-flop, flop-to-macro, etc.)
                    </span>
                </h3>
            </div>
            '''
            content += self._generate_path_table(internal_paths, timing_type, 'internal')
            if internal_file:
                content += f'<p style="margin: 10px 0 0 0; font-size: 11px; color: #666;"><strong>Source:</strong> {self._make_source_clickable(internal_file)}</p>'
        elif internal_paths:  # Has paths but no violations
            content += '''
            <div style="background: #d4edda; border: 1px solid #c3e6cb; border-radius: 8px; padding: 15px; margin: 30px 0;">
                <p style="color: #155724; margin: 0; font-size: 14px;">
                    &#x2705; <strong>Internal Paths:</strong> All internal paths meet timing (no violations)
                </p>
            </div>
            '''
        
        content += '</div>'
        return content
    
    def _generate_path_table(self, paths: List[Dict], timing_type: str, path_category: str) -> str:
        """Generate optimized HTML table for paths with clickable rows (no Actions column)
        
        Args:
            paths: List of path dictionaries
            timing_type: 'setup' or 'hold'
            path_category: 'external' or 'internal' (used for unique IDs)
            
        Returns:
            HTML string for path summary table
        """
        if not paths:
            return ''
        
        # Sort paths by slack (worst first) and filter out non-violating paths (slack >= 0)
        sorted_paths = sorted(paths, key=lambda x: x['header'].get('slack', 0))
        violating_paths = [p for p in sorted_paths if p['header'].get('slack', 0) < 0]
        
        # If no violating paths, show a message
        if not violating_paths:
            return '''
            <div style="background: #d4edda; border: 1px solid #c3e6cb; border-radius: 8px; padding: 20px; text-align: center; margin-bottom: 20px;">
                <p style="font-size: 14px; color: #155724; margin: 0;">
                    &#x2705; <strong>No timing violations found</strong> - All paths meet timing
                </p>
            </div>
            '''
        
        # Start table with optimized CSS
        content = '''
        <style>
            .critical-paths-table-optimized {
                width: 100%;
                border-collapse: collapse;
                font-size: 13px;
                box-shadow: 0 2px 8px rgba(0,0,0,0.1);
                table-layout: fixed;
            }
            
            .critical-paths-table-optimized th {
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: white;
                padding: 12px 8px;
                font-weight: 600;
                text-align: center;
                border: 1px solid #ddd;
            }
            
            .critical-paths-table-optimized td {
                padding: 10px 8px;
                border: 1px solid #ddd;
                vertical-align: middle;
            }
            
            /* Clickable path rows */
            .path-row-clickable {
                cursor: pointer;
                transition: background-color 0.2s, transform 0.1s;
            }
            
            .path-row-clickable:hover {
                background-color: #e3f2fd !important;
                transform: translateX(2px);
            }
            
            .path-row-clickable:active {
                transform: translateX(0px);
            }
            
            /* Column widths */
            .critical-paths-table-optimized th:nth-child(1),
            .critical-paths-table-optimized td:nth-child(1) {
                width: 40px;
                text-align: center;
                font-weight: 600;
            }
            
            .critical-paths-table-optimized th:nth-child(2),
            .critical-paths-table-optimized td:nth-child(2) {
                width: 60px;
                text-align: right;
                font-family: 'Courier New', Courier, monospace;
                font-weight: 600;
                padding-right: 10px;
            }
            
            .critical-paths-table-optimized th:nth-child(3),
            .critical-paths-table-optimized td:nth-child(3) {
                width: 350px;
                font-family: 'Courier New', Courier, monospace;
                font-size: 12px;
                text-align: left;
            }
            
            .critical-paths-table-optimized th:nth-child(4),
            .critical-paths-table-optimized td:nth-child(4) {
                width: 350px;
                font-family: 'Courier New', Courier, monospace;
                font-size: 12px;
                text-align: left;
            }
            
            .critical-paths-table-optimized th:nth-child(5),
            .critical-paths-table-optimized td:nth-child(5) {
                width: 120px;
                text-align: center;
                font-family: 'Courier New', Courier, monospace;
                font-size: 12px;
                white-space: nowrap;
            }
            
            /* Expand icon */
            .expand-icon {
                display: inline-block;
                transition: transform 0.3s;
                color: #667eea;
                font-weight: bold;
                margin-right: 5px;
            }
            
            .expand-icon.expanded {
                transform: rotate(90deg);
            }
            
            /* Path cell with tooltip */
            .path-cell-tooltip {
                position: relative;
                cursor: help !important;
                overflow: hidden;
                text-overflow: ellipsis;
                white-space: nowrap;
            }
            
            /* Override clickable row cursor for path cells */
            .path-row-clickable .path-cell-tooltip {
                cursor: help !important;
            }
            
            .path-cell-tooltip:hover::after {
                content: attr(data-full-path);
                position: absolute;
                left: 0;
                top: 100%;
                background: #fffbea;
                border: 2px solid #ffc107;
                padding: 10px 12px;
                border-radius: 6px;
                font-family: 'Courier New', Courier, monospace;
                font-size: 11px;
                color: #333;
                white-space: pre-wrap;
                word-break: break-all;
                z-index: 1000;
                max-width: 600px;
                box-shadow: 0 4px 12px rgba(0,0,0,0.25);
                pointer-events: none;
            }
            
            /* Trace row */
            .trace-row-optimized {
                background-color: #f8f9fa !important;
                cursor: default !important;
            }
            
            .trace-row-optimized td {
                padding: 0 !important;
            }
        </style>
        
        <div style="overflow-x: auto; margin-bottom: 20px;">
            <table class="critical-paths-table-optimized">
                <thead>
                    <tr>
                        <th>#</th>
                        <th>Slack (ns)</th>
                        <th>Startpoint</th>
                        <th>Endpoint</th>
                        <th>Clock</th>
                    </tr>
                </thead>
                <tbody>
        '''
        
        for path in violating_paths[:20]:  # Limit to top 20 for performance
            slack = path['header'].get('slack', 0)
            slack_color = '#e74c3c' if slack < -0.050 else '#e67e22'
            
            # Get full paths
            beginpoint_full = path['header'].get('beginpoint', 'N/A')
            endpoint_full = path['header'].get('endpoint', 'N/A')
            
            # Truncate paths intelligently (first 25 + last 30)
            beginpoint_display, beginpoint_tooltip = self._truncate_path_smart(beginpoint_full)
            endpoint_display, endpoint_tooltip = self._truncate_path_smart(endpoint_full)
            
            # Get both clocks
            launch_clock = path['header'].get('launch_clock', 'N/A')
            capture_clock = path['header'].get('clock', 'N/A')
            clock_display = f"{launch_clock} &#8594; {capture_clock}"  # â†’ arrow
            
            path_num = path['number']
            
            # Unique ID for each path trace
            trace_id = f"path-trace-{timing_type}-{path_category}-{path_num}"
            
            # Row background color based on severity
            row_bg = '#fff5f5' if slack < -0.050 else '#fffbf0'
            
            content += f'''
                <tr class="path-row-clickable" onclick="togglePathTraceOptimized('{trace_id}', event)" data-path-id="{trace_id}" style="background: {row_bg};">
                    <td><span class="expand-icon" id="icon-{trace_id}">&#9654;</span>{path_num}</td>
                    <td style="color: {slack_color}; font-weight: bold;">{slack:.3f}</td>
                    <td class="path-cell-tooltip" data-full-path="{beginpoint_tooltip}" title="{beginpoint_tooltip}">{beginpoint_display}</td>
                    <td class="path-cell-tooltip" data-full-path="{endpoint_tooltip}" title="{endpoint_tooltip}">{endpoint_display}</td>
                    <td>{clock_display}</td>
                </tr>
                <tr id="{trace_id}" class="trace-row-optimized" style="display: none;">
                    <td colspan="5" style="padding: 0; border: 1px solid #ddd; text-align: left;">
                        <div style="background: #f9f9f9; padding: 20px; margin: 10px; text-align: left;">
                            <h4 style="color: #2c3e50; margin-top: 0;">Path Details</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 15px; margin-bottom: 15px; font-size: 12px;">
                                <div><strong>Slack:</strong> {slack:.3f} ns</div>
                                <div><strong>Required Time:</strong> {path['header'].get('required_time', 'N/A')} ns</div>
                                <div><strong>Arrival Time:</strong> {path['header'].get('arrival_time', 'N/A')} ns</div>
                                <div><strong>Path Group:</strong> {path['header'].get('path_group', 'N/A')}</div>
                                <div><strong>Analysis View:</strong> {path['header'].get('analysis_view', 'N/A')}</div>
                                <div><strong>Launch Clock:</strong> {path['header'].get('launch_clock', 'N/A')}</div>
                            </div>
                            <div style="background: white; border: 1px solid #ddd; border-radius: 5px; padding: 15px;">
                                <div style="display: flex; justify-content: flex-start; align-items: center; gap: 15px; margin-bottom: 15px; position: sticky; top: 0; background: white; z-index: 10; padding-bottom: 10px; border-bottom: 2px solid #667eea;">
                                    <button onclick="openTraceFullScreen('{trace_id}')" 
                                            style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border: none; padding: 8px 16px; border-radius: 4px; cursor: pointer; font-size: 12px; font-weight: bold; box-shadow: 0 2px 4px rgba(0,0,0,0.2); transition: transform 0.2s; flex-shrink: 0;"
                                            onmouseover="this.style.transform='scale(1.05)'" 
                                            onmouseout="this.style.transform='scale(1)'">
                                        &#128442; View Full Screen
                                    </button>
                                    <h5 style="color: #667eea; margin: 0;">
                                        &#128195; Complete Timing Path Trace
                                        <span style="font-size: 11px; color: #95a5a6; font-weight: normal; margin-left: 10px;">
                                            (Full cell-by-cell timing breakdown with all columns)
                                        </span>
                                    </h5>
                                </div>
                                <div class="trace-container" style="position: relative; max-width: 100%;">
                                    <!-- Main trace content with native scrollbar hidden -->
                                    <div id="{trace_id}-wrapper" class="trace-scroll-wrapper" style="overflow: hidden; height: 500px; width: 100%; border: 1px solid #dee2e6; border-radius: 4px; background: #f8f9fa; position: relative;">
                                        <div id="{trace_id}-content-wrapper" style="position: absolute; left: 0; top: 0; width: max-content; transition: transform 0.05s ease-out; will-change: transform;">
                                            <pre id="{trace_id}-content" style="font-family: 'Courier New', Courier, monospace; font-size: 10px; line-height: 1.3; margin: 0; padding: 12px; white-space: pre; display: inline-block; min-width: 3000px;">{self._format_path_trace(path)}</pre>
                                        </div>
                                    </div>
                                    
                                    <!-- Custom scrollbar -->
                                    <div id="{trace_id}-scrollbar-track" style="position: relative; width: 100%; height: 16px; background: #e0e0e0; border: 1px solid #bdbdbd; border-radius: 8px; margin-top: 8px; cursor: pointer;">
                                        <div id="{trace_id}-scrollbar-thumb" style="position: absolute; left: 0; top: 0; height: 100%; width: 150px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 8px; cursor: grab; transition: background 0.2s; box-shadow: 0 2px 4px rgba(0,0,0,0.2);">
                                        </div>
                                    </div>
                                    
                                    <script>
                                        (function() {{
                                            const wrapper = document.getElementById('{trace_id}-wrapper');
                                            const contentWrapper = document.getElementById('{trace_id}-content-wrapper');
                                            const content = document.getElementById('{trace_id}-content');
                                            const track = document.getElementById('{trace_id}-scrollbar-track');
                                            const thumb = document.getElementById('{trace_id}-scrollbar-thumb');
                                            
                                            if (!wrapper || !contentWrapper || !content || !track || !thumb) return;
                                            
                                            let currentTranslateX = 0;
                                            let isDragging = false;
                                            let startX = 0;
                                            let thumbStartX = 0;
                                            
                                            
                                            // Hover effect
                                            thumb.addEventListener('mouseenter', function() {{
                                                if (!isDragging) {{
                                                    thumb.style.background = 'linear-gradient(135deg, #5568d3 0%, #653a8b 100%)';
                                                }}
                                            }});
                                            
                                            thumb.addEventListener('mouseleave', function() {{
                                                if (!isDragging) {{
                                                    thumb.style.background = 'linear-gradient(135deg, #667eea 0%, #764ba2 100%)';
                                                }}
                                            }});
                                            
                                            // Dragging the thumb
                                            thumb.addEventListener('mousedown', function(e) {{
                                                isDragging = true;
                                                startX = e.clientX;
                                                thumbStartX = thumb.offsetLeft;
                                                thumb.style.cursor = 'grabbing';
                                                thumb.style.background = 'linear-gradient(135deg, #4c5fd1 0%, #5a2f7a 100%)';
                                                contentWrapper.style.transition = 'none';
                                                e.preventDefault();
                                                e.stopPropagation();
                                            }});
                                            
                                            document.addEventListener('mousemove', function(e) {{
                                                if (!isDragging) return;
                                                
                                                const deltaX = e.clientX - startX;
                                                const newThumbX = thumbStartX + deltaX;
                                                const maxThumbPos = track.clientWidth - thumb.clientWidth;
                                                const boundedX = Math.max(0, Math.min(newThumbX, maxThumbPos));
                                                
                                                thumb.style.left = boundedX + 'px';
                                                
                                                // Calculate transform position
                                                const scrollPercent = boundedX / maxThumbPos;
                                                const contentWidth = content.offsetWidth;
                                                const visibleWidth = wrapper.clientWidth;
                                                const maxScroll = Math.max(0, contentWidth - visibleWidth);
                                                const newTranslateX = -scrollPercent * maxScroll;
                                                
                                                if (maxScroll > 0) {{
                                                    currentTranslateX = newTranslateX;
                                                    contentWrapper.style.transform = `translateX(${{newTranslateX}}px)`;
                                                }}
                                            }});
                                            
                                            document.addEventListener('mouseup', function(e) {{
                                                if (isDragging) {{
                                                    isDragging = false;
                                                    thumb.style.cursor = 'grab';
                                                    thumb.style.background = 'linear-gradient(135deg, #667eea 0%, #764ba2 100%)';
                                                    contentWrapper.style.transition = 'transform 0.05s ease-out';
                                                }}
                                            }});
                                            
                                            // Clicking on track
                                            track.addEventListener('click', function(e) {{
                                                if (e.target === thumb || e.target.parentElement === thumb) return;
                                                
                                                const trackRect = track.getBoundingClientRect();
                                                const clickX = e.clientX - trackRect.left;
                                                const maxThumbPos = track.clientWidth - thumb.clientWidth;
                                                const newLeft = Math.max(0, Math.min(clickX - thumb.clientWidth/2, maxThumbPos));
                                                
                                                thumb.style.left = newLeft + 'px';
                                                
                                                const scrollPercent = newLeft / maxThumbPos;
                                                const contentWidth = content.offsetWidth;
                                                const visibleWidth = wrapper.clientWidth;
                                                const maxScroll = Math.max(0, contentWidth - visibleWidth);
                                                const newTranslateX = -scrollPercent * maxScroll;
                                                
                                                if (maxScroll > 0) {{
                                                    currentTranslateX = newTranslateX;
                                                    contentWrapper.style.transform = `translateX(${{newTranslateX}}px)`;
                                                }}
                                            }});
                                        }})();
                                    </script>
                                    
                                </div>
                                <p style="margin: 10px 0 0 0; font-size: 11px; color: #7f8c8d;">
                                    <strong>Tip:</strong> Use the scrollbar below the trace to scroll horizontally, or click "View Full Screen" for better visibility
                                </p>
                            </div>
                        </div>
                    </td>
                </tr>
            '''
        
        content += '''
                </tbody>
            </table>
        </div>
        '''
        
        return content
    
    def _generate_critical_paths_tab_content(self, paths: List[Dict], timing_type: str, 
                                            external_file: Optional[str], internal_file: Optional[str]) -> str:
        """Generate critical paths tab with detailed path traces (LEGACY METHOD - kept for compatibility)
        
        Args:
            paths: List of parsed path dictionaries
            timing_type: 'setup' or 'hold'
            external_file: Path to external paths report
            internal_file: Path to internal paths report
            
        Returns:
            HTML string for critical paths tab
        """
        tab_id = f"{timing_type}-paths-tab"
        title_color = '#27ae60' if timing_type == 'setup' else '#e67e22'
        
        content = f'''
        <div id="{tab_id}" class="timing-tab-content" style="display: none;">
            <h2 style="color: {title_color}; margin-bottom: 15px;">Critical Paths - {timing_type.upper()}</h2>
            <p style="color: #666; margin-bottom: 25px;">Detailed timing path traces for the most critical {timing_type} violations</p>
        '''
        
        if not paths:
            content += f'<div style="background: #f0f8ff; border: 2px solid #3498db; border-radius: 8px; padding: 20px; text-align: center;">'
            content += f'<p style="font-size: 16px; color: #2c3e50; margin: 0;">No critical {timing_type} paths found or reports not available</p>'
            content += '</div></div>'
            return content
        
        # Sort paths by slack (worst first)
        sorted_paths = sorted(paths, key=lambda x: x['header'].get('slack', 0))
        
        # Generate path summary table
        content += '''
        <div style="overflow-x: auto; margin-bottom: 30px;">
            <table style="width: 100%; border-collapse: collapse; font-size: 13px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
                <thead>
                    <tr style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;">
                        <th style="padding: 12px; text-align: center; border: 1px solid #ddd;">#</th>
                        <th style="padding: 12px; text-align: center; border: 1px solid #ddd;">Actions</th>
                        <th style="padding: 12px; text-align: center; border: 1px solid #ddd;">Slack (ns)</th>
                        <th style="padding: 12px; text-align: left; border: 1px solid #ddd;">Status</th>
                        <th style="padding: 12px; text-align: left; border: 1px solid #ddd;">Type</th>
                        <th style="padding: 12px; text-align: left; border: 1px solid #ddd;">Startpoint</th>
                        <th style="padding: 12px; text-align: left; border: 1px solid #ddd;">Endpoint</th>
                        <th style="padding: 12px; text-align: left; border: 1px solid #ddd;">Clock</th>
                    </tr>
                </thead>
                <tbody>
        '''
        
        for path in sorted_paths[:20]:  # Limit to top 20 for performance
            slack = path['header'].get('slack', 0)
            slack_color = '#27ae60' if slack >= 0 else ('#f39c12' if slack >= -0.050 else '#e74c3c')
            status = path['header'].get('status', 'UNKNOWN')
            check_type = path['header'].get('check_type', 'N/A')
            beginpoint = path['header'].get('beginpoint', 'N/A')
            endpoint = path['header'].get('endpoint', 'N/A')
            clock = path['header'].get('clock', 'N/A')
            path_num = path['number']
            
            # Truncate long pin names
            beginpoint_short = beginpoint[:40] + '...' if len(beginpoint) > 40 else beginpoint
            endpoint_short = endpoint[:40] + '...' if len(endpoint) > 40 else endpoint
            
            # Unique ID for each path trace
            trace_id = f"path-trace-{timing_type}-{path_num}"
            
            content += f'''
                <tr style="background: {'#fff5f5' if slack < -0.050 else ('#fffbf0' if slack < 0 else 'white')}; border-bottom: 1px solid #eee;">
                    <td style="padding: 10px; text-align: center; border: 1px solid #ddd; font-weight: bold;">{path_num}</td>
                    <td style="padding: 10px; text-align: center; border: 1px solid #ddd;">
                        <button onclick="togglePathTrace('{trace_id}')" 
                                style="background: #3498db; color: white; border: none; padding: 6px 12px; border-radius: 4px; cursor: pointer; font-size: 11px; transition: transform 0.2s;"
                                onmouseover="this.style.transform='scale(1.05)'; this.style.background='#2980b9';" 
                                onmouseout="this.style.transform='scale(1)'; this.style.background='#3498db';">
                            View Trace
                        </button>
                    </td>
                    <td style="padding: 10px; text-align: center; border: 1px solid #ddd; font-weight: bold; color: {slack_color};">{slack:.3f}</td>
                    <td style="padding: 10px; border: 1px solid #ddd;"><span style="background: {slack_color}; color: white; padding: 3px 8px; border-radius: 3px; font-size: 11px; font-weight: bold;">{status}</span></td>
                    <td style="padding: 10px; border: 1px solid #ddd; font-size: 12px;">{check_type}</td>
                    <td style="padding: 10px; border: 1px solid #ddd; font-family: monospace; font-size: 11px;" title="{beginpoint}">{beginpoint_short}</td>
                    <td style="padding: 10px; border: 1px solid #ddd; font-family: monospace; font-size: 11px;" title="{endpoint}">{endpoint_short}</td>
                    <td style="padding: 10px; border: 1px solid #ddd; font-family: monospace; font-size: 11px;">{clock}</td>
                </tr>
                <tr id="{trace_id}" style="display: none;">
                    <td colspan="8" style="padding: 0; border: 1px solid #ddd; text-align: left;">
                        <div style="background: #f9f9f9; padding: 20px; margin: 10px; text-align: left;">
                            <h4 style="color: #2c3e50; margin-top: 0;">Path Details</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 15px; margin-bottom: 15px; font-size: 12px;">
                                <div><strong>Slack:</strong> {slack:.3f} ns</div>
                                <div><strong>Required Time:</strong> {path['header'].get('required_time', 'N/A')} ns</div>
                                <div><strong>Arrival Time:</strong> {path['header'].get('arrival_time', 'N/A')} ns</div>
                                <div><strong>Path Group:</strong> {path['header'].get('path_group', 'N/A')}</div>
                                <div><strong>Analysis View:</strong> {path['header'].get('analysis_view', 'N/A')}</div>
                                <div><strong>Launch Clock:</strong> {path['header'].get('launch_clock', 'N/A')}</div>
                            </div>
                            <div style="background: white; border: 1px solid #ddd; border-radius: 5px; padding: 15px;">
                                <div style="display: flex; justify-content: flex-start; align-items: center; gap: 15px; margin-bottom: 15px; position: sticky; top: 0; background: white; z-index: 10; padding-bottom: 10px; border-bottom: 2px solid #667eea;">
                                    <button onclick="openTraceFullScreen('{trace_id}')" 
                                            style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border: none; padding: 8px 16px; border-radius: 4px; cursor: pointer; font-size: 12px; font-weight: bold; box-shadow: 0 2px 4px rgba(0,0,0,0.2); transition: transform 0.2s; flex-shrink: 0;"
                                            onmouseover="this.style.transform='scale(1.05)'" 
                                            onmouseout="this.style.transform='scale(1)'">
                                        &#128442; View Full Screen
                                    </button>
                                    <h5 style="color: #667eea; margin: 0;">
                                        &#128195; Complete Timing Path Trace
                                        <span style="font-size: 11px; color: #95a5a6; font-weight: normal; margin-left: 10px;">
                                            (Full cell-by-cell timing breakdown with all columns)
                                        </span>
                                    </h5>
                                </div>
                                <div class="trace-container" style="position: relative; max-width: 100%;">
                                    <!-- Main trace content with native scrollbar hidden -->
                                    <div id="{trace_id}-wrapper" class="trace-scroll-wrapper" style="overflow: hidden; height: 500px; width: 100%; border: 1px solid #dee2e6; border-radius: 4px; background: #f8f9fa; position: relative;">
                                        <div id="{trace_id}-content-wrapper" style="position: absolute; left: 0; top: 0; width: max-content; transition: transform 0.05s ease-out; will-change: transform;">
                                            <pre id="{trace_id}-content" style="font-family: 'Courier New', Courier, monospace; font-size: 10px; line-height: 1.3; margin: 0; padding: 12px; white-space: pre; display: inline-block; min-width: 3000px;">{self._format_path_trace(path)}</pre>
                                        </div>
                                    </div>
                                    
                                    <!-- Custom scrollbar -->
                                    <div id="{trace_id}-scrollbar-track" style="position: relative; width: 100%; height: 16px; background: #e0e0e0; border: 1px solid #bdbdbd; border-radius: 8px; margin-top: 8px; cursor: pointer;">
                                        <div id="{trace_id}-scrollbar-thumb" style="position: absolute; left: 0; top: 0; height: 100%; width: 150px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 8px; cursor: grab; transition: background 0.2s; box-shadow: 0 2px 4px rgba(0,0,0,0.2);">
                                        </div>
                                    </div>
                                    
                                    <script>
                                        (function() {{
                                            const wrapper = document.getElementById('{trace_id}-wrapper');
                                            const contentWrapper = document.getElementById('{trace_id}-content-wrapper');
                                            const content = document.getElementById('{trace_id}-content');
                                            const track = document.getElementById('{trace_id}-scrollbar-track');
                                            const thumb = document.getElementById('{trace_id}-scrollbar-thumb');
                                            
                                            if (!wrapper || !contentWrapper || !content || !track || !thumb) return;
                                            
                                            let currentTranslateX = 0;
                                            let isDragging = false;
                                            let startX = 0;
                                            let thumbStartX = 0;
                                            
                                            
                                            // Hover effect
                                            thumb.addEventListener('mouseenter', function() {{
                                                if (!isDragging) {{
                                                    thumb.style.background = 'linear-gradient(135deg, #5568d3 0%, #653a8b 100%)';
                                                }}
                                            }});
                                            
                                            thumb.addEventListener('mouseleave', function() {{
                                                if (!isDragging) {{
                                                    thumb.style.background = 'linear-gradient(135deg, #667eea 0%, #764ba2 100%)';
                                                }}
                                            }});
                                            
                                            // Dragging the thumb
                                            thumb.addEventListener('mousedown', function(e) {{
                                                isDragging = true;
                                                startX = e.clientX;
                                                thumbStartX = thumb.offsetLeft;
                                                thumb.style.cursor = 'grabbing';
                                                thumb.style.background = 'linear-gradient(135deg, #4c5fd1 0%, #5a2f7a 100%)';
                                                contentWrapper.style.transition = 'none';
                                                e.preventDefault();
                                                e.stopPropagation();
                                            }});
                                            
                                            document.addEventListener('mousemove', function(e) {{
                                                if (!isDragging) return;
                                                
                                                const deltaX = e.clientX - startX;
                                                const newThumbX = thumbStartX + deltaX;
                                                const maxThumbPos = track.clientWidth - thumb.clientWidth;
                                                const boundedX = Math.max(0, Math.min(newThumbX, maxThumbPos));
                                                
                                                thumb.style.left = boundedX + 'px';
                                                
                                                // Calculate transform position
                                                const scrollPercent = boundedX / maxThumbPos;
                                                const contentWidth = content.offsetWidth;
                                                const visibleWidth = wrapper.clientWidth;
                                                const maxScroll = Math.max(0, contentWidth - visibleWidth);
                                                const newTranslateX = -scrollPercent * maxScroll;
                                                
                                                if (maxScroll > 0) {{
                                                    currentTranslateX = newTranslateX;
                                                    contentWrapper.style.transform = `translateX(${{newTranslateX}}px)`;
                                                }}
                                            }});
                                            
                                            document.addEventListener('mouseup', function(e) {{
                                                if (isDragging) {{
                                                    isDragging = false;
                                                    thumb.style.cursor = 'grab';
                                                    thumb.style.background = 'linear-gradient(135deg, #667eea 0%, #764ba2 100%)';
                                                    contentWrapper.style.transition = 'transform 0.05s ease-out';
                                                }}
                                            }});
                                            
                                            // Clicking on track
                                            track.addEventListener('click', function(e) {{
                                                if (e.target === thumb || e.target.parentElement === thumb) return;
                                                
                                                const trackRect = track.getBoundingClientRect();
                                                const clickX = e.clientX - trackRect.left;
                                                const maxThumbPos = track.clientWidth - thumb.clientWidth;
                                                const newLeft = Math.max(0, Math.min(clickX - thumb.clientWidth/2, maxThumbPos));
                                                
                                                thumb.style.left = newLeft + 'px';
                                                
                                                const scrollPercent = newLeft / maxThumbPos;
                                                const contentWidth = content.offsetWidth;
                                                const visibleWidth = wrapper.clientWidth;
                                                const maxScroll = Math.max(0, contentWidth - visibleWidth);
                                                const newTranslateX = -scrollPercent * maxScroll;
                                                
                                                if (maxScroll > 0) {{
                                                    currentTranslateX = newTranslateX;
                                                    contentWrapper.style.transform = `translateX(${{newTranslateX}}px)`;
                                                }}
                                            }});
                                        }})();
                                    </script>
                                    
                                </div>
                                <p style="margin: 10px 0 0 0; font-size: 11px; color: #7f8c8d;">
                                    <strong>Tip:</strong> Use the scrollbar below the trace to scroll horizontally, or click "View Full Screen" for better visibility
                                </p>
                            </div>
                        </div>
                    </td>
                </tr>
            '''
        
        content += '''
                </tbody>
            </table>
        </div>
        '''
        
        # Add source files
        content += '<div style="background: #f9f9f9; border-left: 4px solid #3498db; padding: 15px; margin-top: 20px; font-size: 12px; color: #666;">'
        content += '<strong style="color: #2c3e50;">Data Sources:</strong><br>'
        if external_file:
            content += f'External Paths: {self._make_source_clickable(external_file)}<br>'
        if internal_file:
            content += f'Internal Paths: {self._make_source_clickable(internal_file)}'
        content += '</div>'
        
        content += '</div>'
        return content
    
    def _truncate_path_smart(self, path: str, max_length: int = 75) -> tuple:
        """
        Intelligently truncate long hierarchical paths.
        Shows beginning and end for context.
        
        Args:
            path: Full hierarchical path
            max_length: Maximum display length (default: 75)
        
        Returns:
            tuple: (truncated_display, full_path_for_tooltip)
        
        Example:
            Full: pmux_channel/g_shiftreg_i2ir2icl2_cause_res_credit_inc_fanout0__out_e4_10610222_0_feed_out/g_ff1/ps_pack_reg[0]/Q
            Display: pmux_channel/g_shiftreg_i2ir2i...ps_pack_reg[0]/Q
            Tooltip: [full path]
        """
        if not path or path == 'N/A':
            return (path, path)
        
        if len(path) <= max_length:
            return (path, path)
        
        # Show first 30 chars + "..." + last 40 chars (more context than before)
        first_part = path[:30]
        last_part = path[-40:]
        truncated = f"{first_part}...{last_part}"
        
        return (truncated, path)
    
    def _format_path_trace(self, path: Dict) -> str:
        """Format path trace for display with FULL timing path table
        
        Args:
            path: Path dictionary with timing_path data
            
        Returns:
            Formatted string for path trace
        """
        # Check if we have the full timing path table
        if path.get('timing_path_full'):
            # Display the complete path data
            formatted = "=" * 150 + "\n"
            formatted += f"COMPLETE PATH TRACE - Path #{path['number']}\n"
            formatted += f"From: {path['header'].get('beginpoint', 'N/A')}\n"
            formatted += f"To:   {path['header'].get('endpoint', 'N/A')}\n"
            formatted += f"Slack: {path['header'].get('slack', 'N/A')} ns\n"
            
            # Add path type info if available
            if path['header'].get('check_type'):
                formatted += f"Type: {path['header'].get('check_type')}\n"
            if path['header'].get('status'):
                formatted += f"Status: {path['header'].get('status')}\n"
                
            formatted += "=" * 150 + "\n\n"
            
            # Add the complete path content (everything between Path N: markers)
            formatted += path['timing_path_full']
            formatted += "\n" + "=" * 150 + "\n"
            
            return formatted
        
        # Fallback: Try to show raw section if available
        if not path.get('timing_path') and not path.get('timing_path_full'):
            formatted = "=" * 150 + "\n"
            formatted += f"TIMING PATH #{path['number']} - Limited Data Available\n"
            formatted += "=" * 150 + "\n\n"
            formatted += f"From: {path['header'].get('beginpoint', 'N/A')}\n"
            formatted += f"To:   {path['header'].get('endpoint', 'N/A')}\n"
            formatted += f"Slack: {path['header'].get('slack', 'N/A')} ns\n"
            formatted += f"Analysis View: {path['header'].get('analysis_view', 'N/A')}\n\n"
            
            # Try to show raw section if available
            if path.get('raw_section'):
                formatted += "Raw path data (first 2000 characters):\n"
                formatted += "-" * 150 + "\n"
                formatted += path['raw_section'][:2000] + "\n"
                formatted += "-" * 150 + "\n\n"
                formatted += "NOTE: Full cell-by-cell timing path table not found in this section.\n"
                formatted += "The report may not contain detailed timing breakdown for this path.\n"
            else:
                formatted += "No detailed timing data available for this path.\n"
            
            return formatted
        
        # If we have some timing_path data but not the full table
        if path.get('timing_path') and not path.get('timing_path_full'):
            formatted = "SIMPLIFIED TIMING PATH (Partial data extracted)\n"
            formatted += "=" * 100 + "\n"
            formatted += f"Path #{path['number']}: {path['header'].get('beginpoint', 'N/A')} -> {path['header'].get('endpoint', 'N/A')}\n"
            formatted += f"Slack: {path['header'].get('slack', 'N/A')} ns\n"
            formatted += "=" * 100 + "\n\n"
            
            # Show available path lines
            for idx, cell in enumerate(path['timing_path'][:20], 1):
                formatted += f"{cell.get('line', 'N/A')}\n"
            
            if len(path['timing_path']) > 20:
                formatted += f"\n... ({len(path['timing_path']) - 20} more cells in path) ...\n"
            
            return formatted
        
        # Should never reach here
        return "No timing data available"
    
    def _generate_timing_tab_styles_and_scripts(self) -> str:
        """Generate CSS and JavaScript for timing tabs
        
        Returns:
            HTML string with styles and scripts
        """
        return '''
        <style>
            .timing-tabs {
                display: flex;
                border-bottom: 2px solid #3498db;
                margin-bottom: 20px;
                background: #f8f9fa;
                border-radius: 8px 8px 0 0;
                overflow: hidden;
            }
            .timing-tab-btn {
                flex: 1;
                padding: 15px 20px;
                background: transparent;
                border: none;
                cursor: pointer;
                font-size: 14px;
                font-weight: 600;
                color: #5a6c7d;
                transition: all 0.3s;
                position: relative;
            }
            .timing-tab-btn:hover {
                background: rgba(52, 152, 219, 0.1);
                color: #2980b9;
            }
            .timing-tab-btn.active {
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: white;
            }
            .timing-tab-btn.active::after {
                content: '';
                position: absolute;
                bottom: -2px;
                left: 0;
                right: 0;
                height: 2px;
                background: #667eea;
            }
            .timing-tab-content {
                animation: fadeIn 0.3s;
            }
            @keyframes fadeIn {
                from { opacity: 0; }
                to { opacity: 1; }
            }
            
            /* Hide native scrollbar (we have custom one below) */
            .trace-scroll-wrapper {
                overflow-x: scroll !important;
                overflow-y: auto !important;
                position: relative;
                scrollbar-width: none !important;  /* Firefox */
                -ms-overflow-style: none !important;  /* IE/Edge */
            }
            .trace-scroll-wrapper::-webkit-scrollbar {
                display: none !important;  /* Chrome/Safari */
            }
            
            /* GIANT 30px scrollbar */
            .trace-scroll-wrapper::-webkit-scrollbar {
                height: 30px !important;  /* GIANT HEIGHT */
                width: 30px !important;
                -webkit-appearance: none;
            }
            
        </style>
        
        <script>
            function openTimingTab(evt, tabId) {
                // Hide all tab contents
                var tabContents = document.getElementsByClassName('timing-tab-content');
                for (var i = 0; i < tabContents.length; i++) {
                    tabContents[i].style.display = 'none';
                }
                
                // Remove active class from all buttons
                var tabBtns = document.getElementsByClassName('timing-tab-btn');
                for (var i = 0; i < tabBtns.length; i++) {
                    tabBtns[i].className = tabBtns[i].className.replace(' active', '');
                }
                
                // Show current tab and mark button as active
                document.getElementById(tabId).style.display = 'block';
                evt.currentTarget.className += ' active';
            }
            
            function togglePathTrace(traceId) {
                var trace = document.getElementById(traceId);
                if (trace.style.display === 'none') {
                    trace.style.display = 'table-row';
                } else {
                    trace.style.display = 'none';
                }
            }
            
            // Toggle trace for optimized table (clickable rows)
            function togglePathTraceOptimized(traceId, event) {
                // Don't toggle if clicking on a path cell (for tooltip)
                if (event && event.target.classList.contains('path-cell-tooltip')) {
                    return;
                }
                
                var trace = document.getElementById(traceId);
                var icon = document.getElementById('icon-' + traceId);
                
                if (trace.style.display === 'none' || trace.style.display === '') {
                    trace.style.display = 'table-row';
                    if (icon) icon.classList.add('expanded');
                } else {
                    trace.style.display = 'none';
                    if (icon) icon.classList.remove('expanded');
                }
            }
            
            // Full screen trace modal
            function openTraceFullScreen(traceId) {
                // Get the trace content
                var traceContent = document.getElementById(traceId + '-content');
                if (!traceContent) return;
                
                // Create modal overlay
                var modal = document.createElement('div');
                modal.id = 'fullscreen-trace-modal';
                modal.style.cssText = `
                    position: fixed;
                    top: 0;
                    left: 0;
                    width: 100%;
                    height: 100%;
                    background: rgba(0, 0, 0, 0.95);
                    z-index: 10000;
                    display: flex;
                    flex-direction: column;
                    padding: 20px;
                    box-sizing: border-box;
                `;
                
                // Create header with close button
                var header = document.createElement('div');
                header.style.cssText = `
                    display: flex;
                    justify-content: space-between;
                    align-items: center;
                    margin-bottom: 15px;
                    color: white;
                `;
                
                var title = document.createElement('h2');
                title.textContent = 'ðŸ“‹ Full Screen Timing Path Trace';
                title.style.cssText = 'margin: 0; font-size: 24px; color: #667eea;';
                
                var closeBtn = document.createElement('button');
                closeBtn.innerHTML = 'âœ• Close';
                closeBtn.style.cssText = `
                    background: #e74c3c;
                    color: white;
                    border: none;
                    padding: 10px 20px;
                    font-size: 16px;
                    font-weight: bold;
                    border-radius: 5px;
                    cursor: pointer;
                    transition: background 0.2s;
                `;
                closeBtn.onmouseover = function() { this.style.background = '#c0392b'; };
                closeBtn.onmouseout = function() { this.style.background = '#e74c3c'; };
                closeBtn.onclick = function() { document.body.removeChild(modal); };
                
                header.appendChild(title);
                header.appendChild(closeBtn);
                
                // Create scrollable content container
                var contentContainer = document.createElement('div');
                contentContainer.style.cssText = `
                    flex: 1;
                    overflow: auto;
                    background: #f8f9fa;
                    border: 2px solid #667eea;
                    border-radius: 8px;
                    padding: 20px;
                    box-shadow: 0 4px 20px rgba(0, 0, 0, 0.5);
                `;
                
                // Clone the trace content
                var clonedContent = traceContent.cloneNode(true);
                clonedContent.id = '';
                clonedContent.style.cssText = `
                    font-family: 'Courier New', Courier, monospace;
                    font-size: 11px;
                    line-height: 1.4;
                    margin: 0;
                    white-space: pre;
                    color: #2c3e50;
                `;
                
                contentContainer.appendChild(clonedContent);
                
                // Add info footer
                var footer = document.createElement('div');
                footer.style.cssText = `
                    margin-top: 15px;
                    padding: 10px;
                    background: #34495e;
                    color: #ecf0f1;
                    border-radius: 5px;
                    font-size: 13px;
                    text-align: center;
                `;
                footer.innerHTML = 'ðŸ’¡ <strong>Tip:</strong> Scroll horizontally and vertically to view all timing path details | Press ESC to close';
                
                // Assemble modal
                modal.appendChild(header);
                modal.appendChild(contentContainer);
                modal.appendChild(footer);
                
                // Add to page
                document.body.appendChild(modal);
                
                // Close on ESC key
                document.addEventListener('keydown', function escHandler(e) {
                    if (e.key === 'Escape') {
                        document.body.removeChild(modal);
                        document.removeEventListener('keydown', escHandler);
                    }
                });
                
                // Close on background click
                modal.addEventListener('click', function(e) {
                    if (e.target === modal) {
                        document.body.removeChild(modal);
                    }
                });
            }
        </script>
        '''
    
    def _get_images_html_content(self) -> str:
        """Extract images content for unified HTML with full categorization
        
        Returns:
            HTML string for images section
        """
        try:
            # Find images directory
            images_dir = os.path.join(self.workarea, f"pnr_flow/nv_flow/{self.design_info.top_hier}/{self.design_info.ipo}/REPs/IMAGES")
            
            if not os.path.exists(images_dir):
                return '<div class="no-data">No debug images directory found</div>'
            
            # Find all image files (excluding hidden files starting with .)
            image_files = []
            for ext in ['*.png', '*.jpg', '*.jpeg', '*.gif']:
                all_files = glob.glob(os.path.join(images_dir, ext))
                # Filter out hidden files
                image_files.extend([f for f in all_files if not os.path.basename(f).startswith('.')])
            
            if not image_files:
                return f'<div class="no-data">No images found<br><small>{images_dir}</small></div>'
            
            # Categorize images
            categorized_images = self._categorize_images(image_files)
            
            content = '<div class="section-header">Debug Images Gallery</div>'
            content += f'<p><strong>Total Images:</strong> {len(image_files)} | <strong>Directory:</strong> {images_dir}</p>'
            
            # Add styles
            content += '''
            <style>
                .category-container {
                    margin: 20px 0;
                    border: 1px solid #ddd;
                    border-radius: 8px;
                    overflow: hidden;
                }
                .category-header {
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    color: white;
                    padding: 15px 20px;
                    cursor: pointer;
                    display: flex;
                    justify-content: space-between;
                    align-items: center;
                    font-weight: bold;
                    font-size: 16px;
                }
                .category-header:hover {
                    background: linear-gradient(135deg, #764ba2 0%, #667eea 100%);
                }
                .category-toggle {
                    transition: transform 0.3s ease;
                }
                .category-toggle.collapsed {
                    transform: rotate(-90deg);
                }
                .category-content {
                    padding: 20px;
                    background: #f9f9f9;
                    display: block;
                }
                .category-content.collapsed {
                    display: none;
                }
                .subcategory-header {
                    background: #34495e;
                    color: white;
                    padding: 10px 15px;
                    margin: 15px 0 10px 0;
                    border-radius: 5px;
                    font-weight: bold;
                }
                .image-grid {
                    display: grid;
                    grid-template-columns: repeat(auto-fill, minmax(220px, 1fr));
                    gap: 15px;
                    margin: 15px 0;
                }
                .image-card {
                    border: 1px solid #ddd;
                    border-radius: 8px;
                    padding: 10px;
                    background: white;
                    box-shadow: 0 2px 5px rgba(0,0,0,0.1);
                    transition: transform 0.2s ease, box-shadow 0.2s ease;
                }
                .image-card:hover {
                    transform: translateY(-5px);
                    box-shadow: 0 4px 12px rgba(0,0,0,0.2);
                }
                .image-card img {
                    width: 100%;
                    height: 160px;
                    object-fit: contain;
                    border-radius: 5px;
                    cursor: pointer;
                    background: #f5f5f5;
                }
                .image-name {
                    font-size: 10px;
                    margin-top: 8px;
                    color: #666;
                    word-break: break-all;
                    line-height: 1.3;
                }
                .image-description {
                    font-size: 9px;
                    color: #999;
                    margin-top: 4px;
                    font-style: italic;
                }
                .image-count {
                    background: rgba(255,255,255,0.2);
                    padding: 2px 8px;
                    border-radius: 12px;
                    font-size: 12px;
                }
                .image-modal {
                    display: none;
                    position: fixed;
                    z-index: 10000;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    background-color: rgba(0,0,0,0.95);
                    justify-content: center;
                    align-items: center;
                }
                .image-modal.active {
                    display: flex;
                }
                .image-modal img {
                    max-width: 95%;
                    max-height: 95%;
                    border-radius: 5px;
                }
                .image-modal-close {
                    position: absolute;
                    top: 20px;
                    right: 40px;
                    color: white;
                    font-size: 40px;
                    font-weight: bold;
                    cursor: pointer;
                }
                .show-more-btn {
                    display: block;
                    margin: 15px auto;
                    padding: 10px 30px;
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    color: white;
                    border: none;
                    border-radius: 25px;
                    font-size: 14px;
                    font-weight: bold;
                    cursor: pointer;
                    transition: transform 0.2s ease, box-shadow 0.2s ease;
                }
                .show-more-btn:hover {
                    transform: translateY(-2px);
                    box-shadow: 0 4px 12px rgba(0,0,0,0.3);
                }
                .show-more-btn:active {
                    transform: translateY(0);
                }
                .image-card.hidden {
                    display: none;
                }
            </style>
            '''
            
            # Category priority order
            category_priority = [
                'Synthesis Flow',
                'Floorplan/Layout',
                'Placement',
                'Clock Tree',
                'Routing',
                'Timing',
                'Power',
                'Signal Integrity',
                'DRC/DRV',
                'DRC Violation Snapshots',
                'ECO/Signoff Analysis',
                'Other'
            ]
            
            # Generate categorized sections
            for category in category_priority:
                if category not in categorized_images:
                    continue
                    
                # Count images in this category
                category_image_count = sum(len(categorized_images[category][subcategory]) 
                                         for subcategory in categorized_images[category])
                
                if category_image_count == 0:
                    continue
                    
                cat_id = category.replace(' ', '_').replace('/', '_')
                content += f'''
                <div class="category-container">
                    <div class="category-header" onclick="toggleImgCategory('{cat_id}')">
                        <span>{category}</span>
                        <span><span class="image-count">{category_image_count} images</span> <span class="category-toggle" id="toggle_{cat_id}">â–¼</span></span>
                    </div>
                    <div class="category-content" id="content_{cat_id}">
                '''
                
                # Generate subcategories
                subcat_index = 0
                for subcategory in categorized_images[category]:
                    images = categorized_images[category][subcategory]
                    if not images:
                        continue
                    
                    subcat_id = f"{cat_id}_subcat_{subcat_index}"
                    subcat_index += 1
                    initial_show = 12  # Show first 12 images by default
                    
                    content += f'''
                        <div class="subcategory-header">{subcategory} ({len(images)} images)</div>
                        <div class="image-grid" id="grid_{subcat_id}">
                    '''
                    
                    for idx, img_data in enumerate(images):
                        img_path = img_data['path']
                        img_name = img_data['name']
                        img_desc = img_data['description']
                        img_url = f"file://{os.path.abspath(img_path)}"
                        
                        # Add 'hidden' class to images beyond initial_show count
                        hidden_class = ' hidden' if idx >= initial_show else ''
                        
                        content += f'''
                            <div class="image-card{hidden_class}">
                                <img src="{img_url}" alt="{img_name}" onclick="showImageModal(this.src)" title="{img_desc}">
                                <div class="image-name">{img_name}</div>
                                <div class="image-description">{img_desc}</div>
                            </div>
                        '''
                    
                    content += '</div>'  # Close image-grid
                    
                    # Add "Show More" button if there are more than initial_show images
                    if len(images) > initial_show:
                        remaining = len(images) - initial_show
                        content += f'''
                        <button class="show-more-btn" onclick="showMoreImages('{subcat_id}', this)">
                            Show {remaining} More Images
                        </button>
                        '''
                
                content += '''
                    </div>
                </div>
                '''
            
            # Add image modal and scripts
            content += '''
            <div id="imageModal" class="image-modal" onclick="hideImageModal()">
                <span class="image-modal-close">&times;</span>
                <img id="modalImage" src="">
            </div>
            <script>
                function showImageModal(src) {
                    document.getElementById('modalImage').src = src;
                    document.getElementById('imageModal').classList.add('active');
                }
                function hideImageModal() {
                    document.getElementById('imageModal').classList.remove('active');
                }
                function toggleImgCategory(categoryId) {
                    var content = document.getElementById('content_' + categoryId);
                    var toggle = document.getElementById('toggle_' + categoryId);
                    if (content.classList.contains('collapsed')) {
                        content.classList.remove('collapsed');
                        toggle.classList.remove('collapsed');
                        toggle.textContent = 'â–¼';
                    } else {
                        content.classList.add('collapsed');
                        toggle.classList.add('collapsed');
                        toggle.textContent = 'â–¶';
                    }
                }
                function showMoreImages(subcatId, button) {
                    var grid = document.getElementById('grid_' + subcatId);
                    var hiddenImages = grid.querySelectorAll('.image-card.hidden');
                    
                    // Show next batch of images (12 at a time)
                    var showCount = Math.min(12, hiddenImages.length);
                    for (var i = 0; i < showCount; i++) {
                        hiddenImages[i].classList.remove('hidden');
                    }
                    
                    // Update button text or hide it if all images are shown
                    var remainingHidden = grid.querySelectorAll('.image-card.hidden').length;
                    if (remainingHidden > 0) {
                        button.textContent = 'Show ' + remainingHidden + ' More Images';
                    } else {
                        button.style.display = 'none';
                    }
                }
            </script>
            '''
            
            return content
            
        except Exception as e:
            import traceback
            return f'<div class="no-data">Error loading images: {e}<br><pre>{traceback.format_exc()}</pre></div>'
    
    def _categorize_images(self, image_files: List[str]) -> Dict[str, List[str]]:
        """Categorize images into sections based on patterns
        
        Args:
            image_files: List of image file paths
            
        Returns:
            Dictionary mapping category names to lists of image paths
        """
        import re
        
        # Image categories with patterns (same as standalone report)
        image_categories = {
            'Synthesis Flow': {
                'RTL-to-Gate Analysis': [
                    (r'rtl2gate_layout', 95, 'RTL-to-gate layout visualization'),
                    (r'rtl2gate_glbroutecongestion', 90, 'Global routing congestion from synthesis'),
                    (r'rtl2gate_traces.*clock.*color', 85, 'Clock trace analysis from synthesis'),
                    (r'rtl2gate_traces.*color', 80, 'Signal trace analysis from synthesis'),
                ],
                'Clock Trace Analysis': [
                    (r'rtl2gate_traces_i1_clk_color', 90, 'i1 clock trace analysis'),
                    (r'rtl2gate_traces_i2_clk_color', 90, 'i2 clock trace analysis'),
                    (r'rtl2gate_traces_m1_clk_color', 85, 'm1 clock trace analysis'),
                    (r'rtl2gate_traces_clkgate_color', 80, 'Clock gating trace analysis'),
                ],
                'Signal Trace Analysis': [
                    (r'rtl2gate_traces_regin_color', 75, 'Register input trace analysis'),
                    (r'rtl2gate_traces_regout_color', 75, 'Register output trace analysis'),
                    (r'rtl2gate_traces_feedthrough_color', 70, 'Feedthrough trace analysis'),
                    (r'rtl2gate_traces_wrapper_path_color', 70, 'Wrapper path trace analysis'),
                ]
            },
            'DRC Violation Snapshots': {
                'Metal Short Violations': [
                    (r'.*metal_short.*zoom', 95, 'Metal short violation snapshot'),
                    (r'.*metal_short', 90, 'Metal short violation'),
                ],
                'End of Line Keepout Violations': [
                    (r'.*endofline_keepout.*zoom', 95, 'End of line keepout violation snapshot'),
                    (r'.*endofline_keepout', 90, 'End of line keepout violation'),
                ],
                'Via Stack Violations': [
                    (r'maxviastack.*v3s', 90, 'Via stack violation V3S layer'),
                    (r'maxviastack.*v4t', 90, 'Via stack violation V4T layer'),
                    (r'maxviastack', 85, 'Via stack violations'),
                ],
                'Spacing Violations': [
                    (r'parallel_run_length_spacing', 85, 'Parallel run length spacing violation'),
                    (r'spacing.*violation', 80, 'Spacing violations'),
                ],
                'General DRC Snapshots': [
                    (r'.*zoom.*\.png$', 80, 'DRC violation snapshot'),
                    (r'.*drc.*snapshot', 75, 'DRC snapshot'),
                    (r'.*violation.*zoom', 70, 'Violation snapshot'),
                ]
            },
            'ECO/Signoff Analysis': {
                'ECO Impact Analysis': [
                    (r'eco.*images.*eco_in', 90, 'ECO input analysis'),
                    (r'cell_displacement.*eco_in', 85, 'Cell displacement ECO analysis'),
                    (r'cell_displacement', 80, 'Cell displacement analysis'),
                    (r'eco_summary.*drc', 85, 'ECO DRC summary'),
                ],
                'Legalization Issues': [
                    (r'bad_legalizations', 80, 'Bad legalization cases'),
                    (r'legalization.*issue', 75, 'Legalization issues'),
                ]
            },
            'Timing': {
                'Hold Violations': [
                    (r'timing\.hold\.external\.(input|output)\.worst.*failed.*paths', 100, 'Critical hold timing violations'),
                    (r'timing\.hold\.external\.(input|output)\.path_slack_heatmap', 95, 'Hold timing slack heatmap'),
                    (r'timing\.hold\.internal\.(input|output)\.worst.*failed.*paths', 90, 'Internal hold violations'),
                    (r'timing\.hold\..*violations', 85, 'Hold timing violations'),
                    (r'timing\.hold', 80, 'General hold timing analysis'),
                ],
                'Setup Violations': [
                    (r'timing\.setup\.external\.(input|output)\.worst.*failed.*paths', 100, 'Critical setup timing violations'),
                    (r'timing\.setup\.external\.(input|output)\.path_slack_heatmap', 95, 'Setup timing slack heatmap'),
                    (r'timing\.setup\.internal\.(input|output)\.worst.*failed.*paths', 90, 'Internal setup violations'),
                    (r'timing\.setup\..*violations', 85, 'Setup timing violations'),
                    (r'timing\.setup', 80, 'General setup timing analysis'),
                ],
                'Input/Output Timing': [
                    (r'timing\..*external\.input', 85, 'Input timing analysis'),
                    (r'timing\..*external\.output', 85, 'Output timing analysis'),
                    (r'timing\..*io.*constraints', 75, 'I/O timing constraints'),
                    (r'timing\..*external', 70, 'External timing analysis'),
                ],
                'General Timing': [
                    (r'timing\..*slack.*histogram', 85, 'Timing slack distribution'),
                    (r'timing\..*summary', 80, 'Overall timing summary'),
                    (r'timing\..*paths', 75, 'Critical path analysis'),
                    (r'timing\..*report', 70, 'General timing report'),
                    (r'timing\.', 65, 'Timing analysis'),
                ]
            },
            'DRC/DRV': {
                'Design Rule Violations': [
                    (r'drc.*violations', 90, 'Design rule check violations'),
                    (r'drv.*violations', 90, 'Design rule violations'),
                    (r'drc.*summary', 85, 'DRC summary report'),
                    (r'spacing.*violations', 80, 'Metal spacing violations'),
                    (r'width.*violations', 80, 'Wire width violations'),
                ],
                'Layer Violations': [
                    (r'drc.*layer', 80, 'Layer-specific DRC violations'),
                    (r'drv.*layer', 80, 'Layer-specific violations'),
                ],
                'DRC Analysis': [
                    (r'drc\.all\.class', 85, 'DRC violations by class'),
                    (r'drc\.all\.criticality', 85, 'DRC violations by criticality'),
                    (r'drc\.all\.interaction', 80, 'DRC interaction analysis'),
                    (r'drc\.all\.layer', 80, 'DRC violations by layer'),
                    (r'drc\.all\.type', 80, 'DRC violations by type'),
                    (r'drc\.signal_clock_or_preroute', 75, 'Signal/clock/preroute DRC'),
                    (r'drc\.', 70, 'General DRC analysis'),
                ],
                'DRV Analysis': [
                    (r'drv\.max_capacitance', 85, 'Maximum capacitance violations'),
                    (r'drv\.max_transition.*net\.clock', 85, 'Clock net max transition violations'),
                    (r'drv\.max_transition.*net\.signal', 80, 'Signal net max transition violations'),
                    (r'drv\.max_transition', 80, 'Maximum transition violations'),
                    (r'drv\.max_fanout', 75, 'Maximum fanout violations'),
                    (r'drv\.', 70, 'General DRV analysis'),
                ]
            },
            'Power': {
                'Power Density': [
                    (r'power_density\.total', 95, 'Total power density distribution'),
                    (r'power_density\.switching', 90, 'Switching power density'),
                    (r'power_density\.leakage', 90, 'Leakage power density'),
                    (r'power_density', 85, 'Power density analysis'),
                ],
                'Rail Analysis': [
                    (r'rail_analysis', 85, 'Power rail analysis'),
                    (r'ir_drop', 85, 'IR drop analysis'),
                ],
                'General Power': [
                    (r'power\..*histogram', 80, 'Power distribution'),
                    (r'power\..*summary', 75, 'Power summary'),
                    (r'power\.', 70, 'Power analysis'),
                ]
            },
            'Signal Integrity': {
                'Crosstalk Analysis': [
                    (r'crosstalk', 85, 'Signal crosstalk analysis'),
                    (r'coupling', 80, 'Coupling analysis'),
                ],
                'SI Violations': [
                    (r'si.*violations', 85, 'Signal integrity violations'),
                    (r'noise.*violations', 80, 'Noise violations'),
                ]
            },
            'Floorplan/Layout': {
                'Floorplan': [
                    (r'floorplan', 85, 'Floorplan view'),
                    (r'fp_view', 80, 'Floorplan visualization'),
                ],
                'Placement Density': [
                    (r'placement.*density', 85, 'Cell placement density'),
                    (r'density.*map', 80, 'Density heatmap'),
                ],
                'Macros and Blockages': [
                    (r'macro.*placement', 80, 'Macro placement'),
                    (r'blockage', 75, 'Placement blockages'),
                ]
            },
            'Placement': {
                'Placement Quality': [
                    (r'placement.*congestion', 90, 'Placement congestion'),
                    (r'placement.*legalization', 85, 'Legalization analysis'),
                    (r'placement.*quality', 80, 'Placement quality'),
                ],
                'Via Ladder': [
                    (r'via_ladder', 85, 'Via ladder visualization'),
                ]
            },
            'Clock Tree': {
                'Clock Tree QoR': [
                    (r'clock_tree.*arc_wire_resistance', 95, 'Arc wire resistance analysis'),
                    (r'clock_tree.*spine_qor', 90, 'Spine QoR analysis'),
                    (r'clock_tree.*qor', 85, 'Clock tree quality of results'),
                ],
                'Clock Latency': [
                    (r'clock.*latency', 85, 'Clock latency analysis'),
                    (r'clock.*skew', 85, 'Clock skew analysis'),
                ],
                'General Clock': [
                    (r'clock_tree', 80, 'Clock tree analysis'),
                    (r'clock\.', 75, 'Clock analysis'),
                ]
            },
            'Routing': {
                'Routing Congestion': [
                    (r'routing.*congestion', 90, 'Routing congestion analysis'),
                    (r'congestion.*map', 85, 'Congestion heatmap'),
                ],
                'Route Length': [
                    (r'routing\.route_length\.net_type', 85, 'Route length by net type'),
                    (r'routing\.route_length', 80, 'Wire length distribution'),
                ],
                'General Routing': [
                    (r'routing\.', 75, 'Routing analysis'),
                    (r'route\.', 70, 'Route visualization'),
                ]
            },
            'Other': {
                'Uncategorized': []
            }
        }
        
        # Initialize categorized storage
        categorized = {}
        for category in image_categories:
            categorized[category] = {}
            for subcategory in image_categories[category]:
                categorized[category][subcategory] = []
        
        # Categorize each image
        for image_path in image_files:
            image_name = os.path.basename(image_path).lower()
            best_match = None
            best_score = 0
            best_category = 'Other'
            best_subcategory = 'Uncategorized'
            best_description = 'Debug image'
            
            # Check against all patterns
            for category, subcategories in image_categories.items():
                for subcategory, patterns in subcategories.items():
                    for pattern, score, description in patterns:
                        if re.search(pattern, image_name) and score > best_score:
                            best_score = score
                            best_category = category
                            best_subcategory = subcategory
                            best_description = description
            
            # P&R flow stage priority boosts
            flow_stage_boosts = {
                'postroute': 50,
                'route': 30,
                'cts': 20,
                'place': 10,
                'plan': 5
            }
            
            flow_stage_names = {
                'postroute': 'Post-route',
                'route': 'Routing',
                'cts': 'CTS',
                'place': 'Placement',
                'plan': 'Floorplan'
            }
            
            for stage, boost in flow_stage_boosts.items():
                if re.search(rf'\.{stage}\.custom\.', image_name):
                    best_score += boost
                    best_description += f' ({flow_stage_names[stage]} stage)'
                    break
            
            # Default score for uncategorized
            if best_score == 0:
                best_score = 10
            
            # Add to appropriate category
            categorized[best_category][best_subcategory].append({
                'path': image_path,
                'name': os.path.basename(image_path),
                'score': best_score,
                'description': best_description
            })
        
        # Sort images within each subcategory by score (descending)
        for category in categorized:
            for subcategory in categorized[category]:
                categorized[category][subcategory].sort(key=lambda x: x['score'], reverse=True)
        
        return categorized
    
    def _generate_unified_dc_html(self) -> Optional[str]:
        """Generate unified DC (Synthesis) HTML report
        
        Returns:
            HTML filename if generated successfully, None otherwise
        """
        try:
            from datetime import datetime
            import base64
            
            # Generate timestamp for filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            html_filename = f"{self.design_info.top_hier}_{os.environ.get('USER', 'avice')}_DC_comprehensive_{timestamp}.html"
            html_path = os.path.abspath(html_filename)
            
            # Read and encode logo
            logo_data = ""
            logo_path = os.path.join(os.path.dirname(__file__), "assets/images/avice_logo.png")
            if os.path.exists(logo_path):
                with open(logo_path, 'rb') as f:
                    logo_data = base64.b64encode(f.read()).decode('utf-8')
            
            print(f"{Color.CYAN}Generating Unified DC HTML Report...{Color.RESET}")
            
            # Extract DC version and error statistics
            dc_log = os.path.join(self.workarea, "syn_flow/dc/log/dc.log")
            dc_version = self._extract_dc_version_for_html(dc_log)
            dc_errors, dc_warnings = self._extract_dc_errors_for_html(dc_log)
            
            # Collect content from helper methods
            qor_data_content = self._get_dc_qor_html_content()
            beflow_config_content = self._get_dc_beflow_html_content()
            images_content = self._get_dc_images_html_content()
            
            html_content = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DC Comprehensive Report - {self.design_info.top_hier}</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }}
        .container {{
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            display: flex;
            align-items: center;
            gap: 20px;
        }}
        .logo {{
            height: 60px;
            cursor: pointer;
        }}
        .header-text h1 {{
            font-size: 28px;
            margin-bottom: 10px;
        }}
        .header-text p {{
            font-size: 14px;
            opacity: 0.9;
        }}
        .logo-modal {{
            display: none;
            position: fixed;
            z-index: 9999;
            left: 0;
            top: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0,0,0,0.9);
            justify-content: center;
            align-items: center;
        }}
        .logo-modal.active {{
            display: flex;
        }}
        .logo-modal-content {{
            max-width: 90%;
            max-height: 90%;
            border-radius: 10px;
        }}
        .logo-modal-close {{
            position: absolute;
            top: 20px;
            right: 35px;
            color: #f1f1f1;
            font-size: 40px;
            font-weight: bold;
            cursor: pointer;
        }}
        .tab-navigation {{
            display: flex;
            background: #34495e;
            border-bottom: 3px solid #667eea;
        }}
        .tab-btn {{
            flex: 1;
            padding: 15px 20px;
            background: #34495e;
            color: white;
            border: none;
            cursor: pointer;
            font-size: 16px;
            font-weight: bold;
            transition: all 0.3s ease;
            border-right: 1px solid #2c3e50;
        }}
        .tab-btn:last-child {{
            border-right: none;
        }}
        .tab-btn:hover {{
            background: #2c3e50;
        }}
        .tab-btn.active {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            transform: translateY(-2px);
        }}
        .tab-content {{
            display: none;
            padding: 30px;
            animation: fadeIn 0.3s ease;
        }}
        .tab-content.active {{
            display: block;
        }}
        @keyframes fadeIn {{
            from {{ opacity: 0; transform: translateY(-10px); }}
            to {{ opacity: 1; transform: translateY(0); }}
        }}
        .section-header {{
            font-size: 24px;
            color: #667eea;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #667eea;
        }}
        .no-data {{
            padding: 40px;
            text-align: center;
            color: #999;
            font-size: 18px;
            background: #f5f5f5;
            border-radius: 10px;
        }}
        table {{
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }}
        th, td {{
            padding: 12px;
            text-align: left;
            border: 1px solid #ddd;
        }}
        th {{
            background-color: #34495e;
            color: white;
            font-weight: bold;
        }}
        tr:nth-child(even) {{
            background-color: #f9f9f9;
        }}
        tr:hover {{
            background-color: #f0f0f0;
        }}
        
        /* Copyright Footer */
        .footer {{
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            text-align: center;
            padding: 20px;
            margin-top: 40px;
            border-radius: 10px;
            font-size: 14px;
        }}
        
        .footer p {{
            margin: 5px 0;
        }}
        
        .footer strong {{
            color: #00ff00;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">"""
            
            if logo_data:
                html_content += f"""
            <img class='logo' src='data:image/png;base64,{logo_data}' alt='AVICE Logo' onclick="showLogoModal()" title="Click to enlarge">"""
            
            html_content += f"""
            <div class="header-text">
                <h1>DC (Synthesis) Comprehensive Report</h1>
                <p>Design: {self.design_info.top_hier} | IPO: {self.design_info.ipo}</p>
                <p>DC Version: <strong>{dc_version}</strong> | Errors: <strong style="color: {'#e74c3c' if dc_errors > 0 else '#27ae60'}">{dc_errors}</strong> | Warnings: <strong>{dc_warnings:,}</strong></p>
                <p>Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
            </div>
        </div>
        
        <!-- Logo Modal -->
        <div id="logoModal" class="logo-modal" onclick="hideLogoModal()">
            <span class="logo-modal-close">&times;</span>"""
            
            if logo_data:
                html_content += f"""
            <img class="logo-modal-content" src='data:image/png;base64,{logo_data}' alt='AVICE Logo'>"""
            
            html_content += """
        </div>
        
        <!-- Tab Navigation -->
        <div class="tab-navigation">
            <button class="tab-btn active" onclick="openTab(event, 'qor-data')">QoR Data</button>
            <button class="tab-btn" onclick="openTab(event, 'beflow-config')">BeFlow Config</button>
            <button class="tab-btn" onclick="openTab(event, 'images')">Synthesis Images</button>
        </div>
        
        <!-- QoR Data Tab -->
        <div id="qor-data" class="tab-content active">"""
            html_content += qor_data_content
            html_content += """
        </div>
        
        <!-- BeFlow Config Tab -->
        <div id="beflow-config" class="tab-content">"""
            html_content += beflow_config_content
            html_content += """
        </div>
        
        <!-- Synthesis Images Tab -->
        <div id="images" class="tab-content">"""
            html_content += images_content
            html_content += """
        </div>
    </div>
    
    <script>
        // Tab switching function
        function openTab(evt, tabName) {
            var tabContents = document.getElementsByClassName('tab-content');
            for (var i = 0; i < tabContents.length; i++) {
                tabContents[i].classList.remove('active');
            }
            
            var tabBtns = document.getElementsByClassName('tab-btn');
            for (var i = 0; i < tabBtns.length; i++) {
                tabBtns[i].classList.remove('active');
            }
            
            document.getElementById(tabName).classList.add('active');
            evt.currentTarget.classList.add('active');
        }
        
        // Logo modal functions
        function showLogoModal() {
            document.getElementById('logoModal').classList.add('active');
        }
        
        function hideLogoModal() {
            document.getElementById('logoModal').classList.remove('active');
        }
        
        // Allow ESC key to close logo modal
        document.addEventListener('keydown', function(event) {
            if (event.key === 'Escape') {
                hideLogoModal();
            }
        });
        
        // Back to top button functionality - wait for DOM to load
        document.addEventListener('DOMContentLoaded', function() {{
            var backToTopBtn = document.getElementById('backToTopBtn');
            if (backToTopBtn) {{
                window.addEventListener('scroll', function() {{
                    if (window.pageYOffset > 300) {{
                        backToTopBtn.style.display = 'block';
                    }} else {{
                        backToTopBtn.style.display = 'none';
                    }}
                }});
                
                backToTopBtn.addEventListener('click', function() {{
                    window.scrollTo(0, 0);
                }});
            }}
        }});
    </script>
    
    <button id="backToTopBtn" style="display: none; position: fixed; bottom: 30px; right: 30px; 
            z-index: 99; border: none; outline: none; background-color: #667eea; color: white; 
            cursor: pointer; padding: 15px 20px; border-radius: 50px; font-size: 16px; 
            font-weight: bold; box-shadow: 0 4px 6px rgba(0,0,0,0.3); transition: all 0.3s ease;"
            onmouseover="this.style.backgroundColor='#5568d3'; this.style.transform='scale(1.1)';"
            onmouseout="this.style.backgroundColor='#667eea'; this.style.transform='scale(1)';">
        â†‘ Top
    </button>
    
    <!-- Copyright Footer -->
    <div class="footer">
        <p><strong>AVICE DC Comprehensive Report</strong></p>
        <p>Copyright (c) 2025 Alon Vice (avice)</p>
        <p>Contact: avice@nvidia.com</p>
    </div>
</body>
</html>"""
            
            # Write HTML to file
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            # Determine display path (will be moved to html/ or test_outputs/html/ by _organize_html_files)
            html_output_dir = self._get_html_output_dir()
            display_path = os.path.relpath(html_output_dir, os.getcwd())
            
            print(f"  {Color.GREEN}[OK] Unified DC HTML Report Generated{Color.RESET}")
            print(f"  Open with: /home/utils/firefox-118.0.1/firefox {Color.MAGENTA}{display_path}/{os.path.basename(html_path)}{Color.RESET} &")
            
            return html_path
            
        except Exception as e:
            print(f"  {Color.RED}Error generating unified DC HTML: {e}{Color.RESET}")
            import traceback
            traceback.print_exc()
            return None
    
    def _get_dc_qor_html_content(self) -> str:
        """Extract DC QoR data content for unified HTML
        
        Returns:
            HTML string for DC QoR section
        """
        try:
            content = '<div class="section-header">QoR Data (Quality of Results)</div>'
            
            # Find QoR report
            qor_report = os.path.join(self.workarea, f"syn_flow/dc/reports/{self.design_info.top_hier}_rtl2gate.qor.rpt")
            
            if not os.path.exists(qor_report):
                return content + '<div class="no-data">QoR report not found</div>'
            
            try:
                with open(qor_report, 'r', encoding='utf-8') as f:
                    qor_content = f.read()
                
                # Extract key sections
                content += '<h3 style="color: #667eea; margin-top: 20px;">Timing Summary</h3>'
                
                # Extract timing information
                timing_info = {}
                for line in qor_content.split('\n'):
                    line = line.strip()
                    if 'Timing Path Group' in line or 'clock' in line.lower():
                        if '(Setup)' in line or '(Hold)' in line:
                            parts = line.split()
                            if len(parts) >= 3:
                                group = parts[0]
                                if 'Setup' in line:
                                    timing_info[f"{group}_setup"] = line
                                elif 'Hold' in line:
                                    timing_info[f"{group}_hold"] = line
                
                if timing_info:
                    content += '<table><thead><tr><th>Timing Group</th><th>Information</th></tr></thead><tbody>'
                    for group, info in timing_info.items():
                        content += f'<tr><td>{group}</td><td style="font-family: monospace; font-size: 11px;">{info}</td></tr>'
                    content += '</tbody></table>'
                
                # Extract design metrics
                content += '<h3 style="color: #667eea; margin-top: 30px;">Design Metrics</h3>'
                
                metrics = {}
                in_design_section = False
                for line in qor_content.split('\n'):
                    if 'Design' in line and 'Compiler' not in line:
                        in_design_section = True
                    if in_design_section and ':' in line:
                        parts = line.split(':', 1)
                        if len(parts) == 2:
                            key = parts[0].strip()
                            value = parts[1].strip()
                            if key and value and len(key) < 50:
                                metrics[key] = value
                    if in_design_section and line.strip() == '':
                        if metrics:
                            break
                
                if metrics:
                    content += '<table><thead><tr><th>Metric</th><th>Value</th></tr></thead><tbody>'
                    for key, value in list(metrics.items())[:20]:  # Limit to first 20 metrics
                        content += f'<tr><td><strong>{key}</strong></td><td>{value}</td></tr>'
                    content += '</tbody></table>'
                else:
                    content += '<p>No design metrics found in QoR report</p>'
                
                # Add full report with link
                content += '<h3 style="color: #667eea; margin-top: 30px;">Full QoR Report</h3>'
                qor_report_abs = os.path.abspath(qor_report)
                content += f'<p><strong>Report Location:</strong> <a href="file://{qor_report_abs}" style="color: #667eea; text-decoration: none;">{qor_report}</a></p>'
                content += '<pre style="background: #f5f5f5; padding: 15px; border-radius: 5px; overflow-x: auto; max-height: 600px; font-size: 11px; line-height: 1.4;">'
                content += qor_content  # Full content, no truncation
                content += '</pre>'
                
            except Exception as e:
                content += f'<div class="no-data">Error reading QoR report: {e}</div>'
            
            return content
            
        except Exception as e:
            import traceback
            return f'<div class="no-data">Error loading QoR data: {e}<br><pre>{traceback.format_exc()}</pre></div>'
    
    def _get_dc_beflow_html_content(self) -> str:
        """Extract BeFlow configuration content for unified HTML
        
        Returns:
            HTML string for DC BeFlow section
        """
        try:
            content = '<div class="section-header">BeFlow Configuration</div>'
            
            # Find BeFlow config file
            beflow_config = os.path.join(self.workarea, "syn_flow/dc/beflow_config.yaml")
            
            if not os.path.exists(beflow_config):
                return content + '<div class="no-data">BeFlow configuration file not found</div>'
            
            try:
                import yaml
                with open(beflow_config, 'r', encoding='utf-8') as f:
                    config_data = yaml.safe_load(f)
                
                if not config_data:
                    return content + '<div class="no-data">BeFlow configuration is empty</div>'
                
                # Display configuration in organized sections
                content += '<p>Synthesis flow configuration parameters extracted from beflow_config.yaml</p>'
                
                # Group configurations by category
                categories = {
                    'Design': ['design', 'top_hier', 'unit', 'module'],
                    'Clocks': ['clock', 'clk', 'frequency', 'period'],
                    'Timing': ['timing', 'constraint', 'sdc', 'uncertainty'],
                    'Area': ['area', 'utilization', 'density'],
                    'Power': ['power', 'leakage', 'dynamic'],
                    'Technology': ['tech', 'library', 'lib', 'cell'],
                }
                
                def find_matching_keys(data, keywords, prefix=''):
                    """Recursively find keys matching keywords"""
                    matches = {}
                    if isinstance(data, dict):
                        for key, value in data.items():
                            full_key = f"{prefix}.{key}" if prefix else key
                            key_lower = key.lower()
                            
                            # Check if key matches any keyword
                            if any(kw in key_lower for kw in keywords):
                                if isinstance(value, (str, int, float, bool)):
                                    matches[full_key] = value
                                elif isinstance(value, list) and len(value) < 10:
                                    matches[full_key] = ', '.join(str(v) for v in value)
                            
                            # Recursively search nested dicts
                            if isinstance(value, dict):
                                nested = find_matching_keys(value, keywords, full_key)
                                matches.update(nested)
                    
                    return matches
                
                # Extract and display by category
                for category, keywords in categories.items():
                    matches = find_matching_keys(config_data, keywords)
                    
                    if matches:
                        content += f'<h3 style="color: #667eea; margin-top: 30px;">{category} Configuration</h3>'
                        content += '<table><thead><tr><th>Parameter</th><th>Value</th></tr></thead><tbody>'
                        for key, value in sorted(matches.items()):
                            # Truncate very long values
                            value_str = str(value)
                            if len(value_str) > 100:
                                value_str = value_str[:97] + '...'
                            content += f'<tr><td><strong>{key}</strong></td><td>{value_str}</td></tr>'
                        content += '</tbody></table>'
                
                # Add general/uncategorized parameters
                content += '<h3 style="color: #667eea; margin-top: 30px;">General Configuration</h3>'
                
                def flatten_dict(data, prefix='', max_depth=3, current_depth=0):
                    """Flatten nested dictionary"""
                    items = {}
                    if current_depth >= max_depth or not isinstance(data, dict):
                        return items
                    
                    for key, value in data.items():
                        full_key = f"{prefix}.{key}" if prefix else key
                        if isinstance(value, dict):
                            items.update(flatten_dict(value, full_key, max_depth, current_depth + 1))
                        elif isinstance(value, (str, int, float, bool)):
                            items[full_key] = value
                        elif isinstance(value, list) and len(value) < 10:
                            items[full_key] = ', '.join(str(v) for v in value)
                    
                    return items
                
                all_params = flatten_dict(config_data, max_depth=2)
                
                # Show first 30 general parameters
                if all_params:
                    content += '<table><thead><tr><th>Parameter</th><th>Value</th></tr></thead><tbody>'
                    for key, value in list(sorted(all_params.items()))[:30]:
                        value_str = str(value)
                        if len(value_str) > 100:
                            value_str = value_str[:97] + '...'
                        content += f'<tr><td><strong>{key}</strong></td><td>{value_str}</td></tr>'
                    content += '</tbody></table>'
                    
                    if len(all_params) > 30:
                        content += f'<p style="color: #666; font-style: italic; margin-top: 10px;">Showing 30 of {len(all_params)} parameters. See full file for complete configuration.</p>'
                
                # Add link to full config file
                content += '<h3 style="color: #667eea; margin-top: 30px;">Full Configuration File</h3>'
                content += f'<p><strong>File Location:</strong> {beflow_config}</p>'
                
                # Show first part of raw YAML
                with open(beflow_config, 'r', encoding='utf-8') as f:
                    yaml_content = f.read()
                
                content += '<pre style="background: #f5f5f5; padding: 15px; border-radius: 5px; overflow-x: auto; max-height: 500px; font-size: 11px; line-height: 1.4;">'
                content += yaml_content[:3000]  # First 3000 characters
                if len(yaml_content) > 3000:
                    content += '\n\n... (file truncated, see full file for complete configuration)'
                content += '</pre>'
                
            except ImportError:
                # If yaml module not available, show raw file
                content += '<p style="color: #e74c3c;">YAML parser not available. Showing raw configuration file.</p>'
                with open(beflow_config, 'r', encoding='utf-8') as f:
                    yaml_content = f.read()
                content += '<pre style="background: #f5f5f5; padding: 15px; border-radius: 5px; overflow-x: auto; max-height: 600px; font-size: 11px; line-height: 1.4;">'
                content += yaml_content
                content += '</pre>'
            except Exception as e:
                content += f'<div class="no-data">Error reading BeFlow configuration: {e}</div>'
            
            return content
            
        except Exception as e:
            import traceback
            return f'<div class="no-data">Error loading BeFlow config: {e}<br><pre>{traceback.format_exc()}</pre></div>'
    
    def _get_dc_images_html_content(self) -> str:
        """Extract synthesis images content for unified HTML
        
        Returns:
            HTML string for DC images section
        """
        try:
            # Find DC images directory
            images_dir = os.path.join(self.workarea, "syn_flow/dc/reports/color")
            
            if not os.path.exists(images_dir):
                return '<div class="no-data">No synthesis images directory found</div>'
            
            # Find all image files
            image_files = []
            for ext in ['*.png', '*.jpg', '*.jpeg', '*.gif']:
                image_files.extend(glob.glob(os.path.join(images_dir, ext)))
            
            if not image_files:
                return f'<div class="no-data">No synthesis images found<br><small>{images_dir}</small></div>'
            
            # Categorize images (synthesis images will mostly be in "Synthesis Flow" category)
            categorized_images = self._categorize_images(image_files)
            
            content = '<div class="section-header">Synthesis Images Gallery</div>'
            content += f'<p><strong>Total Images:</strong> {len(image_files)} | <strong>Directory:</strong> {images_dir}</p>'
            
            # Add styles (reuse from PnR images)
            content += '''
            <style>
                .category-container {
                    margin: 20px 0;
                    border: 1px solid #ddd;
                    border-radius: 8px;
                    overflow: hidden;
                }
                .category-header {
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    color: white;
                    padding: 15px 20px;
                    cursor: pointer;
                    display: flex;
                    justify-content: space-between;
                    align-items: center;
                    font-weight: bold;
                    font-size: 16px;
                }
                .category-header:hover {
                    background: linear-gradient(135deg, #764ba2 0%, #667eea 100%);
                }
                .category-toggle {
                    transition: transform 0.3s ease;
                }
                .category-toggle.collapsed {
                    transform: rotate(-90deg);
                }
                .category-content {
                    padding: 20px;
                    background: #f9f9f9;
                    display: block;
                }
                .category-content.collapsed {
                    display: none;
                }
                .subcategory-header {
                    background: #34495e;
                    color: white;
                    padding: 10px 15px;
                    margin: 15px 0 10px 0;
                    border-radius: 5px;
                    font-weight: bold;
                }
                .image-grid {
                    display: grid;
                    grid-template-columns: repeat(auto-fill, minmax(220px, 1fr));
                    gap: 15px;
                    margin: 15px 0;
                }
                .image-card {
                    border: 1px solid #ddd;
                    border-radius: 8px;
                    padding: 10px;
                    background: white;
                    box-shadow: 0 2px 5px rgba(0,0,0,0.1);
                    transition: transform 0.2s ease, box-shadow 0.2s ease;
                }
                .image-card:hover {
                    transform: translateY(-5px);
                    box-shadow: 0 4px 12px rgba(0,0,0,0.2);
                }
                .image-card img {
                    width: 100%;
                    height: 160px;
                    object-fit: contain;
                    border-radius: 5px;
                    cursor: pointer;
                    background: #f5f5f5;
                }
                .image-name {
                    font-size: 10px;
                    margin-top: 8px;
                    color: #666;
                    word-break: break-all;
                    line-height: 1.3;
                }
                .image-description {
                    font-size: 9px;
                    color: #999;
                    margin-top: 4px;
                    font-style: italic;
                }
                .image-count {
                    background: rgba(255,255,255,0.2);
                    padding: 2px 8px;
                    border-radius: 12px;
                    font-size: 12px;
                }
                .image-modal {
                    display: none;
                    position: fixed;
                    z-index: 10000;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    background-color: rgba(0,0,0,0.95);
                    justify-content: center;
                    align-items: center;
                }
                .image-modal.active {
                    display: flex;
                }
                .image-modal img {
                    max-width: 95%;
                    max-height: 95%;
                    border-radius: 5px;
                }
                .image-modal-close {
                    position: absolute;
                    top: 20px;
                    right: 40px;
                    color: white;
                    font-size: 40px;
                    font-weight: bold;
                    cursor: pointer;
                }
            </style>
            '''
            
            # Category priority order
            category_priority = [
                'Synthesis Flow',
                'Floorplan/Layout',
                'Placement',
                'Clock Tree',
                'Routing',
                'Timing',
                'Power',
                'Signal Integrity',
                'DRC/DRV',
                'DRC Violation Snapshots',
                'ECO/Signoff Analysis',
                'Other'
            ]
            
            # Generate categorized sections
            for category in category_priority:
                if category not in categorized_images:
                    continue
                    
                # Count images in this category
                category_image_count = sum(len(categorized_images[category][subcategory]) 
                                         for subcategory in categorized_images[category])
                
                if category_image_count == 0:
                    continue
                    
                cat_id = category.replace(' ', '_').replace('/', '_')
                content += f'''
                <div class="category-container">
                    <div class="category-header" onclick="toggleImgCategory('{cat_id}')">
                        <span>{category}</span>
                        <span><span class="image-count">{category_image_count} images</span> <span class="category-toggle" id="toggle_{cat_id}">â–¼</span></span>
                    </div>
                    <div class="category-content" id="content_{cat_id}">
                '''
                
                # Generate subcategories
                subcat_index = 0
                for subcategory in categorized_images[category]:
                    images = categorized_images[category][subcategory]
                    if not images:
                        continue
                    
                    subcat_id = f"{cat_id}_subcat_{subcat_index}"
                    subcat_index += 1
                    initial_show = 12  # Show first 12 images by default
                    
                    content += f'''
                        <div class="subcategory-header">{subcategory} ({len(images)} images)</div>
                        <div class="image-grid" id="grid_{subcat_id}">
                    '''
                    
                    for idx, img_data in enumerate(images):
                        img_path = img_data['path']
                        img_name = img_data['name']
                        img_desc = img_data['description']
                        img_url = f"file://{os.path.abspath(img_path)}"
                        
                        # Add 'hidden' class to images beyond initial_show count
                        hidden_class = ' hidden' if idx >= initial_show else ''
                        
                        content += f'''
                            <div class="image-card{hidden_class}">
                                <img src="{img_url}" alt="{img_name}" onclick="showImageModal(this.src)" title="{img_desc}">
                                <div class="image-name">{img_name}</div>
                                <div class="image-description">{img_desc}</div>
                            </div>
                        '''
                    
                    content += '</div>'  # Close image-grid
                    
                    # Add "Show More" button if there are more than initial_show images
                    if len(images) > initial_show:
                        remaining = len(images) - initial_show
                        content += f'''
                        <button class="show-more-btn" onclick="showMoreImages('{subcat_id}', this)">
                            Show {remaining} More Images
                        </button>
                        '''
                
                content += '''
                    </div>
                </div>
                '''
            
            # Add image modal and scripts
            content += '''
            <div id="imageModal" class="image-modal" onclick="hideImageModal()">
                <span class="image-modal-close">&times;</span>
                <img id="modalImage" src="">
            </div>
            <script>
                function showImageModal(src) {
                    document.getElementById('modalImage').src = src;
                    document.getElementById('imageModal').classList.add('active');
                }
                function hideImageModal() {
                    document.getElementById('imageModal').classList.remove('active');
                }
                function toggleImgCategory(categoryId) {
                    var content = document.getElementById('content_' + categoryId);
                    var toggle = document.getElementById('toggle_' + categoryId);
                    if (content.classList.contains('collapsed')) {
                        content.classList.remove('collapsed');
                        toggle.classList.remove('collapsed');
                        toggle.textContent = 'â–¼';
                    } else {
                        content.classList.add('collapsed');
                        toggle.classList.add('collapsed');
                        toggle.textContent = 'â–¶';
                    }
                }
            </script>
            '''
            
            return content
            
        except Exception as e:
            import traceback
            return f'<div class="no-data">Error loading images: {e}<br><pre>{traceback.format_exc()}</pre></div>'
    
    def _generate_timing_histogram_html(self) -> Optional[str]:
        """Generate HTML report with timing histogram tables
        
        Returns:
            HTML filename if generated successfully, None otherwise
        """
        try:
            # Define stage priority order
            pnr_stages = ['postroute', 'route', 'cts', 'place', 'plan']
            timing_file = None
            found_stage = None
            
            # Try each stage in priority order
            for stage in pnr_stages:
                # Use wildcard for IPO in filename since it can differ from directory name (e.g., ipo1000 dir with ipo1400 in filenames)
                timing_pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{self.design_info.ipo}/REPs/SUMMARY/{self.design_info.top_hier}.*.{stage}.timing.setup.rpt.gz"
                timing_files = self.file_utils.find_files(timing_pattern, self.workarea)
                if timing_files:
                    timing_file = timing_files[0]
                    found_stage = stage
                    break
            
            if timing_file:
                # Extract the last three histogram tables (category, sub-category, and sub-category+scenario)
                result = self.file_utils.run_command(f"zcat {timing_file} | grep -n 'histogram' | grep '|'")
                if result.strip():
                    histogram_lines = result.strip().split('\n')
                    if len(histogram_lines) >= 4:
                        # Get the last 4 tables: category, scenario, sub-category, and sub-category + scenario
                        table_category_start = int(histogram_lines[-4].split(':')[0])  # Category breakdown
                        table_scenario_start = int(histogram_lines[-3].split(':')[0])  # Scenario breakdown
                        table_subcat_start = int(histogram_lines[-2].split(':')[0])    # Sub-category breakdown
                        table_subcat_scenario_start = int(histogram_lines[-1].split(':')[0])  # Sub-category + scenario breakdown
                        
                        # Find the end of category table (it ends before scenario table starts)
                        table_category_end = table_scenario_start - 1
                        
                        # Extract category table
                        table_category_result = self.file_utils.run_command(f"zcat {timing_file} | sed -n '{table_category_start},{table_category_end}p'")
                        
                        # Find the end of sub-category table (it ends before sub-category + scenario table starts)
                        table_subcat_end = table_subcat_scenario_start - 1
                        
                        # Extract sub-category table
                        table_subcat_result = self.file_utils.run_command(f"zcat {timing_file} | sed -n '{table_subcat_start},{table_subcat_end}p'")
                        
                        # Extract sub-category + scenario table - get remaining lines
                        table_subcat_scenario_result = self.file_utils.run_command(f"zcat {timing_file} | sed -n '{table_subcat_scenario_start},$p'")
                        
                        if table_category_result.strip() and table_subcat_result.strip() and table_subcat_scenario_result.strip():
                            # Generate HTML content for timing histogram
                            html_content = self._create_timing_histogram_html(table_category_result.strip(), table_subcat_result.strip(), table_subcat_scenario_result.strip(), found_stage)
                            
                            # Save HTML file
                            html_filename = f"{self.design_info.top_hier}_{os.environ.get('USER', 'avice')}_innovus_timing_histogram_{self.design_info.ipo}.html"
                            html_path = os.path.join(os.getcwd(), html_filename)
                            
                            with open(html_path, 'w', encoding='utf-8') as f:
                                f.write(html_content)
                            
                            # Determine display path
                            html_output_dir = self._get_html_output_dir()
                            display_path = os.path.relpath(html_output_dir, os.getcwd())
                            
                            print(f"\n  {Color.CYAN}Timing Histogram HTML Report:{Color.RESET}")
                            print(f"  Open with: /home/utils/firefox-118.0.1/firefox {Color.MAGENTA}{display_path}/{html_filename}{Color.RESET} &")
                            return os.path.abspath(html_path)  # Return absolute path for master dashboard
                        
        except Exception as e:
            print(f"  Error generating timing histogram HTML: {e}")
        
        return None
    
    def _create_timing_histogram_html(self, category_data: str, sub_category_data: str, scenario_data: str, stage: str) -> str:
        """Create HTML content for timing histogram tables"""
        # Read and encode logo
        logo_data = ""
        logo_path = os.path.join(os.path.dirname(__file__), "assets/images/avice_logo.png")
        if os.path.exists(logo_path):
            with open(logo_path, "rb") as logo_file:
                logo_data = base64.b64encode(logo_file.read()).decode('utf-8')
        
        # Process the three separate tables
        category_table = [line.strip() for line in category_data.split('\n') if line.strip()]
        sub_category_table = [line.strip() for line in sub_category_data.split('\n') if line.strip()]
        scenario_table = [line.strip() for line in scenario_data.split('\n') if line.strip()]
        
        html = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Timing Histogram Analysis - {self.design_info.top_hier}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
        .container {{ max-width: 95%; margin: 0 auto; background-color: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        h1 {{ color: #2c3e50; text-align: center; margin-bottom: 30px; border-bottom: 3px solid #3498db; padding-bottom: 10px; }}
        h2 {{ color: #34495e; margin-top: 30px; }}
        .info {{ background-color: #ecf0f1; padding: 15px; border-radius: 5px; margin-bottom: 20px; }}
        table {{ border-collapse: collapse; width: 100%; margin: 20px 0; font-size: 12px; }}
        th, td {{ border: 1px solid #bdc3c7; padding: 6px; text-align: center; }}
        th {{ background-color: #34495e; color: white; font-weight: bold; }}
        tr:nth-child(even) {{ background-color: #f9f9f9; }}
        .external {{ background-color: #e8f5e8; }}
        .internal {{ background-color: #fff3cd; }}
        .histogram-cell {{ font-family: 'Courier New', monospace; font-size: 11px; }}
        .legend {{ background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin-top: 20px; border-left: 4px solid #3498db; }}
        .legend h4 {{ margin-top: 0; color: #2c3e50; }}
        .legend ul {{ margin: 10px 0; padding-left: 20px; }}
        .legend li {{ margin: 5px 0; }}
        .header {{ 
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            padding: 30px;
            display: grid;
            grid-template-columns: auto 1fr;
            gap: 20px;
            align-items: center;
            border-radius: 15px 15px 0 0;
            margin: -20px -20px 0 -20px;
        }}
        .logo {{
            width: 80px;
            height: 80px;
            border-radius: 10px;
            background: white;
            padding: 10px;
            cursor: pointer;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }}
        .logo:hover {{
            transform: scale(1.05);
            box-shadow: 0 8px 16px rgba(0,0,0,0.3);
        }}
        .header-text h1 {{
            font-size: 28px;
            margin: 0 0 8px 0;
            color: white;
            border: none;
        }}
        .header-text p {{
            opacity: 0.9;
            font-size: 14px;
            margin: 0;
        }}
        .logo-modal {{
            display: none;
            position: fixed;
            z-index: 9999;
            left: 0;
            top: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0,0,0,0.9);
            justify-content: center;
            align-items: center;
        }}
        .logo-modal.active {{
            display: flex;
        }}
        .logo-modal-content {{
            max-width: 90%;
            max-height: 90%;
            border-radius: 10px;
        }}
        .logo-modal-close {{
            position: absolute;
            top: 20px;
            right: 35px;
            color: #f1f1f1;
            font-size: 40px;
            font-weight: bold;
            cursor: pointer;
        }}
        .logo-modal-close:hover {{
            color: #bbb;
        }}
        
        /* Copyright Footer */
        .footer {{
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            text-align: center;
            padding: 20px;
            margin-top: 40px;
            border-radius: 10px;
            font-size: 14px;
        }}
        
        .footer p {{
            margin: 5px 0;
        }}
        
        .footer strong {{
            color: #00ff00;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <img class='logo' src='data:image/png;base64,{logo_data}' alt='AVICE Logo' onclick="showLogoModal()" title="Click to enlarge">
            <div class="header-text">
                <h1>Timing Histogram Analysis</h1>
                <p>Design: {self.design_info.top_hier} | IPO: {self.design_info.ipo} | Stage: {stage.upper()}</p>
                <p>Workarea: {self.workarea_abs} | Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
            </div>
        </div>
        
        <!-- Logo Modal -->
        <div id="logoModal" class="logo-modal" onclick="hideLogoModal()">
            <span class="logo-modal-close">&times;</span>
            <img class="logo-modal-content" src='data:image/png;base64,{logo_data}' alt='AVICE Logo'>
        </div>
        
                <h2>Category Breakdown</h2>
                <p>Timing analysis by category showing external and internal timing paths.</p>
                <table>
                    <thead>
                        <tr>
                            <th>Type</th>
                            <th>Category</th>
                            <th>WNS (ns)</th>
                            <th>TNS (ns)</th>
                            <th>FEP</th>
                            <th colspan="11">Histogram (Path Count by Slack Range)</th>
                        </tr>
                        <tr>
                            <th></th>
                            <th></th>
                            <th></th>
                            <th></th>
                            <th></th>
                            <th>-0.010</th>
                            <th>-0.020</th>
                            <th>-0.030</th>
                            <th>-0.040</th>
                            <th>-0.050</th>
                            <th>-0.060</th>
                            <th>-0.070</th>
                            <th>-0.080</th>
                            <th>-0.090</th>
                            <th>-0.100</th>
                            <th>-Inf</th>
                        </tr>
                    </thead>
                    <tbody>
        """
        
        # Process category table
        for line in category_table:
            if '=' in line or 'histogram' in line.lower() or 'type' in line.lower():
                continue
            # Check if line has data (contains numbers and |)
            if '|' in line and any(char.isdigit() or char == '-' for char in line):
                parts = [part.strip() for part in line.split('|')]
                if len(parts) >= 6:  # Minimum columns for category table
                    # Determine type class based on first column or content
                    first_col = parts[0].strip() if len(parts) > 0 else ''
                    if 'external' in first_col.lower():
                        type_class = 'external'
                    elif 'internal' in first_col.lower():
                        type_class = 'internal'
                    else:
                        # Default to internal for rows without explicit type
                        type_class = 'internal'
                    
                    html += f"                <tr class=\"{type_class}\">\n"
                    html += f"                    <td>{first_col}</td>\n"  # type
                    html += f"                    <td>{parts[1] if len(parts) > 1 and parts[1].strip() else ''}</td>\n"  # category
                    html += f"                    <td>{parts[2] if len(parts) > 2 and parts[2].strip() else ''}</td>\n"  # wns
                    html += f"                    <td>{parts[3] if len(parts) > 3 and parts[3].strip() else ''}</td>\n"  # tns
                    html += f"                    <td>{parts[4] if len(parts) > 4 and parts[4].strip() else ''}</td>\n"  # fep
                    # Histogram data - split the histogram string into individual values
                    if len(parts) > 5:
                        histogram_data = parts[5].split()
                        for i, value in enumerate(histogram_data):
                            if i < 11:  # Limit to 11 histogram columns
                                html += f"                    <td class=\"histogram-cell\">{value}</td>\n"
                        # Fill remaining histogram columns if needed
                        for i in range(len(histogram_data), 11):
                            html += f"                    <td class=\"histogram-cell\"></td>\n"
                    else:
                        # Fill all histogram columns if no data
                        for i in range(11):
                            html += f"                    <td class=\"histogram-cell\"></td>\n"
                    html += "                </tr>\n"
        
        html += """            </tbody>
            </table>
            
            <h2>Sub-Category Breakdown</h2>
            <p>Timing analysis by category and sub-category showing external and internal timing paths.</p>
        <table>
            <thead>
                <tr>
                    <th>Type</th>
                    <th>Category</th>
                    <th>Sub-Category</th>
                    <th>WNS (ns)</th>
                    <th>TNS (ns)</th>
                    <th>FEP</th>
                    <th colspan="11">Histogram (Path Count by Slack Range)</th>
                </tr>
                <tr>
                    <th></th>
                    <th></th>
                    <th></th>
                    <th></th>
                    <th></th>
                    <th></th>
                    <th>-0.010</th>
                    <th>-0.020</th>
                    <th>-0.030</th>
                    <th>-0.040</th>
                    <th>-0.050</th>
                    <th>-0.060</th>
                    <th>-0.070</th>
                    <th>-0.080</th>
                    <th>-0.090</th>
                    <th>-0.100</th>
                    <th>-Inf</th>
                </tr>
            </thead>
            <tbody>
"""
        
        # Process sub-category table
        for line in sub_category_table:
            if '=' in line or 'histogram' in line.lower() or 'type' in line.lower():
                continue
            # Check if line has data (contains numbers and |)
            if '|' in line and any(char.isdigit() or char == '-' for char in line):
                parts = [part.strip() for part in line.split('|')]
                if len(parts) >= 7:  # Minimum columns for sub-category table
                    # Determine type class based on first column or content
                    first_col = parts[0].strip() if len(parts) > 0 else ''
                    if 'external' in first_col.lower():
                        type_class = 'external'
                    elif 'internal' in first_col.lower():
                        type_class = 'internal'
                    else:
                        # Default to internal for rows without explicit type
                        type_class = 'internal'
                    
                    html += f"                <tr class=\"{type_class}\">\n"
                    html += f"                    <td>{first_col}</td>\n"  # type
                    html += f"                    <td>{parts[1] if len(parts) > 1 else ''}</td>\n"  # category
                    html += f"                    <td>{parts[2] if len(parts) > 2 else ''}</td>\n"  # sub_category
                    html += f"                    <td>{parts[3] if len(parts) > 3 else ''}</td>\n"  # wns
                    html += f"                    <td>{parts[4] if len(parts) > 4 else ''}</td>\n"  # tns
                    html += f"                    <td>{parts[5] if len(parts) > 5 else ''}</td>\n"  # fep
                    # Histogram data - split the histogram string into individual values
                    if len(parts) > 6:
                        histogram_data = parts[6].split()
                        for i, value in enumerate(histogram_data):
                            if i < 11:  # Limit to 11 histogram columns
                                html += f"                    <td class=\"histogram-cell\">{value}</td>\n"
                        # Fill remaining histogram columns if needed
                        for i in range(len(histogram_data), 11):
                            html += f"                    <td class=\"histogram-cell\"></td>\n"
                    else:
                        # Fill all histogram columns if no data
                        for i in range(11):
                            html += f"                    <td class=\"histogram-cell\"></td>\n"
                    html += "                </tr>\n"
        
        html += """            </tbody>
        </table>
        
        <h2>Scenario Breakdown</h2>
        <p>Timing analysis by category, sub-category, and scenario showing both typical and high-temperature conditions.</p>
        <table>
            <thead>
                <tr>
                    <th>Type</th>
                    <th>Category</th>
                    <th>Sub-Category</th>
                    <th>Scenario</th>
                    <th>WNS (ns)</th>
                    <th>TNS (ns)</th>
                    <th>FEP</th>
                    <th colspan="11">Histogram (Path Count by Slack Range)</th>
                </tr>
                <tr>
                    <th></th>
                    <th></th>
                    <th></th>
                    <th></th>
                    <th></th>
                    <th></th>
                    <th></th>
                    <th>-0.010</th>
                    <th>-0.020</th>
                    <th>-0.030</th>
                    <th>-0.040</th>
                    <th>-0.050</th>
                    <th>-0.060</th>
                    <th>-0.070</th>
                    <th>-0.080</th>
                    <th>-0.090</th>
                    <th>-0.100</th>
                    <th>-Inf</th>
                </tr>
            </thead>
            <tbody>
"""
        
        # Process scenario table
        for line in scenario_table:
            if '=' in line or 'histogram' in line.lower() or 'type' in line.lower():
                continue
            # Check if line has data (contains numbers and |)
            if '|' in line and any(char.isdigit() or char == '-' for char in line):
                parts = [part.strip() for part in line.split('|')]
                if len(parts) >= 8:  # Minimum columns for scenario table
                    # Determine type based on category - look for the first non-empty category
                    type_class = 'external'  # Default fallback
                    category = ''
                    for i in range(1, min(4, len(parts))):
                        if parts[i].strip() and parts[i].strip() not in ['', '............', '..................']:
                            category = parts[i].strip()
                            if category in ['output', 'input']:
                                type_class = 'external'
                            else:
                                type_class = 'internal'
                            break
                    
                    html += f"                <tr class=\"{type_class}\">\n"
                    html += f"                    <td>{parts[0] if len(parts) > 0 and parts[0].strip() else ''}</td>\n"  # type
                    html += f"                    <td>{parts[1] if len(parts) > 1 and parts[1].strip() else ''}</td>\n"  # category
                    html += f"                    <td>{parts[2] if len(parts) > 2 and parts[2].strip() else ''}</td>\n"  # sub_category
                    html += f"                    <td>{parts[3] if len(parts) > 3 and parts[3].strip() else ''}</td>\n"  # scenario
                    html += f"                    <td>{parts[4] if len(parts) > 4 and parts[4].strip() else ''}</td>\n"  # wns
                    html += f"                    <td>{parts[5] if len(parts) > 5 and parts[5].strip() else ''}</td>\n"  # tns
                    html += f"                    <td>{parts[6] if len(parts) > 6 and parts[6].strip() else ''}</td>\n"  # fep
                    # Histogram data - split the histogram string into individual values
                    if len(parts) > 7:
                        histogram_data = parts[7].split()
                        for i, value in enumerate(histogram_data):
                            if i < 11:  # Limit to 11 histogram columns
                                html += f"                    <td class=\"histogram-cell\">{value}</td>\n"
                        # Fill remaining histogram columns if needed
                        for i in range(len(histogram_data), 11):
                            html += f"                    <td class=\"histogram-cell\"></td>\n"
                    else:
                        # Fill all histogram columns if no data
                        for i in range(11):
                            html += f"                    <td class=\"histogram-cell\"></td>\n"
                    html += "                </tr>\n"
        
        html += """            </tbody>
        </table>
        
        <div class="legend">
            <h4>Legend</h4>
            <ul>
                <li><strong>WNS:</strong> Worst Negative Slack (ns)</li>
                <li><strong>TNS:</strong> Total Negative Slack (ns)</li>
                <li><strong>FEP:</strong> Failing Endpoint Count</li>
                <li><strong>Histogram:</strong> Number of paths in each slack range</li>
            </ul>
            <h4>Path Types</h4>
            <ul>
                <li><span class="external" style="padding: 2px 6px; border-radius: 3px;">External:</span> Input/Output timing paths</li>
                <li><span class="internal" style="padding: 2px 6px; border-radius: 3px;">Internal:</span> Register-to-register timing paths</li>
            </ul>
            <h4>Categories</h4>
            <ul>
                <li><strong>Output:</strong> Flop-to-port paths</li>
                <li><strong>Input:</strong> Port-to-flop and port-to-clock-gate paths</li>
                <li><strong>Reg_to_reg:</strong> Flop-to-flop paths</li>
                <li><strong>Reg_to_cgate:</strong> Flop-to-clock-gate paths</li>
            </ul>
        </div>
    </div>
    
    <script>
        // Logo modal functions
        function showLogoModal() {{
            document.getElementById('logoModal').classList.add('active');
        }}
        
        function hideLogoModal() {{
            document.getElementById('logoModal').classList.remove('active');
        }}
        
        // Allow ESC key to close logo modal
        document.addEventListener('keydown', function(event) {{
            if (event.key === 'Escape') {{
                hideLogoModal();
            }}
        }});
        
        // Back to top button functionality - wait for DOM to load
        document.addEventListener('DOMContentLoaded', function() {{
            var backToTopBtn = document.getElementById('backToTopBtn');
            if (backToTopBtn) {{
                window.addEventListener('scroll', function() {{
                    if (window.pageYOffset > 300) {{
                        backToTopBtn.style.display = 'block';
                    }} else {{
                        backToTopBtn.style.display = 'none';
                    }}
                }});
                
                backToTopBtn.addEventListener('click', function() {{
                    window.scrollTo(0, 0);
                }});
            }}
        }});
    </script>
    
    <button id="backToTopBtn" style="display: none; position: fixed; bottom: 30px; right: 30px; 
            z-index: 99; border: none; outline: none; background-color: #667eea; color: white; 
            cursor: pointer; padding: 15px 20px; border-radius: 50px; font-size: 16px; 
            font-weight: bold; box-shadow: 0 4px 6px rgba(0,0,0,0.3); transition: all 0.3s ease;"
            onmouseover="this.style.backgroundColor='#5568d3'; this.style.transform='scale(1.1)';"
            onmouseout="this.style.backgroundColor='#667eea'; this.style.transform='scale(1)';">
        â†‘ Top
    </button>
    
    <!-- Copyright Footer -->
    <div class="footer">
        <p><strong>AVICE Timing Histogram Report</strong></p>
        <p>Copyright (c) 2025 Alon Vice (avice)</p>
        <p>Contact: avice@nvidia.com</p>
    </div>
</body>
</html>
"""
        
        return html
    
    def _generate_clock_html_report(self, ipo_clock_data: List[Dict] = None, best_ipo: Dict[str, Any] = None, status: str = "PASS") -> str:
        """Generate HTML report for clock analysis with unified IPO+clock table
        
        Args:
            ipo_clock_data: List of IPO clock data dictionaries
            best_ipo: Best IPO dictionary
            status: Overall status (PASS/WARN/FAIL)
            
        Returns:
            HTML string containing clock report
        """
        if not ipo_clock_data:
            return ""
        
        try:
            # Load the Avice logo
            logo_base64 = ""
            logo_path = os.path.join(os.path.dirname(__file__), "assets/images/avice_logo.png")
            if os.path.exists(logo_path):
                with open(logo_path, "rb") as logo_file:
                    logo_base64 = base64.b64encode(logo_file.read()).decode('utf-8')
            
            # Get current timestamp
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            username = os.environ.get('USER', 'avice')
            
            # HTML filename
            html_filename = f"{username}_clock_analysis_{self.design_info.top_hier}_{self.design_info.ipo}_{timestamp}.html"
            
            # Status color and icon
            if status == "PASS":
                status_color = "#27ae60"
                status_icon = "âœ“"
            elif status == "WARN":
                status_color = "#f39c12"
                status_icon = "âš "
            else:  # FAIL
                status_color = "#e74c3c"
                status_icon = "âœ—"
            
            # Build multi-IPO comparison table HTML (if multiple IPOs)
            ipo_comparison_html = ""
            if ipo_clock_data and len(ipo_clock_data) > 1:
                # Use provided best_ipo or find it
                best_ipo_data = best_ipo if best_ipo else ipo_clock_data[0]
                
                ipo_comparison_html = """
                <div style="background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); margin-bottom: 30px;">
                    <h2 style="color: #2c3e50; margin-bottom: 20px; font-size: 24px; border-bottom: 3px solid #667eea; padding-bottom: 10px;">
                        ðŸ” Multi-IPO Clock Latency Comparison
                    </h2>
                    <p style="color: #7f8c8d; margin-bottom: 20px; font-size: 14px;">
                        Compare clock latencies across all IPOs to identify the best implementation
                    </p>
                    <table style="width: 100%; border-collapse: collapse; margin-top: 15px; font-size: 0.9em;">
                        <thead>
                            <tr style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;">
                                <th style="padding: 10px; text-align: left; font-weight: 600;">IPO</th>
                                <th style="padding: 10px; text-align: center; font-weight: 600;" colspan="2">Innovus</th>
                                <th style="padding: 10px; text-align: center; font-weight: 600;">PT Max</th>
                                <th style="padding: 10px; text-align: center; font-weight: 600;">Combined Max</th>
                                <th style="padding: 10px; text-align: center; font-weight: 600;">Status</th>
                            </tr>
                            <tr style="background: #9b7cc4; color: white; font-size: 0.85em;">
                                <th style="padding: 8px; text-align: left;"></th>
                                <th style="padding: 8px; text-align: center;">Max</th>
                                <th style="padding: 8px; text-align: center;">Median</th>
                                <th style="padding: 8px; text-align: center;"></th>
                                <th style="padding: 8px; text-align: center;"></th>
                                <th style="padding: 8px; text-align: center;"></th>
                            </tr>
                        </thead>
                        <tbody>
"""
                
                for ipo_data in ipo_clock_data:
                    innovus_max = ipo_data['innovus_max_latency']
                    innovus_median = ipo_data.get('innovus_median_latency', 0)
                    pt_max = ipo_data['pt_max_latency']
                    combined_max = max(innovus_max, pt_max)
                    ipo_name = ipo_data['ipo']
                    innovus_file = ipo_data.get('innovus_file', '')
                    pt_file = ipo_data.get('pt_file', '')
                    
                    # Determine status and colors
                    if combined_max >= 580:
                        status_badge = '<span style="background: #e74c3c; color: white; padding: 4px 12px; border-radius: 12px; font-weight: 600; font-size: 0.85em;">FAIL</span>'
                        combined_color = "#e74c3c"
                    elif combined_max > 550:
                        status_badge = '<span style="background: #f39c12; color: white; padding: 4px 12px; border-radius: 12px; font-weight: 600; font-size: 0.85em;">WARN</span>'
                        combined_color = "#f39c12"
                    else:
                        status_badge = '<span style="background: #27ae60; color: white; padding: 4px 12px; border-radius: 12px; font-weight: 600; font-size: 0.85em;">PASS</span>'
                        combined_color = "#27ae60"
                    
                    # Highlight best IPO row
                    is_best = ipo_data['ipo'] == best_ipo_data['ipo']
                    row_style = 'background: #e8f5e9; border-left: 4px solid #27ae60;' if is_best else ''
                    best_badge = '<span style="background: #27ae60; color: white; padding: 2px 8px; border-radius: 8px; font-size: 0.75em; margin-left: 8px;">â˜… BEST</span>' if is_best else ''
                    
                    innovus_max_str = f"{innovus_max:.1f} ps" if innovus_max > 0 else "N/A"
                    innovus_median_str = f"{innovus_median:.1f} ps" if innovus_median > 0 else "N/A"
                    pt_str = f"{pt_max:.1f} ps" if pt_max > 0 else "N/A"
                    
                    # Create source file tooltips (shortened paths)
                    innovus_src = innovus_file.replace(self.workarea + '/', '') if self.workarea in innovus_file else os.path.basename(innovus_file) if innovus_file else 'N/A'
                    pt_src = pt_file.replace(self.workarea + '/', '') if self.workarea in pt_file else os.path.basename(pt_file) if pt_file else 'N/A'
                    
                    ipo_comparison_html += f"""
                        <tr style="{row_style} border-bottom: 1px solid #ecf0f1;">
                            <td style="padding: 12px; font-weight: 600; font-family: monospace;">{ipo_name}{best_badge}</td>
                            <td style="padding: 12px; text-align: center; color: #34495e;" title="Source: {innovus_src}">{innovus_max_str}</td>
                            <td style="padding: 12px; text-align: center; color: #7f8c8d; font-style: italic;" title="Source: {innovus_src}">{innovus_median_str}</td>
                            <td style="padding: 12px; text-align: center; color: #34495e;" title="Source: {pt_src}">{pt_str}</td>
                            <td style="padding: 12px; text-align: center; color: {combined_color}; font-weight: bold; font-size: 1.05em;">{combined_max:.1f} ps</td>
                            <td style="padding: 12px; text-align: center;">{status_badge}</td>
                        </tr>
"""
                
                # Add source file information section
                source_files_html = """
                    <div style="margin-top: 20px; padding: 15px; background: #f8f9fa; border-radius: 6px; border-left: 4px solid #95a5a6;">
                        <h4 style="color: #2c3e50; margin-bottom: 12px; font-size: 0.95em;">ðŸ“ Source Files</h4>
                        <div style="font-size: 0.85em; color: #555;">
"""
                
                for ipo_data in ipo_clock_data:
                    innovus_file = ipo_data.get('innovus_file', '')
                    pt_file = ipo_data.get('pt_file', '')
                    ipo_name = ipo_data['ipo']
                    
                    source_files_html += f"""
                            <div style="margin-bottom: 10px; padding: 8px; background: white; border-radius: 4px;">
                                <strong style="color: #667eea;">{ipo_name}:</strong><br>
"""
                    
                    if innovus_file:
                        innovus_path = innovus_file.replace(self.workarea + '/', '') if self.workarea in innovus_file else innovus_file
                        source_files_html += f"""
                                <span style="margin-left: 15px;">â€¢ Innovus: <code style="background: #ecf0f1; padding: 2px 6px; border-radius: 3px; font-size: 0.9em;">{innovus_path}</code></span><br>
"""
                    
                    if pt_file:
                        pt_path = pt_file.replace(self.workarea + '/', '') if self.workarea in pt_file else pt_file
                        source_files_html += f"""
                                <span style="margin-left: 15px;">â€¢ PT: <code style="background: #ecf0f1; padding: 2px 6px; border-radius: 3px; font-size: 0.9em;">{pt_path}</code></span>
"""
                    
                    source_files_html += """
                            </div>
"""
                
                source_files_html += """
                        </div>
                    </div>
"""
                
                ipo_comparison_html += """
                        </tbody>
                    </table>
                    
                    <div style="margin-top: 20px; padding: 15px; background: #e3f2fd; border-left: 4px solid #2196f3; border-radius: 4px;">
                        <strong style="color: #1565c0;">ðŸ’¡ Recommendation:</strong>
                        <span style="color: #424242; margin-left: 8px;">IPO <strong>""" + best_ipo_data['ipo'] + f"""</strong> is recommended (stage: {best_ipo_data.get('innovus_stage', 'N/A')}).</span>
                    </div>
                    
""" + source_files_html + """
                </div>
"""
            
            # Build unified clock table HTML (all IPOs, all clocks)
            unified_clock_html = """
            <div style="background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); margin-bottom: 30px;">
                <h2 style="color: #2c3e50; margin-bottom: 20px; font-size: 24px; border-bottom: 3px solid #667eea; padding-bottom: 10px;">
                    ðŸ“Š Detailed Clock Latency - All IPOs
                </h2>
                <table style="width: 100%; border-collapse: collapse; margin-top: 15px; font-size: 0.9em;">
                    <thead>
                        <tr style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;">
                            <th rowspan="2" style="padding: 10px; text-align: left; font-weight: 600; border-right: 2px solid white;">IPO</th>
                            <th rowspan="2" style="padding: 10px; text-align: center; font-weight: 600; border-right: 2px solid white;">Stage</th>
                            <th rowspan="2" style="padding: 10px; text-align: left; font-weight: 600; border-right: 2px solid white;">Clock</th>
                            <th rowspan="2" style="padding: 10px; text-align: center; font-weight: 600; border-right: 2px solid white;">Cycle<br>Time (ns)</th>
                            <th colspan="2" style="padding: 10px; text-align: center; font-weight: 600; border-right: 2px solid white;">Innovus</th>
                            <th colspan="2" style="padding: 10px; text-align: center; font-weight: 600; border-right: 2px solid white;">PT</th>
                            <th rowspan="2" style="padding: 10px; text-align: center; font-weight: 600;">Diff<br>(Inn-PT)</th>
                        </tr>
                        <tr style="background: #9b7cc4; color: white; font-size: 0.9em;">
                            <th style="padding: 8px; text-align: center; border-right: 1px solid rgba(255,255,255,0.3);">Median (ns)</th>
                            <th style="padding: 8px; text-align: center; border-right: 2px solid white;">Max (ns)</th>
                            <th style="padding: 8px; text-align: center; border-right: 1px solid rgba(255,255,255,0.3);">Max (ns)</th>
                            <th style="padding: 8px; text-align: center; border-right: 2px solid white;">Min (ns)</th>
                        </tr>
                    </thead>
                    <tbody>
"""
            
            for ipo_data in ipo_clock_data:
                ipo = ipo_data['ipo']
                stage = ipo_data.get('innovus_stage', 'none')
                
                # Get all unique clocks
                all_clocks = set()
                all_clocks.update(ipo_data['innovus_data'].keys())
                all_clocks.update(ipo_data['pt_data'].keys())
                
                if not all_clocks:
                    continue
                
                # IPO separator row
                is_best = best_ipo and ipo == best_ipo['ipo']
                separator_style = 'background: #e8f5e9; border-left: 4px solid #27ae60;' if is_best else 'background: #f5f5f5; border-left: 4px solid #95a5a6;'
                best_badge = ' ðŸ†' if is_best else ''
                
                unified_clock_html += f"""
                        <tr class="ipo-separator">
                            <td colspan="8" style="{separator_style} font-weight: bold; padding: 10px; color: #2c3e50;">
                                {ipo}{best_badge} ({stage})
                            </td>
                        </tr>
"""
                
                for clock in sorted(all_clocks):
                    # Extract cycle time
                    cycle_time_str = "N/A"
                    if clock in ipo_data.get('clock_cycles', {}):
                        cycle_time_ns = ipo_data['clock_cycles'][clock]
                        cycle_time_str = f"{cycle_time_ns:.2f}"
                    
                    # Extract Innovus values
                    if clock in ipo_data['innovus_data']:
                        inn_tuple = ipo_data['innovus_data'][clock]
                        inn_max = inn_tuple[0] / 1000
                        inn_median = inn_tuple[2] / 1000 if len(inn_tuple) > 2 else 0
                        inn_median_str = f"{inn_median:.3f}" if inn_median > 0 else "N/A"
                        inn_max_str = f"{inn_max:.3f}" if inn_max > 0 else "N/A"
                        inn_max_color = "#e74c3c" if inn_max > 0.55 else "#2c3e50"
                    else:
                        inn_max = 0
                        inn_median_str = "N/A"
                        inn_max_str = "N/A"
                        inn_max_color = "#7f8c8d"
                    
                    # Extract PT values
                    if clock in ipo_data['pt_data']:
                        pt_tuple = ipo_data['pt_data'][clock]
                        pt_max = pt_tuple[0] / 1000
                        pt_min = pt_tuple[1] / 1000 if len(pt_tuple) > 1 else 0
                        pt_max_str = f"{pt_max:.3f}" if pt_max > 0 else "N/A"
                        pt_min_str = f"{pt_min:.3f}" if pt_min > 0 else "N/A"
                        pt_max_color = "#e74c3c" if pt_max > 0.55 else "#2c3e50"
                    else:
                        pt_max = 0
                        pt_max_str = "N/A"
                        pt_min_str = "N/A"
                        pt_max_color = "#7f8c8d"
                    
                    # Calculate diff (Innovus Max - PT Max)
                    if inn_max > 0 and pt_max > 0:
                        diff = inn_max - pt_max
                        diff_str = f"{diff:+.3f}"  # Show sign (+/-)
                        # Positive = Innovus slower (worse) = red, Negative = Innovus faster (better) = green
                        if diff > 0.050:  # Innovus significantly slower
                            diff_color = "#e74c3c"
                            diff_weight = "bold"
                        elif diff < -0.050:  # Innovus significantly faster
                            diff_color = "#27ae60"
                            diff_weight = "bold"
                        else:  # Small difference
                            diff_color = "#34495e"
                            diff_weight = "normal"
                    else:
                        diff_str = "N/A"
                        diff_color = "#7f8c8d"
                        diff_weight = "normal"
                    
                    unified_clock_html += f"""
                        <tr style="border-bottom: 1px solid #ecf0f1;">
                            <td style="padding: 10px; font-family: monospace; font-size: 0.9em; border-right: 1px solid #ecf0f1;">{ipo}</td>
                            <td style="padding: 10px; text-align: center; border-right: 1px solid #ecf0f1;">
                                <span style="background: {'#27ae60' if stage == 'postroute' else '#3498db' if stage == 'route' else '#f39c12' if stage in ['place', 'postcts'] else '#95a5a6'}; 
                                             color: white; padding: 3px 8px; border-radius: 10px; font-size: 0.75em; font-weight: 600;">{stage}</span>
                            </td>
                            <td style="padding: 10px; font-family: monospace; font-weight: 500; border-right: 1px solid #ecf0f1;">{clock}</td>
                            <td style="padding: 10px; text-align: center; color: #2c3e50; font-weight: 600; border-right: 1px solid #ecf0f1;">{cycle_time_str}</td>
                            <td style="padding: 10px; text-align: center; color: #7f8c8d; font-style: italic; border-right: 1px solid #ecf0f1;">{inn_median_str}</td>
                            <td style="padding: 10px; text-align: center; color: {inn_max_color}; font-weight: {'bold' if inn_max_color == '#e74c3c' else 'normal'}; border-right: 1px solid #ecf0f1;">{inn_max_str}</td>
                            <td style="padding: 10px; text-align: center; color: {pt_max_color}; font-weight: {'bold' if pt_max_color == '#e74c3c' else 'normal'}; border-right: 1px solid #ecf0f1;">{pt_max_str}</td>
                            <td style="padding: 10px; text-align: center; color: #34495e; border-right: 1px solid #ecf0f1;">{pt_min_str}</td>
                            <td style="padding: 10px; text-align: center; color: {diff_color}; font-weight: {diff_weight};">{diff_str}</td>
                        </tr>
"""
            
            unified_clock_html += """
                    </tbody>
                </table>
                
                <div style="margin-top: 20px; padding: 15px; background: #fff3cd; border-left: 4px solid #f39c12; border-radius: 4px;">
                    <strong style="color: #856404;">ðŸ’¡ Legend:</strong>
                    <ul style="color: #856404; margin: 8px 0 0 20px; padding: 0;">
                        <li>Values exceeding 550ps (0.55ns) are highlighted in <strong>bold red</strong></li>
                        <li><strong style="color: #e74c3c;">Positive Diff</strong>: Innovus slower than PT (worse)</li>
                        <li><strong style="color: #27ae60;">Negative Diff</strong>: Innovus faster than PT (better)</li>
                        <li>Differences > Â±50ps are highlighted in <strong>bold</strong></li>
                    </ul>
                </div>
            </div>
"""
            
            # Add source files section with tablog links
            source_files_section = """
            <div style="background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); margin-bottom: 30px;">
                <h2 style="color: #2c3e50; margin-bottom: 20px; font-size: 24px; border-bottom: 3px solid #667eea; padding-bottom: 10px;">
                    ðŸ“ Source Reports
                </h2>
"""
            
            for ipo_data in ipo_clock_data:
                ipo = ipo_data['ipo']
                stage = ipo_data.get('innovus_stage', 'none')
                innovus_file = ipo_data.get('innovus_file', '')
                pt_file = ipo_data.get('pt_file', '')
                
                if not innovus_file and not pt_file:
                    continue
                
                source_files_section += f"""
                <div style="margin-bottom: 25px; padding: 20px; background: #f8f9fa; border-radius: 8px; border-left: 4px solid #667eea;">
                    <h3 style="color: #667eea; margin: 0 0 15px 0; font-size: 18px;">
                        {ipo} <span style="background: {'#27ae60' if stage == 'postroute' else '#3498db' if stage == 'route' else '#f39c12' if stage in ['place', 'postcts'] else '#95a5a6'}; 
                                      color: white; padding: 3px 8px; border-radius: 10px; font-size: 0.7em; font-weight: 600; margin-left: 8px;">{stage}</span>
                    </h3>
"""
                
                if innovus_file:
                    innovus_rel_path = innovus_file.replace(self.workarea + '/', '') if self.workarea in innovus_file else os.path.basename(innovus_file)
                    source_files_section += f"""
                    <div style="margin-bottom: 12px;">
                        <div style="display: flex; align-items: center; gap: 10px;">
                            <span style="color: #2c3e50; font-weight: 600; min-width: 80px;">Innovus:</span>
                            <button onclick="openLogWithServer('{innovus_file}', event)" 
                                    style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border: none; 
                                           padding: 6px 16px; border-radius: 6px; cursor: pointer; font-size: 0.9em; font-weight: 500; 
                                           transition: all 0.3s ease; box-shadow: 0 2px 4px rgba(0,0,0,0.1);"
                                    onmouseover="this.style.transform='translateY(-2px)'; this.style.boxShadow='0 4px 8px rgba(0,0,0,0.2)';"
                                    onmouseout="this.style.transform='translateY(0)'; this.style.boxShadow='0 2px 4px rgba(0,0,0,0.1)';">
                                ðŸ“Š View in tablog
                            </button>
                            <a href="file://{innovus_file}" style="color: #3498db; text-decoration: none; font-size: 0.9em;">ðŸ“„</a>
                        </div>
                        <code style="display: block; margin-top: 8px; padding: 8px 12px; background: white; border-radius: 4px; 
                                     font-size: 0.85em; color: #555; border: 1px solid #e0e0e0; word-break: break-all;">{innovus_rel_path}</code>
                    </div>
"""
                
                if pt_file:
                    pt_rel_path = pt_file.replace(self.workarea + '/', '') if self.workarea in pt_file else os.path.basename(pt_file)
                    source_files_section += f"""
                    <div style="margin-bottom: 12px;">
                        <div style="display: flex; align-items: center; gap: 10px;">
                            <span style="color: #2c3e50; font-weight: 600; min-width: 80px;">PT:</span>
                            <button onclick="openLogWithServer('{pt_file}', event)" 
                                    style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border: none; 
                                           padding: 6px 16px; border-radius: 6px; cursor: pointer; font-size: 0.9em; font-weight: 500; 
                                           transition: all 0.3s ease; box-shadow: 0 2px 4px rgba(0,0,0,0.1);"
                                    onmouseover="this.style.transform='translateY(-2px)'; this.style.boxShadow='0 4px 8px rgba(0,0,0,0.2)';"
                                    onmouseout="this.style.transform='translateY(0)'; this.style.boxShadow='0 2px 4px rgba(0,0,0,0.1)';">
                                ðŸ“Š View in tablog
                            </button>
                            <a href="file://{pt_file}" style="color: #3498db; text-decoration: none; font-size: 0.9em;">ðŸ“„</a>
                        </div>
                        <code style="display: block; margin-top: 8px; padding: 8px 12px; background: white; border-radius: 4px; 
                                     font-size: 0.85em; color: #555; border: 1px solid #e0e0e0; word-break: break-all;">{pt_rel_path}</code>
                    </div>
"""
                
                source_files_section += """
                </div>
"""
            
            source_files_section += """
            </div>
"""
            
            unified_clock_html += source_files_section
            
            # Count unique clocks
            unique_clocks = set()
            for ipo_data in ipo_clock_data:
                unique_clocks.update(ipo_data['innovus_data'].keys())
                unique_clocks.update(ipo_data['pt_data'].keys())
            num_clocks = len(unique_clocks)
            
            # Calculate max latency across all IPOs
            all_max_values = []
            for ipo_data in ipo_clock_data:
                for clock_data in ipo_data['innovus_data'].values():
                    all_max_values.append(clock_data[0])
                for clock_data in ipo_data['pt_data'].values():
                    all_max_values.append(clock_data[0])
            max_latency_ps = max(all_max_values) if all_max_values else 0
            
            # File links now shown in source files section
            file_links_html = ""
            
            # Generate HTML
            html = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AVICE Clock Analysis - {self.design_info.top_hier}</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
            min-height: 100vh;
        }}
        
        .container {{
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 12px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.3);
            overflow: hidden;
        }}
        
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        
        .logo {{
            position: absolute;
            left: 30px;
            top: 20px;
            cursor: pointer;
            transition: transform 0.3s ease;
        }}
        
        .logo img {{
            max-height: 80px;
            max-width: 80px;
            height: auto;
            width: auto;
            border-radius: 8px;
            box-shadow: 0 2px 6px rgba(0,0,0,0.3);
        }}
        
        .logo img:hover {{
            transform: scale(1.1);
            box-shadow: 0 4px 12px rgba(0,0,0,0.5);
        }}
        
        .image-expanded {{
            position: fixed;
            top: 0;
            left: 0;
            width: 100vw;
            height: 100vh;
            background-color: rgba(0, 0, 0, 0.95);
            z-index: 10000;
            display: flex;
            justify-content: center;
            align-items: center;
            cursor: pointer;
        }}
        
        .image-expanded img {{
            max-width: 90%;
            max-height: 90%;
            border: 3px solid white;
            border-radius: 10px;
            box-shadow: 0 0 50px rgba(255,255,255,0.3);
        }}
        
        .header h1 {{
            font-size: 2.5em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
        }}
        
        .header p {{
            font-size: 1.2em;
            opacity: 0.95;
        }}
        
        .content {{
            padding: 30px;
        }}
        
        .summary-card {{
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            border-radius: 10px;
            padding: 25px;
            margin-bottom: 30px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }}
        
        .summary-row {{
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 15px;
            padding-bottom: 15px;
            border-bottom: 1px solid rgba(0,0,0,0.1);
        }}
        
        .summary-row:last-child {{
            margin-bottom: 0;
            padding-bottom: 0;
            border-bottom: none;
        }}
        
        .summary-label {{
            font-weight: 600;
            color: #555;
            font-size: 1.1em;
        }}
        
        .summary-value {{
            font-size: 1.3em;
            font-weight: bold;
        }}
        
        .status-badge {{
            display: inline-block;
            padding: 8px 20px;
            border-radius: 25px;
            font-weight: bold;
            font-size: 1.2em;
            color: white;
            background-color: {status_color};
        }}
        
        .section {{
            margin-bottom: 30px;
        }}
        
        .section h2 {{
            color: #667eea;
            margin-bottom: 15px;
            padding-bottom: 10px;
            border-bottom: 3px solid #667eea;
            font-size: 1.8em;
        }}
        
        table {{
            width: 100%;
            border-collapse: collapse;
            margin-top: 15px;
            background: white;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }}
        
        thead {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
        }}
        
        th {{
            padding: 15px;
            text-align: left;
            font-weight: 600;
            font-size: 1.05em;
        }}
        
        td {{
            padding: 12px 15px;
            border-bottom: 1px solid #e0e0e0;
        }}
        
        tr:last-child td {{
            border-bottom: none;
        }}
        
        tr:hover {{
            background-color: #f5f7fa;
        }}
        
        .pass {{
            color: #27ae60;
        }}
        
        .warn {{
            color: #f39c12;
        }}
        
        .fail {{
            color: #e74c3c;
        }}
        
        .info-box {{
            background: #e8f4f8;
            border-left: 4px solid #3498db;
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
        }}
        
        .info-box h3 {{
            color: #2980b9;
            margin-bottom: 10px;
            font-size: 1.2em;
        }}
        
        .info-box p {{
            color: #555;
            line-height: 1.6;
        }}
        
        .threshold-legend {{
            display: flex;
            gap: 20px;
            margin-top: 15px;
            flex-wrap: wrap;
        }}
        
        .threshold-item {{
            display: flex;
            align-items: center;
            gap: 8px;
        }}
        
        .threshold-dot {{
            width: 12px;
            height: 12px;
            border-radius: 50%;
        }}
        
        .footer {{
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            text-align: center;
            padding: 20px;
            margin-top: 40px;
            border-radius: 10px;
            font-size: 14px;
        }}
        
        .footer p {{
            margin: 5px 0;
        }}
        
        .footer strong {{
            color: #00ff00;
        }}
        
        a {{
            color: #3498db;
            text-decoration: none;
        }}
        
        a:hover {{
            text-decoration: underline;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <div class="logo">
                <img src="data:image/png;base64,{logo_base64}" alt="Avice Logo" onclick="expandImage(this)">
            </div>
            <h1>â±ï¸ Clock Analysis Report</h1>
            <p>{self.design_info.top_hier} â€¢ {self.design_info.ipo} â€¢ {datetime.now().strftime("%B %d, %Y %H:%M")}</p>
        </div>
        
        <div class="content">
            <!-- Summary Card -->
            <div class="summary-card">
                <div class="summary-row">
                    <span class="summary-label">Overall Status:</span>
                    <span class="status-badge">{status_icon} {status}</span>
                </div>
                <div class="summary-row">
                    <span class="summary-label">Design:</span>
                    <span class="summary-value">{self.design_info.top_hier}</span>
                </div>
                <div class="summary-row">
                    <span class="summary-label">IPO:</span>
                    <span class="summary-value">{self.design_info.ipo}</span>
                </div>
                <div class="summary-row">
                    <span class="summary-label">Number of Clocks:</span>
                    <span class="summary-value">{num_clocks}</span>
                </div>
                <div class="summary-row">
                    <span class="summary-label">Max Clock Latency:</span>
                    <span class="summary-value" style="color: {status_color};">{max_latency_ps:.1f} ps ({max_latency_ps/1000:.3f} ns)</span>
                </div>
            </div>
            
            <!-- Unified Clock Table (All IPOs and Clocks) -->
            {unified_clock_html}
            
            <!-- Threshold Legend -->
            <div class="info-box">
                <h3>ðŸŽ¯ Latency Thresholds</h3>
                <div class="threshold-legend">
                    <div class="threshold-item">
                        <div class="threshold-dot" style="background-color: #27ae60;"></div>
                        <span><strong>PASS:</strong> â‰¤ 550 ps (0.55 ns)</span>
                    </div>
                    <div class="threshold-item">
                        <div class="threshold-dot" style="background-color: #f39c12;"></div>
                        <span><strong>WARN:</strong> 550 ps - 580 ps</span>
                    </div>
                    <div class="threshold-item">
                        <div class="threshold-dot" style="background-color: #e74c3c;"></div>
                        <span><strong>FAIL:</strong> â‰¥ 580 ps</span>
                    </div>
                </div>
            </div>
            
            <!-- Analysis Notes -->
            <div class="info-box">
                <h3>â„¹ï¸ Analysis Notes</h3>
                <p>This report shows the combined clock latency data from both Innovus and PrimeTime analysis.</p>
                <p>For each clock, the maximum latency value is displayed. Latency values are color-coded based on design thresholds.</p>
                <p><strong>Scenario:</strong> func.std_tt_0c_0p6v.setup.typical</p>
            </div>
        </div>
        
        <!-- Copyright Footer -->
        <div class="footer">
            <p><strong>AVICE Clock Analysis Report</strong></p>
            <p>Copyright (c) 2025 Alon Vice (avice)</p>
            <p>Contact: avice@nvidia.com</p>
        </div>
    </div>
    
    <!-- Back to Top Button -->
    <button id="backToTopBtn" style="display: none; position: fixed; bottom: 30px; right: 30px; 
            z-index: 99; border: none; outline: none; background-color: #667eea; color: white; 
            cursor: pointer; padding: 15px 20px; border-radius: 50px; font-size: 16px; 
            font-weight: bold; box-shadow: 0 4px 6px rgba(0,0,0,0.3); transition: all 0.3s ease;"
            onmouseover="this.style.backgroundColor='#5568d3'; this.style.transform='scale(1.1)';"
            onmouseout="this.style.backgroundColor='#667eea'; this.style.transform='scale(1)';">
        â†‘ Top
    </button>
    
    <script>
        // Back to top button functionality
        const backToTopBtn = document.getElementById('backToTopBtn');
        if (backToTopBtn) {{
            window.addEventListener('scroll', function() {{
                if (window.pageYOffset > 300) {{
                    backToTopBtn.style.display = 'block';
                }} else {{
                    backToTopBtn.style.display = 'none';
                }}
            }});
            
            backToTopBtn.addEventListener('click', function() {{
                window.scrollTo({{ top: 0, behavior: 'smooth' }});
            }});
        }}
        
        // Expand logo image functionality
        function expandImage(imgElement) {{
            var overlay = document.createElement('div');
            overlay.className = 'image-expanded';
            
            var expandedImg = document.createElement('img');
            expandedImg.src = imgElement.src;
            expandedImg.alt = imgElement.alt;
            
            overlay.appendChild(expandedImg);
            document.body.appendChild(overlay);
            
            // Close on click
            overlay.onclick = function() {{
                if (document.body.contains(overlay)) {{
                    document.body.removeChild(overlay);
                }}
            }};
        }}
        
        // Open log with server (with fallback to clipboard) - Standard tablog integration
        function openLogWithServer(logfile, event) {{
            if (event) {{
                event.preventDefault();
            }}
            
            const serverUrl = 'http://localhost:8888/open_log?file=' + encodeURIComponent(logfile);
            
            // Try to open via server
            fetch(serverUrl, {{ method: 'GET', mode: 'cors' }})
                .then(function(response) {{
                    if (response.ok) {{
                        showToast('âœ“ Opening in tablog...', 'success');
                    }} else {{
                        throw new Error('Server returned error');
                    }}
                }})
                .catch(function(error) {{
                    // Server not running - fallback to clipboard
                    const command = '/home/scratch.avice_vlsi/tablog/tablog "' + logfile + '"';
                    copyToClipboard(command);
                }});
        }}
        
        function copyToClipboard(text, button) {{
            if (navigator.clipboard && navigator.clipboard.writeText) {{
                navigator.clipboard.writeText(text).then(function() {{
                    showToast('âœ“ Copied to clipboard: ' + text.substring(0, 60) + '...', 'success');
                    if (button) {{
                        const originalText = button.innerHTML;
                        button.innerHTML = 'âœ“ Copied!';
                        button.style.background = '#27ae60';
                        setTimeout(function() {{
                            button.innerHTML = originalText;
                            button.style.background = 'linear-gradient(135deg, #667eea 0%, #764ba2 100%)';
                        }}, 2000);
                    }}
                }}).catch(function() {{
                    showToast('âœ— Failed to copy command', 'error');
                }});
            }} else {{
                showToast('âœ— Clipboard not supported', 'error');
            }}
        }}
        
        function showToast(message, type) {{
            const toast = document.createElement('div');
            toast.textContent = message;
            toast.style.cssText = 'position: fixed; bottom: 30px; left: 50%; transform: translateX(-50%); ' +
                'background: ' + (type === 'success' ? '#27ae60' : '#e74c3c') + '; color: white; ' +
                'padding: 15px 25px; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.3); ' +
                'z-index: 10000; font-size: 14px; font-weight: 500; max-width: 80%; text-align: center;';
            document.body.appendChild(toast);
            setTimeout(function() {{ toast.remove(); }}, 3000);
        }}
    </script>
</body>
</html>
"""
            
            # Write HTML file
            with open(html_filename, 'w') as f:
                f.write(html)
            
            # Determine display path
            html_output_dir = self._get_html_output_dir()
            display_path = os.path.relpath(html_output_dir, os.getcwd())
            
            print(f"\n  {Color.GREEN}[OK] Clock HTML Report Generated{Color.RESET}")
            print(f"  Open with: /home/utils/firefox-118.0.1/firefox {Color.MAGENTA}{display_path}/{html_filename}{Color.RESET} &")
            
            return os.path.abspath(html_filename)
            
        except Exception as e:
            print(f"  Error generating clock HTML report: {e}")
            return ""
    
    def _extract_clock_tree_data_with_stage(self, ipo: str, quiet: bool = False) -> Tuple[float, float, Dict[str, Tuple[float, float, float]], str, str]:
        """Extract clock tree data for IPO, trying multiple stages in priority order
        
        Args:
            ipo: IPO name (e.g., 'ipo1400' or 'ipo1600_fixed_ndr')
            quiet: If True, suppress terminal output
            
        Returns:
            Tuple of (max_latency_ps, median_latency_ps, clock_dict, stage, file_path)
            where stage is 'postroute', 'route', 'postcts', 'preroute', 'floorplan', or 'none'
        """
        # Extract base IPO number (ipo#### without suffix) for filename matching
        # ipo1600_fixed_ndr -> ipo1600
        ipo_base = re.match(r'(ipo\d+)', ipo).group(1) if re.match(r'(ipo\d+)', ipo) else ipo
        
        stages_to_try = ['postroute', 'route', 'postcts', 'place', 'preroute', 'floorplan']
        
        for stage in stages_to_try:
            # Use ipo_base for filename, but full ipo for directory path
            pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{ipo}/REPs/SUMMARY/{self.design_info.top_hier}.{ipo_base}.{stage}.clock_tree.skew_and_latency.from_clock_root_source.rpt*"
            clock_files = self.file_utils.find_files(pattern, self.workarea)
            
            if clock_files:
                max_lat, median_lat, clock_dict = self._extract_clock_tree_data(clock_files[0], quiet=True)
                return max_lat, median_lat, clock_dict, stage, clock_files[0]
        
        # No data found
        return 0, 0, {}, 'none', ''
    
    def run_clock_analysis(self) -> None:
        """Run clock analysis with multi-IPO support"""
        self.print_header(FlowStage.CLOCK_ANALYSIS)
        
        # Detect which IPO the root-level PT belongs to
        root_pt_ipo = self._detect_root_pt_ipo()
        root_signoff_exists = os.path.exists(os.path.join(self.workarea, "signoff_flow"))
        
        # Discover all IPO locations
        ipo_clock_data = []  # List of {ipo, innovus_data, pt_data, stage, ...}
        all_ipos = self.design_info.all_ipos if self.design_info.all_ipos else [self.design_info.ipo]
        
        # Collect clock data from all IPOs
        for ipo in all_ipos:
            ipo_data = {
                'ipo': ipo,
                'innovus_data': {},
                'innovus_max_latency': 0,
                'innovus_median_latency': 0,
                'innovus_stage': 'none',
                'innovus_file': '',
                'pt_data': {},
                'pt_max_latency': 0,
                'pt_file': '',
                'clock_cycles': {}  # Dictionary: {clock_name: cycle_time_ns}
            }
            
            # 1. Extract Innovus clock data for this IPO (try multiple stages)
            innovus_max, innovus_median, innovus_clocks, stage, innovus_file = self._extract_clock_tree_data_with_stage(ipo, quiet=True)
            if innovus_file:
                ipo_data['innovus_file'] = innovus_file
                ipo_data['innovus_data'] = innovus_clocks
                ipo_data['innovus_max_latency'] = innovus_max
                ipo_data['innovus_median_latency'] = innovus_median
                ipo_data['innovus_stage'] = stage
            
            # 1b. Extract clock cycle times from .data file
            pnr_stages = ['postroute', 'route', 'cts', 'place', 'plan']
            for data_stage in pnr_stages:
                data_pattern = f"pnr_flow/nv_flow/{self.design_info.top_hier}/{ipo}/reports/{self.design_info.top_hier}_*_report_{self.design_info.top_hier}_*_{data_stage}.func.std_tt_0c_0p6v.setup.typical.data"
                data_files = self.file_utils.find_files(data_pattern, self.workarea)
                if data_files:
                    # Extract clock cycle times
                    try:
                        with open(data_files[0], 'r') as f:
                            for line in f:
                                # Look for lines like: m1_clk_Cycle_Time = 1.25
                                if '_Cycle_Time = ' in line:
                                    parts = line.strip().split(' = ')
                                    if len(parts) == 2:
                                        param_name = parts[0].strip()
                                        param_value = parts[1].strip()
                                        # Extract clock name: "m1_clk_Cycle_Time" -> "m1_clk"
                                        clock_name = param_name.replace('_Cycle_Time', '')
                                        try:
                                            cycle_time = float(param_value)
                                            ipo_data['clock_cycles'][clock_name] = cycle_time
                                        except ValueError:
                                            pass
                    except (OSError, UnicodeDecodeError):
                        pass
                    break  # Found data file, no need to check other stages
            
            # 2. Extract PT clock data for this IPO (try multiple locations)
            pt_locations_to_try = []
            
            # Per-IPO PT location
            pt_locations_to_try.append(
                f"pnr_flow/nv_flow/{self.design_info.top_hier}/{ipo}/nbu_signoff/signoff_flow/auto_pt"
            )
            
            # Root-level PT location (only for the IPO that matches)
            # Match on base IPO number: ipo2000_ndr_test3 matches ipo2000
            ipo_base = re.match(r'(ipo\d+)', ipo).group(1) if re.match(r'(ipo\d+)', ipo) else ipo
            root_pt_base = re.match(r'(ipo\d+)', root_pt_ipo).group(1) if root_pt_ipo and re.match(r'(ipo\d+)', root_pt_ipo) else root_pt_ipo
            if root_pt_ipo and (ipo == root_pt_ipo or ipo_base == root_pt_base or ipo.startswith(root_pt_ipo)) and root_signoff_exists:
                pt_locations_to_try.append("signoff_flow/auto_pt")
            
            # Search for PT clock latency file in these locations
            for pt_base_path in pt_locations_to_try:
                pt_dir = os.path.join(self.workarea, pt_base_path)
                if not os.path.exists(pt_dir):
                    continue
                
                # Find latest work directory
                work_dirs = glob.glob(os.path.join(pt_dir, "work_*"))
                if not work_dirs:
                    continue
                
                # Sort by modification time, get latest
                work_dirs.sort(key=lambda x: os.path.getmtime(x), reverse=True)
                latest_work = work_dirs[0]
                
                # Look for clock latency file
                pt_clock_file = os.path.join(
                    latest_work,
                    f"func.std_tt_0c_0p6v.setup.typical/reports/timing_reports/{self.design_info.top_hier}_func.std_tt_0c_0p6v.setup.typical.clock_latency"
                )
                
                if os.path.exists(pt_clock_file):
                    ipo_data['pt_file'] = pt_clock_file
                    pt_max, pt_clocks = self._extract_pt_clock_latency(pt_clock_file, quiet=True)
                    ipo_data['pt_data'] = pt_clocks
                    ipo_data['pt_max_latency'] = pt_max
                    break  # Found PT data, no need to check other locations
            
            # Determine stage for IPOs with PT-only
            if not ipo_data['innovus_data'] and ipo_data['pt_data']:
                ipo_data['innovus_stage'] = 'PT-only'
            
            # Only add IPO if we found at least some clock data
            if ipo_data['innovus_data'] or ipo_data['pt_data']:
                ipo_clock_data.append(ipo_data)
        
        # Display detailed analysis
        if not ipo_clock_data:
            print(f"{Color.YELLOW}No clock analysis data found{Color.RESET}")
            return
        
        # Find best IPO (lowest max latency across all clocks)
        best_ipo = None
        best_max_latency = float('inf')
        stage_priority = {'postroute': 1, 'route': 2, 'postcts': 3, 'place': 4, 'preroute': 5, 'floorplan': 6, 'PT-only': 7}
        
        for ipo_data in ipo_clock_data:
            # Calculate max latency for this IPO
            all_max_values = []
            
            for clock_data in ipo_data['innovus_data'].values():
                all_max_values.append(clock_data[0])  # max_ps
            
            for clock_data in ipo_data['pt_data'].values():
                all_max_values.append(clock_data[0])  # max_ps
            
            if not all_max_values:
                continue
            
            ipo_max = max(all_max_values)
            ipo_stage = ipo_data.get('innovus_stage', 'PT-only')
            ipo_priority = stage_priority.get(ipo_stage, 999)
            
            # Select best: lowest max latency, then best stage
            if (ipo_max < best_max_latency) or \
               (ipo_max == best_max_latency and ipo_priority < stage_priority.get(best_ipo.get('innovus_stage', 'PT-only') if best_ipo else 'PT-only', 999)):
                best_ipo = ipo_data
                best_max_latency = ipo_max
        
        if best_ipo:
            print(f"\n{Color.GREEN}Best IPO: {best_ipo['ipo']} (Max Latency: {best_max_latency:.1f}ps, {best_ipo['innovus_stage']}){Color.RESET}")
        
        # Print unified clock table
        print(f"\n{Color.CYAN}Detailed Clock Latency Analysis:{Color.RESET}\n")
        print(f"  {'IPO':<20} {'Stage':<11} {'Clock':<15} {'Cycle Time':<12} {'Innovus Median':<16} {'Innovus Max':<13} {'PT Max':<13} {'PT Min':<13}")
        print(f"  {'-'*20} {'-'*11} {'-'*15} {'-'*12} {'-'*16} {'-'*13} {'-'*13} {'-'*13}")
        
        for ipo_data in ipo_clock_data:
            ipo = ipo_data['ipo']
            stage = ipo_data.get('innovus_stage', 'none')
            
            # Get all unique clocks from both Innovus and PT
            all_clocks = set()
            all_clocks.update(ipo_data['innovus_data'].keys())
            all_clocks.update(ipo_data['pt_data'].keys())
            
            if not all_clocks:
                continue  # Skip IPOs with no data
            
            for clock in sorted(all_clocks):
                # Extract cycle time
                cycle_time_str = "N/A"
                if clock in ipo_data['clock_cycles']:
                    cycle_time_ns = ipo_data['clock_cycles'][clock]
                    cycle_time_str = f"{cycle_time_ns:.2f}ns"
                
                # Extract Innovus values
                if clock in ipo_data['innovus_data']:
                    inn_tuple = ipo_data['innovus_data'][clock]
                    inn_max = inn_tuple[0] / 1000  # ps to ns
                    inn_median = inn_tuple[2] / 1000 if len(inn_tuple) > 2 else 0
                    inn_max_base = f"{inn_max:.3f}ns" if inn_max > 0 else "N/A"
                    inn_median_str = f"{inn_median:.3f}ns" if inn_median > 0 else "N/A"
                    
                    # Red highlight if > 550ps (with padding to maintain alignment)
                    if inn_max > 0.55:
                        inn_max_str = f"{Color.RED}{inn_max_base:<13}{Color.RESET}"
                    else:
                        inn_max_str = f"{inn_max_base:<13}"
                else:
                    inn_max_str = f"{'N/A':<13}"
                    inn_median_str = "N/A"
                
                # Extract PT values
                if clock in ipo_data['pt_data']:
                    pt_tuple = ipo_data['pt_data'][clock]
                    pt_max = pt_tuple[0] / 1000  # ps to ns
                    pt_min = pt_tuple[1] / 1000 if len(pt_tuple) > 1 else 0
                    pt_max_base = f"{pt_max:.3f}ns" if pt_max > 0 else "N/A"
                    pt_min_str = f"{pt_min:.3f}ns" if pt_min > 0 else "N/A"
                    
                    # Red highlight if > 550ps (with padding to maintain alignment)
                    if pt_max > 0.55:
                        pt_max_str = f"{Color.RED}{pt_max_base:<13}{Color.RESET}"
                    else:
                        pt_max_str = f"{pt_max_base:<13}"
                else:
                    pt_max_str = f"{'N/A':<13}"
                    pt_min_str = "N/A"
                
                # Print row - don't add format width for colored strings as they already have it
                print(f"  {ipo:<20} {stage:<11} {clock:<15} {cycle_time_str:<12} {inn_median_str:<16} {inn_max_str} {pt_max_str} {pt_min_str:<13}")
            
            # Blank line between IPOs for visual separation
            print()
        
        # Print legend
        print(f"{Color.CYAN}Note: Values > 550ps (0.55ns) are highlighted in red{Color.RESET}")
        
        # Print source files
        print(f"\n{Color.CYAN}Source Files:{Color.RESET}")
        for ipo_data in ipo_clock_data:
            if ipo_data['innovus_file']:
                stage_info = f" ({ipo_data['innovus_stage']})"
                rel_path = ipo_data['innovus_file'].replace(self.workarea + '/', '') if self.workarea in ipo_data['innovus_file'] else ipo_data['innovus_file']
                print(f"  {ipo_data['ipo']} (Innovus{stage_info}): {rel_path}")
            if ipo_data['pt_file']:
                rel_path = ipo_data['pt_file'].replace(self.workarea + '/', '') if self.workarea in ipo_data['pt_file'] else ipo_data['pt_file']
                print(f"  {ipo_data['ipo']} (PT): {rel_path}")
        
        # Determine overall status based on best IPO's max latency
        # Thresholds: FAIL â‰¥ 580ps, WARN 550ps < latency < 580ps, PASS â‰¤ 550ps
        status = "PASS"
        issues = []
        
        if best_max_latency >= 580:
            status = "FAIL"
            issues.append(f"Max clock latency: {best_max_latency:.1f}ps (threshold: <580ps)")
        elif best_max_latency > 550:
            status = "WARN"
            issues.append(f"Max clock latency: {best_max_latency:.1f}ps (threshold: â‰¤550ps)")
        
        # Prepare key metrics
        key_metrics = {}
        if len(ipo_clock_data) > 1:
            key_metrics["Best IPO"] = f"{best_ipo['ipo']} ({best_max_latency:.1f}ps, {best_ipo['innovus_stage']})"
            key_metrics["Total IPOs"] = str(len(ipo_clock_data))
        else:
            key_metrics["Max Latency"] = f"{best_max_latency:.1f}ps ({ipo_clock_data[0]['innovus_stage']})"
        
        # Generate HTML report with unified clock table
        html_file = self._generate_clock_html_report(
            ipo_clock_data=ipo_clock_data,
            best_ipo=best_ipo,
            status=status
        )
        
        # Add section summary for master dashboard
        self._add_section_summary(
            section_name="Clock Analysis",
            section_id="clock",
            stage=FlowStage.CLOCK_ANALYSIS,
            status=status,
            key_metrics=key_metrics,
            html_file=html_file,
            priority=3,
            issues=issues,
            icon="[Clock]"
        )
    
    def run_formal_verification(self) -> None:
        """Run formal verification analysis"""
        self.print_header(FlowStage.FORMAL_VERIFICATION)
        
        # Extract and display RTL tag
        rtl_readme_path = os.path.join(self.workarea, "rbv", "README")
        rtl_tag = "N/A"
        if os.path.isfile(rtl_readme_path):
            try:
                with open(rtl_readme_path, 'r') as f:
                    lines = f.readlines()
                    if len(lines) >= 2:
                        # Line 2 contains: TAG: <rtl_tag>
                        tag_line = lines[1].strip()
                        if tag_line.startswith("TAG:"):
                            rtl_tag = tag_line.split("TAG:", 1)[1].strip()
                print(f"{Color.CYAN}RTL Tag: {Color.RESET}{rtl_tag}")
            except (OSError, UnicodeDecodeError):
                pass
        
        # Check multibit mapping files (critical for formal verification)
        print(f"\n{Color.CYAN}Multibit Mapping Files Check:{Color.RESET}")
        multibit_status, multibit_issues, multibit_results = self._check_multibit_mapping_files()
        
        formal_log_pattern = "formal_flow/*_vs_*_fm/log/*_vs_*_fm.log"
        formal_files = self.file_utils.find_files(formal_log_pattern, self.workarea)
        
        # Track formal verification results for master dashboard
        formal_results = []
        overall_status = "NOT_RUN"
        issues = []
        latest_formal_end = 0  # Initialize before use
        
        # Add multibit mapping issues to overall issues
        if multibit_issues:
            issues.extend(multibit_issues)
        
        if formal_files:
            # Get latest formal end time for comparison with ECO
            
            for log_file in formal_files:
                self.print_file_info(log_file, "Formal Log")
                status, runtime, flow_name, passing_pts, failing_pts, compare_tbl, failing_list = self._extract_formal_verification_status(log_file)
                formal_results.append((flow_name, status, runtime, passing_pts, failing_pts, compare_tbl, failing_list))
                
                # Extract and display timestamps
                formal_end_time = self._display_formal_timestamps(log_file)
                if formal_end_time and formal_end_time > latest_formal_end:
                    latest_formal_end = formal_end_time
            
            # Check if ECO was run after formal (potential issue)
            self._check_formal_vs_eco_timestamps(latest_formal_end)
            
            # Determine overall status based on all formal results
            statuses = [result[1] for result in formal_results]
            
            # Check multibit status first (critical for formal to succeed)
            if multibit_status == "FAIL":
                overall_status = "FAIL"
            # Check for CRASHED flows (highest priority - tool error)
            elif any("CRASHED" in s for s in statuses):
                crashed_flows = [f"{r[0]}: {r[1]}" for r in formal_results if "CRASHED" in r[1]]
                overall_status = "FAIL"
                issues.extend(crashed_flows)
            elif "FAILED" in statuses:
                overall_status = "FAIL"
                # Include failing compare points in issues
                for r in formal_results:
                    if r[1] == "FAILED":
                        if r[4] > 0:  # failing_pts > 0
                            issues.append(f"{r[0]}: FAILED ({r[4]} failing points)")
                        else:
                            issues.append(f"{r[0]}: FAILED")
            elif multibit_status == "WARN":
                overall_status = "WARN"
            elif "UNRESOLVED" in statuses:
                overall_status = "WARN"
                unresolved_flows = [f"{r[0]}: UNRESOLVED" for r in formal_results if r[1] == "UNRESOLVED"]
                issues.extend(unresolved_flows)
            elif "SUCCEEDED" in statuses:
                overall_status = "PASS"
            elif "RUNNING" in statuses:
                overall_status = "WARN"
                issues.append("Formal verification still running")
        else:
            print("No formal verification logs found")
            # Even without formal logs, multibit status matters
            if multibit_status == "FAIL":
                overall_status = "FAIL"
            elif multibit_status == "WARN":
                overall_status = "WARN"
            else:
                overall_status = "NOT_RUN"
        
        # Build key metrics
        key_metrics = {"Design": self.design_info.top_hier, "RTL Tag": rtl_tag}
        for flow_name, status, runtime, passing_pts, failing_pts, compare_tbl, failing_list in formal_results:
            if status == "FAILED" and failing_pts > 0:
                key_metrics[flow_name] = f"{status} ({failing_pts} fail, {runtime})"
            else:
                key_metrics[flow_name] = f"{status} ({runtime})"
        
        # Generate comprehensive HTML report for formal verification
        html_path = ""
        if formal_results or multibit_results:
            html_path = self._generate_formal_html_report(formal_results, rtl_tag, latest_formal_end, multibit_results)
            if html_path:
                html_filename = os.path.basename(html_path)
                
                # Determine display path
                html_output_dir = self._get_html_output_dir()
                display_path = os.path.relpath(html_output_dir, os.getcwd())
                
                print(f"\n  {Color.CYAN}Formal Verification HTML Report:{Color.RESET}")
                print(f"  Open with: /home/utils/firefox-118.0.1/firefox {Color.MAGENTA}{display_path}/{html_filename}{Color.RESET} &")
        
        # Add section summary for master dashboard
        self._add_section_summary(
            section_name="Formal Verification",
            section_id="formal",
            stage=FlowStage.FORMAL_VERIFICATION,
            status=overall_status,
            key_metrics=key_metrics,
            html_file=html_path,
            priority=1 if overall_status == "FAIL" else (2 if overall_status == "WARN" else 3),
            issues=issues,
            icon="[Formal]"
        )
    
    def _generate_formal_html_report(self, formal_results: List[Dict[str, Any]], rtl_tag: str, latest_formal_end: str, multibit_results=None) -> Optional[str]:
        """Generate comprehensive HTML report for Formal Verification
        
        Args:
            formal_results: List of formal verification result dictionaries
            rtl_tag: RTL tag string
            latest_formal_end: Latest formal end timestamp
            multibit_results: Optional multibit mapping results
            
        Returns:
            HTML filename if generated successfully, None otherwise
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            username = os.environ.get('USER', 'avice')
            html_filename = f"{self.design_info.top_hier}_{username}_formal_verification_{timestamp}.html"
            html_path = os.path.abspath(html_filename)
            
            # Determine overall status
            statuses = [result[1] for result in formal_results] if formal_results else []
            overall_status = "PASS"
            if any("CRASHED" in s for s in statuses):
                overall_status = "CRASHED"
            elif "FAILED" in statuses:
                overall_status = "FAILED"
            elif "UNRESOLVED" in statuses:
                overall_status = "UNRESOLVED"
            elif "RUNNING" in statuses:
                overall_status = "RUNNING"
            elif "SUCCEEDED" in statuses:
                overall_status = "PASS"
            
            status_color = {
                "PASS": "#28a745",
                "FAILED": "#dc3545",
                "CRASHED": "#dc3545",
                "UNRESOLVED": "#ffc107",
                "RUNNING": "#17a2b8"
            }.get(overall_status, "#6c757d")
            
            # Generate HTML content
            html_content = self._generate_formal_html_content(
                formal_results, rtl_tag, overall_status, status_color, html_filename, multibit_results
            )
            
            # Write HTML file
            with open(html_path, 'w') as f:
                f.write(html_content)
            
            return html_path
            
        except Exception as e:
            print(f"  Error generating Formal HTML report: {e}")
            import traceback
            traceback.print_exc()
            return ""
    
    def _generate_formal_html_content(self, formal_results: List[Dict[str, Any]], rtl_tag: str, overall_status: str, status_color: str, html_filename: str, multibit_results=None) -> str:
        """Generate HTML content for formal verification report
        
        Args:
            formal_results: List of formal verification results
            rtl_tag: RTL tag string
            overall_status: Overall verification status
            status_color: Color for status display
            html_filename: Output HTML filename
            multibit_results: Optional multibit mapping results
            
        Returns:
            Complete HTML string for formal verification report
        """
        
        # Load and encode logo
        logo_data = ""
        logo_path = os.path.join(os.path.dirname(__file__), "assets", "images", "avice_logo.png")
        try:
            with open(logo_path, 'rb') as f:
                import base64
                logo_data = base64.b64encode(f.read()).decode('utf-8')
        except:
            pass  # If logo not found, continue without it
        
        # Count statuses
        total_flows = len(formal_results)
        passed_flows = sum(1 for r in formal_results if r[1] == "SUCCEEDED")
        failed_flows = sum(1 for r in formal_results if r[1] == "FAILED")
        crashed_flows = sum(1 for r in formal_results if "CRASHED" in r[1])
        unresolved_flows = sum(1 for r in formal_results if r[1] == "UNRESOLVED")
        running_flows = sum(1 for r in formal_results if r[1] == "RUNNING")
        
        # Calculate total compare points
        total_passing = sum(r[3] for r in formal_results)
        total_failing = sum(r[4] for r in formal_results)
        
        # Generate flow cards HTML
        flow_cards_html = ""
        for flow_name, status, runtime, passing_pts, failing_pts, compare_table, failing_points_list in formal_results:
            status_class = {
                "SUCCEEDED": "status-pass",
                "FAILED": "status-fail",
                "UNRESOLVED": "status-warn",
                "RUNNING": "status-running"
            }.get(status, "status-fail" if "CRASHED" in status else "status-unknown")
            
            status_icon = {
                "SUCCEEDED": "âœ“",
                "FAILED": "âœ—",
                "UNRESOLVED": "âš ",
                "RUNNING": "â†»"
            }.get(status, "âš " if "CRASHED" not in status else "ðŸ’¥")
            
            # Compare points section
            compare_points_html = ""
            if status == "FAILED" and (passing_pts > 0 or failing_pts > 0):
                total_pts = passing_pts + failing_pts
                pass_percentage = (passing_pts / total_pts * 100) if total_pts > 0 else 0
                
                # Generate matched compare points table HTML
                compare_table_html = ""
                if compare_table:
                    compare_table_html = """
                <div class="compare-table-section">
                    <h4>Matched Compare Points Breakdown</h4>
                    <table class="compare-table">
                        <thead>
                            <tr>
                                <th>Category</th>
                                <th>BBPin</th>
                                <th>Loop</th>
                                <th>BBNet</th>
                                <th>Cut</th>
                                <th>Port</th>
                                <th>DFF</th>
                                <th>LAT</th>
                                <th>TOTAL</th>
                            </tr>
                        </thead>
                        <tbody>
                    """
                    
                    if 'passing' in compare_table:
                        p = compare_table['passing']
                        compare_table_html += f"""
                            <tr class="passing-row">
                                <td><strong>Passing (equivalent)</strong></td>
                                <td>{p.get('BBPin', 0):,}</td>
                                <td>{p.get('Loop', 0):,}</td>
                                <td>{p.get('BBNet', 0):,}</td>
                                <td>{p.get('Cut', 0):,}</td>
                                <td>{p.get('Port', 0):,}</td>
                                <td>{p.get('DFF', 0):,}</td>
                                <td>{p.get('LAT', 0):,}</td>
                                <td><strong>{p.get('TOTAL', 0):,}</strong></td>
                            </tr>
                        """
                    
                    if 'failing' in compare_table:
                        f = compare_table['failing']
                        compare_table_html += f"""
                            <tr class="failing-row">
                                <td><strong>Failing (not equivalent)</strong></td>
                                <td>{f.get('BBPin', 0):,}</td>
                                <td>{f.get('Loop', 0):,}</td>
                                <td>{f.get('BBNet', 0):,}</td>
                                <td>{f.get('Cut', 0):,}</td>
                                <td>{f.get('Port', 0):,}</td>
                                <td>{f.get('DFF', 0):,}</td>
                                <td>{f.get('LAT', 0):,}</td>
                                <td><strong>{f.get('TOTAL', 0):,}</strong></td>
                            </tr>
                        """
                    
                    if 'not_compared' in compare_table and compare_table['not_compared']:
                        compare_table_html += """
                            <tr class="separator-row">
                                <td colspan="9"><strong>Not Compared</strong></td>
                            </tr>
                        """
                        for name, count in compare_table['not_compared'].items():
                            compare_table_html += f"""
                            <tr class="not-compared-row">
                                <td>&nbsp;&nbsp;{name}</td>
                                <td colspan="7"></td>
                                <td>{count:,}</td>
                            </tr>
                            """
                    
                    compare_table_html += """
                        </tbody>
                    </table>
                </div>
                    """
                
                # Generate failing points list HTML
                failing_points_html = ""
                if failing_points_list:
                    failing_points_html = f"""
                <div class="failing-points-section">
                    <h4>Failing Compare Points ({len(failing_points_list)} points)</h4>
                    <div class="failing-points-list">
                    """
                    for point in failing_points_list:
                        failing_points_html += f'<div class="failing-point">âœ— {point}</div>\n'
                    failing_points_html += """
                    </div>
                </div>
                    """
                
                compare_points_html = f"""
                <div class="compare-points">
                    <h4>Compare Points Summary</h4>
                    <div class="points-grid">
                        <div class="point-card pass">
                            <div class="point-label">Passing</div>
                            <div class="point-value">{passing_pts:,}</div>
                            <div class="point-percentage">{pass_percentage:.2f}%</div>
                        </div>
                        <div class="point-card fail">
                            <div class="point-label">Failing</div>
                            <div class="point-value">{failing_pts:,}</div>
                            <div class="point-percentage">{100-pass_percentage:.2f}%</div>
                        </div>
                        <div class="point-card total">
                            <div class="point-label">Total</div>
                            <div class="point-value">{total_pts:,}</div>
                        </div>
                    </div>
                    {compare_table_html}
                    {failing_points_html}
                </div>
                """
            elif status == "SUCCEEDED" and passing_pts > 0:
                compare_points_html = f"""
                <div class="compare-points success">
                    <div class="success-message">
                        <span class="success-icon">âœ“</span>
                        All {passing_pts:,} compare points passed
                    </div>
                </div>
                """
            
            flow_cards_html += f"""
            <div class="flow-card {status_class}">
                <div class="flow-header">
                    <h3>{flow_name}</h3>
                    <span class="status-badge {status_class}">{status_icon} {status}</span>
                </div>
                <div class="flow-details">
                    <div class="detail-item">
                        <span class="detail-label">Runtime:</span>
                        <span class="detail-value">{runtime}</span>
                    </div>
                </div>
                {compare_points_html}
            </div>
            """
        
        # Generate HTML
        html = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Formal Verification Report - {self.design_info.top_hier}</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
            color: #333;
        }}
        
        .container {{
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 12px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.2);
            overflow: hidden;
        }}
        
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px 40px;
            text-align: center;
        }}
        
        .header-content {{
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 30px;
            flex-wrap: wrap;
        }}
        
        .logo {{
            max-width: 120px;
            height: auto;
            filter: drop-shadow(0 4px 6px rgba(0,0,0,0.3));
            cursor: pointer;
            transition: transform 0.3s ease;
        }}
        
        .logo:hover {{
            transform: scale(1.05);
        }}
        
        .header-text {{
            flex: 1;
            min-width: 300px;
        }}
        
        .header h1 {{
            font-size: 2.5em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }}
        
        .header-subtitle {{
            font-size: 1.1em;
            opacity: 0.9;
        }}
        
        /* Logo Modal */
        .logo-modal {{
            display: none;
            position: fixed;
            z-index: 1000;
            left: 0;
            top: 0;
            width: 100%;
            height: 100%;
            overflow: auto;
            background-color: rgba(0,0,0,0.9);
            cursor: pointer;
        }}
        
        .logo-modal-content {{
            margin: auto;
            display: block;
            width: 80%;
            max-width: 700px;
            position: relative;
            top: 50%;
            transform: translateY(-50%);
        }}
        
        .logo-modal-close {{
            position: absolute;
            top: 15px;
            right: 35px;
            color: #f1f1f1;
            font-size: 40px;
            font-weight: bold;
            transition: 0.3s;
        }}
        
        .logo-modal-close:hover,
        .logo-modal-close:focus {{
            color: #bbb;
            text-decoration: none;
            cursor: pointer;
        }}
        
        .summary-section {{
            padding: 30px 40px;
            background: #f8f9fa;
            border-bottom: 3px solid #e9ecef;
        }}
        
        .summary-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }}
        
        .summary-card {{
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            text-align: center;
            border-left: 4px solid #667eea;
        }}
        
        .summary-card.overall {{
            border-left-color: {status_color};
            background: linear-gradient(135deg, white 0%, {status_color}10 100%);
        }}
        
        .summary-card.passed {{
            border-left-color: #28a745;
        }}
        
        .summary-card.failed {{
            border-left-color: #dc3545;
        }}
        
        .summary-card.crashed {{
            border-left-color: #dc3545;
        }}
        
        .summary-card.unresolved {{
            border-left-color: #ffc107;
        }}
        
        .summary-label {{
            font-size: 0.9em;
            color: #666;
            text-transform: uppercase;
            letter-spacing: 1px;
            margin-bottom: 10px;
        }}
        
        .summary-value {{
            font-size: 2.5em;
            font-weight: bold;
            color: #333;
        }}
        
        .rtl-section {{
            padding: 20px 40px;
            background: #fff9e6;
            border-left: 4px solid #ffc107;
            margin: 20px 40px;
            border-radius: 8px;
        }}
        
        .rtl-container {{
            display: flex;
            align-items: center;
            gap: 10px;
            margin-top: 8px;
        }}
        
        .rtl-label {{
            font-weight: bold;
            color: #856404;
            font-size: 0.9em;
            text-transform: uppercase;
            letter-spacing: 1px;
        }}
        
        .rtl-value {{
            flex: 1;
            font-family: 'Courier New', monospace;
            color: #333;
            font-size: 1.1em;
            padding: 10px;
            background: white;
            border-radius: 4px;
            overflow: hidden;
            text-overflow: ellipsis;
            white-space: nowrap;
        }}
        
        .copy-btn {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            padding: 8px 16px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 0.9em;
            font-weight: bold;
            transition: all 0.3s ease;
            white-space: nowrap;
        }}
        
        .copy-btn:hover {{
            transform: scale(1.05);
            box-shadow: 0 4px 8px rgba(102, 126, 234, 0.3);
        }}
        
        .copy-btn:active {{
            transform: scale(0.95);
        }}
        
        .copy-btn.copied {{
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
        }}
        
        .content {{
            padding: 40px;
        }}
        
        .section-title {{
            font-size: 1.8em;
            margin-bottom: 30px;
            color: #333;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
        }}
        
        .flow-card {{
            background: white;
            border: 2px solid #e9ecef;
            border-radius: 8px;
            padding: 25px;
            margin-bottom: 25px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            transition: all 0.3s ease;
        }}
        
        .flow-card:hover {{
            box-shadow: 0 4px 16px rgba(0,0,0,0.15);
            transform: translateY(-2px);
        }}
        
        .flow-card.status-pass {{
            border-left: 6px solid #28a745;
        }}
        
        .flow-card.status-fail {{
            border-left: 6px solid #dc3545;
        }}
        
        .flow-card.status-warn {{
            border-left: 6px solid #ffc107;
        }}
        
        .flow-card.status-running {{
            border-left: 6px solid #17a2b8;
        }}
        
        .flow-header {{
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 20px;
            padding-bottom: 15px;
            border-bottom: 2px solid #f8f9fa;
        }}
        
        .flow-header h3 {{
            color: #333;
            font-size: 1.4em;
        }}
        
        .status-badge {{
            padding: 8px 20px;
            border-radius: 20px;
            font-weight: bold;
            font-size: 0.9em;
            text-transform: uppercase;
            letter-spacing: 1px;
        }}
        
        .status-badge.status-pass {{
            background: #28a745;
            color: white;
        }}
        
        .status-badge.status-fail {{
            background: #dc3545;
            color: white;
        }}
        
        .status-badge.status-warn {{
            background: #ffc107;
            color: #333;
        }}
        
        .status-badge.status-running {{
            background: #17a2b8;
            color: white;
        }}
        
        .flow-details {{
            margin-bottom: 20px;
        }}
        
        .detail-item {{
            display: flex;
            justify-content: space-between;
            padding: 10px 0;
            border-bottom: 1px solid #f8f9fa;
        }}
        
        .detail-label {{
            font-weight: 600;
            color: #666;
        }}
        
        .detail-value {{
            font-family: 'Courier New', monospace;
            color: #333;
        }}
        
        .compare-points {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-top: 15px;
        }}
        
        .compare-points h4 {{
            color: #333;
            margin-bottom: 15px;
            font-size: 1.1em;
        }}
        
        .points-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 15px;
        }}
        
        .point-card {{
            background: white;
            padding: 15px;
            border-radius: 8px;
            text-align: center;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        
        .point-card.pass {{
            border-top: 4px solid #28a745;
        }}
        
        .point-card.fail {{
            border-top: 4px solid #dc3545;
        }}
        
        .point-card.total {{
            border-top: 4px solid #667eea;
        }}
        
        .point-label {{
            font-size: 0.9em;
            color: #666;
            text-transform: uppercase;
            margin-bottom: 8px;
        }}
        
        .point-value {{
            font-size: 2em;
            font-weight: bold;
            color: #333;
            margin-bottom: 5px;
        }}
        
        .point-percentage {{
            font-size: 0.9em;
            color: #666;
        }}
        
        .compare-points.success {{
            background: linear-gradient(135deg, #28a74510 0%, #28a74520 100%);
            border: 2px solid #28a745;
        }}
        
        .success-message {{
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 10px;
            font-size: 1.2em;
            color: #155724;
            font-weight: 600;
        }}
        
        .success-icon {{
            font-size: 1.5em;
            color: #28a745;
        }}
        
        .compare-table-section {{
            margin-top: 20px;
        }}
        
        .compare-table {{
            width: 100%;
            border-collapse: collapse;
            margin-top: 15px;
            background: white;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        
        .compare-table thead {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
        }}
        
        .compare-table th {{
            padding: 12px 8px;
            text-align: center;
            font-weight: bold;
            font-size: 0.9em;
            border: 1px solid rgba(255,255,255,0.2);
        }}
        
        .compare-table td {{
            padding: 10px 8px;
            text-align: center;
            border: 1px solid #e9ecef;
        }}
        
        .compare-table td:first-child {{
            text-align: left;
            padding-left: 15px;
        }}
        
        .compare-table .passing-row {{
            background: linear-gradient(90deg, #28a74510 0%, white 100%);
        }}
        
        .compare-table .passing-row td:last-child {{
            color: #28a745;
            font-weight: bold;
        }}
        
        .compare-table .failing-row {{
            background: linear-gradient(90deg, #dc354510 0%, white 100%);
        }}
        
        .compare-table .failing-row td:last-child {{
            color: #dc3545;
            font-weight: bold;
        }}
        
        .compare-table .separator-row {{
            background: #f8f9fa;
            font-weight: bold;
        }}
        
        .compare-table .separator-row td {{
            padding: 12px 15px;
            text-align: left;
            border-top: 2px solid #667eea;
        }}
        
        .compare-table .not-compared-row {{
            background: white;
        }}
        
        .compare-table .not-compared-row:hover {{
            background: #f8f9fa;
        }}
        
        .failing-points-section {{
            margin-top: 20px;
            background: #fff5f5;
            border: 2px solid #dc3545;
            border-radius: 8px;
            padding: 15px;
        }}
        
        .failing-points-section h4 {{
            color: #dc3545;
            margin-bottom: 15px;
        }}
        
        .failing-points-list {{
            max-height: 400px;
            overflow-y: auto;
            background: white;
            border-radius: 4px;
            padding: 10px;
        }}
        
        .failing-point {{
            padding: 8px 12px;
            margin: 5px 0;
            background: #f8f9fa;
            border-left: 4px solid #dc3545;
            font-family: 'Courier New', monospace;
            font-size: 0.85em;
            color: #333;
            border-radius: 4px;
            transition: all 0.2s ease;
        }}
        
        .failing-point:hover {{
            background: #e9ecef;
            transform: translateX(5px);
        }}
        
        .footer {{
            background: #f8f9fa;
            padding: 20px 40px;
            text-align: center;
            color: #666;
            border-top: 3px solid #e9ecef;
        }}
        
        .footer-info {{
            font-size: 0.9em;
        }}
        
        @media print {{
            body {{
                background: white;
                padding: 0;
            }}
            
            .container {{
                box-shadow: none;
            }}
        }}
    </style>
</head>
<body>
    <!-- Logo Modal -->
    <div id="logoModal" class="logo-modal" onclick="hideLogoModal()">
        <span class="logo-modal-close">&times;</span>
        <img class="logo-modal-content" id="logoModalImg">
    </div>
    
    <div class="container">
        <div class="header">
            <div class="header-content">
                {f'<img class="logo" src="data:image/png;base64,{logo_data}" alt="AVICE Logo" onclick="showLogoModal()" title="Click to enlarge">' if logo_data else ''}
                <div class="header-text">
                    <h1>ðŸ” Formal Verification Report</h1>
                    <div class="header-subtitle">Design: {self.design_info.top_hier}</div>
                    <div class="header-subtitle">Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</div>
                </div>
            </div>
        </div>
        
        <div class="summary-section">
            <h2>Verification Summary</h2>
            <div class="summary-grid">
                <div class="summary-card overall">
                    <div class="summary-label">Overall Status</div>
                    <div class="summary-value" style="color: {status_color};">{overall_status}</div>
                </div>
                <div class="summary-card">
                    <div class="summary-label">Total Flows</div>
                    <div class="summary-value">{total_flows}</div>
                </div>
                <div class="summary-card passed">
                    <div class="summary-label">Passed</div>
                    <div class="summary-value" style="color: #28a745;">{passed_flows}</div>
                </div>
                <div class="summary-card failed">
                    <div class="summary-label">Failed</div>
                    <div class="summary-value" style="color: #dc3545;">{failed_flows}</div>
                </div>
                {f'''<div class="summary-card crashed">
                    <div class="summary-label">Crashed</div>
                    <div class="summary-value" style="color: #dc3545;">{crashed_flows}</div>
                </div>''' if crashed_flows > 0 else ''}
                {f'''<div class="summary-card unresolved">
                    <div class="summary-label">Unresolved</div>
                    <div class="summary-value" style="color: #ffc107;">{unresolved_flows}</div>
                </div>''' if unresolved_flows > 0 else ''}
            </div>
            {f'''<div class="summary-grid" style="margin-top: 20px;">
                <div class="summary-card passed">
                    <div class="summary-label">Total Passing Points</div>
                    <div class="summary-value" style="color: #28a745;">{total_passing:,}</div>
                </div>
                <div class="summary-card failed">
                    <div class="summary-label">Total Failing Points</div>
                    <div class="summary-value" style="color: #dc3545;">{total_failing:,}</div>
                </div>
            </div>''' if total_passing > 0 or total_failing > 0 else ''}
        </div>
        
        <div class="rtl-section">
            <div class="rtl-label">RTL Tag</div>
            <div class="rtl-container">
                <div class="rtl-value" id="rtl-tag">{rtl_tag}</div>
                <button class="copy-btn" onclick="copyToClipboard('rtl-tag', this)" title="Copy RTL tag">
                    ðŸ“‹ Copy
                </button>
            </div>
        </div>
        
        {self._generate_multibit_mapping_html_section(multibit_results)}
        
        <div class="content">
            <h2 class="section-title">Formal Verification Flows</h2>
            {flow_cards_html}
        </div>
        
        <div class="footer">
            <div class="footer-info">
                <p><strong>AVICE Formal Verification Report</strong></p>
                <p>Report generated by AVICE Workarea Review Tool</p>
                <p>Workarea: {self.workarea}</p>
                <p>File: {html_filename}</p>
                <p>Copyright (c) 2025 Alon Vice (avice)</p>
                <p>Contact: avice@nvidia.com</p>
            </div>
        </div>
    </div>
    
    <!-- Back to Top Button -->
    <button id="backToTopBtn" style="display: none; position: fixed; bottom: 30px; right: 30px; 
            z-index: 9999; border: none; outline: none; background-color: #667eea; color: white; 
            cursor: pointer; padding: 15px 20px; border-radius: 50px; font-size: 16px; 
            font-weight: bold; box-shadow: 0 4px 6px rgba(0,0,0,0.3); transition: all 0.3s ease;"
            onmouseover="this.style.backgroundColor='#5568d3'; this.style.transform='scale(1.1)';"
            onmouseout="this.style.backgroundColor='#667eea'; this.style.transform='scale(1)';">
        â†‘ Top
    </button>
    
    <script>
        // Logo modal functions
        function showLogoModal() {{
            const modal = document.getElementById('logoModal');
            const modalImg = document.getElementById('logoModalImg');
            const logo = document.querySelector('.logo');
            if (modal && modalImg && logo) {{
                modal.style.display = 'block';
                modalImg.src = logo.src;
            }}
        }}
        
        function hideLogoModal() {{
            const modal = document.getElementById('logoModal');
            if (modal) {{
                modal.style.display = 'none';
            }}
        }}
        
        // Back to top button functionality
        const backToTopBtn = document.getElementById('backToTopBtn');
        if (backToTopBtn) {{
            window.addEventListener('scroll', function() {{
                if (window.pageYOffset > 300) {{
                    backToTopBtn.style.display = 'block';
                }} else {{
                    backToTopBtn.style.display = 'none';
                }}
            }});
            
            backToTopBtn.addEventListener('click', function() {{
                window.scrollTo({{ top: 0, behavior: 'smooth' }});
            }});
        }}
        
        // Copy to clipboard function
        function copyToClipboard(elementId, button) {{
            const element = document.getElementById(elementId);
            const text = element.textContent;
            
            // Use modern clipboard API
            if (navigator.clipboard && navigator.clipboard.writeText) {{
                navigator.clipboard.writeText(text).then(function() {{
                    // Success feedback
                    const originalText = button.innerHTML;
                    button.innerHTML = 'âœ… Copied!';
                    button.classList.add('copied');
                    
                    // Reset after 2 seconds
                    setTimeout(function() {{
                        button.innerHTML = originalText;
                        button.classList.remove('copied');
                    }}, 2000);
                }}).catch(function(err) {{
                    console.error('Failed to copy:', err);
                    button.innerHTML = 'âŒ Failed';
                    setTimeout(function() {{
                        button.innerHTML = 'ðŸ“‹ Copy';
                    }}, 2000);
                }});
            }} else {{
                // Fallback for older browsers
                const textArea = document.createElement('textarea');
                textArea.value = text;
                textArea.style.position = 'fixed';
                textArea.style.left = '-999999px';
                document.body.appendChild(textArea);
                textArea.select();
                try {{
                    document.execCommand('copy');
                    button.innerHTML = 'âœ… Copied!';
                    button.classList.add('copied');
                    setTimeout(function() {{
                        button.innerHTML = 'ðŸ“‹ Copy';
                        button.classList.remove('copied');
                    }}, 2000);
                }} catch (err) {{
                    console.error('Fallback copy failed:', err);
                    button.innerHTML = 'âŒ Failed';
                    setTimeout(function() {{
                        button.innerHTML = 'ðŸ“‹ Copy';
                    }}, 2000);
                }}
                document.body.removeChild(textArea);
            }}
        }}
    </script>
</body>
</html>
"""
        return html
    
    def _generate_multibit_mapping_html_section(self, multibit_results: Dict[str, Any]) -> str:
        """Generate HTML section for multibit mapping file check results
        
        Args:
            multibit_results: Dictionary containing multibit mapping data
            
        Returns:
            HTML string for the multibit section
        """
        if not multibit_results:
            return ""
        
        # Count total checks and passed checks
        total_checks = 0
        passed_checks = 0
        
        # Build table rows
        table_rows = ""
        for ipo, ipo_data in multibit_results.items():
            for loc_name, loc_info in ipo_data["locations"].items():
                files_status = loc_info["files_exist"]
                total_checks += 1
                
                # Determine status and color
                if all(files_status):
                    status_label = "âœ“ OK"
                    status_color = "#28a745"
                    passed_checks += 1
                else:
                    status_label = "âœ— MISSING"
                    status_color = "#dc3545"
                
                # Create file status string
                file1_icon = "âœ“" if files_status[0] else "âœ—"
                file2_icon = "âœ“" if files_status[1] else "âœ—"
                file_status_str = f"{file1_icon} multibitMapping.gz<br>{file2_icon} multibitMapping_for_gen_mbff_scandef.gz"
                
                table_rows += f"""
                <tr>
                    <td>{ipo}</td>
                    <td>{loc_name}</td>
                    <td style="font-family: monospace; font-size: 12px;">{file_status_str}</td>
                    <td style="font-weight: bold; color: {status_color};">{status_label}</td>
                </tr>
                """
        
        # Determine overall section status
        if passed_checks == total_checks:
            section_status = "âœ“ PASS"
            section_color = "#28a745"
            section_bg = "#d4edda"
            section_icon = "âœ…"
        else:
            section_status = "âœ— FAIL"
            section_color = "#dc3545"
            section_bg = "#f8d7da"
            section_icon = "âŒ"
        
        html = f"""
        <div class="content" style="margin-top: 30px; background: {section_bg}; border-left: 4px solid {section_color};">
            <h2 class="section-title" style="color: {section_color};">
                {section_icon} Multibit Mapping Files Check
                <span style="float: right; font-size: 18px; font-weight: bold;">{section_status}</span>
            </h2>
            <div style="padding: 15px;">
                <p style="margin-bottom: 15px; color: #333;">
                    <strong>Purpose:</strong> Multibit mapping files are critical for formal verification to prevent non-equivalence points.
                    These files are generated during PnR flow and must be copied to export_innovus and nv_gate_eco directories.
                </p>
                <p style="margin-bottom: 20px; color: #666; font-size: 14px;">
                    <strong>Flow:</strong> PnR generates files â†’ Copied to export_innovus â†’ Copied to nv_gate_eco during ECO â†’ 
                    Copied back to export_innovus after each ECO loop
                </p>
                
                <table style="width: 100%; border-collapse: collapse; background: white;">
                    <thead>
                        <tr style="background: #f8f9fa; border-bottom: 2px solid #dee2e6;">
                            <th style="padding: 12px; text-align: left; border: 1px solid #dee2e6;">IPO</th>
                            <th style="padding: 12px; text-align: left; border: 1px solid #dee2e6;">Location</th>
                            <th style="padding: 12px; text-align: left; border: 1px solid #dee2e6;">File Status</th>
                            <th style="padding: 12px; text-align: center; border: 1px solid #dee2e6;">Result</th>
                        </tr>
                    </thead>
                    <tbody>
                        {table_rows}
                    </tbody>
                </table>
                
                <div style="margin-top: 20px; padding: 15px; background: white; border-radius: 5px;">
                    <p style="margin: 5px 0;"><strong>Summary:</strong> {passed_checks}/{total_checks} location checks passed</p>
                    {f'<p style="margin: 5px 0; color: #dc3545;"><strong>âš  Warning:</strong> Missing files may cause formal verification non-equivalence points!</p>' if passed_checks < total_checks else ''}
                    {f'<p style="margin: 5px 0; color: #666;"><strong>Action:</strong> Verify files were copied correctly during PnR/ECO flows. Re-run copy steps if needed.</p>' if passed_checks < total_checks else ''}
                </div>
            </div>
        </div>
        """
        
        return html
    
    def run_parasitic_extraction(self) -> None:
        """Run parasitic extraction analysis"""
        self.print_header(FlowStage.PARASITIC_EXTRACTION)
        
        # Find all Star runs by looking for summary.rpt files
        summary_pattern = f"export/nv_star/{self.design_info.top_hier}/ipo*/REPs/*.checks.summary.rpt"
        summary_files = self.file_utils.find_files(summary_pattern, self.workarea)
        
        if summary_files:
            # Extract run information from each summary file
            star_runs = []
            for summary_file in summary_files:
                # Extract timestamp from filename (e.g., STAR.setup.smc.1.fdb_ipo1000.10_08_18_15.checks.summary.rpt)
                basename = os.path.basename(summary_file)
                timestamp_match = re.search(r'\.(\d{2}_\d{2}_\d{2}_\d{2})\.checks\.summary\.rpt', basename)
                if timestamp_match:
                    timestamp_str = timestamp_match.group(1)
                    # Parse timestamp: MM_DD_HH_MM
                    try:
                        month, day, hour, minute = timestamp_str.split('_')
                        timestamp_display = f"{month}/{day} {hour}:{minute}"
                    except:
                        timestamp_display = timestamp_str
                    
                    # Extract shorts count from SX-0955 code
                    shorts_count = 0
                    try:
                        with open(summary_file, 'r') as f:
                            for line in f:
                                if 'SX-0955' in line:
                                    # Format: SX-0955     COUNT    WAIVED    TOTAL
                                    parts = line.split()
                                    if len(parts) >= 2:
                                        shorts_count = int(parts[1])
                                    break
                    except:
                        pass
                    
                    # Get file modification time for sorting
                    mtime = os.path.getmtime(summary_file)
                    
                    star_runs.append({
                        'timestamp': timestamp_display,
                        'shorts': shorts_count,
                        'mtime': mtime,
                        'file': summary_file
                    })
            
            # Sort by modification time (oldest first)
            star_runs.sort(key=lambda x: x['mtime'])
            
            # Display Star runs summary table
            print(f"\n{Color.CYAN}Star Extraction Runs:{Color.RESET}")
            print(f"  Total Runs: {len(star_runs)}")
            print(f"\n  {'Run':<6} {'Timestamp':<12} {'Shorts':<8}")
            print(f"  {'-'*6} {'-'*12} {'-'*8}")
            for idx, run in enumerate(star_runs, 1):
                shorts_color = Color.RED if run['shorts'] > 0 else Color.GREEN
                is_latest = " (latest)" if idx == len(star_runs) else ""
                print(f"  {idx:<6} {run['timestamp']:<12} {shorts_color}{run['shorts']:<8}{Color.RESET}{is_latest}")
            
            # Show latest run details
            if star_runs:
                latest_run = star_runs[-1]
                print(f"\n{Color.CYAN}Latest Run Details:{Color.RESET}")
                print(f"  Timestamp: {latest_run['timestamp']}")
                print(f"  Shorts: {Color.GREEN if latest_run['shorts'] == 0 else Color.RED}{latest_run['shorts']}{Color.RESET}")
        else:
            print(f"  {Color.YELLOW}No Star extraction runs found{Color.RESET}")
        
        # Star extraction SPEF files (all corners from latest run)
        print(f"\n{Color.CYAN}SPEF Files (All Corners):{Color.RESET}")
        spef_pattern = f"export/nv_star/{self.design_info.top_hier}/ipo*/IOs/netlists/*.spef.*.gz"
        all_spef_files = self.file_utils.find_files(spef_pattern, self.workarea)
        
        if all_spef_files:
            # Filter out files with .bad. in the name (broken symlinks or test files)
            all_spef_files = [f for f in all_spef_files if '.bad.' not in os.path.basename(f)]
            
            # Get the most recent modification time to group latest run
            if all_spef_files:
                latest_mtime = max(os.path.getmtime(f) for f in all_spef_files)
                # Consider files modified within 5 minutes of latest as part of same run
                latest_spef_files = [f for f in all_spef_files if abs(os.path.getmtime(f) - latest_mtime) < 300]
                
                # Sort by corner name for consistent display
                latest_spef_files.sort()
                
                print(f"  Total SPEF files: {len(latest_spef_files)}")
                if len(latest_spef_files) < 6:
                    print(f"  {Color.YELLOW}Warning: Expected at least 6 SPEF files, found {len(latest_spef_files)}{Color.RESET}")
                else:
                    print(f"  {Color.GREEN}[OK] All required SPEF files present{Color.RESET}")
                
                # Show location once
                if latest_spef_files:
                    spef_dir = os.path.dirname(latest_spef_files[0])
                    print(f"  Location: {spef_dir}")
                    print(f"\n  {'Corner':<25} {'Size':>8}")
                    print(f"  {'-'*25} {'-'*8}")
                    
                    # Extract corner information and display
                    for spef_file in latest_spef_files:
                        basename = os.path.basename(spef_file)
                        # Extract corner name (e.g., typical_T0, cworst_CCworst_T0, etc.)
                        corner_match = re.search(r'\.spef\.([^.]+)\.gz', basename)
                        if corner_match:
                            corner = corner_match.group(1)
                            file_size_bytes = os.path.getsize(spef_file)
                            file_size_gb = file_size_bytes / (1024**3)
                            print(f"  {corner:<25} {file_size_gb:>6.2f} GB")
                        else:
                            print(f"  {basename}")
        else:
            print(f"  {Color.YELLOW}No SPEF files found{Color.RESET}")
        
        # SPEF info file for typical corner
        spef_info_pattern = f"export/nv_star/{self.design_info.top_hier}/ipo*/IOs/netlists/*.typical_T0.spef_info"
        spef_info_files = self.file_utils.find_files(spef_info_pattern, self.workarea)
        
        if spef_info_files:
            # Get the most recent SPEF info file
            latest_spef_info = max(spef_info_files, key=os.path.getmtime)
            self.print_file_info(latest_spef_info, "SPEF Info (Typical Corner)")
            
            print(f"\n{Color.CYAN}Star Extraction Configuration:{Color.RESET}")
            try:
                with open(latest_spef_info, 'r') as f:
                    content = f.read()
                
                # Extract date
                date_match = re.search(r'date:\s*(.+)', content)
                if date_match:
                    date_str = date_match.group(1).strip()
                    print(f"  Date: {date_str}")
                
                # Extract opens and shorts counts
                opens_match = re.search(r'opens:\s*(\d+)', content)
                shorts_match = re.search(r'shorts:\s*(\d+)', content)
                
                opens_count = int(opens_match.group(1)) if opens_match else 0
                shorts_count = int(shorts_match.group(1)) if shorts_match else 0
                
                print(f"  Opens: {opens_count}")
                print(f"  Shorts: {shorts_count}")
                
                # Extract project_rdl_file
                project_rdl_match = re.search(r'project_rdl_file:\s*(.+)', content)
                if project_rdl_match:
                    project_rdl = project_rdl_match.group(1).strip()
                    print(f"  Project RDL: {project_rdl}")
                
                # Extract GDS_LAYER_MAP_FILE
                gds_layer_map_match = re.search(r'GDS_LAYER_MAP_FILE:\s*(.+)', content)
                if gds_layer_map_match:
                    gds_layer_map = gds_layer_map_match.group(1).strip()
                    print(f"  GDS Layer Map: {gds_layer_map}")
                
                # Extract MAPPING_FILE
                mapping_file_match = re.search(r'MAPPING_FILE:\s*(.+)', content)
                if mapping_file_match:
                    mapping_file = mapping_file_match.group(1).strip()
                    print(f"  Mapping File: {mapping_file}")
                
            except Exception as e:
                print(f"  Error reading SPEF info: {e}")
                # Fallback to original grep method
                matches = self.file_utils.grep_file(r"opens|shorts|date:", latest_spef_info)
                for match in matches:
                    print(f"  {match}")
        
        # Star extraction detailed shorts report (generated only when shorts exist)
        shorts_pattern = f"export/nv_star/{self.design_info.top_hier}/ipo*/REPs/*.star_extraction_shorts.rpt"
        shorts_files = self.file_utils.find_files(shorts_pattern, self.workarea)
        
        if shorts_files:
            self.print_file_info(shorts_files[0], "Star Detailed Shorts Report")
            
            # Identify which run this shorts report is from
            if summary_files:  # If we have star_runs data
                shorts_mtime = os.path.getmtime(shorts_files[0])
                
                # Find the run with the closest timestamp (within 5 minutes)
                matching_run = None
                min_time_diff = float('inf')
                for idx, run in enumerate(star_runs, 1):
                    time_diff = abs(run['mtime'] - shorts_mtime)
                    if time_diff < min_time_diff and time_diff < 300:  # Within 5 minutes
                        min_time_diff = time_diff
                        matching_run = (idx, run)
                
                if matching_run:
                    run_num, run_data = matching_run
                    is_latest = (run_num == len(star_runs))
                    latest_note = " (latest run)" if is_latest else f" (NOT the latest - latest is run #{len(star_runs)})"
                    print(f"  {Color.YELLOW}Note: This report is from Run #{run_num} ({run_data['timestamp']}){latest_note}{Color.RESET}")
                else:
                    print(f"  {Color.YELLOW}Note: This report shows shorts from a previous run{Color.RESET}")
            else:
                print(f"  {Color.YELLOW}Note: This report shows shorts from the most recent run that had shorts{Color.RESET}")
            
            try:
                if shorts_files[0].endswith('.gz'):
                    with gzip.open(shorts_files[0], 'rt', encoding='utf-8') as f:
                        content = f.read()
                else:
                    with open(shorts_files[0], 'r', encoding='utf-8') as f:
                        content = f.read()
                # Show first 50 lines to avoid overwhelming output
                lines = content.split('\n')[:50]
                print('\n'.join(lines))
                if len(content.split('\n')) > 50:
                    print(f"\n  {Color.YELLOW}... (truncated, see file for full details){Color.RESET}")
            except (OSError, UnicodeDecodeError, gzip.BadGzipFile):
                print("  Unable to read Star Shorts Report")
        
        # Generate Star HTML report
        star_html = self._generate_star_html_report(star_runs if summary_files else [], 
                                                     latest_spef_files if all_spef_files else [],
                                                     latest_spef_info if spef_info_files else None,
                                                     shorts_files[0] if shorts_files else None)
        
        # Determine status based on shorts and SPEF files
        status = "NOT_RUN"
        key_metrics = {"Design": self.design_info.top_hier}
        issues = []
        priority = 3
        
        if summary_files and star_runs:
            latest_shorts = star_runs[-1]['shorts']
            key_metrics["Total Runs"] = len(star_runs)
            key_metrics["Latest Shorts"] = latest_shorts
            
            # Check SPEF files
            spef_count = len(latest_spef_files) if all_spef_files else 0
            key_metrics["SPEF Files"] = spef_count
            
            # Status logic:
            # FAIL: shorts > 0
            # WARN: SPEF files < 6 (missing corners)
            # PASS: shorts == 0 and SPEF files >= 6
            if latest_shorts > 0:
                status = "FAIL"
                priority = 1
                issues.append(f"Star extraction has {latest_shorts} shorts")
            elif spef_count < 6:
                status = "WARN"
                priority = 2
                issues.append(f"Only {spef_count} SPEF files found (expected >= 6)")
            else:
                status = "PASS"
        
        # Add section summary for master dashboard
        self._add_section_summary(
            section_name="Parasitic Extraction (Star)",
            section_id="star",
            stage=FlowStage.PARASITIC_EXTRACTION,
            status=status,
            key_metrics=key_metrics,
            html_file=star_html if star_html else "",
            priority=priority,
            issues=issues,
            icon="[Star]"
        )
    
    def _generate_star_html_report(self, star_runs: List[str], spef_files: List[str], spef_info_file: Optional[str], shorts_file: Optional[str]) -> Optional[str]:
        """Generate comprehensive HTML report for Star extraction analysis
        
        Args:
            star_runs: List of Star run summary files
            spef_files: List of SPEF files found
            spef_info_file: Path to SPEF info file
            shorts_file: Path to shorts file
            
        Returns:
            HTML filename if generated successfully, None otherwise
        """
        try:
            # Generate timestamp for filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            html_filename = f"{self.design_info.top_hier}_{os.environ.get('USER', 'avice')}_star_extraction_{timestamp}.html"
            html_path = os.path.join(os.getcwd(), html_filename)
            
            # Read and encode logo
            logo_data = ""
            logo_path = os.path.join(os.path.dirname(__file__), "assets/images/avice_logo.png")
            if os.path.exists(logo_path):
                with open(logo_path, "rb") as logo_file:
                    logo_data = base64.b64encode(logo_file.read()).decode('utf-8')
            
            # Extract configuration data from spef_info
            config_data = {}
            if spef_info_file and os.path.exists(spef_info_file):
                try:
                    with open(spef_info_file, 'r') as f:
                        content = f.read()
                    config_data['date'] = re.search(r'date:\s*(.+)', content).group(1).strip() if re.search(r'date:\s*(.+)', content) else "N/A"
                    config_data['opens'] = int(re.search(r'opens:\s*(\d+)', content).group(1)) if re.search(r'opens:\s*(\d+)', content) else 0
                    config_data['shorts'] = int(re.search(r'shorts:\s*(\d+)', content).group(1)) if re.search(r'shorts:\s*(\d+)', content) else 0
                    config_data['project_rdl'] = re.search(r'project_rdl_file:\s*(.+)', content).group(1).strip() if re.search(r'project_rdl_file:\s*(.+)', content) else "N/A"
                    config_data['gds_layer_map'] = re.search(r'GDS_LAYER_MAP_FILE:\s*(.+)', content).group(1).strip() if re.search(r'GDS_LAYER_MAP_FILE:\s*(.+)', content) else "N/A"
                    config_data['mapping_file'] = re.search(r'MAPPING_FILE:\s*(.+)', content).group(1).strip() if re.search(r'MAPPING_FILE:\s*(.+)', content) else "N/A"
                except:
                    pass
            
            # Read shorts details if available
            shorts_content = ""
            shorts_run_info = ""
            if shorts_file and os.path.exists(shorts_file):
                try:
                    if shorts_file.endswith('.gz'):
                        with gzip.open(shorts_file, 'rt', encoding='utf-8') as f:
                            shorts_content = f.read()
                    else:
                        with open(shorts_file, 'r', encoding='utf-8') as f:
                            shorts_content = f.read()
                    
                    # Identify which run this shorts report is from
                    if star_runs:
                        shorts_mtime = os.path.getmtime(shorts_file)
                        for idx, run in enumerate(star_runs, 1):
                            time_diff = abs(run['mtime'] - shorts_mtime)
                            if time_diff < 300:  # Within 5 minutes
                                is_latest = (idx == len(star_runs))
                                shorts_run_info = f"Run #{idx} ({run['timestamp']})" + (" - Latest" if is_latest else f" - NOT latest (latest is #{len(star_runs)})")
                                break
                except:
                    shorts_content = "Unable to read shorts report"
            
            # Generate HTML content
            html_content = f"""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Star Extraction Report - {self.design_info.top_hier}</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
            color: #333;
        }}
        
        .container {{
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }}
        
        .header {{
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            padding: 30px;
            display: grid;
            grid-template-columns: auto 1fr;
            gap: 20px;
            align-items: center;
        }}
        
        .logo {{
            width: 80px;
            height: 80px;
            border-radius: 10px;
            background: white;
            padding: 10px;
            cursor: pointer;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }}
        
        .logo:hover {{
            transform: scale(1.05);
            box-shadow: 0 8px 16px rgba(0,0,0,0.3);
        }}
        
        .logo-modal {{
            display: none;
            position: fixed;
            z-index: 9999;
            left: 0;
            top: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0,0,0,0.9);
            justify-content: center;
            align-items: center;
        }}
        
        .logo-modal.active {{
            display: flex;
        }}
        
        .logo-modal-content {{
            max-width: 90%;
            max-height: 90%;
            border-radius: 10px;
        }}
        
        .logo-modal-close {{
            position: absolute;
            top: 20px;
            right: 35px;
            color: #f1f1f1;
            font-size: 40px;
            font-weight: bold;
            cursor: pointer;
        }}
        
        .logo-modal-close:hover {{
            color: #bbb;
        }}
        
        .header-text h1 {{
            font-size: 28px;
            margin-bottom: 8px;
        }}
        
        .header-text p {{
            opacity: 0.9;
            font-size: 14px;
        }}
        
        .content {{
            padding: 30px;
        }}
        
        .summary-cards {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        
        .card {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }}
        
        .card-label {{
            font-size: 12px;
            opacity: 0.9;
            text-transform: uppercase;
            letter-spacing: 1px;
            margin-bottom: 8px;
        }}
        
        .card-value {{
            font-size: 24px;
            font-weight: bold;
        }}
        
        .card.success {{
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
        }}
        
        .card.warning {{
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        }}
        
        .section {{
            margin-bottom: 30px;
            background: #f8f9fa;
            border-radius: 10px;
            padding: 20px;
        }}
        
        .section-title {{
            font-size: 20px;
            font-weight: bold;
            color: #2a5298;
            margin-bottom: 15px;
            padding-bottom: 10px;
            border-bottom: 2px solid #667eea;
        }}
        
        table {{
            width: 100%;
            border-collapse: collapse;
            background: white;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        
        th {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 12px;
            text-align: left;
            font-weight: 600;
            text-transform: uppercase;
            font-size: 12px;
            letter-spacing: 0.5px;
        }}
        
        td {{
            padding: 12px;
            border-bottom: 1px solid #e9ecef;
        }}
        
        tr:hover {{
            background: #f8f9fa;
        }}
        
        .status-clean {{
            color: #28a745;
            font-weight: bold;
        }}
        
        .status-shorts {{
            color: #dc3545;
            font-weight: bold;
        }}
        
        .latest-badge {{
            background: #ffc107;
            color: #000;
            padding: 2px 8px;
            border-radius: 4px;
            font-size: 10px;
            font-weight: bold;
            margin-left: 8px;
        }}
        
        .config-grid {{
            display: grid;
            grid-template-columns: auto 1fr;
            gap: 12px 20px;
            background: white;
            padding: 15px;
            border-radius: 8px;
        }}
        
        .config-label {{
            font-weight: 600;
            color: #495057;
        }}
        
        .config-value {{
            color: #6c757d;
            word-break: break-all;
        }}
        
        .shorts-details {{
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px;
            border-radius: 4px;
            margin-top: 15px;
        }}
        
        .shorts-details-title {{
            font-weight: bold;
            color: #856404;
            margin-bottom: 10px;
        }}
        
        pre {{
            background: #f8f9fa;
            padding: 15px;
            border-radius: 4px;
            overflow-x: auto;
            font-size: 12px;
            line-height: 1.5;
            max-height: 400px;
            overflow-y: auto;
        }}
        
        /* Copyright Footer */
        .footer {{
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            text-align: center;
            padding: 20px;
            margin-top: 40px;
            border-radius: 10px;
            font-size: 14px;
        }}
        
        .footer p {{
            margin: 5px 0;
        }}
        
        .footer strong {{
            color: #00ff00;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">"""
            
            if logo_data:
                html_content += f"""
            <img class='logo' src='data:image/png;base64,{logo_data}' alt='AVICE Logo' onclick="showLogoModal()" title="Click to enlarge">"""
            
            html_content += f"""
            <div class="header-text">
                <h1>Star Extraction Report</h1>
                <p>Design: {self.design_info.top_hier} | IPO: {self.design_info.ipo}</p>
                <p>Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
            </div>
        </div>
        
        <!-- Logo Modal -->
        <div id="logoModal" class="logo-modal" onclick="hideLogoModal()">
            <span class="logo-modal-close">&times;</span>"""
            
            if logo_data:
                html_content += f"""
            <img class="logo-modal-content" src='data:image/png;base64,{logo_data}' alt='AVICE Logo'>"""
            
            html_content += """
        </div>
        
        <div class="content">"""
            
            # Summary Cards
            if star_runs:
                latest_run = star_runs[-1]
                total_runs = len(star_runs)
                shorts_count = latest_run['shorts']
                
                html_content += f"""
            <div class="summary-cards">
                <div class="card">
                    <div class="card-label">Total Runs</div>
                    <div class="card-value">{total_runs}</div>
                </div>
                <div class="card {'success' if shorts_count == 0 else 'warning'}">
                    <div class="card-label">Latest Run Shorts</div>
                    <div class="card-value">{shorts_count}</div>
                </div>
                <div class="card">
                    <div class="card-label">Latest Run</div>
                    <div class="card-value" style="font-size: 18px;">{latest_run['timestamp']}</div>
                </div>
                <div class="card">
                    <div class="card-label">SPEF Files</div>
                    <div class="card-value">{len(spef_files)}</div>
                </div>
            </div>"""
            
            # Star Runs Table
            if star_runs:
                html_content += """
            <div class="section">
                <div class="section-title">Star Extraction Runs History</div>
                <table>
                    <thead>
                        <tr>
                            <th>Run #</th>
                            <th>Timestamp</th>
                            <th>Shorts</th>
                            <th>Status</th>
                        </tr>
                    </thead>
                    <tbody>"""
                
                for idx, run in enumerate(star_runs, 1):
                    is_latest = (idx == len(star_runs))
                    status_class = 'status-clean' if run['shorts'] == 0 else 'status-shorts'
                    status_text = 'âœ“ Clean' if run['shorts'] == 0 else f'âš  {run["shorts"]} Short(s)'
                    latest_badge = '<span class="latest-badge">LATEST</span>' if is_latest else ''
                    
                    html_content += f"""
                        <tr>
                            <td><strong>#{idx}</strong></td>
                            <td>{run['timestamp']}{latest_badge}</td>
                            <td class="{status_class}">{run['shorts']}</td>
                            <td class="{status_class}">{status_text}</td>
                        </tr>"""
                
                html_content += """
                    </tbody>
                </table>
            </div>"""
            
            # SPEF Files Table
            if spef_files:
                spef_dir = os.path.dirname(spef_files[0])
                html_content += f"""
            <div class="section">
                <div class="section-title">SPEF Files (All RC Corners)</div>
                <p style="margin-bottom: 15px; color: #6c757d;"><strong>Location:</strong> {spef_dir}</p>
                <table>
                    <thead>
                        <tr>
                            <th>Corner</th>
                            <th>File Size</th>
                            <th>Filename</th>
                        </tr>
                    </thead>
                    <tbody>"""
                
                for spef_file in sorted(spef_files):
                    basename = os.path.basename(spef_file)
                    corner_match = re.search(r'\.spef\.([^.]+)\.gz', basename)
                    corner = corner_match.group(1) if corner_match else basename
                    file_size_gb = os.path.getsize(spef_file) / (1024**3)
                    
                    html_content += f"""
                        <tr>
                            <td><strong>{corner}</strong></td>
                            <td>{file_size_gb:.2f} GB</td>
                            <td>{basename}</td>
                        </tr>"""
                
                html_content += """
                    </tbody>
                </table>
            </div>"""
            
            # Configuration Section
            if config_data:
                html_content += f"""
            <div class="section">
                <div class="section-title">Star Extraction Configuration</div>
                <div class="config-grid">
                    <div class="config-label">Extraction Date:</div>
                    <div class="config-value">{config_data.get('date', 'N/A')}</div>
                    
                    <div class="config-label">Opens:</div>
                    <div class="config-value">{config_data.get('opens', 0)}</div>
                    
                    <div class="config-label">Shorts:</div>
                    <div class="config-value">{config_data.get('shorts', 0)}</div>
                    
                    <div class="config-label">Project RDL File:</div>
                    <div class="config-value">{config_data.get('project_rdl', 'N/A')}</div>
                    
                    <div class="config-label">GDS Layer Map:</div>
                    <div class="config-value">{config_data.get('gds_layer_map', 'N/A')}</div>
                    
                    <div class="config-label">Mapping File:</div>
                    <div class="config-value">{config_data.get('mapping_file', 'N/A')}</div>
                </div>
            </div>"""
            
            # Shorts Details Section
            if shorts_content:
                html_content += f"""
            <div class="section">
                <div class="section-title">Detailed Shorts Report</div>"""
                
                if shorts_run_info:
                    html_content += f"""
                <div class="shorts-details">
                    <div class="shorts-details-title">âš  Note</div>
                    <p>This report is from {shorts_run_info}</p>
                </div>"""
                
                # Limit shorts content to first 100 lines
                shorts_lines = shorts_content.split('\n')[:100]
                shorts_display = '\n'.join(shorts_lines)
                if len(shorts_content.split('\n')) > 100:
                    shorts_display += "\n\n... (truncated, see file for full details)"
                
                html_content += f"""
                <pre>{shorts_display}</pre>
            </div>"""
            
            html_content += """
        </div>
    </div>
    
    <!-- Copyright Footer -->
    <div class="footer">
        <p><strong>AVICE Star Parasitic Extraction Report</strong></p>
        <p>Copyright (c) 2025 Alon Vice (avice)</p>
        <p>Contact: avice@nvidia.com</p>
    </div>
    
    <script>
        function showLogoModal() {
            document.getElementById('logoModal').classList.add('active');
        }
        
        function hideLogoModal() {
            document.getElementById('logoModal').classList.remove('active');
        }
        
        // Allow ESC key to close modal
        document.addEventListener('keydown', function(event) {
            if (event.key === 'Escape') {
                hideLogoModal();
            }
        });
        
        // Back to top button functionality - wait for DOM to load
        document.addEventListener('DOMContentLoaded', function() {{
            var backToTopBtn = document.getElementById('backToTopBtn');
            if (backToTopBtn) {{
                window.addEventListener('scroll', function() {{
                    if (window.pageYOffset > 300) {{
                        backToTopBtn.style.display = 'block';
                    }} else {{
                        backToTopBtn.style.display = 'none';
                    }}
                }});
                
                backToTopBtn.addEventListener('click', function() {{
                    window.scrollTo(0, 0);
                }});
            }}
        }});
    </script>
    
    <button id="backToTopBtn" style="display: none; position: fixed; bottom: 30px; right: 30px; 
            z-index: 99; border: none; outline: none; background-color: #667eea; color: white; 
            cursor: pointer; padding: 15px 20px; border-radius: 50px; font-size: 16px; 
            font-weight: bold; box-shadow: 0 4px 6px rgba(0,0,0,0.3); transition: all 0.3s ease;"
            onmouseover="this.style.backgroundColor='#5568d3'; this.style.transform='scale(1.1)';"
            onmouseout="this.style.backgroundColor='#667eea'; this.style.transform='scale(1)';">
        â†‘ Top
    </button>
</body>
</html>"""
            
            # Write HTML file
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            # Determine display path
            html_output_dir = self._get_html_output_dir()
            display_path = os.path.relpath(html_output_dir, os.getcwd())
            
            print(f"\n{Color.CYAN}  Star Extraction HTML Report:{Color.RESET}")
            print(f"  Open with: /home/utils/firefox-118.0.1/firefox {Color.MAGENTA}{display_path}/{html_filename}{Color.RESET} &")
            
            return os.path.abspath(html_path)
            
        except Exception as e:
            print(f"  Error generating Star HTML report: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _find_project_root_for_pt_search(self) -> str:
        """Find project root for comprehensive PT search
        
        If workarea is IPO-specific (ends with /ipo\d+/nbu_signoff):
          - Auto-detect project root containing all ipo* directories
          - Enables finding ALL PT runs across all IPOs
        
        If workarea is project root:
          - Return workarea as-is
        
        Returns:
            Path to use as base for PT search
        """
        # Check if workarea is IPO-specific (e.g., .../ipo1040/nbu_signoff)
        if re.search(r'/ipo\d+/nbu_signoff/?$', self.workarea):
            # Workarea is IPO-specific - find TRUE project root (*_rbv_* directory)
            
            # Find the project root (the *_rbv_* directory)
            # Example: .../prt_rbv_2025_*/pnr_flow/nv_flow/prt/ipo1040/nbu_signoff
            #          Should return: .../prt_rbv_2025_*/
            match = re.search(r'(.*?/[^/]+_rbv_[^/]+)/', self.workarea)
            if match:
                project_root = match.group(1)
                if os.path.isdir(project_root):
                    return project_root
            
            # Fallback: Go up until we find a directory containing pnr_flow
            current = self.workarea
            for _ in range(6):  # Max 6 levels up
                current = os.path.dirname(current)
                if os.path.exists(os.path.join(current, "pnr_flow")):
                    return current
        
        # Workarea is already project root or not IPO-specific
        return self.workarea
    
    def _timeout_handler(self, signum, frame):
        """Signal handler for operation timeouts"""
        raise TimeoutError("Operation timed out")
    
    def _with_timeout(self, func, timeout_seconds=30, default_return=None):
        """Execute function with timeout - returns default_return if timeout occurs
        
        Args:
            func: Function to execute (should be a lambda or callable with no args)
            timeout_seconds: Maximum execution time in seconds
            default_return: Value to return if timeout occurs
        
        Returns:
            Function result or default_return if timeout
        """
        # Set up the signal handler
        old_handler = signal.signal(signal.SIGALRM, self._timeout_handler)
        signal.alarm(timeout_seconds)
        
        try:
            result = func()
            signal.alarm(0)  # Cancel the alarm
            return result
        except TimeoutError:
            signal.alarm(0)  # Cancel the alarm
            return default_return
        finally:
            signal.signal(signal.SIGALRM, old_handler)  # Restore old handler
    
    def _find_all_auto_pt_locations(self) -> List[Dict[str, Any]]:
        """Find ALL auto_pt directories in workarea (root + per-IPO structures)
        
        Smart detection:
          - If workarea is IPO-specific, finds project root and searches ALL IPOs
          - If workarea is project root, searches from workarea
          - Provides complete PT timeline across all locations
        
        Supports:
          1. Root signoff_flow: project_root/signoff_flow/auto_pt
          2. Per-IPO nbu_signoff: project_root/ipo*/nbu_signoff/signoff_flow/auto_pt
          3. Mixed: Both structures can exist simultaneously
        
        Returns:
            List of dictionaries with location info:
            [{
                'path': '/path/to/auto_pt',
                'type': 'root_signoff' or 'per_ipo',
                'ipo': 'ipo1000' or None,
                'label': 'signoff_flow' or 'IPO1000',
                'status': 'RUNNING' or 'COMPLETED' or 'STALE' or 'NO_WORK_DIRS',
                'work_dirs': [list of work directories],
                'latest_work': 'work_10.11.25_20:52' or None,
                'log_age_minutes': minutes since log update or None
            }, ...]
        """
        locations = []
        
        # Find project root for PT search (might be different from self.workarea)
        search_root = self._find_project_root_for_pt_search()
        
        # Pattern 1: Root signoff_flow
        # If search_root is IPO level (pnr_flow/nv_flow/prt/), go up to find signoff_flow
        # Otherwise search from search_root
        if re.search(r'/pnr_flow/nv_flow/[^/]+/?$', search_root):
            # search_root is at pnr_flow/nv_flow/prt/ level
            # Go up to project root: .../prt_rbv_*/
            project_root_match = re.search(r'(.*?/[^/]+_rbv_[^/]+)/', search_root)
            if project_root_match:
                project_root = project_root_match.group(1)
                root_auto_pt = os.path.join(project_root, "signoff_flow/auto_pt")
                if os.path.exists(root_auto_pt) and os.path.isdir(root_auto_pt):
                    location_info = {
                        'path': root_auto_pt,
                        'type': 'root_signoff',
                        'ipo': None,
                        'label': 'signoff_flow'
                    }
                    self._add_pt_status_info(location_info)
                    locations.append(location_info)
        else:
            # search_root is project root, search directly
            root_auto_pt = os.path.join(search_root, "signoff_flow/auto_pt")
            if os.path.exists(root_auto_pt) and os.path.isdir(root_auto_pt):
                location_info = {
                    'path': root_auto_pt,
                    'type': 'root_signoff',
                    'ipo': None,
                    'label': 'signoff_flow'
                }
                self._add_pt_status_info(location_info)
                locations.append(location_info)
        
        # Pattern 2: Per-IPO nbu_signoff
        # Search pattern: pnr_flow/nv_flow/*/ipo*/nbu_signoff/signoff_flow/auto_pt from search_root
        ipo_pattern = os.path.join(search_root, "pnr_flow/nv_flow/*/ipo*/nbu_signoff/signoff_flow/auto_pt")
        ipo_paths = [p for p in glob.glob(ipo_pattern) if os.path.isdir(p)]
        
        # Show progress if processing many locations
        if len(ipo_paths) > 5:
            print(f"  {Color.CYAN}Analyzing {len(ipo_paths)} IPO locations (this may take a moment)...{Color.RESET}")
        
        for idx, auto_pt_path in enumerate(ipo_paths, 1):
            # Extract IPO from path: .../ipo1000/nbu_signoff/...
            match = re.search(r'/(ipo\d+)/nbu_signoff/', auto_pt_path)
            ipo = match.group(1) if match else 'unknown'
            
            # Show progress for large numbers of locations
            if len(ipo_paths) > 5 and idx % 3 == 0:
                print(f"  {Color.CYAN}  Processing {idx}/{len(ipo_paths)} locations...{Color.RESET}", end='\r')
            
            location_info = {
                'path': auto_pt_path,
                'type': 'per_ipo',
                'ipo': ipo,
                'label': ipo.upper() if ipo != 'unknown' else 'UNKNOWN_IPO'
            }
            # Add status and work directory info
            self._add_pt_status_info(location_info)
            locations.append(location_info)
        
        # Clear progress line if shown
        if len(ipo_paths) > 5:
            print(" " * 80, end='\r')  # Clear the progress line
        
        # Sort locations: root signoff first, then IPOs in alphanumeric order
        locations.sort(key=lambda x: (x['type'] != 'root_signoff', x['label']))
        
        return locations
    
    def _add_pt_status_info(self, location_info: Dict[str, Any]) -> None:
        """Add PT status and work directory info to location dictionary
        
        Modifies location_info in-place to add:
          - status: RUNNING / COMPLETED / STALE / NO_WORK_DIRS / NO_LOG
          - work_dirs: list of work directory paths
          - latest_work: name of most recent work directory
          - log_age_minutes: minutes since log was last modified
        """
        auto_pt_path = location_info['path']
        
        # Find all work directories (with caching for repeated calls)
        # Use a simple cache keyed by path to avoid repeated glob operations
        if not hasattr(self, '_work_dirs_cache'):
            self._work_dirs_cache = {}
        
        if auto_pt_path in self._work_dirs_cache:
            work_dirs = self._work_dirs_cache[auto_pt_path]
        else:
            work_pattern = os.path.join(auto_pt_path, "work_*")
            all_items = glob.glob(work_pattern)
            work_dirs = [item for item in all_items if os.path.isdir(item)]
            work_dirs = sorted(work_dirs, key=os.path.getmtime, reverse=True)
            self._work_dirs_cache[auto_pt_path] = work_dirs
        
        location_info['work_dirs'] = work_dirs
        location_info['latest_work'] = os.path.basename(work_dirs[0]) if work_dirs else None
        
        if not work_dirs:
            location_info['status'] = 'NO_WORK_DIRS'
            location_info['log_age_minutes'] = None
            return
        
        # Check auto_pt.log modification time
        log_file = os.path.join(auto_pt_path, "log/auto_pt.log")
        if not os.path.exists(log_file):
            location_info['status'] = 'NO_LOG'
            location_info['log_age_minutes'] = None
            return
        
        # Calculate log age
        mtime = os.path.getmtime(log_file)
        current_time = time.time()
        minutes_since_update = (current_time - mtime) / 60
        location_info['log_age_minutes'] = int(minutes_since_update)
        
        # Check if latest work directory has HTML report (PRIMARY completion indicator)
        # HTML existence is the definitive sign of completion, regardless of log age
        latest_work = work_dirs[0]
        work_name = os.path.basename(latest_work)
        
        html_patterns = [
            os.path.join(auto_pt_path, f"{work_name}.html"),
            os.path.join(auto_pt_path, f"PT_{work_name}.html"),
            os.path.join(latest_work, f"{work_name}.html")
        ]
        
        has_html = any(os.path.exists(p) for p in html_patterns)
        
        # Status logic: Check HTML first (primary indicator), then log age
        if has_html:
            # HTML exists = PT completed successfully (regardless of log age)
            location_info['status'] = 'COMPLETED'
        elif minutes_since_update < 60:
            # No HTML, recent log = PT is currently running
            location_info['status'] = 'RUNNING'
        else:
            # No HTML, old log = PT was abandoned or failed
            location_info['status'] = 'STALE'
    
    def run_signoff_timing(self) -> None:
        """Run signoff timing analysis with multi-location support"""
        self.print_header(FlowStage.SIGNOFF_TIMING)
        
        # Find all auto_pt locations (root + per-IPO)
        pt_locations = self._find_all_auto_pt_locations()
        
        # If IPO explicitly specified, filter to that IPO's PT location(s)
        requested_ipo = getattr(self.design_info, "ipo", None)
        if requested_ipo and requested_ipo.startswith("ipo"):
            per_ipo_locations = [loc for loc in pt_locations if loc.get("type") == "per_ipo"]
            if per_ipo_locations:
                pt_locations = [loc for loc in per_ipo_locations if loc.get("ipo") == requested_ipo]
                if not pt_locations:
                    print(f"{Color.YELLOW}[WARN] No PT locations found for requested IPO: {requested_ipo}{Color.RESET}")
                    return
        
        if not pt_locations:
            print(f"{Color.YELLOW}No PT (auto_pt) directories found{Color.RESET}")
            print(f"  Searched locations:")
            print(f"    - signoff_flow/auto_pt")
            print(f"    - pnr_flow/nv_flow/*/ipo*/nbu_signoff/signoff_flow/auto_pt")
            return
        
        # Display condensed location summary
        location_labels = [loc['label'] for loc in pt_locations]
        
        # Count statuses
        status_counts = {'RUNNING': 0, 'COMPLETED': 0, 'STALE': 0, 'NO_WORK_DIRS': 0, 'NO_LOG': 0}
        total_work_dirs = 0
        latest_work_info = None
        
        for loc in pt_locations:
            # Count this status
            status = loc.get('status')
            if status and status in status_counts:
                status_counts[status] += 1
            
            total_work_dirs += len(loc.get('work_dirs', []))
            # Track latest work directory across all locations
            if loc['latest_work'] and loc['work_dirs']:
                latest_mtime = os.path.getmtime(loc['work_dirs'][0])
                if latest_work_info is None or latest_mtime > latest_work_info[1]:
                    latest_work_info = (loc['latest_work'], latest_mtime, loc['label'])
        
        # Print condensed summary (unified single line with status and latest)
        print(f"\n{Color.CYAN}PT Locations:{Color.RESET} {len(pt_locations)} ({', '.join(location_labels)})")
        
        # Build unified status line with Status + Latest
        status_parts = []
        if status_counts['COMPLETED'] > 0:
            status_parts.append(f"{Color.GREEN}{status_counts['COMPLETED']} COMPLETED{Color.RESET}")
        if status_counts['RUNNING'] > 0:
            status_parts.append(f"{Color.YELLOW}{status_counts['RUNNING']} RUNNING{Color.RESET}")
        if status_counts['STALE'] > 0:
            status_parts.append(f"{Color.RED}{status_counts['STALE']} STALE{Color.RESET}")
        if status_counts['NO_WORK_DIRS'] > 0:
            status_parts.append(f"{Color.YELLOW}{status_counts['NO_WORK_DIRS']} NO_WORK_DIRS{Color.RESET}")
        if status_counts['NO_LOG'] > 0:
            status_parts.append(f"{Color.YELLOW}{status_counts['NO_LOG']} NO_LOG{Color.RESET}")
        
        # Add latest work info to the same line
        status_line = f"  Status: {', '.join(status_parts)}" if status_parts else "  Status: None"
        if latest_work_info:
            work_name, _, loc_label = latest_work_info
            status_line += f"  |  Latest: {Color.CYAN}{work_name}{Color.RESET} [{loc_label}]"
        
        print(status_line)
        
        # Show PT flow timeline (use root signoff_flow if available, otherwise first location)
        root_loc = next((loc for loc in pt_locations if loc['type'] == 'root_signoff'), pt_locations[0])
        pt_local_flow_dirs = [
            os.path.join(root_loc['path'], f"{self.design_info.top_hier}/local_flow"),
            os.path.join(root_loc['path'], "local_flow")
        ]
        self._show_flow_timeline("PT", pt_local_flow_dirs)
        
        # Generate timing summary HTML report (pass pt_locations for multi-location support)
        pt_html_path, timing_data = self._generate_timing_summary_report(pt_locations)
        
        # Add section summary for master dashboard
        status = "NOT_RUN"
        key_metrics = {}
        issues = []
        priority = 1  # Critical section
        
        if timing_data and len(timing_data) > 0:
            # Get latest work area data
            latest = timing_data[0]
            
            # Calculate status based on setup timing (most critical)
            if 'setup' in latest['scenarios']:
                setup_data = latest['scenarios']['setup']
                external_groups = {'FEEDTHROUGH', 'REGIN', 'REGOUT'}
                
                # Calculate internal WNS (worst of all internal groups)
                internal_wns = None
                internal_tns = 0
                internal_nvp = 0
                
                for group_name, group_data in setup_data['groups'].items():
                    if group_name.upper() not in external_groups:
                        internal_tns += group_data['TNS']
                        internal_nvp += group_data['NVP']
                        if internal_wns is None or group_data['WNS'] < internal_wns:
                            internal_wns = group_data['WNS']
                
                # Determine status based on WNS and TNS thresholds
                # Thresholds:
                #   FAIL: WNS < -0.050 ns OR TNS < -10.0 ns (significant violations)
                #   WARN: WNS < 0 OR TNS < 0 (any violation but minor)
                #   PASS: WNS >= 0 AND TNS >= 0
                if internal_wns is not None:
                    if internal_wns < -0.050 or internal_tns < -10.0:
                        status = "FAIL"
                        issues.append(f"Setup timing violation: WNS = {internal_wns:.3f} ns, TNS = {internal_tns:.2f} ns")
                    elif internal_wns < 0 or internal_tns < 0:
                        status = "WARN"
                        issues.append(f"Setup timing minor violation: WNS = {internal_wns:.3f} ns, TNS = {internal_tns:.2f} ns")
                    else:
                        status = "PASS"
                    
                    key_metrics["Setup WNS"] = f"{internal_wns:.3f} ns"
                    key_metrics["Setup TNS"] = f"{internal_tns:.2f} ns"
                    key_metrics["Setup NVP"] = str(internal_nvp)
            
            # Check hold timing too
            if 'hold' in latest['scenarios']:
                hold_data = latest['scenarios']['hold']
                external_groups = {'FEEDTHROUGH', 'REGIN', 'REGOUT'}
                
                hold_wns = None
                hold_tns = 0
                hold_nvp = 0
                
                for group_name, group_data in hold_data['groups'].items():
                    if group_name.upper() not in external_groups:
                        hold_tns += group_data['TNS']
                        hold_nvp += group_data['NVP']
                        if hold_wns is None or group_data['WNS'] < hold_wns:
                            hold_wns = group_data['WNS']
                
                if hold_wns is not None:
                    # Hold timing thresholds (stricter than setup - hold is harder to fix)
                    #   FAIL: WNS < -0.025 ns OR TNS < -5.0 ns (tighter threshold than setup)
                    #   WARN: WNS < 0 OR TNS < 0 (small violations - acceptable during development)
                    if hold_wns < -0.025 or hold_tns < -5.0:
                        status = "FAIL"  # Significant hold violation
                        issues.append(f"Hold timing violation: WNS = {hold_wns:.3f} ns, TNS = {hold_tns:.2f} ns")
                    elif hold_wns < 0 or hold_tns < 0:
                        # Only upgrade to WARN if not already FAIL from setup
                        if status != "FAIL":
                            status = "WARN"
                        issues.append(f"Hold timing minor violation: WNS = {hold_wns:.3f} ns, TNS = {hold_tns:.2f} ns")
                    
                    key_metrics["Hold WNS"] = f"{hold_wns:.3f} ns"
                    key_metrics["Hold TNS"] = f"{hold_tns:.2f} ns"
            
            # Check DSR Mux Clock Skew thresholds
            # Thresholds: FAIL > 10ps, WARN 5ps < skew <= 10ps, PASS <= 5ps
            dsr_skew_setup = latest.get('dsr_skew_setup')
            dsr_skew_hold = latest.get('dsr_skew_hold')
            
            if dsr_skew_setup is not None:
                key_metrics["DSR Skew (Setup)"] = f"{dsr_skew_setup:.2f} ps"
                if dsr_skew_setup > 10.0:
                    if status != "FAIL":
                        status = "FAIL"
                    issues.append(f"DSR skew (setup) violation: {dsr_skew_setup:.2f} ps (target: <=5ps)")
                elif dsr_skew_setup > 5.0:
                    if status != "FAIL":
                        status = "WARN"
                    issues.append(f"DSR skew (setup) marginal: {dsr_skew_setup:.2f} ps (target: <=5ps)")
            
            if dsr_skew_hold is not None:
                key_metrics["DSR Skew (Hold)"] = f"{dsr_skew_hold:.2f} ps"
                if dsr_skew_hold > 10.0:
                    if status != "FAIL":
                        status = "FAIL"
                    issues.append(f"DSR skew (hold) violation: {dsr_skew_hold:.2f} ps (target: <=5ps)")
                elif dsr_skew_hold > 5.0:
                    if status != "FAIL":
                        status = "WARN"
                    issues.append(f"DSR skew (hold) marginal: {dsr_skew_hold:.2f} ps (target: <=5ps)")
            
            # Check PT waiver gaps (auto_pt_slave.err vs central pt.csv)
            waiver_summary = self._summarize_pt_waiver_gaps(latest)
            if waiver_summary.get("csv_loaded", True):
                key_metrics["PT Waiver Gaps"] = str(waiver_summary.get("unwaived_total", 0))
                if waiver_summary.get("unwaived_total", 0) > 0:
                    if status != "FAIL":
                        status = "WARN"
                    issues.append(f"PT waiver gaps: {waiver_summary.get('unwaived_total', 0)} unwaived entry(ies)")
            else:
                key_metrics["PT Waiver Gaps"] = "SKIP"
                if status != "FAIL":
                    status = "WARN"
                issues.append("PT waiver check skipped (pt.csv not available)")
            
            key_metrics["Work Areas"] = str(len(timing_data))
        
        self._add_section_summary(
            section_name="Signoff Timing (PT)",
            section_id="timing",
            stage=FlowStage.SIGNOFF_TIMING,
            status=status,
            key_metrics=key_metrics,
            html_file=pt_html_path if pt_html_path else "",
            priority=priority,
            issues=issues,
            icon="[PT]"
        )
    
    def _parse_power_rollup_file(self, rollup_file: str) -> Dict[str, Any]:
        """Parse power rollup file (.rep or .csv) and extract power metrics
        
        Args:
            rollup_file: Path to rollup.rep or rollup.csv file
            
        Returns:
            Dictionary containing power metrics (total power, internal, switching, leakage, etc.)
        """
        
        try:
            with open(rollup_file, 'r') as f:
                content = f.read()
            
            power_metrics = {}
            
            # Parse based on file extension
            if rollup_file.endswith('.csv'):
                # CSV format parsing
                lines = content.strip().split('\n')
                for line in lines[1:]:  # Skip header
                    parts = [p.strip() for p in line.split(',')]
                    if len(parts) >= 2:
                        key = parts[0]
                        value = parts[1]
                        try:
                            power_metrics[key] = float(value)
                        except ValueError:
                            power_metrics[key] = value
            else:
                # .rep format parsing (PrimeTime power rollup format)
                # Example format:
                #   Dynamic Power[mW]         	157.375
                #   Leakage Power[mW]         	9.805
                #   Total   Power[mW]         	167.179
                
                # Extract Total Power
                total_match = re.search(r'Total\s+Power\[(\w+)\]\s+(\d+\.?\d*)', content, re.IGNORECASE)
                if total_match:
                    power_metrics['total_power'] = float(total_match.group(2))
                    power_metrics['total_power_unit'] = total_match.group(1)
                
                # Extract Dynamic Power (Internal Power)
                dynamic_match = re.search(r'Dynamic\s+Power\[(\w+)\]\s+(\d+\.?\d*)', content, re.IGNORECASE)
                if dynamic_match:
                    power_metrics['dynamic_power'] = float(dynamic_match.group(2))
                    power_metrics['dynamic_power_unit'] = dynamic_match.group(1)
                    # Use dynamic power as internal power
                    power_metrics['internal_power'] = float(dynamic_match.group(2))
                    power_metrics['internal_power_unit'] = dynamic_match.group(1)
                
                # Extract Leakage Power
                leakage_match = re.search(r'Leakage\s+Power\[(\w+)\]\s+(\d+\.?\d*)', content, re.IGNORECASE)
                if leakage_match:
                    power_metrics['leakage_power'] = float(leakage_match.group(2))
                    power_metrics['leakage_power_unit'] = leakage_match.group(1)
                
                # Extract Glitch Power (if available, use as switching power indicator)
                glitch_match = re.search(r'Glitch\s+Power\[(\w+)\]\s+(\d+\.?\d*)', content, re.IGNORECASE)
                if glitch_match:
                    power_metrics['glitch_power'] = float(glitch_match.group(2))
                    power_metrics['glitch_power_unit'] = glitch_match.group(1)
                
                # Extract additional metrics
                # Main Clock Frequency
                freq_match = re.search(r'Main\s+Clk\s+Freq\[GHz\]\s+(\d+\.?\d*)', content, re.IGNORECASE)
                if freq_match:
                    power_metrics['main_clk_freq_ghz'] = float(freq_match.group(1))
                
                # Annotation Score
                score_match = re.search(r'Annotation\s+Score\s+(\d+\.?\d*)\s*%', content, re.IGNORECASE)
                if score_match:
                    power_metrics['annotation_score'] = float(score_match.group(1))
                
                # Activity Factor (Average AF FF/Q) - Used for test selection
                af_match = re.search(r'Average\s+AF\s+FF/Q\s+(\d+\.?\d*)\s*%', content, re.IGNORECASE)
                if af_match:
                    power_metrics['activity_factor'] = float(af_match.group(1))
            
            return power_metrics if power_metrics else None
            
        except Exception as e:
            # Silently fail if file cannot be parsed
            return None
    
    def _parse_clock_timing_report(self, clock_timing_file: str) -> Dict[str, Any]:
        """Parse clock_timing report to extract clock quality metrics
        
        Extracts per-clock metrics:
          - Maximum setup/hold skew
          - Maximum/minimum active transitions (slew)
          - Latency ranges
        
        Args:
            clock_timing_file: Path to *.clock_timing report file
        
        Returns:
            Dictionary with clock quality data:
            {
                'clocks': {
                    'i1_clk': {
                        'max_setup_skew': 0.189,  # ns
                        'max_hold_skew': 0.196,   # ns
                        'max_transition': 0.054,  # ns
                        'min_transition': 0.004   # ns
                    },
                    ...
                }
            }
        """
        try:
            if not os.path.exists(clock_timing_file):
                return None
            
            clock_data = {}
            current_clock = None
            
            with open(clock_timing_file, 'r') as f:
                for line in f:
                    # Detect clock section: "  Clock: i1_clk"
                    clock_match = re.match(r'\s*Clock:\s+(\S+)', line)
                    if clock_match:
                        current_clock = clock_match.group(1)
                        if current_clock not in clock_data:
                            clock_data[current_clock] = {}
                        continue
                    
                    if current_clock:
                        # Extract maximum setup skew
                        # Format: "  Maximum setup skew:" followed by two lines with pin info and value
                        # Line 1: pin1
                        # Line 2: pin2 0.189 rp-+
                        if 'Maximum setup skew:' in line:
                            # Read next two lines, second line should have the value
                            f.readline()  # Skip first pin line
                            value_line = f.readline()
                            skew_match = re.search(r'(\d+\.\d+)\s+r[pf]', value_line)
                            if skew_match:
                                clock_data[current_clock]['max_setup_skew'] = float(skew_match.group(1))
                        
                        # Extract maximum hold skew (same format)
                        elif 'Maximum hold skew:' in line:
                            f.readline()  # Skip first pin line
                            value_line = f.readline()
                            skew_match = re.search(r'(\d+\.\d+)\s+r[pf]', value_line)
                            if skew_match:
                                clock_data[current_clock]['max_hold_skew'] = float(skew_match.group(1))
                        
                        # Extract maximum active transition (slew)
                        # Format: "  Maximum active transition:" followed by pin and value on same or next line
                        elif 'Maximum active transition:' in line:
                            # Check if value is on the same line
                            trans_match = re.search(r'(\d+\.\d+)\s+r[pf]', line)
                            if trans_match:
                                clock_data[current_clock]['max_transition'] = float(trans_match.group(1))
                            else:
                                # Value might be on next line
                                next_line = f.readline()
                                trans_match = re.search(r'(\d+\.\d+)\s+r[pf]', next_line)
                                if trans_match:
                                    clock_data[current_clock]['max_transition'] = float(trans_match.group(1))
                        
                        # Extract minimum active transition (same format)
                        elif 'Minimum active transition:' in line:
                            trans_match = re.search(r'(\d+\.\d+)\s+r[pf]', line)
                            if trans_match:
                                clock_data[current_clock]['min_transition'] = float(trans_match.group(1))
                            else:
                                next_line = f.readline()
                                trans_match = re.search(r'(\d+\.\d+)\s+r[pf]', next_line)
                                if trans_match:
                                    clock_data[current_clock]['min_transition'] = float(trans_match.group(1))
            
            return {'clocks': clock_data} if clock_data else None
            
        except Exception as e:
            # Silently fail if file cannot be parsed
            return None
    
    def _parse_clock_slopes_and_xcap(self, slopes_xcap_file: str) -> Dict[str, Any]:
        """Parse clock_slopes_and_xcap report to extract violation counts
        
        Extracts:
          - Clock slew violation count
          - Clock X-cap (crosstalk capacitance) violation count
        
        Args:
            slopes_xcap_file: Path to *.clock_slopes_and_xcap report file
        
        Returns:
            Dictionary with violation counts:
            {
                'slew_violations': 5,
                'xcap_violations': 2
            }
        """
        try:
            if not os.path.exists(slopes_xcap_file):
                return None
            
            with open(slopes_xcap_file, 'r') as f:
                content = f.read()
            
            # Look for summary line: "Summary: clock-slope violations: 0    x-cap violations: 1"
            summary_match = re.search(r'Summary:\s+clock-slope violations:\s+(\d+)\s+x-cap violations:\s+(\d+)', content)
            if summary_match:
                return {
                    'slew_violations': int(summary_match.group(1)),
                    'xcap_violations': int(summary_match.group(2))
                }
            
            return None
            
        except Exception as e:
            # Silently fail if file cannot be parsed
            return None
    
    def _parse_constraint_violations(self, work_dir: str, scenario_name: str, design_name: str) -> Dict[str, int]:
        """Parse constraint violation reports (data + clock violations)
        
        Extracts violation counts from:
          - *.all_violators.tran.gz (max transition violations - DATA signals)
          - *.all_violators.cap.gz (max capacitance violations - DATA signals)
          - *.all_violators.max_fanout.gz (max fanout violations - DATA signals)
          - *.clock_violators.gz (max tran/cap violations - CLOCK signals)
        
        Args:
            work_dir: Path to PT work directory
            scenario_name: Scenario name (e.g., 'func.std_tt_125c_0p6v.setup.typical')
            design_name: Design name (e.g., 'prt')
        
        Returns:
            Dictionary with violation counts:
            {
                'data_max_transition': 37,
                'data_max_capacitance': 12,
                'data_max_fanout': 6,
                'clock_max_transition': 0,
                'clock_max_capacitance': 0
            }
        """
        import gzip
        
        violations = {
            'data_max_transition': 0,
            'data_max_capacitance': 0,
            'data_max_fanout': 0,
            'clock_max_transition': 0,
            'clock_max_capacitance': 0
        }
        
        try:
            # Path to reports directory
            reports_dir = os.path.join(work_dir, scenario_name, "reports/timing_reports")
            
            if not os.path.exists(reports_dir):
                return violations
            
            # Parse DATA signal violations (all_violators.*gz files)
            tran_file = os.path.join(reports_dir, f"{design_name}_{scenario_name}.all_violators.tran.gz")
            if os.path.exists(tran_file):
                try:
                    with gzip.open(tran_file, 'rt', encoding='utf-8', errors='ignore') as f:
                        violations['data_max_transition'] = sum(1 for line in f if '(VIOLATED)' in line)
                except Exception as e:
                    pass
            
            cap_file = os.path.join(reports_dir, f"{design_name}_{scenario_name}.all_violators.cap.gz")
            if os.path.exists(cap_file):
                try:
                    with gzip.open(cap_file, 'rt', encoding='utf-8', errors='ignore') as f:
                        violations['data_max_capacitance'] = sum(1 for line in f if '(VIOLATED)' in line)
                except Exception as e:
                    pass
            
            fanout_file = os.path.join(reports_dir, f"{design_name}_{scenario_name}.all_violators.max_fanout.gz")
            if os.path.exists(fanout_file):
                try:
                    with gzip.open(fanout_file, 'rt', encoding='utf-8', errors='ignore') as f:
                        violations['data_max_fanout'] = sum(1 for line in f if '(VIOLATED)' in line)
                except Exception as e:
                    pass
            
            # Parse CLOCK signal violations (clock_violators.gz file)
            clock_viol_file = os.path.join(reports_dir, f"{design_name}_{scenario_name}.clock_violators.gz")
            if os.path.exists(clock_viol_file):
                try:
                    with gzip.open(clock_viol_file, 'rt', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                        # Count violations in max_transition section
                        tran_section = re.search(r'max_transition.*?(?=max_capacitance|$)', content, re.DOTALL)
                        if tran_section:
                            violations['clock_max_transition'] = content.count('(VIOLATED)')
                        # Count violations in max_capacitance section
                        cap_section = re.search(r'max_capacitance.*?(?=max_transition|$)', content, re.DOTALL)
                        if cap_section:
                            violations['clock_max_capacitance'] = cap_section.group(0).count('(VIOLATED)')
                except Exception as e:
                    pass
            
            return violations
            
        except Exception as e:
            # Silently fail if parsing fails
            return violations

    def _load_pt_waiver_patterns(self, csv_path: Optional[str] = None) -> Dict[str, Any]:
        """Load PT waiver CSV and compile RegExp patterns
        
        Args:
            csv_path: Optional path to central pt.csv (defaults to standard path)
            
        Returns:
            Dictionary with waiver metadata and compiled regex list:
            {
                'path': '/abs/path/to/pt.csv',
                'loaded': True/False,
                'regexes': [compiled regexes],
                'error': 'error string if any'
            }
        """
        if not csv_path:
            csv_path = "/home/nbu_be_tools/beflow/1.0/site/bytool/synopsys/pt/pt.csv"
        
        csv_path = os.path.abspath(csv_path)
        
        # Use cached patterns if already loaded
        cached = getattr(self, "_pt_waiver_cache", None)
        if cached and cached.get("path") == csv_path:
            return cached
        
        waiver_data = {
            "path": csv_path,
            "loaded": False,
            "regexes": [],
            "error": None
        }
        
        try:
            if not os.path.exists(csv_path):
                waiver_data["error"] = f"pt.csv not found: {csv_path}"
                self._pt_waiver_cache = waiver_data
                return waiver_data
            
            with open(csv_path, "r", encoding="utf-8", errors="ignore") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    pattern = row.get("RegExp") or row.get("Regexp") or row.get("REGEXP")
                    if not pattern:
                        continue
                    try:
                        waiver_data["regexes"].append(re.compile(pattern))
                    except re.error:
                        # Skip invalid regex entries
                        continue
            
            if waiver_data["regexes"]:
                waiver_data["loaded"] = True
            else:
                waiver_data["error"] = "no valid RegExp entries found in pt.csv"
        except Exception as e:
            waiver_data["error"] = str(e)
        
        self._pt_waiver_cache = waiver_data
        return waiver_data
    
    def _parse_auto_pt_slave_err(self, err_file: str) -> List[Dict[str, Any]]:
        """Parse auto_pt_slave.err for error/warning entries
        
        Args:
            err_file: Path to auto_pt_slave.err
            
        Returns:
            List of entries with line number and message
        """
        entries = []
        try:
            if not os.path.exists(err_file):
                return entries
            
            with open(err_file, "r", encoding="utf-8", errors="ignore") as f:
                lines = f.readlines()
            
            current_line = None
            current_msg = None
            current_type = None
            
            for raw_line in lines:
                line = raw_line.rstrip("\n")
                
                if line.startswith("line:"):
                    current_line = line.split(":", 1)[1].strip()
                    continue
                
                msg_match = re.match(r'^(Error:|Warning:|-E-|-W-)\s*(.*)', line)
                if msg_match:
                    prefix = msg_match.group(1)
                    rest = msg_match.group(2).strip()
                    current_msg = f"{prefix} {rest}".strip()
                    current_type = "Error" if prefix in ("Error:", "-E-") else "Warning"
                    continue
                
                if line.startswith("-" * 5):
                    if current_msg:
                        entries.append({
                            "line": current_line,
                            "message": current_msg,
                            "type": current_type
                        })
                    current_line = None
                    current_msg = None
                    current_type = None
            
            if current_msg:
                entries.append({
                    "line": current_line,
                    "message": current_msg,
                    "type": current_type
                })
        
        except Exception:
            return []
        
        return entries
    
    def _summarize_unwaived_messages(self, entries: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Summarize unwaived messages by count with sample line numbers"""
        summary_map = {}
        for entry in entries:
            message = entry.get("message", "").strip()
            if not message:
                continue
            data = summary_map.setdefault(message, {"message": message, "count": 0, "line_numbers": []})
            data["count"] += 1
            line_number = entry.get("line")
            if line_number and len(data["line_numbers"]) < 5:
                data["line_numbers"].append(line_number)
        
        summary = list(summary_map.values())
        summary.sort(key=lambda x: x["count"], reverse=True)
        return summary
    
    def _summarize_pt_waiver_gaps(self, work_data: Dict[str, Any]) -> Dict[str, Any]:
        """Summarize PT waiver gaps for a work directory"""
        summary = {
            "csv_path": None,
            "csv_loaded": True,
            "csv_error": None,
            "unwaived_total": 0,
            "unwaived_unique": 0,
            "by_scenario": {}
        }
        
        if not work_data:
            return summary
        
        csv_info = work_data.get("waiver_csv", {})
        summary["csv_path"] = csv_info.get("path")
        summary["csv_loaded"] = csv_info.get("loaded", True)
        summary["csv_error"] = csv_info.get("error")
        
        for scenario in ("setup", "hold"):
            gaps = work_data.get("scenarios", {}).get(scenario, {}).get("waiver_gaps")
            if not gaps:
                continue
            summary["by_scenario"][scenario] = gaps
            if gaps.get("csv_loaded", True):
                summary["unwaived_total"] += gaps.get("unwaived_total", 0)
                summary["unwaived_unique"] += gaps.get("unwaived_unique", 0)
        
        return summary
    
    def _extract_timing_data_from_work_areas(self, pt_locations: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Extract timing data from all auto_pt work areas with dual-scenario support
        
        Args:
            pt_locations: List of PT location dictionaries from _find_all_auto_pt_locations()
                         If None, falls back to old behavior (root signoff_flow only)
        
        Returns:
            Dictionary containing timing data from all work areas across all locations
        """
        
        # If no locations provided, fall back to old behavior (root signoff_flow only)
        if pt_locations is None:
            work_pattern = os.path.join(self.workarea, "signoff_flow/auto_pt/work_*")
            all_items = glob.glob(work_pattern)
            work_dirs = [item for item in all_items if os.path.isdir(item)]
            work_dirs = sorted(work_dirs, key=os.path.getmtime, reverse=True)
            pt_locations = [{
                'path': os.path.join(self.workarea, "signoff_flow/auto_pt"),
                'label': 'signoff_flow',
                'work_dirs': work_dirs
            }]
        
        # Collect all work directories from all locations
        all_work_dirs = []
        for loc in pt_locations:
            for work_dir in loc['work_dirs']:
                # Add location context to each work directory
                all_work_dirs.append({
                    'path': work_dir,
                    'location_label': loc['label'],
                    'location_path': loc['path']
                })
        
        # Sort by modification time (newest first)
        all_work_dirs = sorted(all_work_dirs, key=lambda x: os.path.getmtime(x['path']), reverse=True)
        
        timing_data = []
        
        # Load PT waiver patterns once (central pt.csv)
        waiver_data = self._load_pt_waiver_patterns()
        waiver_regexes = waiver_data.get("regexes", [])
        waiver_csv_meta = {
            "path": waiver_data.get("path"),
            "loaded": waiver_data.get("loaded", False),
            "error": waiver_data.get("error")
        }
        
        # Check if power corner exists in any work area before processing
        # Power corner: func.std_tt_105c_0p67v.setup.typical (used for power estimation only)
        has_power_corner = False
        for work_info in all_work_dirs:
            work_dir_path = work_info['path']
            power_corner_dir = os.path.join(work_dir_path, 'func.std_tt_105c_0p67v.setup.typical')
            if os.path.isdir(power_corner_dir):
                has_power_corner = True
                break
        
        # Define scenarios to extract: setup, hold, and power (if available)
        # Dynamically discover ALL available corners for setup/hold (all corner types)
        # Power corner (func.std_tt_105c_0p67v.setup.typical) is extracted separately
        scenarios_to_check = {
            'setup': [],  # Will be populated dynamically per work directory
            'hold': [],   # Will be populated dynamically per work directory
            'power': []   # Power corner for power estimation (not timing) - only if has_power_corner
        }
        
        for work_info in all_work_dirs:
            work_dir = work_info['path']
            location_label = work_info['location_label']
            
            # OPTIMIZATION: Use single os.listdir() instead of multiple glob() calls (3-5x faster)
            # Dynamically discover ALL setup/hold corners and HTML files in one pass
            try:
                work_dir_items = os.listdir(work_dir)
            except (OSError, PermissionError):
                # Skip if directory is inaccessible
                continue
            
            # Filter for setup corners (ALL func.std_*.setup.* directories)
            # EXCLUDE func.std_tt_105c_0p67v.setup.typical (used for power analysis only)
            setup_dirs = [item for item in work_dir_items 
                         if item.startswith('func.std_') and '.setup.' in item 
                         and item != 'func.std_tt_105c_0p67v.setup.typical'
                         and os.path.isdir(os.path.join(work_dir, item))]
            scenarios_to_check['setup'] = setup_dirs
            
            # Filter for hold corners (ALL func.std_*.hold.* directories)
            hold_dirs = [item for item in work_dir_items 
                        if item.startswith('func.std_') and '.hold.' in item 
                        and os.path.isdir(os.path.join(work_dir, item))]
            scenarios_to_check['hold'] = hold_dirs
            
            # Filter for power corner (func.std_tt_105c_0p67v.setup.typical only)
            # This corner is used for power estimation, not timing analysis
            # Only check if we know the power corner exists in at least one work area
            if has_power_corner:
                power_corner = [item for item in work_dir_items 
                               if item == 'func.std_tt_105c_0p67v.setup.typical'
                               and os.path.isdir(os.path.join(work_dir, item))]
                scenarios_to_check['power'] = power_corner
            else:
                scenarios_to_check['power'] = []
            
            # Extract work directory name (e.g., work_16.09.25_19:53)
            work_name = os.path.basename(work_dir)
            
            # Look for HTML reports in this work directory and parent auto_pt directory
            html_reports = []
            
            # Filter for HTML files in single pass (no glob needed)
            html_files = [item for item in work_dir_items if item.endswith('.html')]
            for html_file in sorted(html_files, key=lambda x: os.path.getmtime(os.path.join(work_dir, x)), reverse=True):
                # Convert to absolute path for HTML links to work from any location
                html_reports.append(os.path.abspath(os.path.join(work_dir, html_file)))
            
            # Check parent auto_pt directory for work-specific HTML files
            auto_pt_dir = os.path.dirname(work_dir)
            parent_html_patterns = [
                os.path.join(auto_pt_dir, f"PT_{work_name}.html"),
                os.path.join(auto_pt_dir, f"{work_name}.html")
            ]
            for parent_html_pattern in parent_html_patterns:
                if os.path.exists(parent_html_pattern):
                    # Convert to absolute path for HTML links to work from any location
                    html_reports.append(os.path.abspath(parent_html_pattern))
                    break
            
            # Initialize work data structure (add location_label for IPO context)
            work_data = {
                "work_dir": work_name,
                "location_label": location_label,  # NEW: IPO/location context
                "scenarios": {},
                "html_reports": html_reports,
                "dsr_skew_setup": None,  # DSR skew for setup scenario
                "dsr_skew_hold": None,   # DSR skew for hold scenario
                "power_data": None,      # Power corner data (func.std_tt_105c_0p67v.setup.typical)
                "runtime": None,
                "waiver_csv": waiver_csv_meta
            }
            
            # Extract data for each scenario type (setup, hold, and power)
            for scenario_type, scenario_list in scenarios_to_check.items():
                # Power corner is handled separately (extract power metrics, not timing)
                if scenario_type == 'power':
                    if scenario_list:
                        power_corner_name = scenario_list[0]  # func.std_tt_105c_0p67v.setup.typical
                        
                        # Path: $WA/signoff_flow/auto_pt/work_*/func.std_tt_105c_0p67v.setup.typical/reports/power_reports/func.std_tt_105c_0p67v.setup.typical/power_analysis/<fsdb test>/<unit>_rollup.rep
                        power_reports_base = os.path.join(work_dir, power_corner_name, "reports/power_reports", power_corner_name, "power_analysis")
                        
                        if os.path.exists(power_reports_base):
                            try:
                                # Find all FSDB test directories (skip symlinks like "latest")
                                fsdb_tests = []
                                for item in os.listdir(power_reports_base):
                                    fsdb_test_dir = os.path.join(power_reports_base, item)
                                    # Skip symlinks to avoid duplicates
                                    if os.path.isdir(fsdb_test_dir) and not os.path.islink(fsdb_test_dir):
                                        # Look for rollup files: <unit>_rollup.rep or <unit>_rollup.csv
                                        rollup_rep = os.path.join(fsdb_test_dir, f"{self.design_info.top_hier}_rollup.rep")
                                        rollup_csv = os.path.join(fsdb_test_dir, f"{self.design_info.top_hier}_rollup.csv")
                                        
                                        rollup_file = None
                                        if os.path.exists(rollup_rep):
                                            rollup_file = rollup_rep
                                        elif os.path.exists(rollup_csv):
                                            rollup_file = rollup_csv
                                        
                                        if rollup_file:
                                            # Extract power metrics from rollup file
                                            power_metrics = self._parse_power_rollup_file(rollup_file)
                                            if power_metrics:
                                                fsdb_tests.append({
                                                    "test_name": item,
                                                    "rollup_file": rollup_file,
                                                    "metrics": power_metrics
                                                })
                                
                                if fsdb_tests:
                                    work_data["power_data"] = {
                                        "corner": power_corner_name,
                                        "fsdb_tests": fsdb_tests,
                                        "total_tests": len(fsdb_tests),
                                        "note": "Power estimation corner (not timing-critical)"
                                    }
                            except Exception as e:
                                pass
                    continue  # Skip the worst-case selection for power
                
                # For setup/hold: select worst-case corner based on TNS
                best_scenario = None
                best_tns = 0  # Track worst (most negative) TNS to find the scenario with most violations
                
                for scenario in scenario_list:
                    timing_file = os.path.join(work_dir, f"{scenario}/reports/timing_reports/{self.design_info.top_hier}_{scenario}.timing")
                    
                    if os.path.exists(timing_file):
                        try:
                            with open(timing_file, 'r') as f:
                                content = f.read()
                            
                            # Find all Group lines - improved regex to handle asterisks properly
                            groups_data = {}
                            group_pattern = r'Group:\s*\*\*([^*]+)\*\*\s*\|\s*NVP:\s*(\d+)\s*\|\s*WNS:\s*([-\d.]+)\s*\|\s*TNS:\s*([-\d.]+)|Group:\s*([^|]+)\s*\|\s*NVP:\s*(\d+)\s*\|\s*WNS:\s*([-\d.]+)\s*\|\s*TNS:\s*([-\d.]+)'
                            matches = re.findall(group_pattern, content)
                            
                            total_tns = 0
                            all_wns_values = []
                            total_nvp = 0
                            
                            for match in matches:
                                if match[0]:  # Group with asterisks
                                    group_name = match[0].strip()
                                    nvp = int(match[1])
                                    wns = float(match[2])
                                    tns = float(match[3])
                                else:  # Regular group
                                    group_name = match[4].strip()
                                    nvp = int(match[5])
                                    wns = float(match[6])
                                    tns = float(match[7])
                                
                                groups_data[group_name] = {
                                    "NVP": nvp,
                                    "WNS": wns,
                                    "TNS": tns
                                }
                                total_tns += tns
                                total_nvp += nvp
                                all_wns_values.append(wns)
                            
                            # Add 'total' group with aggregated values
                            if groups_data:
                                worst_wns = min(all_wns_values) if all_wns_values else 0.0
                                groups_data['total'] = {
                                    "WNS": worst_wns,
                                    "TNS": total_tns,
                                    "NVP": total_nvp
                                }
                            
                            # If this scenario has worse TNS (or is the first found), use it
                            if best_scenario is None or total_tns < best_tns:
                                best_scenario = scenario
                                best_tns = total_tns
                                work_data["scenarios"][scenario_type] = {
                                    "name": scenario,
                                    "groups": groups_data,
                                    "total_tns": total_tns
                                }
                        except Exception as e:
                            # Silently skip files with errors
                            pass
                
                # Parse auto_pt_slave.err for the selected scenario and compare to central pt.csv
                if scenario_type in ("setup", "hold") and scenario_type in work_data["scenarios"]:
                    scenario_name = work_data["scenarios"][scenario_type]["name"]
                    err_file = os.path.join(work_dir, scenario_name, "auto_pt_slave.err")
                    err_entries = self._parse_auto_pt_slave_err(err_file)
                    if err_entries:
                        if waiver_regexes:
                            unwaived_entries = []
                            for entry in err_entries:
                                message = entry.get("message", "")
                                matched = False
                                for regex in waiver_regexes:
                                    if regex.search(message):
                                        matched = True
                                        break
                                if not matched:
                                    unwaived_entries.append(entry)
                            
                            unwaived_summary = self._summarize_unwaived_messages(unwaived_entries)
                            work_data["scenarios"][scenario_type]["waiver_gaps"] = {
                                "err_file": os.path.abspath(err_file),
                                "total_entries": len(err_entries),
                                "unwaived_total": len(unwaived_entries),
                                "unwaived_unique": len(unwaived_summary),
                                "unwaived_entries": unwaived_entries,
                                "unwaived_summary": unwaived_summary,
                                "csv_loaded": waiver_csv_meta["loaded"],
                                "csv_path": waiver_csv_meta["path"],
                                "csv_error": waiver_csv_meta.get("error")
                            }
                        else:
                            # CSV unavailable or had no valid patterns - mark as skipped
                            work_data["scenarios"][scenario_type]["waiver_gaps"] = {
                                "err_file": os.path.abspath(err_file),
                                "total_entries": len(err_entries),
                                "unwaived_total": 0,
                                "unwaived_unique": 0,
                                "unwaived_entries": [],
                                "unwaived_summary": [],
                                "csv_loaded": False,
                                "csv_path": waiver_csv_meta["path"],
                                "csv_error": waiver_csv_meta.get("error") or "pt.csv patterns unavailable"
                            }
            
            # Extract DSR mux_clock_skew from both setup and hold scenarios
            # Each scenario has its own DSR skew file with different corner-specific values
            
            # Setup scenario DSR skew
            if "setup" in work_data["scenarios"]:
                setup_scenario = work_data["scenarios"]["setup"]["name"]
                dsr_pattern = os.path.join(work_dir, f"{setup_scenario}/reports/timing_reports/*.dsr_mux_clock_skew")
                dsr_files = glob.glob(dsr_pattern)
                
                if dsr_files:
                    try:
                        # Store the DSR source file path for Report Navigator
                        work_data["scenarios"]["setup"]["dsr_source_file"] = os.path.abspath(dsr_files[0])
                        
                        with open(dsr_files[0], 'r') as f:
                            dsr_content = f.read()
                        
                        # Extract clock skew values - look for "I0-I1 skew is:" pattern
                        # Format: "I0-I1 skew is: 0.006949" or "5.1e-05" (in nanoseconds)
                        # Need to handle scientific notation (e.g., 5.1e-05, 1.2E-04)
                        skew_pattern = r'I0-I1\s+skew\s+is:\s*([-\d.eE+-]+)'
                        skew_matches = re.findall(skew_pattern, dsr_content, re.IGNORECASE)
                        
                        if skew_matches:
                            # Convert from nanoseconds to picoseconds (multiply by 1000)
                            work_data["dsr_skew_setup"] = float(skew_matches[0]) * 1000.0
                    
                    except Exception as e:
                        # Silently skip DSR extraction errors
                        pass
            
            # Hold scenario DSR skew
            if "hold" in work_data["scenarios"]:
                hold_scenario = work_data["scenarios"]["hold"]["name"]
                dsr_pattern = os.path.join(work_dir, f"{hold_scenario}/reports/timing_reports/*.dsr_mux_clock_skew")
                dsr_files = glob.glob(dsr_pattern)
                
                if dsr_files:
                    try:
                        # Store the DSR source file path for Report Navigator
                        work_data["scenarios"]["hold"]["dsr_source_file"] = os.path.abspath(dsr_files[0])
                        
                        with open(dsr_files[0], 'r') as f:
                            dsr_content = f.read()
                        
                        # Extract clock skew values - look for "I0-I1 skew is:" pattern  
                        # Format: "I0-I1 skew is: 0.006949" or "5.1e-05" (in nanoseconds)
                        # Need to handle scientific notation (e.g., 5.1e-05, 1.2E-04)
                        skew_pattern = r'I0-I1\s+skew\s+is:\s*([-\d.eE+-]+)'
                        skew_matches = re.findall(skew_pattern, dsr_content, re.IGNORECASE)
                        
                        if skew_matches:
                            # Convert from nanoseconds to picoseconds (multiply by 1000)
                            work_data["dsr_skew_hold"] = float(skew_matches[0]) * 1000.0
                    
                    except Exception as e:
                        # Silently skip DSR extraction errors
                        pass
            
            # NEW Phase B.1: Extract clock quality metrics from setup and hold scenarios
            # Clock quality includes: skew, transitions (slew), and X-cap violations
            clock_quality_data = {}
            
            for scenario_type in ['setup', 'hold']:
                if scenario_type in work_data["scenarios"]:
                    scenario_name = work_data["scenarios"][scenario_type]["name"]
                    timing_reports_dir = os.path.join(work_dir, f"{scenario_name}/reports/timing_reports")
                    
                    # Initialize clock quality data for this scenario
                    clock_quality_data[scenario_type] = {}
                    
                    # Extract clock timing metrics (skew, transitions)
                    clock_timing_pattern = f"{self.design_info.top_hier}_{scenario_name}.clock_timing"
                    clock_timing_file = os.path.join(timing_reports_dir, clock_timing_pattern)
                    
                    if os.path.exists(clock_timing_file):
                        clock_timing_data = self._parse_clock_timing_report(clock_timing_file)
                        if clock_timing_data:
                            clock_quality_data[scenario_type]['clock_timing'] = clock_timing_data
                    
                    # Extract clock slopes and X-cap violations
                    slopes_xcap_pattern = f"{self.design_info.top_hier}_{scenario_name}.clock_slopes_and_xcap"
                    slopes_xcap_file = os.path.join(timing_reports_dir, slopes_xcap_pattern)
                    
                    if os.path.exists(slopes_xcap_file):
                        slopes_xcap_data = self._parse_clock_slopes_and_xcap(slopes_xcap_file)
                        if slopes_xcap_data:
                            clock_quality_data[scenario_type]['violations'] = slopes_xcap_data
            
            # Add clock quality data to work_data if any was found
            if clock_quality_data:
                work_data['clock_quality'] = clock_quality_data
            
            # NEW: Extract constraint violations (data + clock) for setup scenario only (main setup corner)
            if 'setup' in work_data["scenarios"]:
                scenario_name = work_data["scenarios"]['setup']["name"]
                violations = self._parse_constraint_violations(work_dir, scenario_name, self.design_info.top_hier)
                # Add detailed violations to setup scenario data
                work_data["scenarios"]['setup']['data_max_transition'] = violations['data_max_transition']
                work_data["scenarios"]['setup']['data_max_capacitance'] = violations['data_max_capacitance']
                work_data["scenarios"]['setup']['data_max_fanout'] = violations['data_max_fanout']
                work_data["scenarios"]['setup']['clock_max_transition'] = violations['clock_max_transition']
                work_data["scenarios"]['setup']['clock_max_capacitance'] = violations['clock_max_capacitance']
            
            # Extract PT runtime from log file
            log_file = os.path.join(work_dir, "*.log")
            log_files = glob.glob(log_file)
            if log_files:
                try:
                    with open(log_files[0], 'r') as f:
                        log_content = f.read()
                    
                    # Look for "Elapsed time for this session" pattern
                    runtime_pattern = r'Elapsed time for this session:\s*([\d.]+)\s*hours?'
                    runtime_matches = re.findall(runtime_pattern, log_content, re.IGNORECASE)
                    
                    if runtime_matches:
                        work_data["runtime"] = float(runtime_matches[0])
                
                except Exception as e:
                    # Silently skip runtime extraction errors
                    pass
            
            # Only add work data if at least one scenario was found
            if work_data["scenarios"]:
                timing_data.append(work_data)
        
        return timing_data
    
    def _get_pt_html_common_css(self) -> str:
        """Generate common CSS styles for PT HTML reports
        
        Returns:
            CSS string with all common styles for PT reports
        """
        return """
        /* ==================== Global Styles ==================== */
        :root {
            --primary-color: #667eea;
            --secondary-color: #764ba2;
            --success-color: #27ae60;
            --warning-color: #f39c12;
            --danger-color: #e74c3c;
            --info-color: #3498db;
            --gray-light: #ecf0f1;
            --gray-medium: #bdc3c7;
            --gray-dark: #34495e;
            --text-dark: #2c3e50;
            --shadow-sm: 0 2px 4px rgba(0,0,0,0.05);
            --shadow-md: 0 2px 10px rgba(0,0,0,0.1);
            --shadow-lg: 0 8px 16px rgba(0,0,0,0.15);
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 20px;
            background-color: #f5f5f5;
            color: var(--text-dark);
            line-height: 1.6;
        }
        
        .container {
            max-width: 95%;
            margin: 0 auto;
            background-color: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: var(--shadow-md);
        }
        
        /* ==================== Header Styles ==================== */
        .header {
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            padding: 30px;
            display: grid;
            grid-template-columns: auto 1fr;
            gap: 20px;
            align-items: center;
            border-radius: 15px 15px 0 0;
            margin: -20px -20px 20px -20px;
        }
        
        .logo {
            width: 80px;
            height: 80px;
            border-radius: 10px;
            background: white;
            padding: 10px;
            cursor: pointer;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }
        
        .logo:hover {
            transform: scale(1.05);
            box-shadow: var(--shadow-lg);
        }
        
        .header-text h1 {
            font-size: 28px;
            margin: 0 0 8px 0;
            color: white;
            border: none;
        }
        
        .header-text p {
            opacity: 0.9;
            font-size: 14px;
            margin: 0;
        }
        
        /* ==================== Section Styles ==================== */
        .section {
            background-color: white;
            padding: 25px;
            border-radius: 8px;
            margin-bottom: 25px;
            box-shadow: var(--shadow-sm);
            border-left: 4px solid var(--primary-color);
        }
        
        .section-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--gray-light);
            cursor: pointer;
            user-select: none;
        }
        
        .section-header:hover {
            background-color: var(--gray-light);
            padding: 10px;
            margin: -10px -10px 10px -10px;
            border-radius: 5px;
        }
        
        .section-title {
            font-size: 24px;
            color: var(--text-dark);
            margin: 0;
            font-weight: 600;
        }
        
        .section-toggle {
            font-size: 18px;
            color: var(--gray-dark);
            transition: transform 0.3s ease;
        }
        
        .section-toggle.collapsed {
            transform: rotate(-90deg);
        }
        
        .section-content {
            max-height: 10000px;
            overflow: hidden;
            transition: max-height 0.3s ease;
        }
        
        .section-content.collapsed {
            max-height: 0;
        }
        
        /* ==================== Card Styles ==================== */
        .cards-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: var(--shadow-md);
            transition: transform 0.2s ease, box-shadow 0.2s ease;
        }
        
        .card:hover {
            transform: translateY(-5px);
            box-shadow: var(--shadow-lg);
        }
        
        .card-title {
            font-size: 14px;
            font-weight: 500;
            opacity: 0.9;
            margin: 0 0 10px 0;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }
        
        .card-value {
            font-size: 32px;
            font-weight: bold;
            margin: 0;
        }
        
        .card-subtitle {
            font-size: 12px;
            opacity: 0.8;
            margin: 5px 0 0 0;
        }
        
        .card.success {
            background: linear-gradient(135deg, #27ae60 0%, #2ecc71 100%);
        }
        
        .card.warning {
            background: linear-gradient(135deg, #f39c12 0%, #f1c40f 100%);
        }
        
        .card.danger {
            background: linear-gradient(135deg, #e74c3c 0%, #c0392b 100%);
        }
        
        .card.info {
            background: linear-gradient(135deg, #3498db 0%, #2980b9 100%);
        }
        
        /* ==================== Status Badge Styles ==================== */
        .badge {
            display: inline-block;
            padding: 5px 12px;
            border-radius: 20px;
            font-size: 12px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            margin: 0 5px;
        }
        
        .badge.completed {
            background-color: var(--success-color);
            color: white;
        }
        
        .badge.running {
            background-color: var(--warning-color);
            color: white;
            animation: pulse 2s infinite;
        }
        
        .badge.stale {
            background-color: var(--danger-color);
            color: white;
        }
        
        .badge.no-work {
            background-color: var(--gray-medium);
            color: white;
        }
        
        .badge.ipo {
            background-color: var(--info-color);
            color: white;
        }
        
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.7; }
        }
        
        /* ==================== Table Styles ==================== */
        .table-wrapper {
            overflow-x: auto;
            max-width: 100%;
            margin: 20px 0;
            border: 1px solid var(--gray-medium);
            border-radius: 8px;
            box-shadow: var(--shadow-sm);
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 0;
            font-size: 14px;
        }
        
        th, td {
            border: 1px solid var(--gray-medium);
            padding: 12px;
            text-align: left;
        }
        
        th {
            background-color: var(--gray-dark);
            color: white;
            font-weight: bold;
            position: sticky;
            top: 0;
            z-index: 10;
            cursor: pointer;
            user-select: none;
        }
        
        th:hover {
            background-color: #2c3e50;
        }
        
        th.sortable::after {
            content: ' â‡…';
            opacity: 0.5;
        }
        
        th.sorted-asc::after {
            content: ' â–²';
            opacity: 1;
        }
        
        th.sorted-desc::after {
            content: ' â–¼';
            opacity: 1;
        }
        
        tr:nth-child(even) {
            background-color: #f9f9f9;
        }
        
        tr:hover {
            background-color: var(--gray-light);
        }
        
        td.center {
            text-align: center;
        }
        
        td.right {
            text-align: right;
        }
        
        /* ==================== Color Coding for Values ==================== */
        .value-green {
            color: var(--success-color);
            font-weight: 600;
        }
        
        .value-yellow {
            color: var(--warning-color);
            font-weight: 600;
        }
        
        .value-red {
            color: var(--danger-color);
            font-weight: 600;
        }
        
        .bg-green {
            background-color: #d4edda !important;
        }
        
        .bg-yellow {
            background-color: #fff3cd !important;
        }
        
        .bg-red {
            background-color: #f8d7da !important;
        }
        
        /* ==================== Button Styles ==================== */
        .btn {
            display: inline-block;
            padding: 8px 16px;
            border: none;
            border-radius: 5px;
            font-size: 14px;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s ease;
            text-decoration: none;
            color: white;
            margin: 5px;
        }
        
        .btn:hover {
            transform: translateY(-2px);
            box-shadow: var(--shadow-md);
        }
        
        .btn:active {
            transform: translateY(0);
        }
        
        .btn-primary {
            background-color: var(--primary-color);
        }
        
        .btn-primary:hover {
            background-color: #5a67d8;
        }
        
        .btn-success {
            background-color: var(--success-color);
        }
        
        .btn-warning {
            background-color: var(--warning-color);
        }
        
        .btn-danger {
            background-color: var(--danger-color);
        }
        
        .btn-sm {
            padding: 5px 10px;
            font-size: 12px;
        }
        
        /* ==================== Chart Container ==================== */
        .chart-container {
            position: relative;
            height: 400px;
            margin: 20px 0;
            padding: 20px;
            background-color: white;
            border-radius: 8px;
            box-shadow: var(--shadow-sm);
        }
        
        /* ==================== Filter Controls ==================== */
        .filter-controls {
            display: flex;
            gap: 15px;
            flex-wrap: wrap;
            padding: 15px;
            background-color: var(--gray-light);
            border-radius: 8px;
            margin: 15px 0;
        }
        
        .filter-group {
            display: flex;
            flex-direction: column;
            gap: 5px;
        }
        
        .filter-group label {
            font-size: 12px;
            font-weight: 600;
            color: var(--gray-dark);
            text-transform: uppercase;
        }
        
        .filter-group input,
        .filter-group select {
            padding: 8px 12px;
            border: 1px solid var(--gray-medium);
            border-radius: 5px;
            font-size: 14px;
        }
        
        .filter-group input:focus,
        .filter-group select:focus {
            outline: none;
            border-color: var(--primary-color);
            box-shadow: 0 0 0 3px rgba(102, 126, 234, 0.1);
        }
        
        /* ==================== Modal Styles ==================== */
        .modal {
            display: none;
            position: fixed;
            z-index: 9999;
            left: 0;
            top: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0,0,0,0.9);
            justify-content: center;
            align-items: center;
        }
        
        .modal.active {
            display: flex;
        }
        
        .modal-content {
            max-width: 90%;
            max-height: 90%;
            border-radius: 10px;
        }
        
        .modal-close {
            position: absolute;
            top: 20px;
            right: 35px;
            color: #f1f1f1;
            font-size: 40px;
            font-weight: bold;
            cursor: pointer;
            transition: color 0.2s;
        }
        
        .modal-close:hover {
            color: #bbb;
        }
        
        /* ==================== Back to Top Button ==================== */
        .back-to-top {
            position: fixed;
            bottom: 30px;
            right: 30px;
            background-color: var(--primary-color);
            color: white;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            display: none;
            justify-content: center;
            align-items: center;
            cursor: pointer;
            box-shadow: var(--shadow-lg);
            transition: all 0.3s ease;
            z-index: 1000;
            font-size: 24px;
            border: none;
        }
        
        .back-to-top:hover {
            background-color: #5a67d8;
            transform: scale(1.1);
        }
        
        .back-to-top.visible {
            display: flex;
        }
        
        /* ==================== Footer Styles ==================== */
        .footer {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            text-align: center;
            padding: 20px;
            border-radius: 0 0 15px 15px;
            margin: 30px -20px -20px -20px;
        }
        
        .footer p {
            margin: 5px 0;
        }
        
        .footer a {
            color: #ffd700;
            text-decoration: none;
        }
        
        .footer a:hover {
            text-decoration: underline;
        }
        
        /* ==================== Expandable Rows ==================== */
        .expandable-row {
            cursor: pointer;
        }
        
        .expandable-row:hover {
            background-color: var(--gray-light) !important;
        }
        
        .expand-icon {
            transition: transform 0.2s ease;
            display: inline-block;
        }
        
        .expand-icon.expanded {
            transform: rotate(90deg);
        }
        
        .details-row {
            display: none;
        }
        
        .details-row.visible {
            display: table-row;
        }
        
        .details-cell {
            background-color: #f9f9f9;
            padding: 15px;
        }
        
        /* ==================== Responsive Design ==================== */
        @media (max-width: 768px) {
            .container {
                padding: 10px;
            }
            
            .header {
                grid-template-columns: 1fr;
                text-align: center;
                padding: 20px;
            }
            
            .logo {
                margin: 0 auto;
            }
            
            .cards-grid {
                grid-template-columns: 1fr;
            }
            
            .filter-controls {
                flex-direction: column;
            }
            
            table {
                font-size: 12px;
            }
            
            th, td {
                padding: 8px;
            }
        }
        
        /* ==================== Print Styles ==================== */
        @media print {
            body {
                background-color: white;
                margin: 0;
            }
            
            .container {
                box-shadow: none;
                max-width: 100%;
            }
            
            .header {
                border-radius: 0;
                margin: 0;
            }
            
            .back-to-top,
            .btn,
            .filter-controls {
                display: none !important;
            }
            
            .section {
                page-break-inside: avoid;
            }
            
            .chart-container {
                page-break-inside: avoid;
            }
            
            table {
                page-break-inside: auto;
            }
            
            tr {
                page-break-inside: avoid;
                page-break-after: auto;
            }
        }
        """
    
    def _get_pt_html_common_js(self) -> str:
        """Generate common JavaScript for PT HTML reports
        
        Returns:
            JavaScript string with all common functions for PT reports
        """
        return """
        // ==================== Tablog Server Integration ====================
        function checkTablogServer() {
            return fetch('http://localhost:8888/status', {
                method: 'GET',
                mode: 'no-cors'
            }).then(() => true).catch(() => false);
        }
        
        function openLogWithServer(logfile, event) {
            if (event) event.preventDefault();
            
            checkTablogServer().then(serverAvailable => {
                if (serverAvailable) {
                    fetch(`http://localhost:8888/open_log?file=${encodeURIComponent(logfile)}`, {
                        method: 'GET',
                        mode: 'no-cors'
                    }).then(() => {
                        showToast('Opening log in tablog...', 'success');
                    }).catch(() => {
                        copyToClipboard(logfile);
                    });
                } else {
                    copyToClipboard(logfile);
                }
            });
        }
        
        function copyToClipboard(text) {
            navigator.clipboard.writeText(text).then(() => {
                showToast('Path copied to clipboard!', 'info');
            }).catch(() => {
                // Fallback for older browsers
                const textarea = document.createElement('textarea');
                textarea.value = text;
                document.body.appendChild(textarea);
                textarea.select();
                document.execCommand('copy');
                document.body.removeChild(textarea);
                showToast('Path copied to clipboard!', 'info');
            });
        }
        
        // ==================== Toast Notifications ====================
        function showToast(message, type = 'info') {
            const toast = document.createElement('div');
            toast.className = `toast toast-${type}`;
            toast.textContent = message;
            toast.style.cssText = `
                position: fixed;
                bottom: 80px;
                right: 30px;
                background-color: ${type === 'success' ? '#27ae60' : type === 'error' ? '#e74c3c' : '#3498db'};
                color: white;
                padding: 15px 20px;
                border-radius: 5px;
                box-shadow: 0 4px 12px rgba(0,0,0,0.15);
                z-index: 10000;
                animation: slideIn 0.3s ease;
            `;
            
            document.body.appendChild(toast);
            
            setTimeout(() => {
                toast.style.animation = 'slideOut 0.3s ease';
                setTimeout(() => toast.remove(), 300);
            }, 3000);
        }
        
        // ==================== Modal Functions ====================
        function showLogoModal() {
            const modal = document.getElementById('logoModal');
            if (modal) modal.classList.add('active');
        }
        
        function hideLogoModal() {
            const modal = document.getElementById('logoModal');
            if (modal) modal.classList.remove('active');
        }
        
        // ==================== Back to Top Button ====================
        function initBackToTop() {
            const button = document.querySelector('.back-to-top');
            if (!button) return;
            
            window.addEventListener('scroll', () => {
                if (window.pageYOffset > 300) {
                    button.classList.add('visible');
                } else {
                    button.classList.remove('visible');
                }
            });
            
            button.addEventListener('click', () => {
                window.scrollTo({
                    top: 0,
                    behavior: 'smooth'
                });
            });
        }
        
        // ==================== Section Collapsible ====================
        function toggleSection(sectionId) {
            const content = document.getElementById(sectionId);
            const toggle = document.getElementById(`toggle-${sectionId}`);
            
            if (content && toggle) {
                content.classList.toggle('collapsed');
                toggle.classList.toggle('collapsed');
                
                // Save state to localStorage
                const isCollapsed = content.classList.contains('collapsed');
                localStorage.setItem(`section-${sectionId}`, isCollapsed ? 'collapsed' : 'expanded');
            }
        }
        
        function initCollapsibleSections() {
            // Restore collapsed states from localStorage
            document.querySelectorAll('.section-content').forEach(section => {
                const sectionId = section.id;
                const savedState = localStorage.getItem(`section-${sectionId}`);
                
                if (savedState === 'collapsed') {
                    section.classList.add('collapsed');
                    const toggle = document.getElementById(`toggle-${sectionId}`);
                    if (toggle) toggle.classList.add('collapsed');
                }
            });
        }
        
        function expandAllSections() {
            document.querySelectorAll('.section-content').forEach(section => {
                section.classList.remove('collapsed');
                const toggle = document.getElementById(`toggle-${section.id}`);
                if (toggle) toggle.classList.remove('collapsed');
                localStorage.setItem(`section-${section.id}`, 'expanded');
            });
        }
        
        function collapseAllSections() {
            document.querySelectorAll('.section-content').forEach(section => {
                section.classList.add('collapsed');
                const toggle = document.getElementById(`toggle-${section.id}`);
                if (toggle) toggle.classList.add('collapsed');
                localStorage.setItem(`section-${section.id}`, 'collapsed');
            });
        }
        
        // ==================== Table Sorting ====================
        function sortTable(tableId, columnIndex, dataType = 'string') {
            const table = document.getElementById(tableId);
            if (!table) return;
            
            const tbody = table.querySelector('tbody');
            const rows = Array.from(tbody.querySelectorAll('tr'));
            const headers = table.querySelectorAll('th');
            const currentHeader = headers[columnIndex];
            
            // Determine sort direction
            let ascending = true;
            if (currentHeader.classList.contains('sorted-asc')) {
                ascending = false;
            }
            
            // Clear all sort indicators
            headers.forEach(h => {
                h.classList.remove('sorted-asc', 'sorted-desc');
            });
            
            // Set new sort indicator
            currentHeader.classList.add(ascending ? 'sorted-asc' : 'sorted-desc');
            
            // Sort rows
            rows.sort((a, b) => {
                let aVal = a.cells[columnIndex].textContent.trim();
                let bVal = b.cells[columnIndex].textContent.trim();
                
                // Handle different data types
                if (dataType === 'number') {
                    aVal = parseFloat(aVal.replace(/[^0-9.-]/g, '')) || 0;
                    bVal = parseFloat(bVal.replace(/[^0-9.-]/g, '')) || 0;
                    return ascending ? aVal - bVal : bVal - aVal;
                } else {
                    return ascending ? 
                        aVal.localeCompare(bVal) : 
                        bVal.localeCompare(aVal);
                }
            });
            
            // Reorder rows
            rows.forEach(row => tbody.appendChild(row));
        }
        
        // ==================== Table Filtering ====================
        function filterTable(tableId, filterValue, columnIndex) {
            const table = document.getElementById(tableId);
            if (!table) return;
            
            const rows = table.querySelectorAll('tbody tr');
            filterValue = filterValue.toLowerCase();
            
            rows.forEach(row => {
                if (!columnIndex && columnIndex !== 0) {
                    // Search all columns
                    const text = row.textContent.toLowerCase();
                    row.style.display = text.includes(filterValue) ? '' : 'none';
                } else {
                    // Search specific column
                    const cell = row.cells[columnIndex];
                    if (cell) {
                        const text = cell.textContent.toLowerCase();
                        row.style.display = text.includes(filterValue) ? '' : 'none';
                    }
                }
            });
            
            // Update visible count
            const visibleRows = Array.from(rows).filter(r => r.style.display !== 'none').length;
            const totalRows = rows.length;
            console.log(`Showing ${visibleRows} of ${totalRows} rows`);
        }
        
        // ==================== Expandable Rows ====================
        function toggleRow(rowId) {
            const detailsRow = document.getElementById(`details-${rowId}`);
            const icon = document.getElementById(`icon-${rowId}`);
            
            if (detailsRow && icon) {
                detailsRow.classList.toggle('visible');
                icon.classList.toggle('expanded');
            }
        }
        
        function toggleReportExpand(expandId) {
            const expandSection = document.getElementById(expandId);
            const button = event.target;
            
            if (expandSection) {
                if (expandSection.style.display === 'none' || expandSection.style.display === '') {
                    expandSection.style.display = 'grid';
                    button.innerHTML = button.innerHTML.replace('â–¼ Show', 'â–² Hide');
                } else {
                    expandSection.style.display = 'none';
                    button.innerHTML = button.innerHTML.replace('â–² Hide', 'â–¼ Show');
                }
            }
        }
        
        // ==================== Export Functions ====================
        function exportTableToCSV(tableId, filename = 'export.csv') {
            const table = document.getElementById(tableId);
            if (!table) return;
            
            const rows = table.querySelectorAll('tr');
            const csv = [];
            
            rows.forEach(row => {
                const cols = row.querySelectorAll('td, th');
                const rowData = Array.from(cols).map(col => {
                    let text = col.textContent.trim();
                    // Escape quotes
                    text = text.replace(/"/g, '""');
                    // Wrap in quotes if contains comma
                    return text.includes(',') ? `"${text}"` : text;
                });
                csv.push(rowData.join(','));
            });
            
            const csvContent = csv.join('\\n');
            const blob = new Blob([csvContent], { type: 'text/csv' });
            const url = window.URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = filename;
            a.click();
            window.URL.revokeObjectURL(url);
            
            showToast('Table exported to CSV!', 'success');
        }
        
        function copyTableToClipboard(tableId) {
            const table = document.getElementById(tableId);
            if (!table) return;
            
            const range = document.createRange();
            range.selectNode(table);
            window.getSelection().removeAllRanges();
            window.getSelection().addRange(range);
            document.execCommand('copy');
            window.getSelection().removeAllRanges();
            
            showToast('Table copied to clipboard!', 'success');
        }
        
        function exportChartToPNG(chartId, filename = 'chart.png') {
            const canvas = document.getElementById(chartId);
            if (!canvas) return;
            
            canvas.toBlob(blob => {
                const url = window.URL.createObjectURL(blob);
                const a = document.createElement('a');
                a.href = url;
                a.download = filename;
                a.click();
                window.URL.revokeObjectURL(url);
                
                showToast('Chart exported to PNG!', 'success');
            });
        }
        
        // ==================== Search Function ====================
        function globalSearch(searchTerm) {
            searchTerm = searchTerm.toLowerCase();
            const sections = document.querySelectorAll('.section');
            let matchCount = 0;
            
            sections.forEach(section => {
                const text = section.textContent.toLowerCase();
                if (text.includes(searchTerm)) {
                    section.style.display = '';
                    // Expand section if collapsed
                    const content = section.querySelector('.section-content');
                    if (content && content.classList.contains('collapsed')) {
                        content.classList.remove('collapsed');
                    }
                    matchCount++;
                } else {
                    section.style.display = searchTerm ? 'none' : '';
                }
            });
            
            if (searchTerm) {
                showToast(`Found ${matchCount} matching section(s)`, 'info');
            }
        }
        
        // ==================== Initialization ====================
        document.addEventListener('DOMContentLoaded', function() {
            initBackToTop();
            initCollapsibleSections();
            
            // Add click handler for logo modal
            const logo = document.querySelector('.logo');
            if (logo) {
                logo.addEventListener('click', showLogoModal);
            }
            
            // Add click handler for modal close
            const modalClose = document.querySelector('.modal-close');
            if (modalClose) {
                modalClose.addEventListener('click', hideLogoModal);
            }
            
            // Close modal on background click
            const modal = document.getElementById('logoModal');
            if (modal) {
                modal.addEventListener('click', function(e) {
                    if (e.target === modal) {
                        hideLogoModal();
                    }
                });
            }
            
            console.log('PT HTML Report initialized successfully');
        });
        
        // Add animation keyframes
        const style = document.createElement('style');
        style.textContent = `
            @keyframes slideIn {
                from { transform: translateX(100%); opacity: 0; }
                to { transform: translateX(0); opacity: 1; }
            }
            @keyframes slideOut {
                from { transform: translateX(0); opacity: 1; }
                to { transform: translateX(100%); opacity: 0; }
            }
        `;
        document.head.appendChild(style);
        """
    
    def _get_pt_html_header(self, title: str, subtitle: str = "") -> str:
        """Generate HTML header with logo and branding
        
        Args:
            title: Main title for the report
            subtitle: Optional subtitle
        
        Returns:
            HTML string for header section
        """
        # Read and encode logo as base64
        logo_data = ""
        logo_path = os.path.join(os.path.dirname(__file__), "assets/images/avice_logo.png")
        if os.path.exists(logo_path):
            with open(logo_path, "rb") as logo_file:
                logo_data = base64.b64encode(logo_file.read()).decode('utf-8')
        
        subtitle_html = f"<p>{subtitle}</p>" if subtitle else ""
        
        return f"""
        <div class="header">
            <img src="data:image/png;base64,{logo_data}" alt="Alon Vice Logo" class="logo" onclick="showLogoModal()">
            <div class="header-text">
                <h1>{title}</h1>
                {subtitle_html}
            </div>
        </div>
        
        <!-- Logo Modal -->
        <div id="logoModal" class="modal">
            <span class="modal-close" onclick="hideLogoModal()">&times;</span>
            <img src="data:image/png;base64,{logo_data}" alt="Alon Vice Logo" class="modal-content">
        </div>
        """
    
    def _get_pt_html_footer(self) -> str:
        """Generate HTML footer with copyright and contact info
        
        Returns:
            HTML string for footer section
        """
        timestamp = datetime.now().strftime("%B %d, %Y at %H:%M:%S")
        
        return f"""
        <div class="footer">
            <p><strong>Auto PT Timing Summary Report</strong></p>
            <p>Copyright &copy; 2025 Alon Vice (avice)</p>
            <p>Contact: <a href="mailto:avice@nvidia.com">avice@nvidia.com</a></p>
            <p>Generated: {timestamp}</p>
        </div>
        
        <!-- Back to Top Button -->
        <button class="back-to-top" onclick="window.scrollTo({{top: 0, behavior: 'smooth'}})">
            â†‘
        </button>
        """
    
    def _get_chartjs_library(self) -> str:
        """Get Chart.js library for visualizations
        
        Returns:
            Script tag with Chart.js CDN link
        """
        return """
        <!-- Chart.js Library -->
        <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.js"></script>
        """
    
    def _prepare_enhanced_timing_data(self, timing_data: List[Dict], pt_locations: List[Dict]) -> Dict[str, Any]:
        """Prepare enhanced timing data structure with all metadata
        
        Args:
            timing_data: List of timing data per work directory
            pt_locations: List of PT location info from _find_all_auto_pt_locations
        
        Returns:
            Enhanced data structure with locations, quality metrics, etc.
        """
        enhanced_data = {
            'timing_data': timing_data,
            'pt_locations': pt_locations if pt_locations else [],
            'location_metadata': {},
            'summary_stats': {},
            'quality_stats': {},
            'dsr_stats': {}
        }
        
        if not timing_data:
            return enhanced_data
        
        # Calculate location metadata
        location_counts = {}
        for loc in pt_locations:
            status = loc.get('status', 'UNKNOWN')
            location_counts[status] = location_counts.get(status, 0) + 1
        
        enhanced_data['location_metadata'] = {
            'total_locations': len(pt_locations),
            'status_counts': location_counts,
            'locations_list': [loc.get('label', loc.get('location', 'Unknown')) for loc in pt_locations]
        }
        
        # Calculate summary stats from latest work directory
        if timing_data:
            latest = timing_data[0]
            setup_data = latest.get('scenarios', {}).get('setup', {})
            hold_data = latest.get('scenarios', {}).get('hold', {})
            
            # Get total WNS/TNS/NVP from 'total' group
            def calc_scenario_totals(scenario_data):
                """Get total WNS/TNS/NVP from the 'total' group"""
                groups = scenario_data.get('groups', {})
                total_group = groups.get('total', {})
                
                if not total_group:
                    return {'wns': 'N/A', 'tns': 'N/A', 'nvp': 'N/A'}
                
                return {
                    'wns': total_group.get('WNS', 'N/A'),
                    'tns': total_group.get('TNS', 'N/A'),
                    'nvp': total_group.get('NVP', 'N/A')
                }
            
            setup_totals = calc_scenario_totals(setup_data)
            hold_totals = calc_scenario_totals(hold_data)
            waiver_summary = self._summarize_pt_waiver_gaps(latest)
            
            enhanced_data['summary_stats'] = {
                'total_work_dirs': len(timing_data),
                'latest_work_dir': latest.get('work_dir', 'N/A'),
                'setup_wns': setup_totals['wns'],
                'setup_tns': setup_totals['tns'],
                'setup_nvp': setup_totals['nvp'],
                'hold_wns': hold_totals['wns'],
                'hold_tns': hold_totals['tns'],
                'hold_nvp': hold_totals['nvp'],
                'pt_waiver_unwaived_total': waiver_summary.get('unwaived_total', 0),
                'pt_waiver_unwaived_unique': waiver_summary.get('unwaived_unique', 0),
                'pt_waiver_csv_loaded': waiver_summary.get('csv_loaded', True),
                'pt_waiver_csv_error': waiver_summary.get('csv_error')
            }
            
            # Calculate quality stats
            data_tran = setup_data.get('data_max_transition', 0)
            data_cap = setup_data.get('data_max_capacitance', 0)
            data_fanout = setup_data.get('data_max_fanout', 0)
            clock_tran = setup_data.get('clock_max_transition', 0)
            clock_cap = setup_data.get('clock_max_capacitance', 0)
            
            clock_quality = latest.get('clock_quality', {}).get('setup', {}).get('violations', {})
            clock_slew = clock_quality.get('slew_violations', 0)
            clock_xcap = clock_quality.get('xcap_violations', 0)
            
            enhanced_data['quality_stats'] = {
                'data_total': data_tran + data_cap + data_fanout,
                'clock_total': clock_tran + clock_cap + clock_slew + clock_xcap,
                'data_tran': data_tran,
                'data_cap': data_cap,
                'data_fanout': data_fanout,
                'clock_tran': clock_tran,
                'clock_cap': clock_cap,
                'clock_slew': clock_slew,
                'clock_xcap': clock_xcap
            }
            
            # Calculate DSR skew stats
            # DSR skew is stored at work_data level: work_data["dsr_skew_setup"] and work_data["dsr_skew_hold"]
            dsr_values = []
            for wd in timing_data:
                setup_dsr = wd.get('dsr_skew_setup', None)
                hold_dsr = wd.get('dsr_skew_hold', None)
                if setup_dsr is not None:
                    dsr_values.append(setup_dsr)
                if hold_dsr is not None:
                    dsr_values.append(hold_dsr)
            
            if dsr_values:
                enhanced_data['dsr_stats'] = {
                    'min': min(dsr_values),
                    'max': max(dsr_values),
                    'avg': sum(dsr_values) / len(dsr_values)
                }
        
        return enhanced_data
    
    def _generate_overview_dashboard_html(self, enhanced_data: Dict[str, Any]) -> str:
        """Generate Overview Dashboard section with quick stats
        
        Args:
            enhanced_data: Enhanced timing data with metadata
        
        Returns:
            HTML string for overview dashboard
        """
        timing_data = enhanced_data['timing_data']
        pt_locations = enhanced_data['pt_locations']
        location_metadata = enhanced_data['location_metadata']
        summary_stats = enhanced_data['summary_stats']
        quality_stats = enhanced_data['quality_stats']
        dsr_stats = enhanced_data['dsr_stats']
        
        # Design Information Card
        design_name = self.design_info.top_hier
        workarea = self.workarea
        timestamp = datetime.now().strftime("%B %d, %Y at %H:%M:%S")
        
        # PT Locations Summary
        total_locations = location_metadata.get('total_locations', 0)
        status_counts = location_metadata.get('status_counts', {})
        locations_list = location_metadata.get('locations_list', [])
        
        # Create location badges
        location_badges = " ".join([
            f'<span class="badge ipo">{loc}</span>' 
            for loc in locations_list[:10]  # Show first 10
        ])
        if len(locations_list) > 10:
            location_badges += f' <span class="badge info">+{len(locations_list) - 10} more</span>'
        
        # Status badges
        status_html = ""
        for status, count in status_counts.items():
            badge_class = {
                'COMPLETED': 'completed',
                'RUNNING': 'running',
                'STALE': 'stale',
                'NO_WORK_DIRS': 'no-work'
            }.get(status, 'no-work')
            status_html += f'<span class="badge {badge_class}">{count} {status}</span>'
        
        # Quick Stats - Timing Violations
        setup_wns = summary_stats.get('setup_wns', 'N/A')
        setup_tns = summary_stats.get('setup_tns', 'N/A')
        hold_wns = summary_stats.get('hold_wns', 'N/A')
        hold_tns = summary_stats.get('hold_tns', 'N/A')
        
        # Determine timing card class
        timing_class = 'success'
        if isinstance(setup_wns, (int, float)) and isinstance(setup_tns, (int, float)):
            if setup_wns < -0.050 or setup_tns < -1.0:
                timing_class = 'danger'
            elif setup_wns < 0 or setup_tns < 0:
                timing_class = 'warning'
        
        # Quick Stats - Quality Metrics
        data_total = quality_stats.get('data_total', 0)
        clock_total = quality_stats.get('clock_total', 0)
        quality_total = data_total + clock_total
        
        quality_class = 'success' if quality_total == 0 else 'danger' if quality_total > 100 else 'warning'
        
        # Quick Stats - DSR Skew
        dsr_min = dsr_stats.get('min', 'N/A')
        dsr_max = dsr_stats.get('max', 'N/A')
        
        dsr_class = 'success'
        if isinstance(dsr_max, (int, float)):
            if dsr_max > 20:
                dsr_class = 'danger'
            elif dsr_max > 10:
                dsr_class = 'warning'
        
        # Quick Stats - PT Waiver Gaps
        waiver_total = summary_stats.get('pt_waiver_unwaived_total', 0)
        waiver_unique = summary_stats.get('pt_waiver_unwaived_unique', 0)
        waiver_csv_loaded = summary_stats.get('pt_waiver_csv_loaded', True)
        waiver_class = 'info'
        if waiver_csv_loaded:
            if waiver_total == 0:
                waiver_class = 'success'
            elif waiver_total <= 10:
                waiver_class = 'warning'
            else:
                waiver_class = 'danger'
        
        # Quick Stats - Work Directories
        total_work_dirs = summary_stats.get('total_work_dirs', 0)
        latest_work = summary_stats.get('latest_work_dir', 'N/A')
        
        return f"""
        <div class="section">
            <div class="section-header" onclick="toggleSection('overview-content')">
                <h2 class="section-title">ðŸ“Š Overview Dashboard</h2>
                <span id="toggle-overview-content" class="section-toggle">â–¼</span>
            </div>
            <div id="overview-content" class="section-content">
                <!-- Design Information -->
                <div class="info">
                    <h3>Design Information</h3>
                    <div class="info-grid">
                        <div class="info-item">
                            <strong>Design Name</strong>
                            {design_name}
                        </div>
                        <div class="info-item">
                            <strong>Workarea</strong>
                            <span style="font-size: 11px; word-break: break-all;">{workarea}</span>
                            <button class="btn btn-sm btn-primary" onclick="copyToClipboard('{workarea}')">Copy Path</button>
                        </div>
                        <div class="info-item">
                            <strong>Report Generated</strong>
                            {timestamp}
                        </div>
                    </div>
                </div>
                
                <!-- PT Locations Summary -->
                <div class="info">
                    <h3>PT Locations Summary</h3>
                    <div class="info-grid">
                        <div class="info-item">
                            <strong>Total PT Locations</strong>
                            {total_locations}
                        </div>
                        <div class="info-item">
                            <strong>Status Breakdown</strong>
                            {status_html if status_html else 'No locations found'}
                        </div>
                        <div class="info-item" style="grid-column: 1 / -1;">
                            <strong>Locations</strong><br>
                            {location_badges if location_badges else 'No locations detected'}
                        </div>
                        <div class="info-item">
                            <strong>Latest Work Directory</strong>
                            {latest_work}
                        </div>
                    </div>
                </div>
                
                <!-- Quick Stats Cards -->
                <h3 style="margin-top: 30px; margin-bottom: 15px;">Quick Statistics</h3>
                <div class="cards-grid">
                    <!-- Timing Violations Card -->
                    <div class="card {timing_class}">
                        <div class="card-title">Timing (Latest)</div>
                        <div class="card-value">
                            Setup: {f"{setup_wns:.3f}" if isinstance(setup_wns, (int, float)) else setup_wns} ns
                        </div>
                        <div class="card-subtitle">
                            Hold: {f"{hold_wns:.3f}" if isinstance(hold_wns, (int, float)) else hold_wns} ns | 
                            TNS: {f"{setup_tns:.2f}" if isinstance(setup_tns, (int, float)) else setup_tns} / 
                            {f"{hold_tns:.2f}" if isinstance(hold_tns, (int, float)) else hold_tns} ns
                        </div>
                    </div>
                    
                    <!-- Quality Metrics Card -->
                    <div class="card {quality_class}">
                        <div class="card-title">Quality Violations</div>
                        <div class="card-value">{quality_total}</div>
                        <div class="card-subtitle">
                            Data: {data_total} | Clock: {clock_total}
                        </div>
                    </div>
                    
                    <!-- PT Waiver Gaps Card -->
                    <div class="card {waiver_class}">
                        <div class="card-title">PT Waiver Gaps</div>
                        <div class="card-value">
                            {waiver_total if waiver_csv_loaded else 'CSV Missing'}
                        </div>
                        <div class="card-subtitle">
                            Unique: {waiver_unique if waiver_csv_loaded else 'N/A'}
                        </div>
                    </div>
                    
                    <!-- DSR Skew Card -->
                    <div class="card {dsr_class}">
                        <div class="card-title">DSR Skew Range</div>
                        <div class="card-value">
                            {f"{dsr_min:.2f}" if isinstance(dsr_min, (int, float)) else dsr_min} - 
                            {f"{dsr_max:.2f}" if isinstance(dsr_max, (int, float)) else dsr_max} ps
                        </div>
                        <div class="card-subtitle">
                            Setup/Hold Corners
                        </div>
                    </div>
                    
                    <!-- Work Directories Card -->
                    <div class="card info">
                        <div class="card-title">Work Directories</div>
                        <div class="card-value">{total_work_dirs}</div>
                        <div class="card-subtitle">
                            Total PT runs analyzed
                        </div>
                    </div>
                </div>
            </div>
        </div>
        """
    
    def _generate_location_explorer_html(self, pt_locations: List[Dict[str, Any]], timing_data: List[Dict]) -> str:
        """Generate Location Explorer section with expandable table
        
        Args:
            pt_locations: List of PT location info
            timing_data: List of timing data per work directory
        
        Returns:
            HTML string for location explorer
        """
        if not pt_locations:
            return """
            <div class="section">
                <div class="section-header" onclick="toggleSection('location-content')">
                    <h2 class="section-title">ðŸ“ PT Location Explorer</h2>
                    <span id="toggle-location-content" class="section-toggle">â–¼</span>
                </div>
                <div id="location-content" class="section-content">
                    <p>No PT locations detected.</p>
                </div>
            </div>
            """
        
        # Build location table rows
        table_rows = ""
        for idx, loc in enumerate(pt_locations):
            location = loc.get('location', 'Unknown')
            label = loc.get('label', location)
            status = loc.get('status', 'UNKNOWN')
            work_dirs = loc.get('work_dirs', [])
            latest_work = loc.get('latest_work', 'N/A')
            work_dir_count = len(work_dirs)
            
            # Status badge
            badge_class = {
                'COMPLETED': 'completed',
                'RUNNING': 'running',
                'STALE': 'stale',
                'NO_WORK_DIRS': 'no-work'
            }.get(status, 'no-work')
            status_badge = f'<span class="badge {badge_class}">{status}</span>'
            
            # Expandable row
            expand_icon = 'â–¶' if work_dirs else ''
            row_id = f"loc-{idx}"
            
            # Main row
            table_rows += f"""
            <tr class="expandable-row" onclick="toggleRow('{row_id}')">
                <td>
                    <span id="icon-{row_id}" class="expand-icon">{expand_icon}</span>
                    <strong>{label}</strong>
                </td>
                <td class="center">{status_badge}</td>
                <td class="center">{work_dir_count}</td>
                <td>{latest_work}</td>
                <td class="center">
                    <button class="btn btn-sm btn-primary" onclick="event.stopPropagation(); toggleRow('{row_id}')">
                        View Details
                    </button>
                </td>
            </tr>
            """
            
            # Details row (hidden by default)
            if work_dirs:
                auto_pt_path = loc.get('path', '')
                details_content = "<strong>Work Directories & Reports:</strong><ul style='margin: 10px 0; padding-left: 20px;'>"
                
                for wd_path in work_dirs[:20]:  # Show first 20
                    wd_name = os.path.basename(wd_path)
                    
                    # Find matching timing data
                    wd_timing = next((t for t in timing_data if wd_path in t.get('work_dir', '')), None)
                    runtime_str = ""
                    if wd_timing and 'runtime_hours' in wd_timing:
                        runtime_str = f" - Runtime: {wd_timing['runtime_hours']:.2f}h"
                    
                    details_content += f"<li style='margin-bottom: 20px; border-bottom: 1px solid #e0e0e0; padding-bottom: 15px;'><strong style='font-size: 14px;'>{wd_name}</strong>{runtime_str}"
                    
                    # Find PT HTML report in auto_pt directory (format: work_*.html)
                    work_html = os.path.join(auto_pt_path, f"{wd_name}.html")
                    
                    # Find all timing reports for this work directory
                    # Pattern: work_dir/func.std_tt_0c_0p6v.setup.typical/reports/timing_reports/*
                    timing_reports_pattern = os.path.join(wd_path, "func.std_tt_0c_0p6v.setup.typical/reports/timing_reports/*")
                    report_files = glob.glob(timing_reports_pattern)
                    
                    has_reports = os.path.exists(work_html) or report_files
                    
                    if has_reports:
                        details_content += "<div style='margin-left: 20px; margin-top: 10px;'>"
                        
                        # Show PT HTML report first (direct file:// link, not tablog)
                        if os.path.exists(work_html):
                            abs_path = os.path.abspath(work_html)
                            details_content += f"""
                            <div style='margin-bottom: 10px;'>
                                <a href='file://{abs_path}' target='_blank' style='display: inline-block; padding: 6px 12px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; text-decoration: none; border-radius: 4px; font-weight: bold;'>
                                    ðŸ“Š PT Timing Summary (HTML)
                                </a>
                            </div>
                            """
                        
                        # Show other timing reports (compressed files with tablog)
                        if report_files:
                            details_content += "<div style='margin-top: 12px;'>"
                            details_content += "<small style='color: #666; font-weight: bold;'>ðŸ“ Additional Timing Reports:</small><br>"
                            
                            # Group reports by type
                            sorted_files = sorted(report_files, key=lambda x: os.path.basename(x))
                            
                            # Unique ID for this work directory's expandable section
                            expand_id = f"expand_{wd_name.replace('.', '_').replace(':', '_')}"
                            
                            # Show first 12 files in 3-column grid
                            details_content += "<div style='margin-top: 8px; display: grid; grid-template-columns: repeat(3, 1fr); gap: 8px; background: #f9f9f9; padding: 10px; border-radius: 4px; border: 1px solid #e0e0e0;'>"
                            
                            for report_file in sorted_files[:12]:  # Show first 12 files (4 rows Ã— 3 columns)
                                report_name = os.path.basename(report_file)
                                abs_path = os.path.abspath(report_file)
                                
                                # Shorter display name (remove redundant prefix)
                                display_name = report_name.replace(f"{os.path.basename(wd_path).split('_')[0]}_", "")
                                if len(display_name) > 35:
                                    display_name = display_name[:32] + "..."
                                
                                # Use tablog for viewing compressed files - improved button styling
                                details_content += f"""
                                <div style='display: flex; align-items: center; background: white; padding: 4px 6px; border-radius: 3px; border: 1px solid #ddd;'>
                                    <button class='btn btn-sm' onclick='event.stopPropagation(); openLogWithServer("{abs_path}", event)' style='margin-right: 5px; font-size: 10px; padding: 3px 8px; background: #667eea; color: white; border: none; flex: 0 0 auto;'>
                                        ðŸ”
                                    </button>
                                    <span style='font-size: 10px; color: #555; font-family: monospace; overflow: hidden; text-overflow: ellipsis; white-space: nowrap;' title='{report_name}'>{display_name}</span>
                                </div>
                                """
                            
                            details_content += "</div>"
                            
                            # Expandable section for remaining reports
                            if len(report_files) > 12:
                                remaining_count = len(report_files) - 12
                                details_content += f"""
                                <div style='margin-top: 8px; text-align: center;'>
                                    <button onclick='event.stopPropagation(); toggleReportExpand("{expand_id}")' style='background: #3498db; color: white; border: none; padding: 6px 12px; border-radius: 4px; cursor: pointer; font-size: 11px;'>
                                        â–¼ Show {remaining_count} more reports
                                    </button>
                                </div>
                                <div id='{expand_id}' style='display: none; margin-top: 8px; display: grid; grid-template-columns: repeat(3, 1fr); gap: 8px; background: #f9f9f9; padding: 10px; border-radius: 4px; border: 1px solid #e0e0e0;'>
                                """
                                
                                # Show remaining reports
                                for report_file in sorted_files[12:]:
                                    report_name = os.path.basename(report_file)
                                    abs_path = os.path.abspath(report_file)
                                    
                                    display_name = report_name.replace(f"{os.path.basename(wd_path).split('_')[0]}_", "")
                                    if len(display_name) > 35:
                                        display_name = display_name[:32] + "..."
                                    
                                    details_content += f"""
                                    <div style='display: flex; align-items: center; background: white; padding: 4px 6px; border-radius: 3px; border: 1px solid #ddd;'>
                                        <button class='btn btn-sm' onclick='event.stopPropagation(); openLogWithServer("{abs_path}", event)' style='margin-right: 5px; font-size: 10px; padding: 3px 8px; background: #667eea; color: white; border: none; flex: 0 0 auto;'>
                                            ðŸ”
                                        </button>
                                        <span style='font-size: 10px; color: #555; font-family: monospace; overflow: hidden; text-overflow: ellipsis; white-space: nowrap;' title='{report_name}'>{display_name}</span>
                                    </div>
                                    """
                                
                                details_content += "</div>"
                            
                            details_content += "</div>"
                        
                        details_content += "</div>"
                    
                    details_content += "</li>"
                
                if len(work_dirs) > 20:
                    details_content += f"<li><em>...and {len(work_dirs) - 20} more work directories</em></li>"
                
                details_content += "</ul>"
                
                table_rows += f"""
                <tr id="details-{row_id}" class="details-row">
                    <td colspan="5" class="details-cell">
                        {details_content}
                    </td>
                </tr>
                """
        
        # Filter controls
        filter_html = """
        <div class="filter-controls">
            <div class="filter-group">
                <label>Filter by Status:</label>
                <select id="status-filter" onchange="filterLocationTable()">
                    <option value="">All Statuses</option>
                    <option value="COMPLETED">COMPLETED</option>
                    <option value="RUNNING">RUNNING</option>
                    <option value="STALE">STALE</option>
                    <option value="NO_WORK_DIRS">NO_WORK_DIRS</option>
                </select>
            </div>
            
            <div class="filter-group">
                <label>Search Location:</label>
                <input type="text" id="location-search" placeholder="Type to search..." oninput="filterLocationTable()">
            </div>
            
            <div class="filter-group">
                <button class="btn btn-sm btn-primary" onclick="clearLocationFilters()">Clear Filters</button>
                <button class="btn btn-sm btn-success" onclick="expandAllLocations()">Expand All</button>
                <button class="btn btn-sm btn-warning" onclick="collapseAllLocations()">Collapse All</button>
            </div>
        </div>
        
        <script>
        function filterLocationTable() {
            const statusFilter = document.getElementById('status-filter').value.toLowerCase();
            const searchTerm = document.getElementById('location-search').value.toLowerCase();
            const table = document.getElementById('location-table');
            const rows = table.querySelectorAll('tbody tr.expandable-row');
            
            let visibleCount = 0;
            rows.forEach(row => {
                const statusBadge = row.querySelector('.badge');
                const locationText = row.cells[0].textContent.toLowerCase();
                const status = statusBadge ? statusBadge.textContent.toLowerCase() : '';
                
                const statusMatch = !statusFilter || status.includes(statusFilter);
                const searchMatch = !searchTerm || locationText.includes(searchTerm);
                
                if (statusMatch && searchMatch) {
                    row.style.display = '';
                    // Also show details row if it exists
                    const detailsRow = row.nextElementSibling;
                    if (detailsRow && detailsRow.classList.contains('details-row')) {
                        if (detailsRow.classList.contains('visible')) {
                            detailsRow.style.display = '';
                        }
                    }
                    visibleCount++;
                } else {
                    row.style.display = 'none';
                    // Hide details row
                    const detailsRow = row.nextElementSibling;
                    if (detailsRow && detailsRow.classList.contains('details-row')) {
                        detailsRow.style.display = 'none';
                    }
                }
            });
            
            console.log(`Showing ${visibleCount} of ${rows.length} locations`);
        }
        
        function clearLocationFilters() {
            document.getElementById('status-filter').value = '';
            document.getElementById('location-search').value = '';
            filterLocationTable();
        }
        
        function expandAllLocations() {
            document.querySelectorAll('.details-row').forEach(row => {
                row.classList.add('visible');
                row.style.display = '';
            });
            document.querySelectorAll('.expand-icon').forEach(icon => {
                icon.classList.add('expanded');
            });
        }
        
        function collapseAllLocations() {
            document.querySelectorAll('.details-row').forEach(row => {
                row.classList.remove('visible');
                row.style.display = 'none';
            });
            document.querySelectorAll('.expand-icon').forEach(icon => {
                icon.classList.remove('expanded');
            });
        }
        </script>
        """
        
        return f"""
        <div class="section">
            <div class="section-header" onclick="toggleSection('location-content')">
                <h2 class="section-title">ðŸ“ PT Location Explorer</h2>
                <span id="toggle-location-content" class="section-toggle">â–¼</span>
            </div>
            <div id="location-content" class="section-content">
                <p><strong>{len(pt_locations)} PT location(s) detected</strong> - Click rows to expand details</p>
                
                {filter_html}
                
                <div class="table-wrapper">
                    <table id="location-table">
                        <thead>
                            <tr>
                                <th style="text-align: left;">Location</th>
                                <th>Status</th>
                                <th>Work Dirs</th>
                                <th style="text-align: left;">Latest Work</th>
                                <th>Actions</th>
                            </tr>
                        </thead>
                        <tbody>
                            {table_rows}
                        </tbody>
                    </table>
                </div>
            </div>
        </div>
        """
    
    def _generate_quality_metrics_html(self, timing_data: List[Dict], enhanced_data: Dict[str, Any]) -> str:
        """Generate Quality Metrics Dashboard section
        
        Args:
            timing_data: List of timing data per work directory
            enhanced_data: Enhanced data with quality stats
        
        Returns:
            HTML string for quality metrics dashboard
        """
        if not timing_data:
            return """
            <div class="section">
                <div class="section-header" onclick="toggleSection('quality-content')">
                    <h2 class="section-title">âš¡ Quality Metrics Dashboard</h2>
                    <span id="toggle-quality-content" class="section-toggle">â–¼</span>
                </div>
                <div id="quality-content" class="section-content">
                    <p>No quality metrics data available.</p>
                </div>
            </div>
            """
        
        quality_stats = enhanced_data.get('quality_stats', {})
        
        # Summary cards
        data_total = quality_stats.get('data_total', 0)
        clock_total = quality_stats.get('clock_total', 0)
        total_violations = data_total + clock_total
        
        # Color coding
        overall_class = 'success' if total_violations == 0 else 'danger' if total_violations > 100 else 'warning'
        data_class = 'success' if data_total == 0 else 'danger' if data_total > 50 else 'warning'
        clock_class = 'success' if clock_total == 0 else 'danger' if clock_total > 50 else 'warning'
        
        # Build detailed quality table
        table_rows = ""
        for idx, wd in enumerate(timing_data):
            work_dir = wd.get('work_dir', 'N/A')
            setup_data = wd.get('scenarios', {}).get('setup', {})
            clock_quality = wd.get('clock_quality', {}).get('setup', {}).get('violations', {})
            
            # Data violations
            data_tran = setup_data.get('data_max_transition', 0)
            data_cap = setup_data.get('data_max_capacitance', 0)
            data_fanout = setup_data.get('data_max_fanout', 0)
            data_sum = data_tran + data_cap + data_fanout
            
            # Clock violations
            clock_tran = setup_data.get('clock_max_transition', 0)
            clock_cap = setup_data.get('clock_max_capacitance', 0)
            clock_slew = clock_quality.get('slew_violations', 0)
            clock_xcap = clock_quality.get('xcap_violations', 0)
            clock_sum = clock_tran + clock_cap + clock_slew + clock_xcap
            
            total = data_sum + clock_sum
            
            # Color code total
            total_class = 'value-green' if total == 0 else 'value-red' if total > 100 else 'value-yellow'
            
            row_id = f"quality-{idx}"
            expand_icon = 'â–¶'
            
            table_rows += f"""
            <tr class="expandable-row" onclick="toggleRow('{row_id}')">
                <td>
                    <span id="icon-{row_id}" class="expand-icon">{expand_icon}</span>
                    <strong>{work_dir}</strong>
                </td>
                <td class="center"><span class="{total_class}">{total}</span></td>
                <td class="center">{data_sum}</td>
                <td class="center">{clock_sum}</td>
                <td class="center">
                    <button class="btn btn-sm btn-primary" onclick="event.stopPropagation(); toggleRow('{row_id}')">
                        Details
                    </button>
                </td>
            </tr>
            """
            
            # Details row
            details_html = f"""
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                <div>
                    <h4 style="color: #e74c3c;">Data Violations</h4>
                    <ul style="margin: 10px 0;">
                        <li>Max Transition: {data_tran}</li>
                        <li>Max Capacitance: {data_cap}</li>
                        <li>Max Fanout: {data_fanout}</li>
                    </ul>
                </div>
                <div>
                    <h4 style="color: #f39c12;">Clock Violations</h4>
                    <ul style="margin: 10px 0;">
                        <li>Max Transition: {clock_tran}</li>
                        <li>Max Capacitance: {clock_cap}</li>
                        <li>Slew: {clock_slew}</li>
                        <li>X-cap: {clock_xcap}</li>
                    </ul>
                </div>
            </div>
            """
            
            table_rows += f"""
            <tr id="details-{row_id}" class="details-row">
                <td colspan="5" class="details-cell">
                    {details_html}
                </td>
            </tr>
            """
        
        return f"""
        <div class="section">
            <div class="section-header" onclick="toggleSection('quality-content')">
                <h2 class="section-title">âš¡ Quality Metrics Dashboard</h2>
                <span id="toggle-quality-content" class="section-toggle">â–¼</span>
            </div>
            <div id="quality-content" class="section-content">
                <h3 style="margin-bottom: 15px;">Quality Summary</h3>
                <div class="cards-grid">
                    <div class="card {overall_class}">
                        <div class="card-title">Total Violations</div>
                        <div class="card-value">{total_violations}</div>
                        <div class="card-subtitle">All constraint violations</div>
                    </div>
                    
                    <div class="card {data_class}">
                        <div class="card-title">Data Violations</div>
                        <div class="card-value">{data_total}</div>
                        <div class="card-subtitle">
                            Tran: {quality_stats.get('data_tran', 0)} | 
                            Cap: {quality_stats.get('data_cap', 0)} | 
                            Fanout: {quality_stats.get('data_fanout', 0)}
                        </div>
                    </div>
                    
                    <div class="card {clock_class}">
                        <div class="card-title">Clock Violations</div>
                        <div class="card-value">{clock_total}</div>
                        <div class="card-subtitle">
                            Tran: {quality_stats.get('clock_tran', 0)} | 
                            Cap: {quality_stats.get('clock_cap', 0)} | 
                            Slew: {quality_stats.get('clock_slew', 0)} | 
                            Xcap: {quality_stats.get('clock_xcap', 0)}
                        </div>
                    </div>
                </div>
                
                <h3 style="margin-top: 30px; margin-bottom: 15px;">Quality Metrics by Work Directory</h3>
                
                <div style="margin-bottom: 15px;">
                    <button class="btn btn-sm btn-success" onclick="exportTableToCSV('quality-table', 'quality_metrics.csv')">
                        Export to CSV
                    </button>
                    <button class="btn btn-sm btn-primary" onclick="copyTableToClipboard('quality-table')">
                        Copy Table
                    </button>
                </div>
                
                <div class="table-wrapper">
                    <table id="quality-table">
                        <thead>
                            <tr>
                                <th onclick="sortTable('quality-table', 0)">Work Directory</th>
                                <th onclick="sortTable('quality-table', 1, 'number')" class="sortable">Total</th>
                                <th onclick="sortTable('quality-table', 2, 'number')" class="sortable">Data</th>
                                <th onclick="sortTable('quality-table', 3, 'number')" class="sortable">Clock</th>
                                <th>Actions</th>
                            </tr>
                        </thead>
                        <tbody>
                            {table_rows}
                        </tbody>
                    </table>
                </div>
            </div>
        </div>
        """
    
    def _generate_pt_waiver_gap_html(self, timing_data: List[Dict[str, Any]]) -> str:
        """Generate PT waiver gap section (auto_pt_slave.err vs central pt.csv)"""
        if not timing_data:
            return """
            <div class="section">
                <div class="section-header" onclick="toggleSection('waiver-gap-content')">
                    <h2 class="section-title">PT Waiver Gaps</h2>
                    <span id="toggle-waiver-gap-content" class="section-toggle">â–¼</span>
                </div>
                <div id="waiver-gap-content" class="section-content">
                    <p>No timing data available.</p>
                </div>
            </div>
            """
        
        csv_path = None
        csv_loaded = True
        csv_error = None
        for wd in timing_data:
            csv_info = wd.get("waiver_csv", {})
            if csv_info:
                csv_path = csv_info.get("path")
                csv_loaded = csv_info.get("loaded", True)
                csv_error = csv_info.get("error")
                break
        
        csv_link = ""
        if csv_path and os.path.exists(csv_path):
            csv_link = f"<a href='file://{csv_path}' target='_blank'>{csv_path}</a>"
        elif csv_path:
            csv_link = f"{csv_path} (missing)"
        
        if not csv_loaded:
            error_text = csv_error or "pt.csv not available"
            return f"""
            <div class="section">
                <div class="section-header" onclick="toggleSection('waiver-gap-content')">
                    <h2 class="section-title">PT Waiver Gaps</h2>
                    <span id="toggle-waiver-gap-content" class="section-toggle">â–¼</span>
                </div>
                <div id="waiver-gap-content" class="section-content">
                    <div class="info">
                        <h3>Central Waiver CSV</h3>
                        <p>{csv_link}</p>
                    </div>
                    <p style="color: #f39c12;"><strong>Waiver comparison skipped:</strong> {html.escape(error_text)}</p>
                </div>
            </div>
            """
        
        rows_html = ""
        total_unwaived = 0
        code_re = re.compile(r'\(([^()]+)\)')
        
        for wd in timing_data:
            work_name = wd.get("work_dir", "N/A")
            location_label = wd.get("location_label", "")
            work_label = f"{work_name} [{location_label}]" if location_label else work_name
            
            for scenario in ("setup", "hold"):
                gaps = wd.get("scenarios", {}).get(scenario, {}).get("waiver_gaps")
                if not gaps:
                    continue
                
                unwaived_total = gaps.get("unwaived_total", 0)
                unwaived_unique = gaps.get("unwaived_unique", 0)
                if unwaived_total == 0:
                    continue
                
                total_unwaived += unwaived_total
                err_file = gaps.get("err_file", "")
                err_link = f"<a href='file://{err_file}' target='_blank'>auto_pt_slave.err</a>" if err_file else "N/A"
                
                # Category summary by code for faster review
                code_counts = {}
                no_code = 0
                for entry in gaps.get("unwaived_summary", []):
                    msg = entry.get("message", "")
                    matches = code_re.findall(msg)
                    if matches:
                        code = matches[-1]
                        code_counts[code] = code_counts.get(code, 0) + int(entry.get("count", 0))
                    else:
                        no_code += int(entry.get("count", 0))
                
                code_rows = ""
                for code, count in sorted(code_counts.items(), key=lambda x: x[1], reverse=True):
                    code_rows += f"""
                    <tr>
                        <td>{code}</td>
                        <td style="text-align: right;">{count}</td>
                    </tr>
                    """
                if no_code:
                    code_rows += f"""
                    <tr>
                        <td>(no_code)</td>
                        <td style="text-align: right;">{no_code}</td>
                    </tr>
                    """
                
                summary_rows = ""
                for entry in gaps.get("unwaived_summary", []):
                    msg = html.escape(entry.get("message", ""))
                    count = entry.get("count", 0)
                    lines = ", ".join(entry.get("line_numbers", [])) or "N/A"
                    summary_rows += f"""
                    <tr>
                        <td style="text-align: right;">{count}</td>
                        <td><code style="font-size: 11px;">{msg}</code></td>
                        <td style="font-size: 11px;">{lines}</td>
                    </tr>
                    """
                
                details_html = ""
                if summary_rows:
                    details_html = f"""
                    <details>
                        <summary>View {unwaived_unique} message type(s)</summary>
                        <div style="margin-top: 10px;">
                            <table>
                                <thead>
                                    <tr>
                                        <th>Count</th>
                                        <th>Message</th>
                                        <th>Sample Lines</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    {summary_rows}
                                </tbody>
                            </table>
                        </div>
                    </details>
                    """
                
                category_html = ""
                if code_rows:
                    category_html = f"""
                    <details open>
                        <summary>Category Summary</summary>
                        <div style="margin-top: 10px;">
                            <table style="font-size: 12px; width: 100%;">
                                <thead>
                                    <tr>
                                        <th>Code</th>
                                        <th>Count</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    {code_rows}
                                </tbody>
                            </table>
                        </div>
                    </details>
                    """
                
                rows_html += f"""
                <tr>
                    <td>{work_label}</td>
                    <td style="text-transform: uppercase;">{scenario}</td>
                    <td style="text-align: right;">{unwaived_total}</td>
                    <td style="text-align: right;">{unwaived_unique}</td>
                    <td>
                        <button class="btn btn-sm" onclick="openLogWithServer('{err_file}', event)" style="margin-right: 6px;">View in Tablog</button>
                        {err_link}
                    </td>
                    <td>
                        {category_html}
                        {details_html}
                    </td>
                </tr>
                """
        
        if not rows_html:
            return f"""
            <div class="section">
                <div class="section-header" onclick="toggleSection('waiver-gap-content')">
                    <h2 class="section-title">PT Waiver Gaps</h2>
                    <span id="toggle-waiver-gap-content" class="section-toggle">â–¼</span>
                </div>
                <div id="waiver-gap-content" class="section-content">
                    <div class="info">
                        <h3>Central Waiver CSV</h3>
                        <p>{csv_link}</p>
                    </div>
                    <p style="color: #27ae60;"><strong>No unwaived PT errors/warnings found.</strong></p>
                </div>
            </div>
            """
        
        return f"""
        <div class="section">
            <div class="section-header" onclick="toggleSection('waiver-gap-content')">
                <h2 class="section-title">PT Waiver Gaps</h2>
                <span id="toggle-waiver-gap-content" class="section-toggle">â–¼</span>
            </div>
            <div id="waiver-gap-content" class="section-content">
                <div class="info">
                    <h3>Central Waiver CSV</h3>
                    <p>{csv_link}</p>
                </div>
                <p><strong>Total Unwaived Entries:</strong> {total_unwaived}</p>
                <div class="table-wrapper">
                    <table>
                        <thead>
                            <tr>
                                <th style="text-align: left;">Work Directory</th>
                                <th>Scenario</th>
                                <th>Unwaived</th>
                                <th>Unique</th>
                                <th style="text-align: left;">Report</th>
                                <th style="text-align: left;">Details</th>
                            </tr>
                        </thead>
                        <tbody>
                            {rows_html}
                        </tbody>
                    </table>
                </div>
            </div>
        </div>
        """
    
    def _generate_enhanced_timing_summary_html(self, timing_data: List[Dict], enhanced_data: Dict[str, Any]) -> str:
        """Generate Enhanced Unified Timing Summary section
        
        Args:
            timing_data: List of timing data per work directory
            enhanced_data: Enhanced data with summary stats
        
        Returns:
            HTML string for enhanced timing summary
        """
        if not timing_data:
            return """
            <div class="section">
                <div class="section-header" onclick="toggleSection('timing-content')">
                    <h2 class="section-title">â±ï¸ Unified Timing Summary</h2>
                    <span id="toggle-timing-content" class="section-toggle">â–¼</span>
                </div>
                <div id="timing-content" class="section-content">
                    <p>No timing data available.</p>
                </div>
            </div>
            """
        
        # Get all timing groups across all work areas
        all_groups = set()
        for wd in timing_data:
            for scenario_type, scenario_data in wd.get("scenarios", {}).items():
                all_groups.update(scenario_data.get("groups", {}).keys())
        
        # Separate internal and external groups
        external_groups = {'FEEDTHROUGH', 'REGIN', 'REGOUT'}
        internal_groups = sorted([g for g in all_groups if g not in external_groups])
        external_groups_sorted = sorted([g for g in all_groups if g in external_groups])
        
        # Build timing table for latest work directory (both setup and hold)
        latest_wd = timing_data[0] if timing_data else {}
        work_dir = latest_wd.get('work_dir', 'N/A')
        
        # Setup scenario
        setup_data = latest_wd.get('scenarios', {}).get('setup', {})
        setup_groups = setup_data.get('groups', {})
        # Get total from groups (uppercase keys: WNS, TNS, NVP)
        setup_total = setup_groups.get('total', {})
        setup_wns = setup_total.get('WNS', 'N/A')
        setup_tns = setup_total.get('TNS', 'N/A')
        setup_nvp = setup_total.get('NVP', 'N/A')
        # DSR skew is at work_data level
        setup_dsr = latest_wd.get('dsr_skew_setup', 'N/A')
        
        # Hold scenario
        hold_data = latest_wd.get('scenarios', {}).get('hold', {})
        hold_groups = hold_data.get('groups', {})
        # Get total from groups (uppercase keys: WNS, TNS, NVP)
        hold_total = hold_groups.get('total', {})
        hold_wns = hold_total.get('WNS', 'N/A')
        hold_tns = hold_total.get('TNS', 'N/A')
        hold_nvp = hold_total.get('NVP', 'N/A')
        # DSR skew is at work_data level
        hold_dsr = latest_wd.get('dsr_skew_hold', 'N/A')
        
        # Build table rows - separate for setup and hold
        def build_group_rows(groups_dict, internal_grps, external_grps):
            rows = ""
            # Internal groups (sorted by TNS worst first)
            internal_sorted = sorted(internal_grps, key=lambda g: groups_dict.get(g, {}).get('TNS', 0))
            for group in internal_sorted:
                group_data = groups_dict.get(group, {})
                wns = group_data.get('WNS', 'N/A')
                tns = group_data.get('TNS', 'N/A')
                nvp = group_data.get('NVP', 'N/A')
                
                # Color code
                wns_class = 'value-green' if isinstance(wns, (int, float)) and wns >= 0 else 'value-red' if isinstance(wns, (int, float)) and wns < -0.050 else 'value-yellow'
                tns_class = 'value-green' if isinstance(tns, (int, float)) and tns >= 0 else 'value-red' if isinstance(tns, (int, float)) and tns < -1.0 else 'value-yellow'
                
                rows += f"""
                <tr>
                    <td><strong>{group}</strong></td>
                    <td class="center"><span class="{wns_class}">{f"{wns:.3f}" if isinstance(wns, (int, float)) else wns}</span></td>
                    <td class="center"><span class="{tns_class}">{f"{tns:.2f}" if isinstance(tns, (int, float)) else tns}</span></td>
                    <td class="center">{nvp if isinstance(nvp, int) else 'N/A'}</td>
                </tr>
                """
            
            # External groups at end
            for group in external_grps:
                group_data = groups_dict.get(group, {})
                wns = group_data.get('WNS', 0)
                tns = group_data.get('TNS', 0)
                nvp = group_data.get('NVP', 0)
                
                rows += f"""
                <tr style="opacity: 0.6;">
                    <td><em>{group}</em></td>
                    <td class="center">{f"{wns:.3f}" if isinstance(wns, (int, float)) else wns}</td>
                    <td class="center">{f"{tns:.2f}" if isinstance(tns, (int, float)) else tns}</td>
                    <td class="center">{nvp}</td>
                </tr>
                """
            
            return rows
        
        setup_rows = build_group_rows(setup_groups, internal_groups, external_groups_sorted)
        hold_rows = build_group_rows(hold_groups, internal_groups, external_groups_sorted)
        
        # Calculate internal totals
        setup_wns_vals = [setup_groups.get(g, {}).get('wns', 0) for g in internal_groups if g in setup_groups]
        setup_internal_wns = min(setup_wns_vals) if setup_wns_vals else 'N/A'
        setup_tns_vals = [setup_groups.get(g, {}).get('tns', 0) for g in internal_groups if g in setup_groups]
        setup_internal_tns = sum(setup_tns_vals) if setup_tns_vals else 'N/A'
        setup_nvp_vals = [setup_groups.get(g, {}).get('nvp', 0) for g in internal_groups if g in setup_groups]
        setup_internal_nvp = sum(setup_nvp_vals) if setup_nvp_vals else 'N/A'
        
        hold_wns_vals = [hold_groups.get(g, {}).get('wns', 0) for g in internal_groups if g in hold_groups]
        hold_internal_wns = min(hold_wns_vals) if hold_wns_vals else 'N/A'
        hold_tns_vals = [hold_groups.get(g, {}).get('tns', 0) for g in internal_groups if g in hold_groups]
        hold_internal_tns = sum(hold_tns_vals) if hold_tns_vals else 'N/A'
        hold_nvp_vals = [hold_groups.get(g, {}).get('nvp', 0) for g in internal_groups if g in hold_groups]
        hold_internal_nvp = sum(hold_nvp_vals) if hold_nvp_vals else 'N/A'
        
        return f"""
        <div class="section">
            <div class="section-header" onclick="toggleSection('timing-content')">
                <h2 class="section-title">â±ï¸ Unified Timing Summary</h2>
                <span id="toggle-timing-content" class="section-toggle">â–¼</span>
            </div>
            <div id="timing-content" class="section-content">
                <p><strong>Latest Work Directory:</strong> {work_dir}</p>
                
                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin-top: 20px;">
                    <!-- Setup Scenario -->
                    <div>
                        <h3 style="color: #3498db; margin-bottom: 15px;">Setup Scenario</h3>
                        <div class="info" style="margin-bottom: 15px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px; grid-column: 1 / -1;">
                                <div><strong>WNS:</strong> <span class="{'value-green' if isinstance(setup_wns, (int, float)) and setup_wns >= 0 else 'value-red'}">{f"{setup_wns:.3f}" if isinstance(setup_wns, (int, float)) else setup_wns} ns</span></div>
                                <div><strong>TNS:</strong> <span class="{'value-green' if isinstance(setup_tns, (int, float)) and setup_tns >= 0 else 'value-red'}">{f"{setup_tns:.2f}" if isinstance(setup_tns, (int, float)) else setup_tns} ns</span></div>
                                <div><strong>NVP:</strong> {setup_nvp if isinstance(setup_nvp, int) else 'N/A'}</div>
                                <div><strong>DSR Skew:</strong> <span class="{'value-green' if isinstance(setup_dsr, (int, float)) and setup_dsr <= 10 else 'value-red'}">{f"{setup_dsr:.2f}" if isinstance(setup_dsr, (int, float)) else setup_dsr} ps</span></div>
                            </div>
                        </div>
                        
                        <div class="table-wrapper">
                            <table>
                                <thead>
                                    <tr>
                                        <th>Path Group</th>
                                        <th>WNS (ns)</th>
                                        <th>TNS (ns)</th>
                                        <th>NVP</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr style="background-color: #e8f4f8; font-weight: bold;">
                                        <td>Internal Total</td>
                                        <td class="center">{f"{setup_internal_wns:.3f}" if isinstance(setup_internal_wns, (int, float)) else 'N/A'}</td>
                                        <td class="center">{f"{setup_internal_tns:.2f}" if isinstance(setup_internal_tns, (int, float)) else 'N/A'}</td>
                                        <td class="center">{setup_internal_nvp if isinstance(setup_internal_nvp, int) else 'N/A'}</td>
                                    </tr>
                                    {setup_rows}
                                </tbody>
                            </table>
                        </div>
                    </div>
                    
                    <!-- Hold Scenario -->
                    <div>
                        <h3 style="color: #9b59b6; margin-bottom: 15px;">Hold Scenario</h3>
                        <div class="info" style="margin-bottom: 15px;">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px; grid-column: 1 / -1;">
                                <div><strong>WNS:</strong> <span class="{'value-green' if isinstance(hold_wns, (int, float)) and hold_wns >= 0 else 'value-red'}">{f"{hold_wns:.3f}" if isinstance(hold_wns, (int, float)) else hold_wns} ns</span></div>
                                <div><strong>TNS:</strong> <span class="{'value-green' if isinstance(hold_tns, (int, float)) and hold_tns >= 0 else 'value-red'}">{f"{hold_tns:.2f}" if isinstance(hold_tns, (int, float)) else hold_tns} ns</span></div>
                                <div><strong>NVP:</strong> {hold_nvp if isinstance(hold_nvp, int) else 'N/A'}</div>
                                <div><strong>DSR Skew:</strong> <span class="{'value-green' if isinstance(hold_dsr, (int, float)) and hold_dsr <= 10 else 'value-red'}">{f"{hold_dsr:.2f}" if isinstance(hold_dsr, (int, float)) else hold_dsr} ps</span></div>
                            </div>
                        </div>
                        
                        <div class="table-wrapper">
                            <table>
                                <thead>
                                    <tr>
                                        <th>Path Group</th>
                                        <th>WNS (ns)</th>
                                        <th>TNS (ns)</th>
                                        <th>NVP</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr style="background-color: #f4e8f8; font-weight: bold;">
                                        <td>Internal Total</td>
                                        <td class="center">{f"{hold_internal_wns:.3f}" if isinstance(hold_internal_wns, (int, float)) else 'N/A'}</td>
                                        <td class="center">{f"{hold_internal_tns:.2f}" if isinstance(hold_internal_tns, (int, float)) else 'N/A'}</td>
                                        <td class="center">{hold_internal_nvp if isinstance(hold_internal_nvp, int) else 'N/A'}</td>
                                    </tr>
                                    {hold_rows}
                                </tbody>
                            </table>
                        </div>
                    </div>
                </div>
                
                <div style="margin-top: 20px;">
                    <button class="btn btn-sm btn-success" onclick="exportTableToCSV('timing-table', 'timing_summary.csv')">
                        Export to CSV
                    </button>
                </div>
            </div>
        </div>
        """
    
    def _generate_dsr_skew_trend_html(self, timing_data: List[Dict]) -> str:
        """Generate Enhanced DSR Skew Trend section with Chart.js visualization
        
        Args:
            timing_data: List of timing data per work directory
        
        Returns:
            HTML string for DSR skew trend (or info message if no DSR data found)
        """
        if not timing_data:
            return """
            <div class="section">
                <div class="section-header" onclick="toggleSection('dsr-content')">
                    <h2 class="section-title">ðŸ“ˆ DSR Skew Trend</h2>
                    <span id="toggle-dsr-content" class="section-toggle">â–¼</span>
                </div>
                <div id="dsr-content" class="section-content">
                    <p>No DSR skew data available.</p>
                </div>
            </div>
            """
        
        # Collect DSR skew data for chart
        work_dirs = []
        setup_dsr = []
        hold_dsr = []
        has_any_dsr_data = False
        
        for wd in timing_data:
            work_dir = wd.get('work_dir', 'N/A')
            work_dirs.append(work_dir)
            
            # DSR skew is stored at top level: dsr_skew_setup and dsr_skew_hold
            setup_val = wd.get('dsr_skew_setup', None)
            hold_val = wd.get('dsr_skew_hold', None)
            
            # Check if ANY DSR data exists
            if setup_val is not None or hold_val is not None:
                has_any_dsr_data = True
            
            setup_dsr.append(setup_val if setup_val is not None else 0)
            hold_dsr.append(hold_val if hold_val is not None else 0)
        
        # If no DSR data found in any work directory, return info message
        if not has_any_dsr_data:
            return """
            <div class="section">
                <div class="section-header" onclick="toggleSection('dsr-content')">
                    <h2 class="section-title">ðŸ“ˆ DSR Mux Clock Skew</h2>
                    <span id="toggle-dsr-content" class="section-toggle">â–¼</span>
                </div>
                <div id="dsr-content" class="section-content">
                    <div class="info-box" style="background: #fff3cd; border-left: 4px solid #ffc107; padding: 15px; border-radius: 4px;">
                        <h3 style="color: #856404; margin-top: 0;">DSR Mux Clock Skew Files Not Found</h3>
                        <p style="color: #856404; margin-bottom: 0;">
                            The DSR mux clock skew files (<code>*.dsr_mux_clock_skew</code>) were not found in any work directory.
                            This is expected if the design does not use DSR (Domain-Specific Router) mux clocking architecture.
                        </p>
                    </div>
                </div>
            </div>
            """
        
        # Statistics
        all_values = [v for v in setup_dsr + hold_dsr if v is not None and v != 0]
        if all_values:
            min_dsr = min(all_values)
            max_dsr = max(all_values)
            avg_dsr = sum(all_values) / len(all_values)
        else:
            min_dsr = max_dsr = avg_dsr = 0
        
        # Build table rows
        table_rows = ""
        for idx, wd in enumerate(timing_data):
            work_dir = wd.get('work_dir', 'N/A')
            setup_val = setup_dsr[idx]
            hold_val = hold_dsr[idx]
            
            # Color code
            setup_class = 'value-green' if setup_val <= 10 else 'value-yellow' if setup_val <= 20 else 'value-red'
            hold_class = 'value-green' if hold_val <= 10 else 'value-yellow' if hold_val <= 20 else 'value-red'
            
            # Source files
            setup_source = wd.get('scenarios', {}).get('setup', {}).get('dsr_source_file', 'N/A')
            hold_source = wd.get('scenarios', {}).get('hold', {}).get('dsr_source_file', 'N/A')
            
            table_rows += f"""
            <tr>
                <td><strong>{work_dir}</strong></td>
                <td class="center"><span class="{setup_class}">{f"{setup_val:.2f}" if isinstance(setup_val, (int, float)) else 'N/A'} ps</span></td>
                <td class="center"><span class="{hold_class}">{f"{hold_val:.2f}" if isinstance(hold_val, (int, float)) else 'N/A'} ps</span></td>
                <td style="font-size: 11px;">{setup_source if setup_source != 'N/A' else '-'}</td>
            </tr>
            """
        
        # Prepare chart data (JavaScript arrays)
        work_dirs_js = str(work_dirs)
        setup_dsr_js = str(setup_dsr)
        hold_dsr_js = str(hold_dsr)
        
        return f"""
        <div class="section">
            <div class="section-header" onclick="toggleSection('dsr-content')">
                <h2 class="section-title">ðŸ“ˆ DSR Skew Trend</h2>
                <span id="toggle-dsr-content" class="section-toggle">â–¼</span>
            </div>
            <div id="dsr-content" class="section-content">
                <div class="cards-grid" style="margin-bottom: 20px;">
                    <div class="card info">
                        <div class="card-title">Min DSR Skew</div>
                        <div class="card-value">{f"{min_dsr:.2f}"} ps</div>
                        <div class="card-subtitle">Best across all runs</div>
                    </div>
                    <div class="card warning">
                        <div class="card-title">Max DSR Skew</div>
                        <div class="card-value">{f"{max_dsr:.2f}"} ps</div>
                        <div class="card-subtitle">Worst across all runs</div>
                    </div>
                    <div class="card success">
                        <div class="card-title">Avg DSR Skew</div>
                        <div class="card-value">{f"{avg_dsr:.2f}"} ps</div>
                        <div class="card-subtitle">Average across all runs</div>
                    </div>
                </div>
                
                <h3 style="margin-bottom: 15px;">DSR Skew Progression</h3>
                <div class="chart-container">
                    <canvas id="dsr-chart"></canvas>
                </div>
                
                <h3 style="margin-top: 30px; margin-bottom: 15px;">DSR Skew by Work Directory</h3>
                <div class="table-wrapper">
                    <table>
                        <thead>
                            <tr>
                                <th>Work Directory</th>
                                <th>Setup DSR</th>
                                <th>Hold DSR</th>
                                <th>Source File</th>
                            </tr>
                        </thead>
                        <tbody>
                            {table_rows}
                        </tbody>
                    </table>
                </div>
                
                <script>
                const ctx = document.getElementById('dsr-chart');
                if (ctx) {{
                    new Chart(ctx, {{
                        type: 'line',
                        data: {{
                            labels: {work_dirs_js},
                            datasets: [
                                {{
                                    label: 'Setup DSR Skew (ps)',
                                    data: {setup_dsr_js},
                                    borderColor: '#3498db',
                                    backgroundColor: 'rgba(52, 152, 219, 0.1)',
                                    tension: 0.3,
                                    fill: true
                                }},
                                {{
                                    label: 'Hold DSR Skew (ps)',
                                    data: {hold_dsr_js},
                                    borderColor: '#9b59b6',
                                    backgroundColor: 'rgba(155, 89, 182, 0.1)',
                                    tension: 0.3,
                                    fill: true
                                }}
                            ]
                        }},
                        options: {{
                            responsive: true,
                            maintainAspectRatio: false,
                            plugins: {{
                                legend: {{
                                    display: true,
                                    position: 'top'
                                }},
                                title: {{
                                    display: true,
                                    text: 'DSR Skew Trend Across PT Runs'
                                }}
                            }},
                            scales: {{
                                y: {{
                                    beginAtZero: true,
                                    title: {{
                                        display: true,
                                        text: 'DSR Skew (ps)'
                                    }}
                                }},
                                x: {{
                                    ticks: {{
                                        maxRotation: 45,
                                        minRotation: 45
                                    }}
                                }}
                            }}
                        }}
                    }});
                }}
                </script>
            </div>
        </div>
        """
    
    def _generate_power_analysis_html(self, timing_data: List[Dict]) -> str:
        """Generate Power Analysis section
        
        Args:
            timing_data: List of timing data per work directory
        
        Returns:
            HTML string for power analysis
        """
        # Find work area with power data (prefer latest)
        power_work_to_display = None
        is_latest_power = False
        power_work_name = None
        
        for wd in timing_data:
            if wd.get('power_data'):
                power_work_to_display = wd
                if wd == timing_data[0]:
                    is_latest_power = True
                power_work_name = wd['work_dir']
                break
        
        if not power_work_to_display:
            return """
            <div class="section">
                <div class="section-header" onclick="toggleSection('power-content')">
                    <h2 class="section-title">âš¡ Power Analysis</h2>
                    <span id="toggle-power-content" class="section-toggle">â–¼</span>
                </div>
                <div id="power-content" class="section-content collapsed">
                    <p>No power analysis data available.</p>
                    <p style="color: #f39c12;">Power corner (func.std_tt_105c_0p67v.setup.typical) was not run in any work area</p>
                </div>
            </div>
            """
        
        # Build power data display
        power_data = power_work_to_display['power_data']
        corner = power_data.get('corner', 'N/A')
        note = power_data.get('note', '')
        total_tests = power_data.get('total_tests', 0)
        fsdb_tests = power_data.get('fsdb_tests', [])
        
        # Sort tests by total power (descending)
        sorted_tests = sorted(fsdb_tests, key=lambda x: x['metrics'].get('total_power', 0), reverse=True)
        
        # Build FSDB tests table
        test_rows = ""
        for fsdb_test in sorted_tests:
            test_name = fsdb_test['test_name']
            metrics = fsdb_test['metrics']
            
            total_power = metrics.get('total_power', 0)
            total_unit = metrics.get('total_power_unit', 'mW')
            dynamic_power = metrics.get('dynamic_power', 0)
            dynamic_unit = metrics.get('dynamic_power_unit', 'mW')
            leakage_power = metrics.get('leakage_power', 0)
            leakage_unit = metrics.get('leakage_power_unit', 'mW')
            activity_factor = metrics.get('activity_factor', 'N/A')
            annotation_score = metrics.get('annotation_score', 'N/A')
            
            # Format values
            total_str = f"{total_power:.3f} {total_unit}" if isinstance(total_power, (int, float)) else 'N/A'
            dynamic_str = f"{dynamic_power:.3f} {dynamic_unit}" if isinstance(dynamic_power, (int, float)) else 'N/A'
            leakage_str = f"{leakage_power:.3f} {leakage_unit}" if isinstance(leakage_power, (int, float)) else 'N/A'
            af_str = f"{activity_factor:.2f} %" if isinstance(activity_factor, (int, float)) else 'N/A'
            annot_str = f"{annotation_score:.2f} %" if isinstance(annotation_score, (int, float)) else 'N/A'
            
            test_rows += f"""
            <tr>
                <td>{test_name}</td>
                <td class="right">{total_str}</td>
                <td class="right">{dynamic_str}</td>
                <td class="right">{leakage_str}</td>
                <td class="center">{af_str}</td>
                <td class="center">{annot_str}</td>
            </tr>
            """
        
        # Check if we should show power trend
        power_work_areas = [wd for wd in timing_data if wd.get('power_data')]
        trend_html = ""
        
        if len(power_work_areas) >= 2:
            # Collect all unique test names
            all_test_names = set()
            for wd in power_work_areas:
                for test in wd['power_data']['fsdb_tests']:
                    all_test_names.add(test['test_name'])
            
            # Build trend table
            trend_rows = ""
            for test_name in sorted(all_test_names):
                power_values = []
                for wd in power_work_areas[:5]:  # Show up to 5 most recent
                    for test in wd['power_data']['fsdb_tests']:
                        if test['test_name'] == test_name:
                            if 'total_power' in test['metrics']:
                                power_val = test['metrics']['total_power']
                                unit = test['metrics'].get('total_power_unit', 'mW')
                                power_values.append((wd['work_dir'], power_val, unit))
                            break
                
                if len(power_values) >= 2:
                    latest_power, latest_unit = power_values[0][1], power_values[0][2]
                    oldest_power, oldest_unit = power_values[-1][1], power_values[-1][2]
                    diff = latest_power - oldest_power
                    percent_change = (diff / oldest_power) * 100 if oldest_power != 0 else 0
                    
                    # Color code
                    if diff < 0:
                        trend_class = 'value-green'
                        trend_text = 'decreased'
                    elif diff > 0:
                        trend_class = 'value-red'
                        trend_text = 'increased'
                    else:
                        trend_class = ''
                        trend_text = 'unchanged'
                    
                    trend_rows += f"""
                    <tr>
                        <td>{test_name}</td>
                        <td class="right">{latest_power:.3f} {latest_unit}</td>
                        <td class="right">{oldest_power:.3f} {oldest_unit}</td>
                        <td class="right"><span class="{trend_class}">{diff:+.3f} {latest_unit}</span></td>
                        <td class="right"><span class="{trend_class}">{percent_change:+.1f} %</span></td>
                        <td class="center"><span class="{trend_class}">{trend_text}</span></td>
                    </tr>
                    """
            
            if trend_rows:
                trend_html = f"""
                <h3 style="margin-top: 30px; margin-bottom: 15px;">ðŸ“Š Total Power Trend (All Tests)</h3>
                <div class="table-wrapper">
                    <table>
                        <thead>
                            <tr>
                                <th>Test Name</th>
                                <th style="text-align: right;">Latest</th>
                                <th style="text-align: right;">Oldest</th>
                                <th style="text-align: right;">Change</th>
                                <th style="text-align: right;">% Change</th>
                                <th style="text-align: center;">Trend</th>
                            </tr>
                        </thead>
                        <tbody>
                            {trend_rows}
                        </tbody>
                    </table>
                </div>
                """
        
        # Build final HTML
        latest_note = "" if is_latest_power else f"""
        <p style="color: #f39c12; margin-bottom: 15px;">
            <strong>Note:</strong> Latest work area does not have power corner - showing data from: {power_work_name}
        </p>
        """
        
        return f"""
        <div class="section">
            <div class="section-header" onclick="toggleSection('power-content')">
                <h2 class="section-title">âš¡ Power Analysis</h2>
                <span id="toggle-power-content" class="section-toggle">â–¼</span>
            </div>
            <div id="power-content" class="section-content collapsed">
                <h3 style="margin-bottom: 10px;">Power Analysis Corner: <span style="color: #f39c12;">{corner}</span></h3>
                <p style="color: #888; margin-bottom: 15px;"><em>{note}</em></p>
                {latest_note}
                <p style="margin-bottom: 15px;"><strong>Total FSDB Tests:</strong> {total_tests}</p>
                
                <div class="table-wrapper">
                    <table>
                        <thead>
                            <tr>
                                <th>Test Name</th>
                                <th style="text-align: right;">Total Power</th>
                                <th style="text-align: right;">Dynamic Power</th>
                                <th style="text-align: right;">Leakage Power</th>
                                <th style="text-align: center;">AF (FF/Q)</th>
                                <th style="text-align: center;">Annot Score</th>
                            </tr>
                        </thead>
                        <tbody>
                            {test_rows}
                        </tbody>
                    </table>
                </div>
                
                {trend_html}
            </div>
        </div>
        """
    
    def _generate_report_navigator_html(self, timing_data: List[Dict], pt_locations: List[Dict] = None) -> str:
        """Generate Report Navigator section with high-level summary reports
        
        This section provides quick access to summary/analysis reports organized by IPO location.
        Detailed per-work-directory reports are in the Location Explorer table.
        
        Args:
            timing_data: List of timing data per work directory
            pt_locations: List of PT location dictionaries (optional)
        
        Returns:
            HTML string for report navigator
        """
        if not timing_data:
            return ""
        
        # If pt_locations not provided, organize by work directory (fallback)
        if not pt_locations:
            pt_locations = []
        
        # Organize reports by IPO location (high-level view)
        location_reports = {}
        
        for loc in pt_locations:
            location_label = loc.get('label', loc.get('ipo', 'N/A'))
            auto_pt_path = loc.get('path', '')
            
            if not auto_pt_path or not os.path.exists(auto_pt_path):
                continue
            
            reports = []
            
            # Auto PT Fix log (if exists)
            auto_pt_fix_log = os.path.join(auto_pt_path, "log/auto_pt_fix.log")
            if os.path.exists(auto_pt_fix_log):
                reports.append(('Auto PT Fix Log', os.path.abspath(auto_pt_fix_log), 'ðŸ“‹'))
            
            # Auto PT main log (fallback if no fix log)
            if not reports:
                auto_pt_log = os.path.join(auto_pt_path, "log/auto_pt.log")
                if os.path.exists(auto_pt_log):
                    reports.append(('Auto PT Log', os.path.abspath(auto_pt_log), 'ðŸ“‹'))
            
            # Find one representative DSR skew file per scenario (from latest work dir)
            work_dirs = loc.get('work_dirs', [])
            if work_dirs:
                # Get latest work directory
                latest_wd = work_dirs[0]  # Already sorted by timestamp
                
                # Find matching timing data for this work directory
                wd_timing = next((t for t in timing_data if latest_wd in t.get('work_dir', '')), None)
                if wd_timing:
                    for scenario_name in ['setup', 'hold']:
                        scenario_data = wd_timing.get('scenarios', {}).get(scenario_name, {})
                        dsr_file = scenario_data.get('dsr_source_file', '')
                        if dsr_file and os.path.exists(dsr_file):
                            reports.append((f'DSR Skew Report ({scenario_name.title()})', dsr_file, 'ðŸ“Š'))
            
            # ALWAYS add location to navigator (even if no reports found yet)
            # This ensures all IPO locations are visible
            location_reports[location_label] = reports if reports else [('No summary reports', '', '')]
        
        if not location_reports:
            return f"""
            <div class="section">
                <div class="section-header" onclick="toggleSection('navigator-content')">
                    <h2 class="section-title">ðŸ“‚ Report Navigator</h2>
                    <span id="toggle-navigator-content" class="section-toggle">â–¼</span>
                </div>
                <div id="navigator-content" class="section-content collapsed">
                    <p style="color: #888; font-style: italic;">No summary reports available. See Location Explorer above for per-work-directory reports.</p>
                </div>
            </div>
            """
        
        # Build HTML for each location
        navigator_sections = ""
        for location_label, reports in sorted(location_reports.items()):
            links_html = ""
            for report_name, report_path, icon in reports:
                # Skip empty entries
                if not report_path:
                    links_html += f"""
                    <div style="margin-bottom: 8px; color: #888; font-style: italic; font-size: 12px;">
                        {icon} {report_name}
                    </div>
                    """
                else:
                    links_html += f"""
                    <div style="margin-bottom: 8px; display: flex; align-items: center;">
                        <button class="btn btn-sm" onclick="openLogWithServer('{report_path}', event)" style="margin-right: 8px; background: #667eea; color: white; border: none; padding: 6px 12px; border-radius: 4px; font-size: 11px; cursor: pointer;">
                            {icon} View in Tablog
                        </button>
                        <span style="font-size: 12px; color: #555;">{report_name}</span>
                    </div>
                    """
            
            navigator_sections += f"""
            <div style="margin-bottom: 15px; padding: 12px; border-left: 4px solid #667eea; background: #f5f7fa; border-radius: 4px;">
                <h4 style="margin: 0 0 10px 0; color: #2c3e50; font-size: 14px;">{location_label}</h4>
                {links_html}
            </div>
            """
        
        return f"""
        <div class="section">
            <div class="section-header" onclick="toggleSection('navigator-content')">
                <h2 class="section-title">ðŸ“‚ Report Navigator</h2>
                <span id="toggle-navigator-content" class="section-toggle">â–¼</span>
            </div>
            <div id="navigator-content" class="section-content collapsed">
                <p style="color: #555; margin-bottom: 15px; font-size: 13px;">
                    <strong>Quick access to summary reports by IPO location.</strong> For detailed per-work-directory reports, see the Location Explorer table above.
                </p>
                {navigator_sections}
            </div>
        </div>
        """
    
    def _generate_timing_summary_html(self, timing_data: Dict[str, Any], pt_locations: List[Dict[str, Any]] = None) -> str:
        """Generate comprehensive HTML report with PT timing analysis
        
        REFACTORED: Now uses modular helper functions for maintainability
        
        Args:
            timing_data: Dictionary or List containing timing analysis data
            pt_locations: Optional list of PT location info
            
        Returns:
            HTML string containing comprehensive timing summary
        """
        
        # Handle both old and new data formats
        if isinstance(timing_data, dict):
            timing_data_list = timing_data.get('timing_data', [])
        elif isinstance(timing_data, list):
            timing_data_list = timing_data
        else:
            timing_data_list = []
        
        if not timing_data_list:
            return None
        
        # Prepare enhanced data structure with all metadata
        if not pt_locations:
            pt_locations = []
        enhanced_data = self._prepare_enhanced_timing_data(timing_data_list, pt_locations)
        
        # Generate timestamp for filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        html_filename = f"{self.design_info.top_hier}_{os.environ.get('USER', 'avice')}_PT_timing_summary_{timestamp}.html"
        
        # Get modular HTML components
        css_styles = self._get_pt_html_common_css()
        js_code = self._get_pt_html_common_js()
        chartjs_lib = self._get_chartjs_library()
        header_html = self._get_pt_html_header(
            title=f"Auto PT Timing Summary Report - {self.design_info.top_hier}",
            subtitle=f"Workarea: {self.workarea}"
        )
        footer_html = self._get_pt_html_footer()
        
        # Generate section HTML components
        overview_html = self._generate_overview_dashboard_html(enhanced_data)
        location_html = self._generate_location_explorer_html(pt_locations, timing_data_list)
        quality_html = self._generate_quality_metrics_html(timing_data_list, enhanced_data)
        waiver_html = self._generate_pt_waiver_gap_html(timing_data_list)
        timing_html = self._generate_enhanced_timing_summary_html(timing_data_list, enhanced_data)
        dsr_html = self._generate_dsr_skew_trend_html(timing_data_list)
        power_html = self._generate_power_analysis_html(timing_data_list)
        navigator_html = self._generate_report_navigator_html(timing_data_list, pt_locations)
        
        # Assemble complete HTML document
        html_content = f"""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Auto PT Timing Summary Report - {self.design_info.top_hier}</title>
    <style>
        {css_styles}
    </style>
    {chartjs_lib}
</head>
<body>
    <div class="container">
        {header_html}
        {overview_html}
        {location_html}
        {quality_html}
        {waiver_html}
        {timing_html}
        {dsr_html}
        {power_html}
        {navigator_html}
        {footer_html}
    </div>
    <script>
        {js_code}
    </script>
</body>
</html>"""
        
        # Write HTML file in current working directory to avoid permission issues
        # Use absolute paths in HTML content to ensure links work from any location
        html_path = os.path.join(os.getcwd(), html_filename)
        with open(html_path, 'w') as f:
            f.write(html_content)
        
        return os.path.abspath(html_path)
    
    def _generate_timing_summary_report(self, pt_locations: List[Dict[str, Any]] = None) -> Tuple[Optional[str], List[Dict[str, Any]]]:
        """Generate timing summary report with dual-scenario and DSR skew tracking
        
        Args:
            pt_locations: List of PT location dictionaries from _find_all_auto_pt_locations()
        
        Returns:
            Tuple of (HTML filename if generated successfully or None, timing_data list)
        """
        # Extract timing data from all PT work directories across all locations
        timing_data = self._extract_timing_data_from_work_areas(pt_locations)
        
        if not timing_data:
            print(f"  {Color.YELLOW}No timing data found{Color.RESET}")
            return None, []
        
        # Generate HTML report
        try:
            html_file = self._generate_timing_summary_html(timing_data, pt_locations)
            if html_file:
                print(f"\n  {Color.GREEN}PT Timing Summary HTML: {html_file}{Color.RESET}")
                return html_file, timing_data
        except Exception as e:
            print(f"  {Color.RED}Error generating PT HTML report: {e}{Color.RESET}")
            import traceback
            traceback.print_exc()
        
        return None, timing_data
    
    def _generate_timing_summary_report(self, pt_locations: List[Dict[str, Any]] = None) -> Tuple[Optional[str], List[Dict[str, Any]]]:
        """Generate timing summary report with dual-scenario and DSR skew tracking
        
        Args:
            pt_locations: List of PT location dictionaries from _find_all_auto_pt_locations()
        
        Returns:
            Tuple of (HTML filename if generated successfully or None, timing_data list)
        """
        
        # Extract timing data from all work areas across all locations
        timing_data = self._extract_timing_data_from_work_areas(pt_locations)
        
        if timing_data:
            # Generate HTML report
            html_filename = self._generate_timing_summary_html(timing_data, pt_locations)
            
            if html_filename:
                # Show unified Setup/Hold timing summary table
                if len(timing_data) >= 1:
                    latest = timing_data[0]
                    
                    # Check if we have both setup and hold scenarios
                    has_setup = 'setup' in latest['scenarios']
                    has_hold = 'hold' in latest['scenarios']
                    
                    if has_setup or has_hold:
                        print(f"\n  {Color.CYAN}Timing Summary (Latest: {latest['work_dir']}):{Color.RESET}")
                        
                        # Get scenario data
                        setup_data = latest['scenarios'].get('setup', {})
                        hold_data = latest['scenarios'].get('hold', {})
                        
                        # Get corner names
                        setup_corner = setup_data.get('name', 'N/A') if has_setup else 'N/A'
                        hold_corner = hold_data.get('name', 'N/A') if has_hold else 'N/A'
                        
                        print(f"    Setup Corner: {Color.YELLOW}{setup_corner}{Color.RESET}")
                        print(f"    Hold Corner:  {Color.YELLOW}{hold_corner}{Color.RESET}")
                        
                        # Define external timing groups and groups to exclude from individual display
                        external_groups = {'FEEDTHROUGH', 'REGIN', 'REGOUT'}
                        excluded_groups = external_groups | {'TOTAL'}  # Also exclude 'TOTAL' to avoid duplication
                        
                        # Collect all unique path groups (internal only, excluding total)
                        all_groups = set()
                        if has_setup:
                            all_groups.update([g for g in setup_data.get('groups', {}).keys() if g.upper() not in excluded_groups])
                        if has_hold:
                            all_groups.update([g for g in hold_data.get('groups', {}).keys() if g.upper() not in excluded_groups])
                        
                        # Sort groups alphabetically
                        sorted_groups = sorted(all_groups)
                        
                        # Calculate totals for internal groups (excluding external groups and any 'TOTAL' group from data)
                        setup_total_wns = None
                        setup_total_tns = 0
                        setup_total_nvp = 0
                        hold_total_wns = None
                        hold_total_tns = 0
                        hold_total_nvp = 0
                        
                        if has_setup:
                            for group_name, group_data in setup_data.get('groups', {}).items():
                                if group_name.upper() not in excluded_groups:
                                    setup_total_tns += group_data['TNS']
                                    setup_total_nvp += group_data['NVP']
                                    if setup_total_wns is None or group_data['WNS'] < setup_total_wns:
                                        setup_total_wns = group_data['WNS']
                        
                        if has_hold:
                            for group_name, group_data in hold_data.get('groups', {}).items():
                                if group_name.upper() not in excluded_groups:
                                    hold_total_tns += group_data['TNS']
                                    hold_total_nvp += group_data['NVP']
                                    if hold_total_wns is None or group_data['WNS'] < hold_total_wns:
                                        hold_total_wns = group_data['WNS']
                        
                        # Print unified table header
                        print(f"\n    {'Path Group':<20} {'Setup WNS':>11} {'Setup TNS':>11} {'Setup NVP':>10} {'Hold WNS':>11} {'Hold TNS':>11} {'Hold NVP':>10}")
                        print(f"    {'-'*20} {'-'*11} {'-'*11} {'-'*10} {'-'*11} {'-'*11} {'-'*10}")
                        
                        # Print TOTAL row first (internal groups only)
                        setup_wns_str = f"{setup_total_wns:7.3f} ns" if setup_total_wns is not None else "N/A"
                        setup_tns_str = f"{setup_total_tns:9.2f} ns" if setup_total_wns is not None else "N/A"
                        setup_nvp_str = f"{setup_total_nvp:5d} paths" if setup_total_wns is not None else "N/A"
                        hold_wns_str = f"{hold_total_wns:7.3f} ns" if hold_total_wns is not None else "N/A"
                        hold_tns_str = f"{hold_total_tns:9.2f} ns" if hold_total_wns is not None else "N/A"
                        hold_nvp_str = f"{hold_total_nvp:5d} paths" if hold_total_wns is not None else "N/A"
                        
                        # Color coding for totals
                        setup_wns_color = Color.GREEN if setup_total_wns is not None and setup_total_wns >= 0 else Color.RED if setup_total_wns is not None else Color.RESET
                        setup_tns_color = Color.GREEN if setup_total_wns is not None and setup_total_tns >= 0 else Color.RED if setup_total_wns is not None else Color.RESET
                        hold_wns_color = Color.GREEN if hold_total_wns is not None and hold_total_wns >= 0 else Color.RED if hold_total_wns is not None else Color.RESET
                        hold_tns_color = Color.GREEN if hold_total_wns is not None and hold_total_tns >= 0 else Color.RED if hold_total_wns is not None else Color.RESET
                        
                        print(f"    {'TOTAL (Internal)':<20} {setup_wns_color}{setup_wns_str:>11}{Color.RESET} {setup_tns_color}{setup_tns_str:>11}{Color.RESET} {setup_nvp_str:>10} {hold_wns_color}{hold_wns_str:>11}{Color.RESET} {hold_tns_color}{hold_tns_str:>11}{Color.RESET} {hold_nvp_str:>10}")
                        
                        # Print separator
                        print(f"    {'-'*20} {'-'*11} {'-'*11} {'-'*10} {'-'*11} {'-'*11} {'-'*10}")
                        
                        # Print each path group
                        for group_name in sorted_groups:
                            # Get setup values
                            setup_group = setup_data.get('groups', {}).get(group_name, {})
                            setup_wns = setup_group.get('WNS')
                            setup_tns = setup_group.get('TNS')
                            setup_nvp = setup_group.get('NVP')
                            
                            # Get hold values
                            hold_group = hold_data.get('groups', {}).get(group_name, {})
                            hold_wns = hold_group.get('WNS')
                            hold_tns = hold_group.get('TNS')
                            hold_nvp = hold_group.get('NVP')
                            
                            # Format values
                            setup_wns_str = f"{setup_wns:7.3f} ns" if setup_wns is not None else "N/A"
                            setup_tns_str = f"{setup_tns:9.2f} ns" if setup_tns is not None else "N/A"
                            setup_nvp_str = f"{setup_nvp:5d} paths" if setup_nvp is not None else "N/A"
                            hold_wns_str = f"{hold_wns:7.3f} ns" if hold_wns is not None else "N/A"
                            hold_tns_str = f"{hold_tns:9.2f} ns" if hold_tns is not None else "N/A"
                            hold_nvp_str = f"{hold_nvp:5d} paths" if hold_nvp is not None else "N/A"
                            
                            # Color coding
                            setup_wns_color = Color.GREEN if setup_wns is not None and setup_wns >= 0 else Color.RED if setup_wns is not None else Color.RESET
                            setup_tns_color = Color.GREEN if setup_tns is not None and setup_tns >= 0 else Color.RED if setup_tns is not None else Color.RESET
                            hold_wns_color = Color.GREEN if hold_wns is not None and hold_wns >= 0 else Color.RED if hold_wns is not None else Color.RESET
                            hold_tns_color = Color.GREEN if hold_tns is not None and hold_tns >= 0 else Color.RED if hold_tns is not None else Color.RESET
                            
                            print(f"    {group_name:<20} {setup_wns_color}{setup_wns_str:>11}{Color.RESET} {setup_tns_color}{setup_tns_str:>11}{Color.RESET} {setup_nvp_str:>10} {hold_wns_color}{hold_wns_str:>11}{Color.RESET} {hold_tns_color}{hold_tns_str:>11}{Color.RESET} {hold_nvp_str:>10}")
                    
                # Show unified Quality Metrics table for multiple work areas
                if len(timing_data) >= 1:
                    print(f"\n  {Color.CYAN}Quality Metrics (Setup Corner Only):{Color.RESET}")
                    
                    # Collect quality data for all work areas (setup corner only)
                    quality_data = []
                    
                    for wd in timing_data[:5]:  # Show up to 5 most recent
                        work_name = wd['work_dir']
                        location_label = wd.get('location_label', '')
                        work_label = f"{work_name} [{location_label}]" if location_label else work_name
                        
                        # Extract setup scenario data
                        setup_data = wd.get('scenarios', {}).get('setup', {})
                        
                        # Data signal violations
                        data_max_tran = setup_data.get('data_max_transition', 0)
                        data_max_cap = setup_data.get('data_max_capacitance', 0)
                        data_max_fanout = setup_data.get('data_max_fanout', 0)
                        
                        # Clock signal violations
                        clock_max_tran = setup_data.get('clock_max_transition', 0)
                        clock_max_cap = setup_data.get('clock_max_capacitance', 0)
                        
                        # Clock quality violations (slew, xcap)
                        clock_slew = 0
                        clock_xcap = 0
                        clock_quality = wd.get('clock_quality', {})
                        if 'setup' in clock_quality and 'violations' in clock_quality['setup']:
                            clock_slew = clock_quality['setup']['violations'].get('slew_violations', 0)
                            clock_xcap = clock_quality['setup']['violations'].get('xcap_violations', 0)
                        
                        quality_data.append({
                            'work_label': work_label,
                            'data_max_tran': data_max_tran,
                            'data_max_cap': data_max_cap,
                            'data_max_fanout': data_max_fanout,
                            'clock_max_tran': clock_max_tran,
                            'clock_max_cap': clock_max_cap,
                            'clock_slew': clock_slew,
                            'clock_xcap': clock_xcap
                        })
                    
                    # Print unified quality table
                    if quality_data:
                        print(f"\n    {'Work Directory':<30} {'-- Data Signals --':>24} {'-- Clock Signals --':>25}")
                        print(f"    {'':<30} {'Tran':>7} {'Cap':>7} {'Fanout':>7}   {'Tran':>7} {'Cap':>7} {'Slew':>7} {'XCap':>7}")
                        print(f"    {'-'*30} {'-'*7} {'-'*7} {'-'*7}   {'-'*7} {'-'*7} {'-'*7} {'-'*7}")
                        
                        for qd in quality_data:
                            # Color code violations (0 = green, >0 = red)
                            data_tran_color = Color.GREEN if qd['data_max_tran'] == 0 else Color.RED
                            data_cap_color = Color.GREEN if qd['data_max_cap'] == 0 else Color.RED
                            data_fanout_color = Color.GREEN if qd['data_max_fanout'] == 0 else Color.RED
                            clock_tran_color = Color.GREEN if qd['clock_max_tran'] == 0 else Color.RED
                            clock_cap_color = Color.GREEN if qd['clock_max_cap'] == 0 else Color.RED
                            clock_slew_color = Color.GREEN if qd['clock_slew'] == 0 else Color.RED
                            clock_xcap_color = Color.GREEN if qd['clock_xcap'] == 0 else Color.RED
                            
                            print(f"    {qd['work_label']:<30} {data_tran_color}{qd['data_max_tran']:>7}{Color.RESET} {data_cap_color}{qd['data_max_cap']:>7}{Color.RESET} {data_fanout_color}{qd['data_max_fanout']:>7}{Color.RESET}   {clock_tran_color}{qd['clock_max_tran']:>7}{Color.RESET} {clock_cap_color}{qd['clock_max_cap']:>7}{Color.RESET} {clock_slew_color}{qd['clock_slew']:>7}{Color.RESET} {clock_xcap_color}{qd['clock_xcap']:>7}{Color.RESET}")
                        
                        print(f"\n    {Color.CYAN}[Source: *.all_violators.*.gz, *.clock_violators.gz, *.clock_slopes_and_xcap]{Color.RESET}")
                
                # Show PT waiver gaps (auto_pt_slave.err vs central pt.csv) for latest work area
                waiver_summary = self._summarize_pt_waiver_gaps(latest)
                print(f"\n  {Color.CYAN}PT Waiver Gaps (central pt.csv):{Color.RESET}")
                
                if not waiver_summary.get("csv_loaded", True):
                    error_text = waiver_summary.get("csv_error", "pt.csv not available")
                    print(f"    {Color.YELLOW}[WARN] Waiver check skipped: {error_text}{Color.RESET}")
                elif waiver_summary.get("unwaived_total", 0) == 0:
                    print(f"    {Color.GREEN}[OK] No unwaived PT errors/warnings found{Color.RESET}")
                else:
                    print(f"    {Color.YELLOW}[WARN] Unwaived PT errors/warnings detected{Color.RESET}")
                    print(f"    {'Scenario':<10} {'Unwaived':>9} {'Unique':>8} {'Err File':<20}")
                    print(f"    {'-'*10} {'-'*9} {'-'*8} {'-'*20}")
                    
                    for scenario, gaps in waiver_summary.get("by_scenario", {}).items():
                        if not gaps.get("csv_loaded", True):
                            continue
                        unwaived_total = gaps.get("unwaived_total", 0)
                        if unwaived_total == 0:
                            continue
                        err_file = os.path.basename(gaps.get("err_file", ""))
                        unique_count = gaps.get("unwaived_unique", 0)
                        print(f"    {scenario:<10} {unwaived_total:>9} {unique_count:>8} {err_file:<20}")
                    
                    print(f"    {Color.CYAN}[Source: auto_pt_slave.err]{Color.RESET}")
                
                # Show DSR skew trend if multiple work areas (unified table for setup and hold)
                if len(timing_data) >= 2:
                    # Collect DSR skew data for both scenarios to check if any data exists
                    dsr_data = []
                    setup_values = []
                    hold_values = []
                    
                    for wd in timing_data[:5]:  # Check up to 5 most recent
                        work_name = wd['work_dir']
                        setup_skew = wd.get('dsr_skew_setup')
                        hold_skew = wd.get('dsr_skew_hold')
                        
                        # Color coding for each value
                        setup_color = Color.RESET
                        hold_color = Color.RESET
                        
                        if setup_skew is not None:
                            setup_color = Color.GREEN if setup_skew <= 10 else Color.YELLOW if setup_skew <= 20 else Color.RED
                            setup_values.append(setup_skew)
                        
                        if hold_skew is not None:
                            hold_color = Color.GREEN if hold_skew <= 10 else Color.YELLOW if hold_skew <= 20 else Color.RED
                            hold_values.append(hold_skew)
                        
                        dsr_data.append((work_name, setup_skew, setup_color, hold_skew, hold_color))
                    
                    # Check if ANY DSR data exists (not all None)
                    has_any_dsr_data = any(setup_skew is not None or hold_skew is not None 
                                          for _, setup_skew, _, hold_skew, _ in dsr_data)
                    
                    if has_any_dsr_data:
                        # Display DSR Skew Trend section with data
                        print(f"\n  {Color.CYAN}DSR Skew Trend:{Color.RESET}")
                        
                        # Get corner names from latest work area
                        setup_corner = "N/A"
                        hold_corner = "N/A"
                        if 'setup' in latest.get('scenarios', {}):
                            setup_corner = latest['scenarios']['setup'].get('name', 'N/A')
                        if 'hold' in latest.get('scenarios', {}):
                            hold_corner = latest['scenarios']['hold'].get('name', 'N/A')
                        
                        # Display corner names
                        print(f"    Setup Corner: {Color.YELLOW}{setup_corner}{Color.RESET}")
                        print(f"    Hold Corner:  {Color.YELLOW}{hold_corner}{Color.RESET}")
                        print(f"    {Color.CYAN}[Source: *.dsr_mux_clock_skew]{Color.RESET}")
                        
                        # Print unified table
                        print(f"\n    {'Work Directory':<25} {'Setup Skew':>12} {'Hold Skew':>12}")
                        print(f"    {'-'*25} {'-'*12} {'-'*12}")
                        
                        for work_name, setup_skew, setup_color, hold_skew, hold_color in dsr_data:
                            setup_str = f"{setup_color}{setup_skew:6.2f}{Color.RESET} ps" if setup_skew is not None else "N/A"
                            hold_str = f"{hold_color}{hold_skew:6.2f}{Color.RESET} ps" if hold_skew is not None else "N/A"
                            print(f"    {work_name:<25} {setup_str:>20} {hold_str:>20}")
                        
                        # Show trends below the table
                        print()
                        if len(setup_values) >= 2:
                            first_setup = setup_values[0]
                            last_setup = setup_values[-1]
                            setup_diff = first_setup - last_setup
                            setup_trend_color = Color.GREEN if setup_diff < 0 else Color.RED
                            setup_trend_text = "improved" if setup_diff < 0 else "degraded"
                            print(f"    Setup Trend: {setup_trend_color}{abs(setup_diff):.2f} ps {setup_trend_text}{Color.RESET} (newest vs oldest)")
                        
                        if len(hold_values) >= 2:
                            first_hold = hold_values[0]
                            last_hold = hold_values[-1]
                            hold_diff = first_hold - last_hold
                            hold_trend_color = Color.GREEN if hold_diff < 0 else Color.RED
                            hold_trend_text = "improved" if hold_diff < 0 else "degraded"
                            print(f"    Hold Trend:  {hold_trend_color}{abs(hold_diff):.2f} ps {hold_trend_text}{Color.RESET} (newest vs oldest)")
                    else:
                        # No DSR data found - print informational message instead of empty table
                        print(f"\n  {Color.CYAN}DSR Mux Clock Skew:{Color.RESET}")
                        print(f"    {Color.YELLOW}DSR mux clock skew files not found (design may not use DSR mux clocking){Color.RESET}")
                        print(f"    {Color.CYAN}[Source: *.dsr_mux_clock_skew]{Color.RESET}")
                else:
                    # Show DSR skew for latest if only one work area
                    # Check if any DSR data exists first
                    has_dsr_data = (latest.get('dsr_skew_setup') is not None or 
                                   latest.get('dsr_skew_hold') is not None)
                    
                    if has_dsr_data:
                        print(f"\n  {Color.CYAN}Latest DSR Mux Clock Skew:{Color.RESET}")
                        
                        # Get corner names
                        setup_corner = "N/A"
                        hold_corner = "N/A"
                        if 'setup' in latest.get('scenarios', {}):
                            setup_corner = latest['scenarios']['setup'].get('name', 'N/A')
                        if 'hold' in latest.get('scenarios', {}):
                            hold_corner = latest['scenarios']['hold'].get('name', 'N/A')
                        
                        print(f"    Setup Corner: {Color.YELLOW}{setup_corner}{Color.RESET}")
                        print(f"    Hold Corner:  {Color.YELLOW}{hold_corner}{Color.RESET}")
                        print()
                        
                        if latest.get('dsr_skew_setup') is not None:
                            dsr_val_setup = latest['dsr_skew_setup']
                            dsr_color = Color.GREEN if dsr_val_setup <= 10 else Color.YELLOW if dsr_val_setup <= 20 else Color.RED
                            print(f"    Setup: {dsr_color}{dsr_val_setup:6.2f}{Color.RESET} ps")
                        if latest.get('dsr_skew_hold') is not None:
                            dsr_val_hold = latest['dsr_skew_hold']
                            dsr_color = Color.GREEN if dsr_val_hold <= 10 else Color.YELLOW if dsr_val_hold <= 20 else Color.RED
                            print(f"    Hold:  {dsr_color}{dsr_val_hold:6.2f}{Color.RESET} ps")
                    else:
                        # No DSR data found - print informational message
                        print(f"\n  {Color.CYAN}DSR Mux Clock Skew:{Color.RESET}")
                        print(f"    {Color.YELLOW}DSR mux clock skew files not found (design may not use DSR mux clocking){Color.RESET}")
                        print(f"    {Color.CYAN}[Source: *.dsr_mux_clock_skew]{Color.RESET}")
                
                # Show power corner analysis (func.std_tt_105c_0p67v.setup.typical)
                # If latest work area doesn't have power data, check previous work areas
                power_work_to_display = None
                power_work_name = None
                is_latest_power = False
                
                if latest.get('power_data'):
                    power_work_to_display = latest
                    power_work_name = latest['work_dir']
                    is_latest_power = True
                else:
                    # Check previous work areas for power data (most recent first)
                    for wd in timing_data[1:]:  # Skip first (latest) since we already checked
                        if wd.get('power_data'):
                            power_work_to_display = wd
                            power_work_name = wd['work_dir']
                            break
                
                if power_work_to_display:
                    power_data = power_work_to_display['power_data']
                    print(f"\n  {Color.CYAN}Power Analysis Corner ({power_data['corner']}):{Color.RESET}")
                    print(f"    {Color.YELLOW}Note: Power estimation corner (not timing-critical){Color.RESET}")
                    
                    # Show note if power data is from previous work area
                    if not is_latest_power:
                        print(f"    {Color.YELLOW}[INFO] Latest work area does not have power corner - showing data from: {power_work_name}{Color.RESET}")
                    
                    print(f"    Total FSDB Tests: {power_data['total_tests']}")
                    
                    # Show ALL FSDB tests in unified table (sorted by total power, highest first)
                    print(f"\n    {'Test Name':<50} {'Total Power':>13} {'Dynamic Power':>15} {'Leakage Power':>15} {'AF (FF/Q)':>11} {'Annot Score':>12}")
                    print(f"    {'-'*50} {'-'*13} {'-'*15} {'-'*15} {'-'*11} {'-'*12}")
                    
                    # Sort tests by total power (descending)
                    sorted_tests = sorted(power_data['fsdb_tests'], 
                                        key=lambda x: x['metrics'].get('total_power', 0), 
                                        reverse=True)
                    
                    for fsdb_test in sorted_tests:
                        test_name = fsdb_test['test_name']
                        metrics = fsdb_test['metrics']
                        
                        # Format values with units
                        total_power = f"{metrics.get('total_power', 0):7.3f} {metrics.get('total_power_unit', 'mW')}" if 'total_power' in metrics else "N/A"
                        dynamic_power = f"{metrics.get('dynamic_power', 0):7.3f} {metrics.get('dynamic_power_unit', 'mW')}" if 'dynamic_power' in metrics else "N/A"
                        leakage_power = f"{metrics.get('leakage_power', 0):7.3f} {metrics.get('leakage_power_unit', 'mW')}" if 'leakage_power' in metrics else "N/A"
                        activity_factor = f"{metrics.get('activity_factor', 0):6.2f} %" if 'activity_factor' in metrics else "N/A"
                        annotation_score = f"{metrics.get('annotation_score', 0):6.2f} %" if 'annotation_score' in metrics else "N/A"
                        
                        # Truncate long test names
                        display_name = test_name if len(test_name) <= 50 else test_name[:47] + "..."
                        
                        print(f"    {display_name:<50} {total_power:>13} {dynamic_power:>15} {leakage_power:>15} {activity_factor:>11} {annotation_score:>12}")
                    
                    # Add explanation for N/A values if any exist
                    has_na_values = any('N/A' in str(test['metrics'].get('annotation_score', '')) or 
                                       test['metrics'].get('annotation_score') is None 
                                       for test in power_data['fsdb_tests'])
                    if has_na_values:
                        print(f"\n    {Color.YELLOW}Note: N/A in Annot Score indicates vectorless power tests (no toggle annotation available){Color.RESET}")
                else:
                    # No power corner found in any work area
                    print(f"\n  {Color.CYAN}Power Analysis Corner:{Color.RESET}")
                    print(f"    {Color.YELLOW}Power corner (func.std_tt_105c_0p67v.setup.typical) was not run in any work area{Color.RESET}")
                
                # Show power trend if multiple work areas with power data (unified table for all tests)
                power_work_areas = [wd for wd in timing_data if wd.get('power_data')]
                if len(power_work_areas) >= 2:
                    print(f"\n  {Color.CYAN}Total Power Trend (All Tests):{Color.RESET}")
                    
                    # Collect all unique test names across work areas
                    all_test_names = set()
                    for wd in power_work_areas:
                        for test in wd['power_data']['fsdb_tests']:
                            all_test_names.add(test['test_name'])
                    
                    # Build table data for all tests
                    trend_data = []
                    for test_name in sorted(all_test_names):
                        power_values = []
                        for wd in power_work_areas[:5]:  # Show up to 5 most recent
                            # Find this test in this work area
                            for test in wd['power_data']['fsdb_tests']:
                                if test['test_name'] == test_name:
                                    if 'total_power' in test['metrics']:
                                        power_val = test['metrics']['total_power']
                                        power_values.append((wd['work_dir'], power_val))
                                    break
                        
                        if len(power_values) >= 2:
                            latest_power = power_values[0][1]
                            oldest_power = power_values[-1][1]
                            diff = latest_power - oldest_power
                            percent_change = (diff / oldest_power) * 100 if oldest_power != 0 else 0
                            trend_data.append((test_name, latest_power, oldest_power, diff, percent_change))
                    
                    if trend_data:
                        # Sort by latest power (highest first)
                        trend_data.sort(key=lambda x: x[1], reverse=True)
                        
                        # Print unified table header
                        print(f"\n    {'Test Name':<50} {'Latest':>12} {'Oldest':>12} {'Change':>12} {'% Change':>10} {'Trend':>10}")
                        print(f"    {'-'*50} {'-'*12} {'-'*12} {'-'*12} {'-'*10} {'-'*10}")
                        
                        for test_name, latest_power, oldest_power, diff, percent_change in trend_data:
                            # Truncate test name for display
                            display_test = test_name if len(test_name) <= 50 else test_name[:47] + "..."
                            
                            # Color code the change
                            if diff < 0:
                                trend_color = Color.GREEN
                                trend_text = "decreased"
                            elif diff > 0:
                                trend_color = Color.RED
                                trend_text = "increased"
                            else:
                                trend_color = Color.RESET
                                trend_text = "unchanged"
                            
                            print(f"    {display_test:<50} {latest_power:9.3f} mW {oldest_power:9.3f} mW {trend_color}{diff:+9.3f} mW{Color.RESET} {trend_color}{percent_change:+8.1f} %{Color.RESET} {trend_color}{trend_text:>10}{Color.RESET}")
                
                # Show HTML report link at the end
                # Determine display path (will be moved to html/ or test_outputs/html/ by _organize_html_files)
                html_output_dir = self._get_html_output_dir()
                display_path = os.path.relpath(html_output_dir, os.getcwd())
                
                print(f"\n  {Color.CYAN}PT Timing Summary (Dual-Scenario):{Color.RESET}")
                print(f"    Open with: /home/utils/firefox-118.0.1/firefox {Color.MAGENTA}{display_path}/{os.path.basename(html_filename)}{Color.RESET} &")
                
                # Return HTML path and timing data for dashboard summary
                return html_filename, timing_data
        
        return None, None
    
    def run_physical_verification(self) -> None:
        """Run physical verification analysis"""
        self.print_header(FlowStage.PHYSICAL_VERIFICATION)
        
        # Check if PV was run by looking for PV flow directory and files
        pv_flow_dir = os.path.join(self.workarea, "pv_flow")
        pv_was_run = os.path.exists(pv_flow_dir)
        
        if not pv_was_run:
            print(f"{Color.YELLOW}Physical Verification (PV) flow was not run in this workarea{Color.RESET}")
            
            # Add NOT_RUN summary to master dashboard
            self._add_section_summary(
                section_name="Physical Verification (PV)",
                section_id="pv",
                stage=FlowStage.PHYSICAL_VERIFICATION,
                status="NOT_RUN",
                key_metrics={},
                html_file="",
                priority=2,
                issues=["PV flow was not executed"],
                icon="[PV]"
            )
            return
        
        # Show PV flow timestamps at the beginning
        timeline_result = self._show_pv_flow_timestamps()
        
        # Initialize violation counts and data structures
        lvs_violations = 0
        drc_violations = 0
        antenna_violations = 0
        
        # Data structures for HTML report
        lvs_data = {}
        drc_data = {'total_violations': 0, 'violations': [], 'file_path': ''}
        antenna_data = {'total_violations': 0, 'file_path': ''}
        pv_flow_data = {}
        timeline_data = {}
        
        # Parse timeline data
        if timeline_result and timeline_result[0]:
            start_time, end_time = timeline_result
            if end_time != "RUNNING":
                # Calculate duration
                try:
                    from datetime import datetime
                    start_dt = datetime.strptime(start_time, "%Y/%m/%d %I:%M:%S%p")
                    end_dt = datetime.strptime(end_time.split(' (')[0] if ' (' in end_time else end_time, "%Y/%m/%d %I:%M:%S%p")
                    duration_sec = int((end_dt - start_dt).total_seconds())
                    if duration_sec >= 3600:
                        duration_str = f"{duration_sec//3600}h {(duration_sec%3600)//60}m {duration_sec%60}s"
                    elif duration_sec >= 60:
                        duration_str = f"{(duration_sec%3600)//60}m {duration_sec%60}s"
                    else:
                        duration_str = f"{duration_sec}s"
                    timeline_data = {'start': start_time, 'end': end_time, 'duration': duration_str}
                except:
                    timeline_data = {'start': start_time, 'end': end_time, 'duration': 'N/A'}
            else:
                timeline_data = {'start': start_time, 'end': 'Running', 'duration': 'In progress'}
        
        # LVS errors
        lvs_pattern = f"pv_flow/drc_dir/{self.design_info.top_hier}/lvs_icv_ipo*/{self.design_info.top_hier}_ipo*_fill.LVS_ERRORS"
        lvs_files = self.file_utils.find_files(lvs_pattern, self.workarea)
        
        if lvs_files:
            self.print_file_info(lvs_files[0], "LVS Errors")
            violations = self.lvs_parser.parse_lvs_errors(lvs_files[0])
            lvs_violations = violations['failed_equivalence_points']
            lvs_data = violations.copy()
            lvs_data['file_path'] = os.path.abspath(lvs_files[0])
            
            # Display detailed LVS violation information
            status_color = Color.RED if violations['status'] == 'FAIL' else Color.GREEN
            print(f"  {status_color}Status: {violations['status']}{Color.RESET}")
            print(f"  Failed Equivalence Points: {violations['failed_equivalence_points']}")
            if violations['first_priority_errors'] > 0:
                print(f"  First Priority Errors: {violations['first_priority_errors']}")
            if violations['second_priority_errors'] > 0:
                print(f"  Second Priority Errors: {violations['second_priority_errors']}")
            print(f"  Successful Equivalence Points: {violations['successful_equivalence_points']}")
            
            if violations['failed_equivalence_points'] > 0:
                # Only show non-zero unmatched items
                unmatched_items = []
                if violations['unmatched_schematic_instances'] > 0:
                    unmatched_items.append(f"Schematic Instances: {violations['unmatched_schematic_instances']}")
                if violations['unmatched_schematic_nets'] > 0:
                    unmatched_items.append(f"Schematic Nets: {violations['unmatched_schematic_nets']}")
                if violations['unmatched_layout_instances'] > 0:
                    unmatched_items.append(f"Layout Instances: {violations['unmatched_layout_instances']}")
                if violations['unmatched_layout_nets'] > 0:
                    unmatched_items.append(f"Layout Nets: {violations['unmatched_layout_nets']}")
                if violations['unmatched_schematic_ports'] > 0:
                    unmatched_items.append(f"Schematic Ports: {violations['unmatched_schematic_ports']}")
                if violations['unmatched_layout_ports'] > 0:
                    unmatched_items.append(f"Layout Ports: {violations['unmatched_layout_ports']}")
                
                if unmatched_items:
                    print(f"\n  {Color.YELLOW}Unmatched Items:{Color.RESET}")
                    for item in unmatched_items:
                        print(f"    {item}")
                
                total_unmatched = (violations['unmatched_schematic_instances'] + 
                                 violations['unmatched_schematic_nets'] + 
                                 violations['unmatched_layout_instances'] + 
                                 violations['unmatched_layout_nets'] + 
                                 violations['unmatched_schematic_ports'] + 
                                 violations['unmatched_layout_ports'])
                print(f"    {Color.RED}Total Unmatched Items: {total_unmatched}{Color.RESET}")
            
            print(f"\n  {Color.GREEN}Matched Items:{Color.RESET}")
            print(f"    Instances: {violations['matched_instances']:,}")
            print(f"    Nets:      {violations['matched_nets']:,}")
            print(f"    Ports:     {violations['matched_ports']:,}")
        
        # DRC errors
        drc_pattern = f"pv_flow/drc_dir/{self.design_info.top_hier}/drc_icv_ipo*/{self.design_info.top_hier}_ipo*_fill.LAYOUT_ERRORS"
        drc_files = self.file_utils.find_files(drc_pattern, self.workarea)
        
        if drc_files:
            self.print_file_info(drc_files[0], "DRC Errors")
            drc_violations, drc_violations_list = self._analyze_drc_errors_with_data(drc_files[0])
            drc_data = {
                'total_violations': drc_violations,
                'violations': drc_violations_list,
                'file_path': os.path.abspath(drc_files[0])
            }
        
        # Antenna errors
        antenna_pattern = f"pv_flow/drc_dir/{self.design_info.top_hier}/drc_icv_antenna_ipo*/{self.design_info.top_hier}_ipo*_fill.LAYOUT_ERRORS"
        antenna_files = self.file_utils.find_files(antenna_pattern, self.workarea)
        
        if antenna_files:
            self.print_file_info(antenna_files[0], "Antenna Errors")
            antenna_violations = self._analyze_antenna_errors(antenna_files[0])
            antenna_data = {
                'total_violations': antenna_violations,
                'file_path': os.path.abspath(antenna_files[0])
            }
        else:
            print("  No antenna error report found")
        
        # PV Flow Analysis
        pv_flow_data = self._analyze_pv_flow_with_data()
        
        # Generate HTML report
        pv_html_path = self._generate_pv_html_report(lvs_data, drc_data, antenna_data, pv_flow_data, timeline_data)
        
        if pv_html_path:
            html_filename = os.path.basename(pv_html_path)
            # Determine display path
            html_output_dir = self._get_html_output_dir()
            display_path = os.path.relpath(html_output_dir, os.getcwd())
            
            print(f"\n  {Color.CYAN}Physical Verification HTML Report:{Color.RESET}")
            print(f"  Open with: /home/utils/firefox-118.0.1/firefox {Color.MAGENTA}{display_path}/{html_filename}{Color.RESET} &")
        
        # Determine status based on violation counts
        # Thresholds: LVS > 5, DRC > 100, Antenna > 10 â†’ FAIL
        #             All = 0 â†’ PASS
        #             Otherwise â†’ WARN
        status = "PASS"
        issues = []
        
        # Check if any violation type exceeds FAIL threshold
        if lvs_violations > 5 or drc_violations > 100 or antenna_violations > 10:
            status = "FAIL"
            if lvs_violations > 5:
                issues.append(f"LVS failed equivalence points: {lvs_violations} (threshold: â‰¤5)")
            if drc_violations > 100:
                issues.append(f"DRC violations: {drc_violations} (threshold: â‰¤100)")
            if antenna_violations > 10:
                issues.append(f"Antenna violations: {antenna_violations} (threshold: â‰¤10)")
        # Check if any violation type > 0 but within FAIL threshold (WARN)
        elif lvs_violations > 0 or drc_violations > 0 or antenna_violations > 0:
            status = "WARN"
            if lvs_violations > 0:
                issues.append(f"LVS failed equivalence points: {lvs_violations} (threshold: â‰¤5)")
            if drc_violations > 0:
                issues.append(f"DRC violations: {drc_violations} (threshold: â‰¤100)")
            if antenna_violations > 0:
                issues.append(f"Antenna violations: {antenna_violations} (threshold: â‰¤10)")
        
        # Prepare key metrics
        key_metrics = {
            "LVS Failures": str(lvs_violations),
            "DRC Violations": str(drc_violations),
            "Antenna Violations": str(antenna_violations)
        }
        
        # Add section summary for master dashboard
        self._add_section_summary(
            section_name="Physical Verification (PV)",
            section_id="pv",
            stage=FlowStage.PHYSICAL_VERIFICATION,
            status=status,
            key_metrics=key_metrics,
            html_file=pv_html_path if pv_html_path else "",
            priority=2,
            issues=issues,
            icon="[PV]"
        )
    
    def _show_flow_timeline(self, flow_name: str, local_flow_dirs: List[str]) -> None:
        """Show flow start and end timestamps for any flow type
        
        Args:
            flow_name: Name of the flow
            local_flow_dirs: List of flow directory paths
        """
        for local_flow_dir_pattern in local_flow_dirs:
            # Handle glob patterns in directory paths
            if '*' in local_flow_dir_pattern:
                matching_dirs = glob.glob(local_flow_dir_pattern)
            else:
                matching_dirs = [local_flow_dir_pattern] if os.path.exists(local_flow_dir_pattern) else []
            
            for local_flow_dir in matching_dirs:
                if os.path.exists(local_flow_dir):
                    try:
                        # Find BEGIN and END step files
                        begin_files = glob.glob(os.path.join(local_flow_dir, "STEP__BEGIN__*"))
                        end_files = glob.glob(os.path.join(local_flow_dir, "STEP__END__*"))
                        
                        if begin_files:
                            # Get begin time
                            begin_time = os.path.getmtime(begin_files[0])
                            begin_str = time.strftime("%Y/%m/%d %I:%M:%S%p", time.localtime(begin_time))
                            
                            if end_files:
                                # Flow completed - show full timeline
                                end_time = os.path.getmtime(end_files[0])
                                end_str = time.strftime("%Y/%m/%d %I:%M:%S%p", time.localtime(end_time))
                                
                                # Calculate total duration
                                duration_sec = int(end_time - begin_time)
                                if duration_sec >= 3600:
                                    duration_str = f"{duration_sec//3600}h {(duration_sec%3600)//60}m {duration_sec%60}s"
                                elif duration_sec >= 60:
                                    duration_str = f"{(duration_sec%3600)//60}m {duration_sec%60}s"
                                else:
                                    duration_str = f"{duration_sec}s"
                                
                                print(f"{Color.CYAN}{flow_name} Flow Timeline:{Color.RESET}")
                                print(f"  Started:  {begin_str}")
                                print(f"  Finished: {end_str}")
                                print(f"  Duration: {duration_str}")
                                print()
                                return begin_str, end_str  # Return timestamps for use in runtime table
                            else:
                                # No END file - check if flow actually completed by looking for output files
                                flow_completed_without_end_marker = False
                                
                                if flow_name == "Star":
                                    # Check if SPEF files exist (Star's main output)
                                    spef_pattern = os.path.join(self.workarea, f"export/nv_star/{self.design_info.top_hier}/ipo*/IOs/netlists/*.spef.typical_T0.gz")
                                    spef_files = glob.glob(spef_pattern)
                                    if spef_files:
                                        # SPEF exists - flow completed but END marker missing
                                        flow_completed_without_end_marker = True
                                        # Use the SPEF file timestamp as proxy for end time
                                        end_time = os.path.getmtime(spef_files[0])
                                        end_str = time.strftime("%Y/%m/%d %I:%M:%S%p", time.localtime(end_time))
                                        duration_sec = int(end_time - begin_time)
                                
                                if flow_completed_without_end_marker:
                                    # Show completed timeline with note about missing END marker
                                    if duration_sec >= 3600:
                                        duration_str = f"{duration_sec//3600}h {(duration_sec%3600)//60}m {duration_sec%60}s"
                                    elif duration_sec >= 60:
                                        duration_str = f"{(duration_sec%3600)//60}m {duration_sec%60}s"
                                    else:
                                        duration_str = f"{duration_sec}s"
                                    
                                    print(f"{Color.CYAN}{flow_name} Flow Timeline:{Color.RESET}")
                                    print(f"  Started:  {begin_str}")
                                    print(f"  Finished: {end_str} {Color.YELLOW}(inferred from output files){Color.RESET}")
                                    print(f"  Duration: {duration_str}")
                                    print()
                                    return begin_str, end_str
                        else:
                            # Flow is currently running - calculate elapsed time
                            current_time = time.time()
                            elapsed_sec = int(current_time - begin_time)
                            if elapsed_sec >= 3600:
                                elapsed_str = f"{elapsed_sec//3600}h {(elapsed_sec%3600)//60}m {elapsed_sec%60}s"
                            elif elapsed_sec >= 60:
                                elapsed_str = f"{(elapsed_sec%3600)//60}m {elapsed_sec%60}s"
                            else:
                                elapsed_str = f"{elapsed_sec}s"
                            
                            print(f"{Color.CYAN}{flow_name} Flow Timeline:{Color.RESET}")
                            print(f"  Started: {begin_str}")
                            print(f"  {Color.YELLOW}Status:  RUNNING (elapsed: {elapsed_str}){Color.RESET}")
                            print()
                            return begin_str, "RUNNING"  # Return start time and RUNNING status
                            
                    except Exception as e:
                        print(f"{Color.YELLOW}Could not determine {flow_name} flow timestamps: {e}{Color.RESET}")
                        continue
        
        return None, None  # No timeline found
    
    def _show_pv_flow_timestamps(self) -> None:
        """Show PV flow start and end timestamps"""
        local_flow_dir = os.path.join(self.workarea, f"pv_flow/nv_flow/{self.design_info.top_hier}/local_flow")
        return self._show_flow_timeline("PV", [local_flow_dir])
    
    def _analyze_pv_flow(self) -> None:
        """Analyze PV flow configuration and status"""
        prc_pattern = f"pv_flow/nv_flow/pv_{self.design_info.top_hier}.prc"
        prc_status_pattern = f"pv_flow/nv_flow/pv_{self.design_info.top_hier}.prc.status"
        
        prc_files = self.file_utils.find_files(prc_pattern, self.workarea)
        prc_status_files = self.file_utils.find_files(prc_status_pattern, self.workarea)
        
        if prc_files and prc_status_files:
            print(f"\n{Color.CYAN}PV Flow Analysis:{Color.RESET}")
            self.print_file_info(prc_status_files[0], "PV Flow Status")
            self._parse_pv_flow_status(prc_status_files[0])
            
            self.print_file_info(prc_files[0], "PV Flow Configuration")
            self._parse_pv_flow_config(prc_files[0])
        elif prc_status_files:
            print(f"\n{Color.CYAN}PV Flow Status:{Color.RESET}")
            self.print_file_info(prc_status_files[0], "PV Flow Status")
            self._parse_pv_flow_status(prc_status_files[0])
        else:
            print(f"\n{Color.YELLOW}No PV flow status files found{Color.RESET}")
    
    def _analyze_pv_flow_with_data(self) -> Optional[Dict[str, Any]]:
        """Wrapper for _analyze_pv_flow that returns structured data for HTML generation
        
        Returns:
            Dictionary with PV flow data or None if not found
        """
        prc_pattern = f"pv_flow/nv_flow/pv_{self.design_info.top_hier}.prc"
        prc_status_pattern = f"pv_flow/nv_flow/pv_{self.design_info.top_hier}.prc.status"
        
        prc_files = self.file_utils.find_files(prc_pattern, self.workarea)
        prc_status_files = self.file_utils.find_files(prc_status_pattern, self.workarea)
        
        pv_flow_data = {}
        
        if prc_status_files:
            try:
                with open(prc_status_files[0], 'r') as f:
                    content = f.read()
                
                # Extract flow steps
                steps = []
                lines = content.split('\n')
                
                for line in lines:
                    if line.strip() and not line.startswith('#') and '    ' in line:
                        parts = line.split()
                        if len(parts) >= 6 and parts[3] in ['DONE', 'FAILED', 'RUNNING', 'UNLAUNCHED']:
                            block = parts[0]
                            experiment = parts[1]
                            step = parts[2]
                            status = parts[3]
                            duration = parts[4]
                            
                            # Convert duration to readable format
                            if duration.isdigit() and int(duration) > 0:
                                duration_sec = int(duration)
                                if duration_sec >= 3600:
                                    duration_str = f"{duration_sec//3600}h {(duration_sec%3600)//60}m {duration_sec%60}s"
                                elif duration_sec >= 60:
                                    duration_str = f"{(duration_sec%3600)//60}m {duration_sec%60}s"
                                else:
                                    duration_str = f"{duration_sec}s"
                            else:
                                duration_str = duration
                            
                            steps.append({
                                'block': block,
                                'experiment': experiment,
                                'step': step,
                                'status': status,
                                'duration': duration_str,
                                'duration_sec': int(duration) if duration.isdigit() else 0
                            })
                
                # Group by experiment
                experiments = {}
                for step in steps:
                    exp_key = f"{step['block']}/{step['experiment']}"
                    if exp_key not in experiments:
                        experiments[exp_key] = []
                    experiments[exp_key].append(step)
                
                # Process each experiment
                for exp_name, exp_steps in experiments.items():
                    total_runtime = sum(step['duration_sec'] for step in exp_steps if step['status'] == 'DONE')
                    done_steps = [s for s in exp_steps if s['status'] == 'DONE']
                    unlaunched_steps = [s for s in exp_steps if s['status'] == 'UNLAUNCHED']
                    
                    # Format total runtime
                    if total_runtime >= 3600:
                        total_str = f"{total_runtime//3600}h {(total_runtime%3600)//60}m {total_runtime%60}s"
                    elif total_runtime >= 60:
                        total_str = f"{(total_runtime%3600)//60}m {total_runtime%60}s"
                    else:
                        total_str = f"{total_runtime}s"
                    
                    # Get key steps
                    key_steps = [(s['step'], s['duration']) for s in done_steps 
                                 if s['step'] in ['temp_run_lvs', 'temp_run_drc', 'temp_run_ant', 'drc_lvs', 'ant']]
                    
                    pv_flow_data[exp_name] = {
                        'completed': len(done_steps),
                        'unlaunched': len(unlaunched_steps),
                        'total_runtime': total_str,
                        'key_steps': key_steps
                    }
                    
            except Exception as e:
                print(f"  Error extracting PV flow data: {e}")
        
        # Call regular function to print output
        self._analyze_pv_flow()
        
        return pv_flow_data
    
    def _parse_pv_flow_status(self, status_file: str) -> Dict[str, Any]:
        """Parse PV flow status file and display runtime information
        
        Args:
            status_file: Path to PV flow status file
            
        Returns:
            Dictionary with parsed status data
        """
        try:
            with open(status_file, 'r') as f:
                content = f.read()
            
            # Extract flow steps with status and runtime
            steps = []
            lines = content.split('\n')
            
            for line in lines:
                if line.strip() and not line.startswith('#') and '    ' in line:
                    parts = line.split()
                    if len(parts) >= 6 and parts[3] in ['DONE', 'FAILED', 'RUNNING', 'UNLAUNCHED']:
                        block = parts[0]
                        experiment = parts[1]
                        step = parts[2]
                        status = parts[3]
                        duration = parts[4]
                        
                        # Convert duration to readable format
                        if duration.isdigit() and int(duration) > 0:
                            duration_sec = int(duration)
                            if duration_sec >= 3600:
                                duration_str = f"{duration_sec//3600}h {(duration_sec%3600)//60}m {duration_sec%60}s"
                            elif duration_sec >= 60:
                                duration_str = f"{(duration_sec%3600)//60}m {duration_sec%60}s"
                            else:
                                duration_str = f"{duration_sec}s"
                        else:
                            duration_str = duration
                        
                        steps.append({
                            'block': block,
                            'experiment': experiment,
                            'step': step,
                            'status': status,
                            'duration': duration_str,
                            'duration_sec': int(duration) if duration.isdigit() else 0
                        })
            
            if steps:
                # Group by experiment and show summary
                experiments = {}
                for step in steps:
                    exp_key = f"{step['block']}/{step['experiment']}"
                    if exp_key not in experiments:
                        experiments[exp_key] = []
                    experiments[exp_key].append(step)
                
                for exp_name, exp_steps in experiments.items():
                    # Calculate total runtime for completed steps
                    total_runtime = sum(step['duration_sec'] for step in exp_steps if step['status'] == 'DONE')
                    done_steps = [s for s in exp_steps if s['status'] == 'DONE']
                    failed_steps = [s for s in exp_steps if s['status'] == 'FAILED']
                    unlaunched_steps = [s for s in exp_steps if s['status'] == 'UNLAUNCHED']
                    
                    # Format total runtime
                    if total_runtime >= 3600:
                        total_str = f"{total_runtime//3600}h {(total_runtime%3600)//60}m {total_runtime%60}s"
                    elif total_runtime >= 60:
                        total_str = f"{(total_runtime%3600)//60}m {total_runtime%60}s"
                    else:
                        total_str = f"{total_runtime}s"
                    
                    print(f"  {Color.CYAN}{exp_name}:{Color.RESET}")
                    print(f"    {Color.GREEN}Completed: {len(done_steps)} steps{Color.RESET} (Total runtime: {total_str})")
                    
                    if failed_steps:
                        print(f"    {Color.RED}Failed: {len(failed_steps)} steps{Color.RESET}")
                    if unlaunched_steps:
                        print(f"    {Color.YELLOW}Unlaunched: {len(unlaunched_steps)} steps{Color.RESET}")
                    
                    # Show key step runtimes (only for significant steps)
                    key_steps = [s for s in done_steps if s['step'] in ['temp_run_lvs', 'temp_run_drc', 'temp_run_ant', 'drc_lvs', 'ant']]
                    if key_steps:
                        print(f"    {Color.CYAN}Key step runtimes:{Color.RESET}")
                        for step in key_steps:
                            print(f"      {step['step']}: {step['duration']}")
                    print()
                    
        except Exception as e:
            print(f"  Error parsing PV flow status: {e}")
    
    def _parse_pv_flow_config(self, config_file: str) -> None:
        """Parse PV flow configuration file and display key settings
        
        Args:
            config_file: Path to PV flow configuration file
        """
        try:
            with open(config_file, 'r') as f:
                content = f.read()
            
            # Extract key configuration information
            print(f"  {Color.CYAN}Configuration Summary:{Color.RESET}")
            
            # Extract IPO number
            ipo_match = re.search(r'ipo_number:\s*(\d+)', content)
            if ipo_match:
                print(f"    IPO Number: {ipo_match.group(1)}")
            
            # Extract tool
            tool_match = re.search(r'tool:\s*(\w+)', content)
            if tool_match:
                print(f"    Tool: {tool_match.group(1)}")
            
            # Extract flow sequences
            self._extract_flow_sequences(content)
                
        except Exception as e:
            print(f"  Error parsing PV flow config: {e}")
    
    def _extract_flow_sequences(self, content: str) -> None:
        """Extract and display flow sequences from PRC YAML content
        
        Args:
            content: YAML content string
        """
        try:
            # Extract local_flow sequence using a simpler approach
            local_steps = self._extract_flow_sequence_simple(content, 'local_flow')
            if local_steps:
                print(f"    {Color.CYAN}Local Flow Sequence:{Color.RESET} {' -> '.join(local_steps)}")
            
            # Extract release_flow sequence
            release_steps = self._extract_flow_sequence_simple(content, 'release_flow')
            if release_steps:
                print(f"    {Color.CYAN}Release Flow Sequence:{Color.RESET} {' -> '.join(release_steps)}")
            else:
                # Check if release_flow exists at all
                if 'release_flow:' in content:
                    print(f"    Release Flow Configured: Yes (but no steps found)")
                else:
                    print(f"    Release Flow Configured: No")
                
        except Exception as e:
            print(f"    Error extracting flow sequences: {e}")
    
    def _extract_flow_sequence_simple(self, content: str, flow_type: str) -> Optional[List[str]]:
        """Extract flow sequence using a simpler line-by-line approach
        
        Args:
            content: YAML content string
            flow_type: Type of flow (e.g., 'pnr', 'sta')
            
        Returns:
            List of flow stages or None if not found
        """
        lines = content.split('\n')
        steps = []
        in_flow_sequence = False
        flow_found = False
        
        for line in lines:
            # Look for the flow type (local_flow or release_flow)
            if f'{flow_type}:' in line and line.strip().endswith(':'):
                flow_found = True
                continue
            
            # If we found the flow, look for flow_sequence
            if flow_found and 'flow_sequence:' in line and line.strip().endswith(':'):
                in_flow_sequence = True
                continue
            
            # If we're in flow_sequence, collect steps
            if in_flow_sequence:
                stripped = line.strip()
                # Stop if we hit another section at the same level
                if stripped and not stripped.startswith('-') and not stripped.startswith(' ') and stripped.endswith(':'):
                    break
                
                # Collect step if it starts with -
                if stripped.startswith('- '):
                    step = stripped[2:].strip()
                    # Extract step name (before colon if present)
                    if ':' in step:
                        step = step.split(':')[0].strip()
                    if step and step not in steps:  # Avoid duplicates
                        steps.append(step)
        
        return steps
    
    
    def _analyze_drc_errors(self, drc_file: str) -> None:
        """Analyze DRC errors file and provide detailed breakdown
        
        Args:
            drc_file: Path to DRC report file
        """
        try:
            with open(drc_file, 'r') as f:
                content = f.read()
            
            # Check if CLEAN or ERRORS
            if "LAYOUT ERRORS RESULTS: CLEAN" in content:
                print(f"  {Color.GREEN}Status: CLEAN - No DRC violations{Color.RESET}")
                return 0
            elif "LAYOUT ERRORS RESULTS: ERRORS" in content:
                print(f"  {Color.RED}Status: ERRORS - DRC violations found{Color.RESET}")
            
            # Extract violation details from ERROR SUMMARY section
            violations = []
            total_violations = 0
            
            # Find all violation lines in ERROR SUMMARY
            # Pattern to match rule violations that may span multiple lines
            # Format: RULE_NAME : description text ... X violations found.
            violation_pattern = r'^\s*([A-Za-z0-9._]+)\s*:\s*(.*?)(\d+)\s+violations?\s+found\.'
            matches = re.findall(violation_pattern, content, re.MULTILINE | re.DOTALL)
            
            for rule, description, count in matches:
                count_int = int(count)
                if count_int > 0:  # Only include non-zero violations
                    # Clean up description: remove extra whitespace and newlines
                    desc_clean = ' '.join(description.split())
                    # Truncate if too long
                    if len(desc_clean) > 60:
                        desc_clean = desc_clean[:57] + "..."
                    violations.append((rule, desc_clean, count_int))
                    total_violations += count_int
            
            if violations:
                print(f"  {Color.RED}Total DRC violations: {total_violations}{Color.RESET}")
                print(f"  {Color.CYAN}Violation breakdown:{Color.RESET}")
                
                # Sort by violation count (descending)
                violations.sort(key=lambda x: x[2], reverse=True)
                
                # Create table format
                print(f"    {'Rule':<20} {'Count':<8} {'Description':<60}")
                print(f"    {'-'*20} {'-'*8} {'-'*60}")
                
                for rule, description, count in violations:
                    print(f"    {rule:<20} {count:<8} {description:<60}")
            else:
                # Fallback: Try alternate parsing methods
                print(f"  {Color.YELLOW}Warning: Could not parse DRC violation details with standard pattern{Color.RESET}")
                
                # Try to find ERROR SUMMARY section and extract lines
                summary_match = re.search(r'ERROR SUMMARY.*?(?=\n\n|\Z)', content, re.DOTALL)
                if summary_match:
                    summary_text = summary_match.group(0)
                    # Look for lines with "violation" in ERROR SUMMARY
                    violation_lines = [line.strip() for line in summary_text.split('\n') 
                                      if 'violation' in line.lower() and line.strip()]
                    
                    if violation_lines:
                        print(f"  {Color.CYAN}DRC Violations Found:{Color.RESET}")
                        total_violations = 0
                        for line in violation_lines:
                            print(f"    {line}")
                            # Try to extract count from line
                            numbers = re.findall(r'(\d+)\s+violations?\s+found', line, re.IGNORECASE)
                            if numbers:
                                total_violations += int(numbers[0])
                        print(f"  {Color.RED}Total DRC violations: {total_violations}{Color.RESET}")
                    else:
                        # Last resort: simple counting
                        matches = self.file_utils.grep_file(r".*violation.*found.*", drc_file)
                        total_violations = 0
                        for match in matches:
                            numbers = re.findall(r'\d+', match)
                            if numbers:
                                total_violations += int(numbers[0])
                        print(f"  Total DRC violations: {total_violations}")
                        print(f"  {Color.CYAN}Note: See full details in DRC file{Color.RESET}")
                else:
                    # No ERROR SUMMARY found, show sample content
                    print(f"  {Color.CYAN}Sample from DRC file (first 10 lines with 'violation'):{Color.RESET}")
                    violation_sample = [line for line in content.split('\n') if 'violation' in line.lower()][:10]
                    for line in violation_sample:
                        print(f"    {line.strip()}")
                    print(f"  {Color.YELLOW}Please check the full DRC file for details{Color.RESET}")
            
            # Return total violation count
            return total_violations
                
        except Exception as e:
            print(f"  Error analyzing DRC file: {e}")
            return 0
    
    def _analyze_drc_errors_with_data(self, drc_file: str) -> Optional[Dict[str, Any]]:
        """Wrapper for _analyze_drc_errors that returns both count and violations list for HTML generation
        
        Args:
            drc_file: Path to DRC report file
            
        Returns:
            Dictionary with DRC error data or None if not found
        """
        try:
            with open(drc_file, 'r') as f:
                content = f.read()
            
            violations = []
            total_violations = 0
            
            # Check if CLEAN or ERRORS
            if "LAYOUT ERRORS RESULTS: CLEAN" in content:
                # Call regular function to print output
                self._analyze_drc_errors(drc_file)
                return 0, []
            
            # Extract violation details
            violation_pattern = r'^\s*([A-Za-z0-9._]+)\s*:\s*(.*?)(\d+)\s+violations?\s+found\.'
            matches = re.findall(violation_pattern, content, re.MULTILINE | re.DOTALL)
            
            for rule, description, count in matches:
                count_int = int(count)
                if count_int > 0:
                    desc_clean = ' '.join(description.split())
                    if len(desc_clean) > 60:
                        desc_clean = desc_clean[:57] + "..."
                    violations.append((rule, desc_clean, count_int))
                    total_violations += count_int
            
            # Sort by violation count (descending)
            violations.sort(key=lambda x: x[2], reverse=True)
            
            # Call regular function to print output
            self._analyze_drc_errors(drc_file)
            
            return total_violations, violations
                
        except Exception as e:
            print(f"  Error analyzing DRC file: {e}")
            return 0, []
    
    def _analyze_antenna_errors(self, antenna_file: str) -> Optional[Dict[str, Any]]:
        """Analyze antenna errors file and provide detailed information
        
        Args:
            antenna_file: Path to antenna violations file
            
        Returns:
            Dictionary with antenna violation data or None if not found
        """
        try:
            with open(antenna_file, 'r') as f:
                content = f.read()
            
            total_violations = 0
            
            # Extract LAYOUT ERRORS RESULTS
            matches = self.file_utils.grep_file(r"LAYOUT ERRORS RESULTS.*", antenna_file)
            for match in matches:
                # Check if status is CLEAN and color it green
                if "CLEAN" in match.upper():
                    print(f"  {Color.GREEN}Status: {match.strip()}{Color.RESET}")
                    print(f"  {Color.GREEN}No antenna violations found{Color.RESET}")
                    return 0
                else:
                    print(f"  {Color.RED}Status: {match.strip()}{Color.RESET}")
            
            
            # If there are errors, extract error summary and count violations
            if "LAYOUT ERRORS RESULTS: ERRORS" in content:
                result = self.file_utils.run_command(f"sed -n '/ERROR SUMMARY/,/ERROR DETAILS/p' {antenna_file} | sed -e '/^$/d' -e '/ERROR DETAILS/d'")
                if result.strip():
                    print(f"  {Color.CYAN}Error Summary:{Color.RESET}")
                    for line in result.strip().split('\n'):
                        if line.strip():
                            print(f"    {line}")
                            # Try to extract violation count from lines
                            numbers = re.findall(r'(\d+)\s+violations?\s+found', line, re.IGNORECASE)
                            if numbers:
                                total_violations += int(numbers[0])
                
                # If no violations counted from summary, try overall pattern
                if total_violations == 0:
                    violation_pattern = r'(\d+)\s+violations?\s+found'
                    matches = re.findall(violation_pattern, content, re.IGNORECASE)
                    for count in matches:
                        total_violations += int(count)
                
                if total_violations > 0:
                    print(f"  {Color.RED}Total Antenna violations: {total_violations}{Color.RESET}")
            
            return total_violations
                            
        except Exception as e:
            print(f"  Error analyzing antenna file: {e}")
            return 0
    
    def _analyze_gl_check_errors(self, waived_file: str, non_waived_file: str) -> tuple:
        """Analyze GL Check error files and show waived vs non-waived counts per checker
        
        Args:
            waived_file: Path to waived errors file
            non_waived_file: Path to non-waived errors file
            
        Returns:
            Tuple of (waived_checkers, non_waived_checkers, non_waived_errors_detail)
        """
        try:
            # Parse waived errors
            waived_checkers = {}
            waived_lines_total = 0
            if os.path.exists(waived_file):
                try:
                    with open(waived_file, 'r') as f:
                        lines = f.readlines()
                        waived_lines_total = len(lines)
                        for line in lines:
                            # Match pattern: -E-[code](checkerName)
                            match = re.match(r'-E-\[(\d+)\]\((\w+)\)', line)
                            if match:
                                code = match.group(1)
                                checker = match.group(2)
                                key = f"[{code}]({checker})"
                                waived_checkers[key] = waived_checkers.get(key, 0) + 1
                except Exception as e:
                    print(f"  Error reading waived file: {e}")
            
            # Parse non-waived errors
            non_waived_checkers = {}
            non_waived_lines_total = 0
            if os.path.exists(non_waived_file):
                try:
                    with open(non_waived_file, 'r') as f:
                        lines = f.readlines()
                        non_waived_lines_total = len(lines)
                        for line in lines:
                            # Match pattern: -E-[code](checkerName)
                            match = re.match(r'-E-\[(\d+)\]\((\w+)\)', line)
                            if match:
                                code = match.group(1)
                                checker = match.group(2)
                                key = f"[{code}]({checker})"
                                non_waived_checkers[key] = non_waived_checkers.get(key, 0) + 1
                except Exception as e:
                    print(f"  Error reading non-waived file: {e}")
            
            # Combine all checkers
            all_checkers = set(waived_checkers.keys()) | set(non_waived_checkers.keys())
            
            # If we couldn't parse any checkers but files have content, show raw counts
            if not all_checkers and (waived_lines_total > 0 or non_waived_lines_total > 0):
                print(f"\n  {Color.YELLOW}Warning: Could not parse checker format from error files{Color.RESET}")
                print(f"  {Color.CYAN}GL Check Error Summary (Raw):{Color.RESET}")
                if waived_lines_total > 0:
                    print(f"    {Color.GREEN}Waived Errors: {waived_lines_total} lines{Color.RESET}")
                if non_waived_lines_total > 0:
                    print(f"    {Color.RED}Non-Waived Errors: {non_waived_lines_total} lines{Color.RESET}")
                print(f"\n  {Color.CYAN}Error file format sample (first 3 lines from non-waived):{Color.RESET}")
                if os.path.exists(non_waived_file):
                    with open(non_waived_file, 'r') as f:
                        for i, line in enumerate(f):
                            if i >= 3:
                                break
                            print(f"    {line.rstrip()}")
                print(f"\n  Waived Errors File: {waived_file}")
                print(f"  Non-Waived Errors File: {non_waived_file}")
                # Return raw line counts
                total = waived_lines_total + non_waived_lines_total
                return total, waived_lines_total, non_waived_lines_total
            
            if not all_checkers:
                print("  No GL Check errors found")
                return 0, 0, 0
            
            # Calculate totals
            total_waived = sum(waived_checkers.values())
            total_non_waived = sum(non_waived_checkers.values())
            total_errors = total_waived + total_non_waived
            
            # Print summary
            print(f"\n  {Color.CYAN}GL Check Error Summary:{Color.RESET}")
            print(f"    Total Errors: {total_errors}")
            print(f"    {Color.GREEN}Waived: {total_waived}{Color.RESET}")
            if total_non_waived > 0:
                print(f"    {Color.RED}Non-Waived: {total_non_waived}{Color.RESET}")
            else:
                print(f"    {Color.GREEN}Non-Waived: 0 (PASS){Color.RESET}")
            
            # Print table
            print(f"\n  {Color.CYAN}GL Check Results by Checker:{Color.RESET}")
            print(f"  {'Checker [Code](Name)':<40} {'Waived':<10} {'Non-Waived':<12} {'Total':<10}")
            print(f"  {'-'*40} {'-'*10} {'-'*12} {'-'*10}")
            
            # Sort by total count (descending)
            sorted_checkers = sorted(all_checkers, 
                                    key=lambda x: waived_checkers.get(x, 0) + non_waived_checkers.get(x, 0), 
                                    reverse=True)
            
            for checker in sorted_checkers:
                waived_cnt = waived_checkers.get(checker, 0)
                non_waived_cnt = non_waived_checkers.get(checker, 0)
                total_cnt = waived_cnt + non_waived_cnt
                
                # Color non-waived count if > 0
                if non_waived_cnt > 0:
                    non_waived_str = f"{Color.RED}{non_waived_cnt}{Color.RESET}"
                else:
                    non_waived_str = f"{Color.GREEN}{non_waived_cnt}{Color.RESET}"
                
                print(f"  {checker:<40} {waived_cnt:<10} {non_waived_str:<22} {total_cnt:<10}")
            
            # Print file paths
            print(f"\n  Waived Errors File: {waived_file}")
            print(f"  Non-Waived Errors File: {non_waived_file}")
            
            # Return totals for status determination
            return total_errors, total_waived, total_non_waived
                    
        except Exception as e:
            print(f"  Error analyzing GL Check errors: {e}")
            return 0, 0, 0
    
    def _generate_gl_check_html_content(self, waived_checkers: Dict[str, int], non_waived_checkers: Dict[str, int], non_waived_errors_detail: Dict[str, List[str]],
                                        sorted_checkers: List[tuple], total_errors: int, total_waived: int, total_non_waived: int,
                                        allowed_clktree_cells: List[str], dont_use_cells: List[str], key_reports: Dict[str, str], main_logs: Dict[str, str], timestamped_dirs: List[str],
                                        waived_file: str, non_waived_file: str, checker_rules: Optional[Dict[str, Dict[str, str]]] = None, executed_checkers: Optional[List[str]] = None, skipped_checkers: Optional[List[str]] = None) -> str:
        """Generate the HTML content for GL Check report with filterable checker rules dictionary and executed/skipped lists
        
        Args:
            waived_checkers: Dictionary of waived checker counts
            non_waived_checkers: Dictionary of non-waived checker counts
            non_waived_errors_detail: Dictionary of detailed non-waived errors
            sorted_checkers: List of sorted checker tuples
            total_errors: Total error count
            total_waived: Total waived count
            total_non_waived: Total non-waived count
            allowed_clktree_cells: List of allowed clock tree cells
            dont_use_cells: List of dont_use cells
            key_reports: Dictionary of key report paths
            main_logs: Dictionary of main log paths
            timestamped_dirs: List of timestamped directory paths
            waived_file: Path to waived errors file
            non_waived_file: Path to non-waived errors file
            checker_rules: Optional dictionary of checker rules
            executed_checkers: Optional list of executed checkers
            skipped_checkers: Optional list of skipped checkers
            
        Returns:
            HTML content string
        """
        
        # Handle defaults
        if executed_checkers is None:
            executed_checkers = []
        if skipped_checkers is None:
            skipped_checkers = []
        
        # Read and encode logo
        import base64
        logo_data = ""
        logo_path = os.path.join(os.path.dirname(__file__), "assets/images/avice_logo.png")
        if os.path.exists(logo_path):
            with open(logo_path, "rb") as logo_file:
                logo_data = base64.b64encode(logo_file.read()).decode('utf-8')
        
        # Build checker rows HTML
        checker_rows = []
        for checker in sorted_checkers:
            waived_cnt = waived_checkers.get(checker, 0)
            non_waived_cnt = non_waived_checkers.get(checker, 0)
            total_cnt = waived_cnt + non_waived_cnt
            
            status_class = 'status-pass' if non_waived_cnt == 0 else 'status-fail'
            
            # Get error details for this checker
            error_details = non_waived_errors_detail.get(checker, [])
            error_details_html = ""
            if error_details:
                error_details_html = "<div class='error-details'>"
                for i, error in enumerate(error_details[:50], 1):  # Limit to 50 errors
                    error_details_html += f"<div class='error-line'>{i}. {error}</div>"
                if len(error_details) > 50:
                    error_details_html += f"<div class='error-line'>... and {len(error_details) - 50} more errors</div>"
                error_details_html += "</div>"
            
            checker_rows.append(f"""
                <tr class="checker-row {status_class}" data-checker="{checker}" data-waived="{waived_cnt}" data-nonwaived="{non_waived_cnt}" data-total="{total_cnt}">
                    <td>{checker}</td>
                    <td class="num-cell">{waived_cnt}</td>
                    <td class="num-cell {'fail-text' if non_waived_cnt > 0 else 'pass-text'}">{non_waived_cnt}</td>
                    <td class="num-cell">{total_cnt}</td>
                    <td class="action-cell">
                        {'<button class="btn-expand" onclick="toggleErrorDetails(this)">Show Errors</button>' if error_details else '-'}
                    </td>
                </tr>
                {'<tr class="error-details-row" style="display:none;"><td colspan="5">' + error_details_html + '</td></tr>' if error_details else ''}
            """)
        
        # Build allowed clock tree cells HTML
        clktree_cells_html = ""
        if allowed_clktree_cells:
            for cell in allowed_clktree_cells:
                clktree_cells_html += f"<div class='cell-item'>{cell}</div>"
        else:
            clktree_cells_html = "<div class='no-data'>No allowed clock tree cells found</div>"
        
        # Build dont_use cells HTML - limit initial display to 50 cells
        dont_use_cells_html = ""
        dont_use_cells_more_html = ""
        if dont_use_cells:
            # Show first 50 cells
            for cell in dont_use_cells[:50]:
                dont_use_cells_html += f"<div class='cell-item'>{cell}</div>"
            # Hide remaining cells
            if len(dont_use_cells) > 50:
                for cell in dont_use_cells[50:]:
                    dont_use_cells_more_html += f"<div class='cell-item'>{cell}</div>"
        else:
            dont_use_cells_html = "<div class='no-data'>No dont_use cells found</div>"
        
        # Build reports HTML
        reports_html = ""
        if key_reports:
            reports_html = "<div class='reports-container'>"
            for name, filepath in key_reports:
                reports_html += f"""
                    <div class='report-item'>
                        <span class='report-name'>{name}</span>
                        <button class='btn-log' onclick="openLogWithServer('{filepath}', event)" title='Open in tablog: {filepath}'>ðŸ–¥ï¸ tablog</button>
                    </div>
                """
            reports_html += "</div>"
        else:
            reports_html = "<div class='no-data'>No reports found</div>"
        
        # Build logs HTML
        logs_html = ""
        if main_logs:
            logs_html = "<div class='logs-container'>"
            for name, filepath in main_logs:
                logs_html += f"""
                    <div class='report-item'>
                        <span class='report-name'>{name}</span>
                        <button class='btn-log' onclick="openLogWithServer('{filepath}', event)" title='Open in tablog: {filepath}'>ðŸ–¥ï¸ tablog</button>
                    </div>
                """
            logs_html += "</div>"
        else:
            logs_html = "<div class='no-data'>No logs found</div>"
        
        # Build checker rules HTML
        checker_rules_html = ""
        if checker_rules:
            for checker_name in sorted(checker_rules.keys()):
                rule_info = checker_rules[checker_name]
                description = rule_info.get('description', 'No description available')
                elapsed_time = rule_info.get('elapsed_time', 'N/A')
                error_count = rule_info.get('error_count', 0)
                
                # Truncate description for display, show full in tooltip
                desc_display = description[:200] + "..." if len(description) > 200 else description
                
                # Color code error count
                if error_count > 0:
                    error_style = "color: #e74c3c; font-weight: bold;"
                else:
                    error_style = "color: #27ae60;"
                
                checker_rules_html += f"""
                                <tr class="rules-row" data-checker="{checker_name.lower()}" data-desc="{description.lower()}">
                                    <td style="font-weight: 600; color: #667eea;">{checker_name}</td>
                                    <td style="font-size: 13px; line-height: 1.4;" title="{description}">{desc_display}</td>
                                    <td style="text-align: center; {error_style}">{error_count}</td>
                                </tr>
"""
        else:
            checker_rules_html = """
                                <tr>
                                    <td colspan="3" style="text-align: center; color: #999; padding: 40px;">
                                        No checker rules available. gl-check.log may not be present or could not be parsed.
                                    </td>
                                </tr>
"""
        
        # Build run history HTML
        run_history_html = ""
        if timestamped_dirs:
            run_history_html = "<div class='run-history-container'>"
            for run_dir in reversed(timestamped_dirs):  # Most recent first
                formatted_date = run_dir.replace('_', '/', 2).replace('_', ' ', 1).replace('_', ':', 1).replace('_', ':', 1)
                run_history_html += f"<div class='run-item'>{formatted_date}</div>"
            run_history_html += "</div>"
        else:
            run_history_html = "<div class='no-data'>No run history found</div>"
        
        # Calculate status
        overall_status = "PASS" if total_non_waived == 0 else "FAIL"
        status_class = "pass-text" if total_non_waived == 0 else "fail-text"
        
        html = f"""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GL Check Report - {self.design_info.top_hier}</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
            color: #333;
        }}
        
        .container {{
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }}
        
        .header {{
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            padding: 30px;
            display: grid;
            grid-template-columns: auto 1fr;
            gap: 20px;
            align-items: center;
        }}
        
        .logo {{
            width: 80px;
            height: 80px;
            border-radius: 10px;
            background: white;
            padding: 10px;
            cursor: pointer;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }}
        
        .logo:hover {{
            transform: scale(1.05);
            box-shadow: 0 8px 16px rgba(0,0,0,0.3);
        }}
        
        .header-text h1 {{
            font-size: 2.5em;
            margin: 0 0 10px 0;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
        }}
        
        .header-text .workarea-path {{
            font-size: 0.9em;
            opacity: 0.9;
            margin-top: 5px;
        }}
        
        .logo-modal {{
            display: none;
            position: fixed;
            z-index: 9999;
            left: 0;
            top: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0,0,0,0.9);
            justify-content: center;
            align-items: center;
        }}
        
        .logo-modal.active {{
            display: flex;
        }}
        
        .logo-modal-content {{
            max-width: 90%;
            max-height: 90%;
            border-radius: 10px;
        }}
        
        .logo-modal-close {{
            position: absolute;
            top: 20px;
            right: 35px;
            color: #f1f1f1;
            font-size: 40px;
            font-weight: bold;
            cursor: pointer;
        }}
        
        .logo-modal-close:hover {{
            color: #bbb;
        }}
        
        .summary {{
            display: flex;
            flex-wrap: wrap;
            gap: 12px;
            padding: 20px;
            background: #f8f9fa;
        }}
        
        .summary-card {{
            flex: 1 1 calc(20% - 12px);
            min-width: 160px;
            background: white;
            padding: 12px 10px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            text-align: center;
            transition: transform 0.3s;
            min-height: 90px;
            display: flex;
            flex-direction: column;
            justify-content: center;
        }}
        
        @media (max-width: 899px) {{
            .summary-card {{
                flex: 1 1 calc(33.333% - 12px);
            }}
        }}
        
        @media (max-width: 599px) {{
            .summary-card {{
                flex: 1 1 calc(50% - 12px);
            }}
        }}
        
        .summary-card:hover {{
            transform: translateY(-3px);
            box-shadow: 0 4px 15px rgba(0,0,0,0.15);
        }}
        
        .summary-card .label {{
            font-size: 0.75em;
            color: #666;
            margin-bottom: 6px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            font-weight: 600;
        }}
        
        .summary-card .value {{
            font-size: 2em;
            font-weight: bold;
            line-height: 1;
        }}
        
        .summary-card .status {{
            font-size: 1.6em;
            font-weight: bold;
            padding: 6px;
            border-radius: 5px;
            line-height: 1;
        }}
        
        .pass-text {{
            color: #28a745;
        }}
        
        .fail-text {{
            color: #dc3545;
        }}
        
        .content {{
            padding: 20px;
        }}
        
        .section {{
            margin-bottom: 15px;
        }}
        
        .section-header {{
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 15px 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-radius: 10px;
            cursor: pointer;
            margin-bottom: 15px;
            transition: all 0.3s;
        }}
        
        .section-header:hover {{
            transform: translateX(5px);
        }}
        
        .section-header h2 {{
            font-size: 1.5em;
        }}
        
        .section-header .toggle-icon {{
            font-size: 1.2em;
            transition: transform 0.3s;
        }}
        
        .section-header.collapsed .toggle-icon {{
            transform: rotate(-90deg);
        }}
        
        .section-content {{
            padding: 20px;
            background: #f8f9fa;
            border-radius: 10px;
            margin-bottom: 10px;
        }}
        
        .section-content.collapsed {{
            display: none;
        }}
        
        table {{
            width: 100%;
            border-collapse: collapse;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        
        th {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 600;
            cursor: pointer;
            user-select: none;
        }}
        
        th:hover {{
            background: linear-gradient(135deg, #5568d3 0%, #653a8b 100%);
        }}
        
        td {{
            padding: 12px 15px;
            border-bottom: 1px solid #e0e0e0;
        }}
        
        tr.checker-row:hover {{
            background: #f0f0f0;
        }}
        
        .num-cell {{
            text-align: right;
            font-weight: 600;
        }}
        
        .action-cell {{
            text-align: center;
        }}
        
        .status-pass {{
            border-left: 4px solid #28a745;
        }}
        
        .status-fail {{
            border-left: 4px solid #dc3545;
        }}
        
        .error-details-row {{
            background: #fff3cd !important;
        }}
        
        .error-details {{
            max-height: 400px;
            overflow-y: auto;
            padding: 15px;
            background: white;
            border-radius: 5px;
            font-family: 'Courier New', monospace;
            font-size: 0.85em;
        }}
        
        .error-line {{
            padding: 5px;
            border-bottom: 1px solid #e0e0e0;
            word-wrap: break-word;
        }}
        
        .error-line:last-child {{
            border-bottom: none;
        }}
        
        .cell-item {{
            display: inline-block;
            background: white;
            padding: 6px 12px;
            margin: 4px;
            border-radius: 5px;
            border: 1px solid #ddd;
            font-family: 'Courier New', monospace;
            font-size: 0.85em;
            transition: all 0.3s;
        }}
        
        .cell-item:hover {{
            background: #667eea;
            color: white;
            border-color: #667eea;
            transform: scale(1.05);
        }}
        
        .reports-container, .logs-container {{
            column-count: 2;
            column-gap: 15px;
        }}
        
        @media (max-width: 900px) {{
            .reports-container, .logs-container {{
                column-count: 1;
            }}
        }}
        
        .report-item {{
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 10px 12px;
            background: white;
            margin-bottom: 8px;
            border-radius: 5px;
            border-left: 4px solid #667eea;
            transition: all 0.3s;
            break-inside: avoid;
        }}
        
        .report-item:hover {{
            transform: translateX(5px);
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        
        .report-name {{
            font-weight: 500;
            font-size: 0.9em;
        }}
        
        .run-history-container {{
            column-count: 3;
            column-gap: 15px;
        }}
        
        @media (max-width: 1200px) {{
            .run-history-container {{
                column-count: 2;
            }}
        }}
        
        @media (max-width: 768px) {{
            .run-history-container {{
                column-count: 1;
            }}
        }}
        
        .run-item {{
            padding: 8px 12px;
            background: white;
            margin-bottom: 6px;
            border-radius: 5px;
            border-left: 4px solid #28a745;
            font-family: 'Courier New', monospace;
            font-size: 0.85em;
            break-inside: avoid;
        }}
        
        .btn-expand, .btn-log {{
            padding: 8px 16px;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            font-weight: 500;
            transition: all 0.3s;
        }}
        
        .btn-expand {{
            background: #667eea;
            color: white;
        }}
        
        .btn-expand:hover {{
            background: #5568d3;
            transform: scale(1.05);
        }}
        
        .btn-log {{
            background: #28a745;
            color: white;
        }}
        
        .btn-log:hover {{
            background: #218838;
            transform: scale(1.05);
        }}
        
        .search-box {{
            width: 100%;
            padding: 12px 20px;
            border: 2px solid #667eea;
            border-radius: 25px;
            font-size: 1em;
            margin-bottom: 20px;
            transition: all 0.3s;
        }}
        
        .search-box:focus {{
            outline: none;
            border-color: #764ba2;
            box-shadow: 0 0 10px rgba(102, 126, 234, 0.3);
        }}
        
        .no-data {{
            text-align: center;
            padding: 30px;
            color: #999;
            font-style: italic;
        }}
        
        .filter-buttons {{
            display: flex;
            gap: 10px;
            margin-bottom: 20px;
            flex-wrap: wrap;
        }}
        
        .filter-btn {{
            padding: 10px 20px;
            border: 2px solid #667eea;
            background: white;
            color: #667eea;
            border-radius: 25px;
            cursor: pointer;
            font-weight: 500;
            transition: all 0.3s;
        }}
        
        .filter-btn:hover, .filter-btn.active {{
            background: #667eea;
            color: white;
        }}
        
        .chart-container {{
            background: white;
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 20px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        
        /* Back to Top Button */
        .back-to-top {{
            position: fixed;
            bottom: 30px;
            right: 30px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            border: none;
            cursor: pointer;
            font-size: 1.5em;
            box-shadow: 0 4px 12px rgba(0,0,0,0.3);
            z-index: 1000;
            display: none;
            align-items: center;
            justify-content: center;
            transition: all 0.3s;
        }}
        
        .back-to-top:hover {{
            transform: translateY(-5px);
            box-shadow: 0 6px 16px rgba(0,0,0,0.4);
        }}
        
        .back-to-top.visible {{
            display: flex;
        }}
        
        /* Quick Navigation */
        .quick-nav {{
            position: fixed;
            top: 50%;
            right: 20px;
            transform: translateY(-50%);
            background: white;
            border-radius: 10px;
            padding: 15px 10px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.2);
            z-index: 999;
            max-height: 80vh;
            overflow-y: auto;
        }}
        
        .quick-nav-item {{
            display: block;
            padding: 8px 12px;
            margin: 3px 0;
            color: #667eea;
            text-decoration: none;
            border-radius: 5px;
            font-size: 0.85em;
            transition: all 0.3s;
            white-space: nowrap;
        }}
        
        .quick-nav-item:hover {{
            background: #667eea;
            color: white;
        }}
        
        .quick-nav-toggle {{
            position: fixed;
            top: 50%;
            right: 20px;
            transform: translateY(-50%);
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            padding: 12px 6px;
            border-radius: 8px 0 0 8px;
            cursor: pointer;
            font-size: 0.9em;
            font-weight: bold;
            writing-mode: vertical-rl;
            text-orientation: mixed;
            box-shadow: 0 4px 12px rgba(0,0,0,0.3);
            z-index: 998;
            transition: all 0.3s;
        }}
        
        .quick-nav-toggle:hover {{
            padding-right: 10px;
        }}
        
        /* Sticky Summary Mode */
        .summary.sticky {{
            position: sticky;
            top: 0;
            z-index: 100;
            margin-bottom: 0;
            border-radius: 0;
            box-shadow: 0 4px 12px rgba(0,0,0,0.2);
        }}
        
        /* Compact mode for sections */
        .section-content {{
            padding: 15px;
            background: #f8f9fa;
            border-radius: 10px;
            margin-bottom: 10px;
        }}
        
        @media (max-width: 768px) {{
            .quick-nav, .quick-nav-toggle {{
                display: none;
            }}
        }}
        
        /* Copyright Footer */
        .footer {{
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            text-align: center;
            padding: 20px;
            margin-top: 40px;
            border-radius: 10px;
            font-size: 14px;
        }}
        
        .footer p {{
            margin: 5px 0;
        }}
        
        .footer strong {{
            color: #00ff00;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <img class='logo' src='data:image/png;base64,{logo_data}' alt='AVICE Logo' onclick="showLogoModal()" title="Click to enlarge">
            <div class="header-text">
                <h1>GL Check Analysis Report</h1>
                <div class="workarea-path">Workarea: {os.path.abspath(self.workarea)}</div>
                <div class="workarea-path">Design: {self.design_info.top_hier}</div>
            </div>
        </div>
        
        <!-- Logo Modal -->
        <div id="logoModal" class="logo-modal" onclick="hideLogoModal()">
            <span class="logo-modal-close">&times;</span>
            <img class="logo-modal-content" src='data:image/png;base64,{logo_data}' alt='AVICE Logo'>
        </div>
        
        <div class="summary" id="summary">
            <div class="summary-card">
                <div class="label">Overall Status</div>
                <div class="status {status_class}">{overall_status}</div>
            </div>
            <div class="summary-card">
                <div class="label">Total Errors</div>
                <div class="value">{total_errors}</div>
            </div>
            <div class="summary-card">
                <div class="label">Waived</div>
                <div class="value pass-text">{total_waived}</div>
            </div>
            <div class="summary-card">
                <div class="label">Non-Waived</div>
                <div class="value {status_class}">{total_non_waived}</div>
            </div>
            <div class="summary-card">
                <div class="label">Total Runs</div>
                <div class="value">{len(timestamped_dirs)}</div>
            </div>
        </div>
        
        <div class="content">
            <!-- Error Analysis Section -->
            <div class="section" id="error-analysis">
                <div class="section-header" onclick="toggleSection(this)">
                    <h2>Error Analysis by Checker</h2>
                    <span class="toggle-icon">â–¼</span>
                </div>
                <div class="section-content">
                    <input type="text" class="search-box" id="checkerSearch" placeholder="Search checkers..." oninput="filterCheckers()" onkeyup="filterCheckers()">
                    
                    <div class="filter-buttons">
                        <button class="filter-btn active" onclick="filterByStatus('all', this)">All Checkers</button>
                        <button class="filter-btn" onclick="filterByStatus('fail', this)">With Non-Waived Only</button>
                        <button class="filter-btn" onclick="filterByStatus('pass', this)">Fully Waived Only</button>
                    </div>
                    
                    <table id="checkersTable">
                        <thead>
                            <tr>
                                <th onclick="sortTable(0)">Checker [Code](Name) â†•</th>
                                <th onclick="sortTable(1)">Waived â†•</th>
                                <th onclick="sortTable(2)">Non-Waived â†•</th>
                                <th onclick="sortTable(3)">Total â†•</th>
                                <th>Actions</th>
                            </tr>
                        </thead>
                        <tbody>
                            {''.join(checker_rows)}
                        </tbody>
                    </table>
                </div>
            </div>
            
            <!-- Unified Checker Information Section -->
            <div class="section" id="checker-info">
                <div class="section-header collapsed" onclick="toggleSection(this)">
                    <h2>Checker Information ({len(checker_rules or {{}})} checkers: {len(executed_checkers)} executed, {len(skipped_checkers)} skipped)</h2>
                    <span class="toggle-icon">â–¼</span>
                </div>
                <div class="section-content collapsed">
                    <p style="margin-bottom: 20px; color: #555;">
                        <strong>Description:</strong> Complete checker information including execution status, descriptions, and rules extracted from gl-check.log.
                    </p>
                    
                    <!-- Execution Status Summary -->
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-bottom: 30px;">
                        <div style="padding: 20px; background: linear-gradient(135deg, #e8f5e9 0%, #c8e6c9 100%); border-radius: 8px; border-left: 4px solid #4caf50;">
                            <h3 style="margin: 0 0 15px 0; color: #2e7d32; font-size: 18px;">âœ“ Executed Checkers ({len(executed_checkers)})</h3>
                            <div style="max-height: 200px; overflow-y: auto;">
                                <div style="display: flex; flex-wrap: wrap; gap: 6px;">
"""
        
        if executed_checkers:
            for checker in executed_checkers:
                html += f"""
                                    <span style="padding: 4px 10px; background: white; border-radius: 4px; font-size: 12px; border: 1px solid #4caf50;">{checker}</span>
"""
        else:
            html += """
                                    <span style="color: #666;">No executed checkers</span>
"""
        
        html += f"""
                                </div>
                            </div>
                        </div>
                        
                        <div style="padding: 20px; background: linear-gradient(135deg, #fff3e0 0%, #ffe0b2 100%); border-radius: 8px; border-left: 4px solid #ff9800;">
                            <h3 style="margin: 0 0 15px 0; color: #e65100; font-size: 18px;">âŠ˜ Skipped Checkers ({len(skipped_checkers)})</h3>
                            <div style="max-height: 200px; overflow-y: auto;">
                                <div style="display: flex; flex-wrap: wrap; gap: 6px;">
"""
        
        if skipped_checkers:
            for checker in skipped_checkers:
                html += f"""
                                    <span style="padding: 4px 10px; background: white; border-radius: 4px; font-size: 12px; border: 1px solid #ff9800;">{checker}</span>
"""
        else:
            html += """
                                    <span style="color: #666;">No skipped checkers</span>
"""
        
        html += f"""
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Checker Rules Dictionary -->
                    <h3 style="color: #667eea; margin-bottom: 15px; font-size: 18px;">ðŸ“– Checker Rules Dictionary</h3>
                    <p style="margin-bottom: 15px; color: #555; font-size: 14px;">
                        Detailed descriptions and rules for each checker. Use the search box to filter by name or description.
                    </p>
                    
                    <div class="search-box" style="margin-bottom: 20px;">
                        <input type="text" id="rulesSearch" placeholder="ðŸ” Search checker names or descriptions..." 
                               onkeyup="filterRules()" style="width: 100%; padding: 12px; font-size: 14px; border: 2px solid #667eea; border-radius: 8px;">
                        <div style="margin-top: 10px; color: #666; font-size: 13px;">
                            <span id="rulesCount">{len(checker_rules or {{}})}</span> checker(s) shown | 
                            <a href="javascript:void(0)" onclick="document.getElementById('rulesSearch').value=''; filterRules();" style="color: #667eea;">Clear Filter</a>
                        </div>
                    </div>
                    
                    <div style="max-height: 600px; overflow-y: auto;">
                        <table class="error-table" id="rulesTable">
                            <thead style="position: sticky; top: 0; background: white; z-index: 10;">
                                <tr>
                                    <th style="width: 30%;">Checker Name</th>
                                    <th style="width: 60%;">Description / Rule</th>
                                    <th style="width: 10%; text-align: center;">Errors</th>
                                </tr>
                            </thead>
                            <tbody>
{checker_rules_html}
                            </tbody>
                        </table>
                    </div>
                </div>
            </div>
            
            <!-- Legacy anchor for backward compatibility -->
            <div id="checker-rules"></div>
            <div id="executed-checkers"></div>
            <div id="skipped-checkers"></div>
            
            <!-- Allowed Clock Tree Cells Section -->
            <div class="section" id="clock-cells">
                <div class="section-header collapsed" onclick="toggleSection(this)">
                    <h2>Allowed Clock Tree Cells ({len(allowed_clktree_cells)} cells)</h2>
                    <span class="toggle-icon">â–¼</span>
                </div>
                <div class="section-content collapsed">
                    <input type="text" class="search-box" id="cellSearch" placeholder="Search cells..." oninput="filterCells()" onkeyup="filterCells()">
                    <div id="cellsContainer">
                        {clktree_cells_html}
                    </div>
                </div>
            </div>
            
            <!-- Don't Use Cells Section -->
            <div class="section" id="dont-use">
                <div class="section-header collapsed" onclick="toggleSection(this)">
                    <h2>Don't Use Cells ({len(dont_use_cells)} cells)</h2>
                    <span class="toggle-icon">â–¼</span>
                </div>
                <div class="section-content collapsed">
                    <input type="text" class="search-box" id="dontUseCellSearch" placeholder="Search dont_use cells..." oninput="filterDontUseCells()" onkeyup="filterDontUseCells()">
                    <div id="dontUseCellsContainer">
                        {dont_use_cells_html}
                        {'<div id="dontUseCellsMore" style="display:none;">' + dont_use_cells_more_html + '</div>' if dont_use_cells_more_html else ''}
                    </div>
                    {f'<button class="btn-show-more" onclick="toggleDontUseCells(this)" style="margin-top: 15px; width: 100%; padding: 12px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border: none; border-radius: 8px; cursor: pointer; font-size: 1em; font-weight: bold; transition: all 0.3s;">Show {len(dont_use_cells) - 50} More Cells</button>' if len(dont_use_cells) > 50 else ''}
                </div>
            </div>
            
            <!-- Key Reports Section -->
            <div class="section" id="reports">
                <div class="section-header collapsed" onclick="toggleSection(this)">
                    <h2>Key Reports ({len(key_reports)} reports)</h2>
                    <span class="toggle-icon">â–¼</span>
                </div>
                <div class="section-content collapsed">
                    {reports_html}
                </div>
            </div>
            
            <!-- Logs & Source Files Section (Combined) -->
            <div class="section" id="logs">
                <div class="section-header collapsed" onclick="toggleSection(this)">
                    <h2>Logs & Source Files ({len(main_logs) + 2} files)</h2>
                    <span class="toggle-icon">â–¼</span>
                </div>
                <div class="section-content collapsed">
                    {logs_html}
                    <div class="report-item">
                        <span class="report-name">Waived Errors File</span>
                        <button class="btn-log" onclick="openLogWithServer('{waived_file}', event)">ðŸ–¥ï¸ tablog</button>
                    </div>
                    <div class="report-item">
                        <span class="report-name">Non-Waived Errors File</span>
                        <button class="btn-log" onclick="openLogWithServer('{non_waived_file}', event)">ðŸ–¥ï¸ tablog</button>
                    </div>
                </div>
            </div>
            
            <!-- Run History Section -->
            <div class="section" id="history">
                <div class="section-header collapsed" onclick="toggleSection(this)">
                    <h2>Run History ({len(timestamped_dirs)} runs)</h2>
                    <span class="toggle-icon">â–¼</span>
                </div>
                <div class="section-content collapsed">
                    {run_history_html}
                </div>
            </div>
        </div>
    </div>
    
    <!-- Back to Top Button -->
    <button class="back-to-top" id="backToTop" onclick="scrollToTop()" title="Back to Top">&#8593;</button>
    
    <!-- Quick Navigation Toggle -->
    <button class="quick-nav-toggle" id="quickNavToggle" onclick="toggleQuickNav()">MENU</button>
    
    <!-- Quick Navigation Menu -->
    <div class="quick-nav" id="quickNav" style="display: none;">
        <a href="javascript:void(0)" class="quick-nav-item" onclick="scrollToSection('summary')">Summary</a>
        <a href="javascript:void(0)" class="quick-nav-item" onclick="scrollToSection('error-analysis')">Error Analysis</a>
        <a href="javascript:void(0)" class="quick-nav-item" onclick="scrollToSection('checker-info')">Checker Info</a>
        <a href="javascript:void(0)" class="quick-nav-item" onclick="scrollToSection('clock-cells')">Clock Cells</a>
        <a href="javascript:void(0)" class="quick-nav-item" onclick="scrollToSection('dont-use')">Don't Use</a>
        <a href="javascript:void(0)" class="quick-nav-item" onclick="scrollToSection('reports')">Reports</a>
        <a href="javascript:void(0)" class="quick-nav-item" onclick="scrollToSection('logs')">Logs & Files</a>
        <a href="javascript:void(0)" class="quick-nav-item" onclick="scrollToSection('history')">History</a>
    </div>
    
    <script>
        let currentFilter = 'all';
        
        // Initialize search boxes when page loads
        document.addEventListener('DOMContentLoaded', function() {{
            const checkerSearch = document.getElementById('checkerSearch');
            const cellSearch = document.getElementById('cellSearch');
            
            if (checkerSearch) {{
                checkerSearch.addEventListener('input', filterCheckers);
                checkerSearch.addEventListener('keyup', filterCheckers);
            }}
            
            if (cellSearch) {{
                cellSearch.addEventListener('input', filterCells);
                cellSearch.addEventListener('keyup', filterCells);
            }}
        }});
        
        function toggleSection(header) {{
            const content = header.nextElementSibling;
            const icon = header.querySelector('.toggle-icon');
            
            content.classList.toggle('collapsed');
            header.classList.toggle('collapsed');
        }}
        
        function toggleErrorDetails(btn) {{
            const row = btn.closest('tr');
            const detailsRow = row.nextElementSibling;
            
            if (detailsRow && detailsRow.classList.contains('error-details-row')) {{
                if (detailsRow.style.display === 'none') {{
                    detailsRow.style.display = 'table-row';
                    btn.textContent = 'Hide Errors';
                }} else {{
                    detailsRow.style.display = 'none';
                    btn.textContent = 'Show Errors';
                }}
            }}
        }}
        
        function filterCheckers() {{
            const searchTerm = document.getElementById('checkerSearch').value.toLowerCase();
            const rows = document.querySelectorAll('#checkersTable tbody tr.checker-row');
            
            // Convert NodeList to Array for compatibility with older browsers
            Array.prototype.slice.call(rows).forEach(function(row) {{
                const checker = row.getAttribute('data-checker').toLowerCase();
                const matchesSearch = checker.includes(searchTerm);
                const matchesFilter = currentFilter === 'all' || 
                                     (currentFilter === 'fail' && parseInt(row.getAttribute('data-nonwaived')) > 0) ||
                                     (currentFilter === 'pass' && parseInt(row.getAttribute('data-nonwaived')) === 0);
                
                if (matchesSearch && matchesFilter) {{
                    row.style.display = '';
                    const detailsRow = row.nextElementSibling;
                    if (detailsRow && detailsRow.classList.contains('error-details-row')) {{
                        if (detailsRow.style.display === 'table-row') {{
                            detailsRow.style.display = 'table-row';
                        }}
                    }}
                }} else {{
                    row.style.display = 'none';
                    const detailsRow = row.nextElementSibling;
                    if (detailsRow && detailsRow.classList.contains('error-details-row')) {{
                        detailsRow.style.display = 'none';
                    }}
                }}
            }});
        }}
        
        function filterByStatus(status, button) {{
            currentFilter = status;
            
            // Update button states
            const buttons = document.querySelectorAll('.filter-btn');
            Array.prototype.slice.call(buttons).forEach(function(btn) {{
                btn.classList.remove('active');
            }});
            button.classList.add('active');
            
            filterCheckers();
        }}
        
        function sortTable(columnIndex) {{
            const table = document.getElementById('checkersTable');
            const tbody = table.querySelector('tbody');
            const rowsNodeList = tbody.querySelectorAll('tr.checker-row');
            const rows = Array.prototype.slice.call(rowsNodeList);
            
            rows.sort(function(a, b) {{
                let aVal, bVal;
                
                if (columnIndex === 0) {{
                    aVal = a.getAttribute('data-checker');
                    bVal = b.getAttribute('data-checker');
                    return aVal.localeCompare(bVal);
                }} else if (columnIndex === 1) {{
                    aVal = parseInt(a.getAttribute('data-waived'));
                    bVal = parseInt(b.getAttribute('data-waived'));
                }} else if (columnIndex === 2) {{
                    aVal = parseInt(a.getAttribute('data-nonwaived'));
                    bVal = parseInt(b.getAttribute('data-nonwaived'));
                }} else if (columnIndex === 3) {{
                    aVal = parseInt(a.getAttribute('data-total'));
                    bVal = parseInt(b.getAttribute('data-total'));
                }}
                
                return bVal - aVal;
            }});
            
            // Re-append rows with their detail rows
            rows.forEach(function(row) {{
                tbody.appendChild(row);
                const detailsRow = row.nextElementSibling;
                if (detailsRow && detailsRow.classList.contains('error-details-row')) {{
                    tbody.appendChild(detailsRow);
                }}
            }});
        }}
        
        function filterCells() {{
            const searchTerm = document.getElementById('cellSearch').value.toLowerCase();
            const container = document.getElementById('cellsContainer');
            const cells = container.querySelectorAll('.cell-item');
            
            // Convert NodeList to Array for compatibility with older browsers
            Array.prototype.slice.call(cells).forEach(function(cell) {{
                const cellText = cell.textContent.toLowerCase();
                cell.style.display = cellText.includes(searchTerm) ? 'inline-block' : 'none';
            }});
        }}
        
        function filterDontUseCells() {{
            const searchTerm = document.getElementById('dontUseCellSearch').value.toLowerCase();
            const container = document.getElementById('dontUseCellsContainer');
            const cells = container.querySelectorAll('.cell-item');
            
            // Convert NodeList to Array for compatibility with older browsers
            Array.prototype.slice.call(cells).forEach(function(cell) {{
                const cellText = cell.textContent.toLowerCase();
                cell.style.display = cellText.includes(searchTerm) ? 'inline-block' : 'none';
            }});
        }}
        
        function filterRules() {{
            const searchTerm = document.getElementById('rulesSearch').value.toLowerCase();
            const rows = document.querySelectorAll('#rulesTable tbody tr.rules-row');
            let visibleCount = 0;
            
            // Convert NodeList to Array for compatibility with older browsers
            Array.prototype.slice.call(rows).forEach(function(row) {{
                const checkerName = row.getAttribute('data-checker').toLowerCase();
                const description = row.getAttribute('data-desc').toLowerCase();
                const matchesSearch = checkerName.includes(searchTerm) || description.includes(searchTerm);
                
                if (matchesSearch) {{
                    row.style.display = '';
                    visibleCount++;
                }} else {{
                    row.style.display = 'none';
                }}
            }});
            
            // Update visible count
            const countElement = document.getElementById('rulesCount');
            if (countElement) {{
                countElement.textContent = visibleCount;
            }}
        }}
        
        function toggleDontUseCells(btn) {{
            const moreSection = document.getElementById('dontUseCellsMore');
            if (moreSection) {{
                if (moreSection.style.display === 'none') {{
                    moreSection.style.display = 'block';
                    btn.textContent = 'Show Less';
                }} else {{
                    moreSection.style.display = 'none';
                    btn.textContent = 'Show {len(dont_use_cells) - 50} More Cells';
                }}
            }}
        }}
        
        // Back to Top functionality
        function scrollToTop() {{
            window.scrollTo({{
                top: 0,
                behavior: 'smooth'
            }});
        }}
        
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {{
            var backToTopBtn = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {{
                backToTopBtn.classList.add('visible');
            }} else {{
                backToTopBtn.classList.remove('visible');
            }}
        }});
        
        // Quick Navigation functions
        function toggleQuickNav() {{
            const quickNav = document.getElementById('quickNav');
            const quickNavToggle = document.getElementById('quickNavToggle');
            
            if (quickNav.style.display === 'none') {{
                quickNav.style.display = 'block';
                quickNavToggle.style.display = 'none';
            }} else {{
                quickNav.style.display = 'none';
                quickNavToggle.style.display = 'block';
            }}
        }}
        
        function scrollToSection(sectionId) {{
            const section = document.getElementById(sectionId);
            if (section) {{
                section.scrollIntoView({{
                    behavior: 'smooth',
                    block: 'start'
                }});
                
                // If section is collapsed, expand it
                const sectionHeader = section.querySelector('.section-header');
                if (sectionHeader && sectionHeader.classList.contains('collapsed')) {{
                    toggleSection(sectionHeader);
                }}
            }}
            
            // Close quick nav after selection
            toggleQuickNav();
            return false;
        }}
        
        // Close quick nav when clicking outside
        document.addEventListener('click', function(event) {{
            const quickNav = document.getElementById('quickNav');
            const quickNavToggle = document.getElementById('quickNavToggle');
            
            if (quickNav && quickNav.style.display === 'block' &&
                !quickNav.contains(event.target) && 
                !quickNavToggle.contains(event.target)) {{
                toggleQuickNav();
            }}
        }});
        
        // Logo modal functions
        function showLogoModal() {{
            document.getElementById('logoModal').classList.add('active');
        }}
        
        function hideLogoModal() {{
            document.getElementById('logoModal').classList.remove('active');
        }}
        
        // Allow ESC key to close logo modal
        document.addEventListener('keydown', function(event) {{
            if (event.key === 'Escape') {{
                hideLogoModal();
            }}
        }});
    </script>
    
    <!-- Tablog Server Integration JavaScript -->
    <script>
        // Show toast notification
        function showToast(message, type) {{
            const toast = document.createElement('div');
            toast.className = 'toast toast-' + type;
            toast.textContent = message;
            toast.style.cssText = `
                position: fixed;
                bottom: 30px;
                right: 30px;
                padding: 15px 25px;
                border-radius: 8px;
                color: white;
                font-weight: 600;
                box-shadow: 0 4px 12px rgba(0,0,0,0.3);
                z-index: 10000;
                opacity: 0;
                transform: translateY(20px);
                transition: all 0.3s ease;
            `;
            
            if (type === 'success') {{
                toast.style.background = 'linear-gradient(135deg, #27ae60 0%, #229954 100%)';
            }} else if (type === 'error') {{
                toast.style.background = 'linear-gradient(135deg, #e74c3c 0%, #c0392b 100%)';
            }} else if (type === 'warning') {{
                toast.style.background = 'linear-gradient(135deg, #f39c12 0%, #d68910 100%)';
            }}
            
            document.body.appendChild(toast);
            
            setTimeout(function() {{
                toast.style.opacity = '1';
                toast.style.transform = 'translateY(0)';
            }}, 100);
            
            setTimeout(function() {{
                toast.style.opacity = '0';
                toast.style.transform = 'translateY(20px)';
                setTimeout(function() {{ toast.remove(); }}, 300);
            }}, 3000);
        }}
        
        // Copy text to clipboard
        function copyToClipboard(text) {{
            navigator.clipboard.writeText(text).then(function() {{
                showToast('âœ“ Command copied to clipboard! Paste in terminal to execute.', 'warning');
            }}).catch(function(err) {{
                showToast('Failed to copy: ' + err, 'error');
            }});
        }}
        
        // Open log with server (with fallback to clipboard)
        function openLogWithServer(logfile, event) {{
            if (event) {{
                event.preventDefault();
            }}
            
            const serverUrl = 'http://localhost:8888/open_log?file=' + encodeURIComponent(logfile);
            
            // Try to open via server
            fetch(serverUrl, {{ method: 'GET', mode: 'cors' }})
                .then(function(response) {{
                    if (response.ok) {{
                        showToast('âœ“ Opening in tablog...', 'success');
                    }} else {{
                        throw new Error('Server returned error');
                    }}
                }})
                .catch(function(error) {{
                    // Server not running - fallback to clipboard
                    const command = '/home/scratch.avice_vlsi/tablog/tablog ' + logfile;
                    copyToClipboard(command);
                }});
            
            return false;
        }}
    </script>
    
    <!-- Copyright Footer -->
    <div class="footer">
        <p><strong>AVICE GL Check Analysis Report</strong></p>
        <p>Copyright (c) 2025 Alon Vice (avice)</p>
        <p>Contact: avice@nvidia.com</p>
    </div>
</body>
</html>
        """
        
        return html
    
    def _parse_gl_check_rules(self, gl_check_log_path: str) -> tuple:
        """Parse checker rules and descriptions from gl-check.log
        
        Args:
            gl_check_log_path: Path to GL Check log file
            
        Returns:
            tuple: (checker_rules, executed_checkers, skipped_checkers)
                checker_rules: Dictionary mapping checker names to their descriptions
                executed_checkers: List of checker names that were executed
                skipped_checkers: List of checker names that were skipped
        """
        checker_rules = {}
        executed_checkers = []
        skipped_checkers = []
        
        if not os.path.exists(gl_check_log_path):
            return checker_rules, executed_checkers, skipped_checkers
        
        try:
            with open(gl_check_log_path, 'r') as f:
                content = f.read()
            
            # Parse executed checkers list
            executed_match = re.search(r'-I-\(<module>\)Executing checkers:\s*(.*?)(?=-[IEW]-|\Z)', content, re.DOTALL)
            if executed_match:
                executed_text = executed_match.group(1)
                # Extract checker names (they appear in the list, one per line or comma-separated)
                executed_checkers = [line.strip() for line in executed_text.strip().split('\n') if line.strip() and not line.strip().startswith('-')]
                # Also handle comma-separated format
                if len(executed_checkers) == 1 and ',' in executed_checkers[0]:
                    executed_checkers = [c.strip() for c in executed_checkers[0].split(',') if c.strip()]
            
            # Parse skipped checkers list
            skipped_match = re.search(r'-I-\(<module>\)Skipping checkers:\s*(.*?)(?=-[IEW]-|\Z)', content, re.DOTALL)
            if skipped_match:
                skipped_text = skipped_match.group(1)
                # Extract checker names
                skipped_checkers = [line.strip() for line in skipped_text.strip().split('\n') if line.strip() and not line.strip().startswith('-')]
                # Also handle comma-separated format
                if len(skipped_checkers) == 1 and ',' in skipped_checkers[0]:
                    skipped_checkers = [c.strip() for c in skipped_checkers[0].split(',') if c.strip()]
            
            # Find all checker blocks using regex
            # Pattern: Start <checker_name> ... <content> ... End <checker_name>
            checker_pattern = re.compile(r'Start\s+(\w+)\s+\.\.\.(.*?)End\s+\1', re.DOTALL)
            
            for match in checker_pattern.finditer(content):
                checker_name = match.group(1)
                checker_content = match.group(2)
                
                # Extract description (lines before first -I-/-E-/-W- line or "End Time:")
                description_lines = []
                error_count = 0
                elapsed_time = "N/A"
                
                for line in checker_content.split('\n'):
                    line_stripped = line.strip()
                    
                    # Extract elapsed time - try multiple formats
                    if line_stripped.startswith('Elapsed Time:'):
                        elapsed_time = line_stripped.replace('Elapsed Time:', '').strip()
                    elif line_stripped.startswith('Elapsed time:'):
                        elapsed_time = line_stripped.replace('Elapsed time:', '').strip()
                    elif 'elapsed time:' in line_stripped.lower():
                        # Handle case-insensitive match
                        parts = line_stripped.split(':', 1)
                        if len(parts) > 1:
                            elapsed_time = parts[1].strip()
                    
                    # Count errors
                    if line_stripped.startswith('-E-'):
                        error_count += 1
                    
                    # Stop collecting description when we hit logs (but NOT End Time - need to get Elapsed Time first)
                    if line_stripped.startswith(('-I-', '-E-', '-W-')):
                        break
                    
                    # Collect description lines (skip empty lines at start, and skip time stamps)
                    if not line_stripped.startswith(('End Time:', 'End time:', 'Elapsed Time:', 'Elapsed time:')):
                        if line_stripped or description_lines:
                            description_lines.append(line_stripped)
                
                # Clean up description
                description = '\n'.join(description_lines).strip()
                
                # Format elapsed time nicely (remove "seconds" suffix if present, keep it clean)
                if elapsed_time != "N/A":
                    # Handle formats like "123 seconds", "123s", "1.5 hours", etc.
                    elapsed_time = elapsed_time.replace(' seconds', 's').replace('seconds', 's')
                    if elapsed_time and elapsed_time[-1] != 's' and elapsed_time[-1] != 'h':
                        # If it's just a number, assume seconds
                        try:
                            float(elapsed_time)
                            elapsed_time = f"{elapsed_time}s"
                        except ValueError:
                            pass
                
                # Store checker info
                checker_rules[checker_name] = {
                    'description': description,
                    'elapsed_time': elapsed_time,
                    'error_count': error_count
                }
        
        except Exception as e:
            print(f"    {Color.YELLOW}[WARN] Error parsing GL Check rules: {e}{Color.RESET}")
        
        return checker_rules, executed_checkers, skipped_checkers
    
    def _generate_gl_check_html_report(self, gl_check_dir: str, waived_file: str, non_waived_file: str, timestamped_dirs: List[str]) -> Optional[str]:
        """Generate comprehensive HTML report for GL Check analysis
        
        Args:
            gl_check_dir: Path to GL check directory
            waived_file: Path to waived errors file
            non_waived_file: Path to non-waived errors file
            timestamped_dirs: List of timestamped directory paths
            
        Returns:
            HTML filename if generated successfully, None otherwise
        """
        try:
            import yaml
            from datetime import datetime
            
            # Parse error files to get checker data
            waived_checkers = {}
            non_waived_checkers = {}
            non_waived_errors_detail = {}  # Store full error messages by checker
            
            if os.path.exists(waived_file):
                with open(waived_file, 'r') as f:
                    for line in f:
                        match = re.match(r'-E-\[(\d+)\]\((\w+)\)(.*)', line)
                        if match:
                            code, checker, message = match.groups()
                            key = f"[{code}]({checker})"
                            waived_checkers[key] = waived_checkers.get(key, 0) + 1
            
            if os.path.exists(non_waived_file):
                with open(non_waived_file, 'r') as f:
                    for line in f:
                        match = re.match(r'-E-\[(\d+)\]\((\w+)\)(.*)', line)
                        if match:
                            code, checker, message = match.groups()
                            key = f"[{code}]({checker})"
                            non_waived_checkers[key] = non_waived_checkers.get(key, 0) + 1
                            if key not in non_waived_errors_detail:
                                non_waived_errors_detail[key] = []
                            non_waived_errors_detail[key].append(line.strip())
            
            # Parse beflow_config.yaml for allowed_clktree_cells_rex and dont_use_cells_by_rex
            beflow_config_file = os.path.join(gl_check_dir, "beflow_config.yaml")
            allowed_clktree_cells = []
            dont_use_cells = []
            if os.path.exists(beflow_config_file):
                try:
                    with open(beflow_config_file, 'r') as f:
                        content = f.read()
                        
                    # Extract allowed_clktree_cells_rex
                    for line in content.split('\n'):
                            if 'allowed_clktree_cells_rex' in line:
                                match = re.search(r'\[(.*)\]', line)
                                if match:
                                    cells_str = match.group(1)
                                    cells = [cell.strip().strip("'\"") for cell in re.findall(r"'([^']*)'", cells_str)]
                                    allowed_clktree_cells = sorted(cells)
                                break
                    
                    # Extract dont_use_cells_by_rex (can be multi-line)
                    dont_use_match = re.search(r'dont_use_cells_by_rex\s*:\s*\[(.*?)\]', content, re.DOTALL)
                    if dont_use_match:
                        cells_str = dont_use_match.group(1)
                        cells = [cell.strip().strip("'\"") for cell in re.findall(r"'([^']*)'", cells_str)]
                        dont_use_cells = sorted(cells)
                except Exception as e:
                    pass
            
            # Find key report files
            reports_dir = os.path.join(gl_check_dir, "reports")
            key_reports = []
            if os.path.exists(reports_dir):
                report_files = {
                    'Clock Tree Report': 'ClockTree.rpt',
                    'Don\'t Use Cells': 'dontUseCells.rpt',
                    'Cell Statistics': 'cellStats.rpt',
                    'Scan Chains': 'ScanChains.rpt',
                    'DFT Clock Tree (Shift)': 'DFT_clocktree_shift.rpt',
                    'DFT Clock Tree (Capture)': 'DFT_clocktree_capture.rpt',
                    'Illegal DCAP Connections': 'illegal_dcap_connections.rpt',
                    'Sequential to Root Clock': 'seq2RootClk.rpt'
                }
                for name, filename in report_files.items():
                    filepath = os.path.join(reports_dir, filename)
                    if os.path.exists(filepath):
                        # Convert to absolute path for HTML links to work from any location
                        key_reports.append((name, os.path.abspath(filepath)))
            
            # Find main log files
            main_logs = []
            log_files = {
                'GL Check Log': 'gl-check.log',
                'BeFlow2YAML Log': 'beflow2yaml.log',
                'Vivid Log': 'vivid.log'
            }
            # Check in latest timestamped directory first
            if timestamped_dirs:
                latest_run_dir = os.path.join(gl_check_dir, timestamped_dirs[-1])
                for name, filename in log_files.items():
                    filepath = os.path.join(latest_run_dir, filename)
                    if os.path.exists(filepath):
                        # Convert to absolute path for HTML links to work from any location
                        main_logs.append((name, os.path.abspath(filepath)))
            # Also check root gl-check directory
            for name, filename in log_files.items():
                filepath = os.path.join(gl_check_dir, filename)
                abs_filepath = os.path.abspath(filepath)
                if os.path.exists(filepath) and (name, abs_filepath) not in main_logs:
                    # Convert to absolute path for HTML links to work from any location
                    main_logs.append((name, abs_filepath))
            
            # Parse checker rules from gl-check.log
            checker_rules = {}
            executed_checkers = []
            skipped_checkers = []
            gl_check_log_path = None
            for log_name, log_path in main_logs:
                if log_name == 'GL Check Log':
                    gl_check_log_path = log_path
                    break
            
            if gl_check_log_path and os.path.exists(gl_check_log_path):
                print(f"    {Color.CYAN}Parsing checker rules from gl-check.log...{Color.RESET}")
                checker_rules, executed_checkers, skipped_checkers = self._parse_gl_check_rules(gl_check_log_path)
                print(f"    {Color.GREEN}[OK] Found {len(checker_rules)} checker rules ({len(executed_checkers)} executed, {len(skipped_checkers)} skipped){Color.RESET}")
            
            # Combine all checkers
            all_checkers = set(waived_checkers.keys()) | set(non_waived_checkers.keys())
            
            # Calculate totals
            total_waived = sum(waived_checkers.values())
            total_non_waived = sum(non_waived_checkers.values())
            total_errors = total_waived + total_non_waived
            
            # Sort checkers by total count
            sorted_checkers = sorted(all_checkers, 
                                    key=lambda x: waived_checkers.get(x, 0) + non_waived_checkers.get(x, 0), 
                                    reverse=True)
            
            # Generate HTML
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            html_filename = f"{self.design_info.top_hier}_{os.environ.get('USER', 'avice')}_gl_check_report_{timestamp}.html"
            html_path = os.path.join(os.getcwd(), html_filename)
            
            html_content = self._generate_gl_check_html_content(
                waived_checkers, non_waived_checkers, non_waived_errors_detail,
                sorted_checkers, total_errors, total_waived, total_non_waived,
                allowed_clktree_cells, dont_use_cells, key_reports, main_logs, timestamped_dirs,
                waived_file, non_waived_file, checker_rules, executed_checkers, skipped_checkers
            )
            
            with open(html_path, 'w') as f:
                f.write(html_content)
            
            # Determine display path
            html_output_dir = self._get_html_output_dir()
            display_path = os.path.relpath(html_output_dir, os.getcwd())
            
            print(f"\n  Open with: /home/utils/firefox-118.0.1/firefox {Color.MAGENTA}{display_path}/{html_filename}{Color.RESET} &")
            
            return os.path.abspath(html_path)
            
        except Exception as e:
            print(f"  Error generating GL Check HTML report: {e}")
            import traceback
            traceback.print_exc()
            return ""
    
    def _generate_pv_html_report(self, lvs_data: Optional[Dict[str, Any]], drc_data: Optional[Dict[str, Any]], antenna_data: Optional[Dict[str, Any]], pv_flow_data: Optional[Dict[str, Any]], timeline_data: Optional[tuple]) -> Optional[str]:
        """Generate comprehensive HTML report for Physical Verification
        
        Args:
            lvs_data: LVS error data or None
            drc_data: DRC error data or None
            antenna_data: Antenna violation data or None
            pv_flow_data: PV flow status data or None
            timeline_data: Timeline data tuple or None
            
        Returns:
            Path to generated HTML file or None if generation fails
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            username = os.environ.get('USER', 'avice')
            html_filename = f"{self.design_info.top_hier}_{username}_pv_report_{timestamp}.html"
            html_path = os.path.abspath(html_filename)
            
            # Determine overall status based on violations
            lvs_violations = lvs_data.get('failed_equivalence_points', 0)
            drc_violations = drc_data.get('total_violations', 0)
            antenna_violations = antenna_data.get('total_violations', 0)
            
            overall_status = "PASS"
            if lvs_violations > 5 or drc_violations > 100 or antenna_violations > 10:
                overall_status = "FAIL"
            elif lvs_violations > 0 or drc_violations > 0 or antenna_violations > 0:
                overall_status = "WARN"
            
            status_color = "#28a745" if overall_status == "PASS" else "#ffc107" if overall_status == "WARN" else "#dc3545"
            
            # Generate HTML content
            html_content = self._generate_pv_html_content(
                lvs_data, drc_data, antenna_data, pv_flow_data, timeline_data,
                overall_status, status_color, html_filename
            )
            
            # Write HTML file
            with open(html_path, 'w') as f:
                f.write(html_content)
            
            return html_path
            
        except Exception as e:
            print(f"  Error generating PV HTML report: {e}")
            import traceback
            traceback.print_exc()
            return ""
    
    def _generate_pv_html_content(self, lvs_data: Optional[Dict[str, Any]], drc_data: Optional[Dict[str, Any]], antenna_data: Optional[Dict[str, Any]], pv_flow_data: Optional[Dict[str, Any]], timeline_data: Optional[tuple],
                                   overall_status: str, status_color: str, html_filename: str) -> str:
        """Generate HTML content for PV report
        
        Args:
            lvs_data: LVS error data or None
            drc_data: DRC error data or None
            antenna_data: Antenna violation data or None
            pv_flow_data: PV flow status data or None
            timeline_data: Timeline data tuple or None
            overall_status: Overall status string
            status_color: Status color string
            html_filename: Name of HTML file
            
        Returns:
            HTML content string
        """
        
        # Load and encode logo
        logo_data = ""
        logo_path = os.path.join(os.path.dirname(__file__), "assets", "images", "avice_logo.png")
        try:
            with open(logo_path, 'rb') as f:
                import base64
                logo_data = base64.b64encode(f.read()).decode('utf-8')
        except:
            pass  # If logo not found, continue without it
        
        # Extract data
        lvs_violations = lvs_data.get('failed_equivalence_points', 0)
        drc_violations = drc_data.get('total_violations', 0)
        antenna_violations = antenna_data.get('total_violations', 0)
        
        # Timeline HTML
        timeline_html = ""
        if timeline_data:
            start_time = timeline_data.get('start', 'N/A')
            end_time = timeline_data.get('end', 'N/A')
            duration = timeline_data.get('duration', 'N/A')
            timeline_html = f"""
        <div class="timeline-section">
            <h3>PV Flow Timeline</h3>
            <div class="timeline-info">
                <div><strong>Started:</strong> {start_time}</div>
                <div><strong>Finished:</strong> {end_time}</div>
                <div><strong>Duration:</strong> {duration}</div>
            </div>
        </div>
"""
        
        # LVS section HTML
        lvs_status_class = "status-pass" if lvs_violations == 0 else "status-fail"
        lvs_html = f"""
        <div class="section {lvs_status_class}">
            <div class="section-header" onclick="toggleSection('lvs-section')">
                <h2>LVS Results</h2>
                <span class="toggle-icon">â–¼</span>
            </div>
            <div class="section-content" id="lvs-section">
                <table>
                    <tr>
                        <th>Metric</th>
                        <th>Value</th>
                    </tr>
                    <tr>
                        <td>Status</td>
                        <td class="{'pass-text' if lvs_data.get('status') != 'FAIL' else 'fail-text'}">{lvs_data.get('status', 'UNKNOWN')}</td>
                    </tr>
                    <tr>
                        <td>Failed Equivalence Points</td>
                        <td class="num-cell {'fail-text' if lvs_violations > 0 else 'pass-text'}">{lvs_violations}</td>
                    </tr>
                    <tr>
                        <td>Successful Equivalence Points</td>
                        <td class="num-cell">{lvs_data.get('successful_equivalence_points', 0)}</td>
                    </tr>
                    <tr>
                        <td>First Priority Errors</td>
                        <td class="num-cell">{lvs_data.get('first_priority_errors', 0)}</td>
                    </tr>
                    <tr>
                        <td>Second Priority Errors</td>
                        <td class="num-cell">{lvs_data.get('second_priority_errors', 0)}</td>
                    </tr>
                </table>
                
                {'<h3>Unmatched Items</h3>' if lvs_violations > 0 else ''}
                {'<table>' if lvs_violations > 0 else ''}
"""
        
        if lvs_violations > 0:
            unmatched_items = [
                ('Schematic Instances', lvs_data.get('unmatched_schematic_instances', 0)),
                ('Schematic Nets', lvs_data.get('unmatched_schematic_nets', 0)),
                ('Layout Instances', lvs_data.get('unmatched_layout_instances', 0)),
                ('Layout Nets', lvs_data.get('unmatched_layout_nets', 0)),
                ('Schematic Ports', lvs_data.get('unmatched_schematic_ports', 0)),
                ('Layout Ports', lvs_data.get('unmatched_layout_ports', 0))
            ]
            
            for item_name, count in unmatched_items:
                if count > 0:
                    lvs_html += f"""
                    <tr>
                        <td>{item_name}</td>
                        <td class="num-cell fail-text">{count}</td>
                    </tr>
"""
            lvs_html += """
                </table>
"""
        
        lvs_html += f"""
                <h3>Matched Items</h3>
                <table>
                    <tr>
                        <td>Instances</td>
                        <td class="num-cell">{lvs_data.get('matched_instances', 0):,}</td>
                    </tr>
                    <tr>
                        <td>Nets</td>
                        <td class="num-cell">{lvs_data.get('matched_nets', 0):,}</td>
                    </tr>
                    <tr>
                        <td>Ports</td>
                        <td class="num-cell">{lvs_data.get('matched_ports', 0):,}</td>
                    </tr>
                </table>
                
                {'<p><strong>File:</strong> <a href="file://' + lvs_data.get('file_path', '') + '" target="_blank">' + os.path.basename(lvs_data.get('file_path', '')) + '</a></p>' if lvs_data.get('file_path') else ''}
            </div>
        </div>
"""
        
        # DRC section HTML
        drc_status_class = "status-pass" if drc_violations == 0 else "status-fail"
        drc_violations_list = drc_data.get('violations', [])
        
        drc_html = f"""
        <div class="section {drc_status_class}">
            <div class="section-header" onclick="toggleSection('drc-section')">
                <h2>DRC Results</h2>
                <span class="toggle-icon">â–¼</span>
            </div>
            <div class="section-content" id="drc-section">
                <div class="summary-cards">
                    <div class="summary-card">
                        <div class="label">Status</div>
                        <div class="status {'pass-text' if drc_violations == 0 else 'fail-text'}">
                            {'CLEAN' if drc_violations == 0 else 'ERRORS'}
                        </div>
                    </div>
                    <div class="summary-card">
                        <div class="label">Total Violations</div>
                        <div class="value {'fail-text' if drc_violations > 0 else 'pass-text'}">{drc_violations}</div>
                    </div>
                </div>
                
                {'<h3>Violation Breakdown</h3>' if drc_violations > 0 else ''}
                {'<table><thead><tr><th>Rule</th><th>Count</th><th>Description</th></tr></thead><tbody>' if drc_violations > 0 else ''}
"""
        
        for rule, desc, count in drc_violations_list:
            drc_html += f"""
                <tr>
                    <td><strong>{rule}</strong></td>
                    <td class="num-cell fail-text">{count}</td>
                    <td>{desc}</td>
                </tr>
"""
        
        if drc_violations > 0:
            drc_html += """
                </tbody></table>
"""
        
        drc_html += f"""
                {'<p><strong>File:</strong> <a href="file://' + drc_data.get('file_path', '') + '" target="_blank">' + os.path.basename(drc_data.get('file_path', '')) + '</a></p>' if drc_data.get('file_path') else ''}
            </div>
        </div>
"""
        
        # Antenna section HTML
        antenna_status_class = "status-pass" if antenna_violations == 0 else "status-fail"
        
        antenna_html = f"""
        <div class="section {antenna_status_class}">
            <div class="section-header" onclick="toggleSection('antenna-section')">
                <h2>Antenna Results</h2>
                <span class="toggle-icon">â–¼</span>
            </div>
            <div class="section-content" id="antenna-section">
                <div class="summary-cards">
                    <div class="summary-card">
                        <div class="label">Status</div>
                        <div class="status {'pass-text' if antenna_violations == 0 else 'fail-text'}">
                            {'CLEAN' if antenna_violations == 0 else 'ERRORS'}
                        </div>
                    </div>
                    <div class="summary-card">
                        <div class="label">Total Violations</div>
                        <div class="value {'fail-text' if antenna_violations > 0 else 'pass-text'}">{antenna_violations}</div>
                    </div>
                </div>
                {'<p><strong>File:</strong> <a href="file://' + antenna_data.get('file_path', '') + '" target="_blank">' + os.path.basename(antenna_data.get('file_path', '')) + '</a></p>' if antenna_data.get('file_path') else ''}
            </div>
        </div>
"""
        
        # PV Flow section HTML
        pv_flow_html = ""
        if pv_flow_data:
            pv_flow_html = """
        <div class="section">
            <div class="section-header" onclick="toggleSection('pv-flow-section')">
                <h2>PV Flow Analysis</h2>
                <span class="toggle-icon">â–¼</span>
            </div>
            <div class="section-content" id="pv-flow-section">
"""
            
            for exp_name, exp_data in pv_flow_data.items():
                completed_steps = exp_data.get('completed', 0)
                unlaunched_steps = exp_data.get('unlaunched', 0)
                total_runtime = exp_data.get('total_runtime', '0s')
                key_steps = exp_data.get('key_steps', [])
                
                pv_flow_html += f"""
                <h3>{exp_name}</h3>
                <div class="flow-summary">
                    <p><strong>Completed:</strong> {completed_steps} steps (Total runtime: {total_runtime})</p>
                    {f'<p><strong>Unlaunched:</strong> {unlaunched_steps} steps</p>' if unlaunched_steps > 0 else ''}
                </div>
                
                {'<h4>Key Step Runtimes:</h4>' if key_steps else ''}
                {'<table><thead><tr><th>Step</th><th>Runtime</th></tr></thead><tbody>' if key_steps else ''}
"""
                
                for step_name, step_runtime in key_steps:
                    pv_flow_html += f"""
                    <tr>
                        <td>{step_name}</td>
                        <td class="num-cell">{step_runtime}</td>
                    </tr>
"""
                
                if key_steps:
                    pv_flow_html += """
                </tbody></table>
"""
            
            pv_flow_html += """
            </div>
        </div>
"""
        
        # Generate full HTML
        html = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Physical Verification Report - {self.design_info.top_hier}</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
            line-height: 1.6;
        }}
        
        .container {{
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.2);
            overflow: hidden;
        }}
        
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        
        .header-content {{
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 30px;
            flex-wrap: wrap;
        }}
        
        .logo {{
            max-width: 120px;
            height: auto;
            filter: drop-shadow(0 4px 6px rgba(0,0,0,0.3));
            cursor: pointer;
            transition: transform 0.3s ease;
        }}
        
        .logo:hover {{
            transform: scale(1.05);
        }}
        
        .header-text {{
            flex: 1;
            min-width: 300px;
        }}
        
        .header h1 {{
            font-size: 2.5em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
        }}
        
        .header .subtitle {{
            font-size: 1.2em;
            opacity: 0.9;
        }}
        
        .header .design-info {{
            margin-top: 15px;
            font-size: 1em;
            opacity: 0.8;
        }}
        
        /* Logo Modal */
        .logo-modal {{
            display: none;
            position: fixed;
            z-index: 1000;
            left: 0;
            top: 0;
            width: 100%;
            height: 100%;
            overflow: auto;
            background-color: rgba(0,0,0,0.9);
            cursor: pointer;
        }}
        
        .logo-modal-content {{
            margin: auto;
            display: block;
            width: 80%;
            max-width: 700px;
            position: relative;
            top: 50%;
            transform: translateY(-50%);
        }}
        
        .logo-modal-close {{
            position: absolute;
            top: 15px;
            right: 35px;
            color: #f1f1f1;
            font-size: 40px;
            font-weight: bold;
            transition: 0.3s;
        }}
        
        .logo-modal-close:hover,
        .logo-modal-close:focus {{
            color: #bbb;
            text-decoration: none;
            cursor: pointer;
        }}
        
        .summary-cards {{
            display: flex;
            flex-wrap: wrap;
            gap: 12px;
            padding: 20px;
            background: #f8f9fa;
            margin-bottom: 20px;
        }}
        
        .summary-card {{
            flex: 1 1 calc(25% - 12px);
            background: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            text-align: center;
            transition: transform 0.3s;
            min-height: 90px;
            display: flex;
            flex-direction: column;
            justify-content: center;
        }}
        
        .summary-card:hover {{
            transform: translateY(-3px);
            box-shadow: 0 4px 15px rgba(0,0,0,0.15);
        }}
        
        .summary-card .label {{
            font-size: 0.75em;
            color: #666;
            margin-bottom: 6px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            font-weight: 600;
        }}
        
        .summary-card .value {{
            font-size: 2em;
            font-weight: bold;
            line-height: 1;
        }}
        
        .summary-card .status {{
            font-size: 1.6em;
            font-weight: bold;
            padding: 6px;
            border-radius: 5px;
            line-height: 1;
        }}
        
        .pass-text {{
            color: #28a745;
        }}
        
        .fail-text {{
            color: #dc3545;
        }}
        
        .content {{
            padding: 20px;
        }}
        
        .section {{
            margin-bottom: 15px;
        }}
        
        .section-header {{
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 15px 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-radius: 10px;
            cursor: pointer;
            margin-bottom: 15px;
            transition: all 0.3s;
        }}
        
        .section-header:hover {{
            transform: translateX(5px);
        }}
        
        .section-header h2 {{
            font-size: 1.5em;
        }}
        
        .section-header .toggle-icon {{
            font-size: 1.2em;
            transition: transform 0.3s;
        }}
        
        .section-header.collapsed .toggle-icon {{
            transform: rotate(-90deg);
        }}
        
        .section-content {{
            padding: 20px;
            background: #f8f9fa;
            border-radius: 10px;
            margin-bottom: 10px;
        }}
        
        .section-content.collapsed {{
            display: none;
        }}
        
        table {{
            width: 100%;
            border-collapse: collapse;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            margin-bottom: 15px;
        }}
        
        th {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 600;
        }}
        
        td {{
            padding: 12px 15px;
            border-bottom: 1px solid #e0e0e0;
        }}
        
        tr:hover {{
            background: #f0f0f0;
        }}
        
        .num-cell {{
            text-align: right;
            font-weight: 600;
        }}
        
        .status-pass {{
            border-left: 4px solid #28a745;
        }}
        
        .status-fail {{
            border-left: 4px solid #dc3545;
        }}
        
        .timeline-section {{
            padding: 20px;
            background: #f8f9fa;
            border-radius: 10px;
            margin-bottom: 20px;
        }}
        
        .timeline-info {{
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 15px;
            margin-top: 10px;
        }}
        
        .timeline-info div {{
            background: white;
            padding: 15px;
            border-radius: 8px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }}
        
        .flow-summary {{
            background: white;
            padding: 15px;
            border-radius: 8px;
            margin-bottom: 15px;
        }}
        
        h3 {{
            color: #667eea;
            margin-bottom: 10px;
            margin-top: 15px;
        }}
        
        h4 {{
            color: #764ba2;
            margin-bottom: 10px;
            margin-top: 10px;
        }}
        
        a {{
            color: #667eea;
            text-decoration: none;
        }}
        
        a:hover {{
            text-decoration: underline;
        }}
        
        .footer {{
            text-align: center;
            padding: 20px;
            background: #f8f9fa;
            color: #666;
            border-top: 1px solid #ddd;
        }}
    </style>
</head>
<body>
    <!-- Logo Modal -->
    <div id="logoModal" class="logo-modal" onclick="hideLogoModal()">
        <span class="logo-modal-close">&times;</span>
        <img class="logo-modal-content" id="logoModalImg">
    </div>
    
    <div class="container">
        <div class="header">
            <div class="header-content">
                {f'<img class="logo" src="data:image/png;base64,{logo_data}" alt="AVICE Logo" onclick="showLogoModal()" title="Click to enlarge">' if logo_data else ''}
                <div class="header-text">
                    <h1>ðŸ” Physical Verification Report</h1>
                    <div class="subtitle">Comprehensive PV Analysis</div>
                    <div class="design-info">
                        Design: {self.design_info.top_hier} | IPO: {self.design_info.ipo}<br>
                        Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
                    </div>
                </div>
            </div>
        </div>
        
        <div class="summary-cards">
            <div class="summary-card">
                <div class="label">Overall Status</div>
                <div class="status" style="color: {status_color};">{overall_status}</div>
            </div>
            <div class="summary-card">
                <div class="label">LVS Failures</div>
                <div class="value {'fail-text' if lvs_violations > 0 else 'pass-text'}">{lvs_violations}</div>
            </div>
            <div class="summary-card">
                <div class="label">DRC Violations</div>
                <div class="value {'fail-text' if drc_violations > 0 else 'pass-text'}">{drc_violations}</div>
            </div>
            <div class="summary-card">
                <div class="label">Antenna Violations</div>
                <div class="value {'fail-text' if antenna_violations > 0 else 'pass-text'}">{antenna_violations}</div>
            </div>
        </div>
        
        {timeline_html}
        
        <div class="content">
            {lvs_html}
            {drc_html}
            {antenna_html}
            {pv_flow_html}
        </div>
        
        <div class="footer">
            <p>Report generated by <strong>avice_wa_review.py</strong></p>
            <p>For questions or issues, contact: avice@nvidia.com</p>
            <p style="margin-top: 10px; font-size: 0.9em;">Alon Vice Tools Â© 2025</p>
        </div>
    </div>
    
    <script>
        function toggleSection(sectionId) {{
            const content = document.getElementById(sectionId);
            const header = content.previousElementSibling;
            
            content.classList.toggle('collapsed');
            header.classList.toggle('collapsed');
        }}
        
        // Back to top button functionality - wait for DOM to load
        document.addEventListener('DOMContentLoaded', function() {{
            var backToTopBtn = document.getElementById('backToTopBtn');
            if (backToTopBtn) {{
                window.addEventListener('scroll', function() {{
                    if (window.pageYOffset > 300) {{
                        backToTopBtn.style.display = 'block';
                    }} else {{
                        backToTopBtn.style.display = 'none';
                    }}
                }});
                
                backToTopBtn.addEventListener('click', function() {{
                    window.scrollTo(0, 0);
                }});
            }}
        }});
        
        // Logo modal functions
        function showLogoModal() {{
            const modal = document.getElementById('logoModal');
            const modalImg = document.getElementById('logoModalImg');
            const logo = document.querySelector('.logo');
            if (modal && modalImg && logo) {{
                modal.style.display = 'block';
                modalImg.src = logo.src;
            }}
        }}
        
        function hideLogoModal() {{
            const modal = document.getElementById('logoModal');
            if (modal) {{
                modal.style.display = 'none';
            }}
        }}
    </script>
    
    <button id="backToTopBtn" style="display: none; position: fixed; bottom: 30px; right: 30px; 
            z-index: 9999; border: none; outline: none; background-color: #667eea; color: white; 
            cursor: pointer; padding: 15px 20px; border-radius: 50px; font-size: 16px; 
            font-weight: bold; box-shadow: 0 4px 6px rgba(0,0,0,0.3); transition: all 0.3s ease;"
            onmouseover="this.style.backgroundColor='#5568d3'; this.style.transform='scale(1.1)';"
            onmouseout="this.style.backgroundColor='#667eea'; this.style.transform='scale(1)';">
        â†‘ Top
    </button>
</body>
</html>
"""
        
        return html
    
    def run_gl_check(self) -> None:
        """Run GL check analysis"""
        self.print_header(FlowStage.GL_CHECK)
        
        # Show GL Check flow timeline
        gl_check_local_flow_dirs = [
            os.path.join(self.workarea, f"signoff_flow/gl-check/{self.design_info.top_hier}/local_flow"),
            os.path.join(self.workarea, f"signoff_flow/gl-check/local_flow"),
            os.path.join(self.workarea, f"signoff_flow/gl-check_{self.design_info.top_hier}/local_flow")
        ]
        self._show_flow_timeline("GL Check", gl_check_local_flow_dirs)
        
        # Detect GL Check run history from timestamped directories
        gl_check_dir = os.path.join(self.workarea, "signoff_flow/gl-check")
        if os.path.exists(gl_check_dir):
            try:
                # Find all timestamped directories (format: YYYY_MM_DD_HH_MM_SS)
                timestamped_dirs = []
                for entry in os.listdir(gl_check_dir):
                    entry_path = os.path.join(gl_check_dir, entry)
                    if os.path.isdir(entry_path) and re.match(r'\d{4}_\d{2}_\d{2}_\d{2}_\d{2}_\d{2}', entry):
                        timestamped_dirs.append(entry)
                
                if timestamped_dirs:
                    timestamped_dirs.sort()
                    print(f"\n  {Color.CYAN}GL Check Run History:{Color.RESET}")
                    print(f"    Total Runs: {len(timestamped_dirs)}")
                    print(f"    First Run: {timestamped_dirs[0].replace('_', '/', 2).replace('_', ' ', 1).replace('_', ':').replace('_', ':')}")
                    print(f"    Latest Run: {timestamped_dirs[-1].replace('_', '/', 2).replace('_', ' ', 1).replace('_', ':').replace('_', ':')}")
            except Exception as e:
                pass  # Silently skip if unable to read directory
        
        # Look for GL check error files (waived vs non-waived)
        # Convert to absolute paths for HTML links to work from any location
        waived_file = os.path.abspath(os.path.join(gl_check_dir, "gl-check.all.waived"))
        non_waived_file = os.path.abspath(os.path.join(gl_check_dir, "gl-check.all.err"))
        
        gl_check_html_path = ""
        total_errors = 0
        total_waived = 0
        total_non_waived = 0
        
        if os.path.exists(waived_file) or os.path.exists(non_waived_file):
            total_errors, total_waived, total_non_waived = self._analyze_gl_check_errors(waived_file, non_waived_file)
            # Generate HTML report
            gl_check_html_path = self._generate_gl_check_html_report(gl_check_dir, waived_file, non_waived_file, timestamped_dirs if 'timestamped_dirs' in locals() else [])
        else:
            # Fallback to old err.long.rep format
            gl_check_pattern = "signoff_flow/gl-check*/reports/err.long.rep"
            gl_check_files = self.file_utils.find_files(gl_check_pattern, self.workarea)
            
            if gl_check_files:
                for report_file in gl_check_files:
                    self.print_file_info(report_file, "GL Check Report")
                    try:
                        with open(report_file, 'r') as f:
                            lines = f.readlines()
                        
                        # Process the report and collect data for table formatting
                        gl_check_data = []
                        for line in lines:
                            line = line.strip()
                            # Skip empty lines, separators, and header lines
                            if not line or line.startswith('---') or 'Checker Name' in line or 'Category' in line:
                                continue
                            # Split by pipe and extract relevant columns (1, 3, 4, 5) - skip Category (col2) as it's same as Checker Name
                            parts = line.split('|')
                            if len(parts) >= 6:
                                checker_name = parts[1].strip() if len(parts) > 1 else ""
                                count = parts[3].strip() if len(parts) > 3 else ""
                                error_id = parts[4].strip() if len(parts) > 4 else ""
                                description = parts[5].strip() if len(parts) > 5 else ""
                                # Only add if we have valid data (checker_name and count are required)
                                if checker_name and count:
                                    gl_check_data.append((checker_name, count, error_id, description))
                        
                        # Print as formatted table
                        if gl_check_data:
                            print(f"\n  {Color.CYAN}GL Check Results:{Color.RESET}")
                            print(f"  {'Checker Name':<30} {'Count':<8} {'Error ID':<10} {'Description'}")
                            print(f"  {'-'*30} {'-'*8} {'-'*10} {'-'*50}")
                            for checker_name, count, error_id, description in gl_check_data:
                                # Truncate description if too long
                                desc_truncated = description[:47] + "..." if len(description) > 50 else description
                                print(f"  {checker_name:<30} {count:<8} {error_id:<10} {desc_truncated}")
                        else:
                            print("  No GL check data found")
                    except Exception as e:
                        print(f"  Error reading GL check report: {e}")
            else:
                print("  Didn't run GL checks")
        
        # Determine status based on non-waived violations (waived are accepted issues)
        # Thresholds: FAIL >= 50, WARN 0 < violations < 50, PASS = 0
        status = "PASS"
        issues = []
        
        if total_non_waived >= 50:
            status = "FAIL"
            issues.append(f"Non-waived violations: {total_non_waived} (threshold: <50)")
        elif total_non_waived > 0:
            status = "WARN"
            issues.append(f"Non-waived violations: {total_non_waived} (threshold: <50)")
        
        # Prepare key metrics
        key_metrics = {
            "Total Violations": str(total_errors),
            "Waived": str(total_waived),
            "Non-Waived": str(total_non_waived)
        }
        
        # Add section summary for master dashboard
        self._add_section_summary(
            section_name="GL Checks",
            section_id="gl-check",
            stage=FlowStage.GL_CHECK,
            status=status,
            key_metrics=key_metrics,
            html_file=gl_check_html_path if gl_check_html_path else "",
            priority=3,
            issues=issues,
            icon="[GL]"
        )
    
    def _find_beflow_path(self) -> Optional[str]:
        """Find the beflow path from PnR flow configuration or common locations
        
        Returns:
            BeFlow path or None if not found
        """
        try:
            # Method 1: Look in PnR logs for NETWORK_FLOW_PATH
            log_files = glob.glob(os.path.join(self.workarea, "pnr_flow/nv_flow/*/ipo*/*.log"))
            for log_file in log_files[:5]:  # Check first 5 logs
                try:
                    with open(log_file, 'r') as f:
                        for line in f:
                            match = re.search(r'NETWORK_FLOW(?:_UTILS)?_(?:PATH|DIR)\s*[:=]\s*([^\s\n]+)', line)
                            if match:
                                path = match.group(1)
                                # Convert flow2_utils path to beflow path if needed
                                if 'flow2_utils' in path:
                                    # Try to find corresponding beflow path
                                    beflow_path = path.replace('flow2_utils', 'beflow')
                                    if os.path.exists(beflow_path):
                                        return beflow_path
                                return path
                except:
                    continue
            
            # Method 2: Try common beflow installation paths
            common_paths = [
                '/home/nbu_be_tools/beflow/1.0',
                '/tools/nbu_be_tools/beflow/1.0',
            ]
            for base_path in common_paths:
                if os.path.exists(base_path):
                    # Find the latest version
                    versions = glob.glob(os.path.join(base_path, '*'))
                    if versions:
                        # Return the first one found (or could sort by date)
                        return sorted(versions)[-1]  # Return latest
        except:
            pass
        return None
    
    def _parse_clock_tree_cells(self) -> set:
        """Parse ClockTree.rpt to get set of all clock tree instance names
        
        Returns:
            Set of instance names that are part of the clock tree
        """
        clock_tree_instances = set()
        try:
            # Look for ClockTree.rpt in gl-check reports directory
            clock_tree_rpt = None
            for gl_dir in ["signoff_flow/gl-check", "signoff_flow/gl_check", "gl_check", "gl-check"]:
                test_path = os.path.join(self.workarea, gl_dir, "reports", "ClockTree.rpt")
                if os.path.exists(test_path):
                    clock_tree_rpt = test_path
                    break
            
            if not clock_tree_rpt:
                return clock_tree_instances
            
            # Parse ClockTree.rpt
            # Format: [lnd]:instance_path/pin(CELL_TYPE)
            with open(clock_tree_rpt, 'r') as f:
                for line in f:
                    # Match pattern: [design]:instance_path/pin(CELL_TYPE)
                    match = re.match(r'\s*\[[^\]]+\]:([^/\(]+)', line)
                    if match:
                        instance_name = match.group(1).strip()
                        if instance_name:
                            clock_tree_instances.add(instance_name)
            
            return clock_tree_instances
            
        except Exception as e:
            return clock_tree_instances
    
    def _get_allowed_clock_cell_patterns(self) -> List:
        """Extract and compile allowed clock tree cell regex patterns from beflow_config.yaml
        
        Returns:
            List of compiled regex patterns for allowed clock tree cells
        """
        compiled_patterns = []
        try:
            # Look for beflow_config.yaml in gl-check directory
            beflow_config = None
            for gl_dir in ["signoff_flow/gl-check", "signoff_flow/gl_check", "gl_check", "gl-check"]:
                test_path = os.path.join(self.workarea, gl_dir, "beflow_config.yaml")
                if os.path.exists(test_path):
                    beflow_config = test_path
                    break
            
            if not beflow_config:
                return compiled_patterns
            
            # Parse beflow_config.yaml for allowed_clktree_cells_rex
            with open(beflow_config, 'r') as f:
                content = f.read()
            
            # Extract allowed_clktree_cells_rex
            # Format: glc allowed_clktree_cells_rex(lnd): ['PATTERN1', 'PATTERN2', ...]
            for line in content.split('\n'):
                if 'allowed_clktree_cells_rex' in line:
                    match = re.search(r'\[(.*)\]', line)
                    if match:
                        cells_str = match.group(1)
                        patterns = [cell.strip().strip("'\"") for cell in re.findall(r"'([^']*)'", cells_str)]
                        
                        # Compile each pattern
                        for pattern in patterns:
                            # Convert TCL regex to Python regex
                            # TCL uses (CS[1-3])? which is already valid Python regex
                            try:
                                # Add word boundaries for exact matching
                                regex_pattern = r'^' + pattern + r'$'
                                compiled_patterns.append(re.compile(regex_pattern))
                            except re.error:
                                pass  # Skip invalid patterns
                    break
            
            return compiled_patterns
            
        except Exception as e:
            return compiled_patterns
    
    def _check_eco_for_clock_violations(self, eco_file: str, eco_commands: List[str]) -> None:
        """Check if ECO modifies clock tree cells with non-allowed cell types
        
        Args:
            eco_file: Path to ECO file
            eco_commands: List of ECO commands
        
        This implements the 'instNotAllowedOnClocks' check:
        1. Extract instance_name and cell_type from each ECO command
        2. Check if instance_name is in clock tree (from ClockTree.rpt)
        3. If yes, check if cell_type matches allowed patterns (from beflow_config.yaml)
        4. If no match, report violation
        
        Args:
            eco_file: Path to ECO file
            eco_commands: List of ECO commands (non-comment lines)
        """
        try:
            # Get clock tree instance names
            clock_tree_instances = self._parse_clock_tree_cells()
            if not clock_tree_instances:
                print(f"    {Color.YELLOW}[SKIP] ClockTree.rpt not found - cannot check instNotAllowedOnClocks{Color.RESET}")
                return
            
            # Get allowed clock cell patterns
            allowed_patterns = self._get_allowed_clock_cell_patterns()
            if not allowed_patterns:
                print(f"    {Color.YELLOW}[SKIP] No allowed clock cell patterns found - cannot check instNotAllowedOnClocks{Color.RESET}")
                return
            
            print(f"    {Color.CYAN}Checking instNotAllowedOnClocks: {len(clock_tree_instances)} clock instances, {len(allowed_patterns)} allowed patterns{Color.RESET}")
            
            # Check each ECO command
            violations = []
            for command_line in eco_commands:
                # Parse ECO command to extract instance and cell type
                # Common formats:
                #   ecoChangeCell -inst {INSTANCE} -cell {CELLTYPE}          (NV Gate ECO)
                #   ecoAddRepeater ... -cell {CELLTYPE} -name {INSTANCE}      (NV Gate ECO/VIVID)
                #   size_cell INSTANCE CELLTYPE                                (PT ECO)
                #   insert_buffer NET CELLTYPE INSTANCE                        (PT ECO)
                #   eco_insert_buffer NET CELLTYPE -cells INSTANCE             (PT ECO)
                
                # Try to extract instance and cell type
                instance_name = None
                cell_type = None
                
                # Pattern 1: ecoChangeCell -inst {INSTANCE} -cell {CELLTYPE}
                match = re.search(r'ecoChangeCell\s+-inst\s+\{([^}]+)\}\s+-cell\s+\{([^}]+)\}', command_line)
                if match:
                    instance_name = match.group(1)
                    cell_type = match.group(2)
                
                # Pattern 2: ecoAddRepeater ... -name {INSTANCE} ... -cell {CELLTYPE}
                # Note: ecoAddRepeater uses -name (not -inst) for new instance name
                # Special handling: NEW buffer won't be in ClockTree.rpt yet (ECO not applied)
                # So we check the PREVIOUS cell (from -term) to see if it's on clock network
                if not instance_name and 'ecoAddRepeater' in command_line:
                    cell_match = re.search(r'-cell\s+\{([^}]+)\}', command_line)
                    name_match = re.search(r'-name\s+\{([^}]+)\}', command_line)
                    term_match = re.search(r'-term\s+\{([^}]+)\}', command_line)
                    
                    if cell_match and name_match and term_match:
                        cell_type = cell_match.group(1)
                        instance_name = name_match.group(1)
                        
                        # Extract previous instance from -term {PREV_INST/PIN}
                        term_path = term_match.group(1)
                        # Remove /PIN suffix to get instance name
                        prev_instance = term_path.rsplit('/', 1)[0] if '/' in term_path else term_path
                        
                        # Check if PREVIOUS instance is in clock tree
                        # If yes, the NEW buffer will also be on clock network
                        if prev_instance in clock_tree_instances:
                            # Mark this for checking (new buffer connects to clock network)
                            pass  # instance_name already set, will be checked below
                        elif instance_name not in clock_tree_instances:
                            # Neither new nor previous instance in clock tree - skip this ECO
                            instance_name = None
                            cell_type = None
                
                # Pattern 3: size_cell INSTANCE CELLTYPE
                if not instance_name:
                    match = re.match(r'size_cell\s+(\S+)\s+(\S+)', command_line)
                    if match:
                        instance_name = match.group(1)
                        cell_type = match.group(2)
                
                # Pattern 4: insert_buffer NET CELLTYPE INSTANCE (or variations)
                if not instance_name:
                    # Look for any command with multiple uppercase words (potential cell types)
                    words = command_line.split()
                    uppercase_words = [w for w in words if re.match(r'^[A-Z][A-Z0-9_]+$', w)]
                    
                    # If we have uppercase words, the last one is likely instance, others might be cell type
                    if len(uppercase_words) >= 2:
                        # Heuristic: cell type usually has T6 or similar suffix
                        for i, word in enumerate(uppercase_words):
                            if re.search(r'T6|T8|MT6|LVT|HVT', word):
                                cell_type = word
                                # Instance could be before or after
                                if i + 1 < len(uppercase_words):
                                    instance_name = uppercase_words[i + 1]
                                elif i > 0:
                                    instance_name = uppercase_words[i - 1]
                                break
                
                # If we found both instance and cell type, check for violations
                if instance_name and cell_type:
                    # Check if instance is in clock tree
                    if instance_name in clock_tree_instances:
                        # Check if cell type matches any allowed pattern
                        is_allowed = False
                        for pattern in allowed_patterns:
                            if pattern.match(cell_type):
                                is_allowed = True
                                break
                        
                        if not is_allowed:
                            violations.append((instance_name, cell_type, command_line))
            
            # Report violations
            if violations:
                print(f"  {Color.RED}[WARN] Found {len(violations)} instNotAllowedOnClocks violations:{Color.RESET}")
                print(f"    {Color.CYAN}Instances on clock tree using non-allowed cell types:{Color.RESET}")
                
                # Group by cell type for better reporting
                violations_by_type = {}
                for inst, cell_type, cmd in violations:
                    if cell_type not in violations_by_type:
                        violations_by_type[cell_type] = []
                    violations_by_type[cell_type].append((inst, cmd))
                
                # Show violations grouped by cell type
                for cell_type, instances in sorted(violations_by_type.items(), key=lambda x: len(x[1]), reverse=True):
                    print(f"      {Color.YELLOW}Cell Type '{cell_type}': {len(instances)} instance(s){Color.RESET}")
                    for inst, cmd in instances[:3]:  # Show first 3 examples
                        print(f"        Instance: {inst}")
                        print(f"        Command:  {cmd[:80]}...")
                    if len(instances) > 3:
                        print(f"        ... and {len(instances) - 3} more instances")
            else:
                print(f"  {Color.GREEN}[OK] No instNotAllowedOnClocks violations found{Color.RESET}")
                
        except Exception as e:
            print(f"  {Color.YELLOW}[WARN] Error checking instNotAllowedOnClocks: {e}{Color.RESET}")
    
    def _check_eco_for_clock_cells_on_data(self, eco_file: str, eco_commands: List[str]) -> None:
        """Check if ECO uses clock cells on data paths (informational)
        
        Args:
            eco_file: Path to ECO file
            eco_commands: List of ECO commands
        
        Clock cells are allowed on data paths but they're larger/stronger cells,
        so tracking their usage helps identify potential area optimization opportunities.
        
        Logic:
        1. Extract instance_name and cell_type from ECO command
        2. Check if cell_type matches any allowed clock cell pattern
        3. If yes, check if instance_name is in ClockTree.rpt
        4. If NOT in ClockTree â†’ clock cell on data path (report as info)
        
        Args:
            eco_file: Path to ECO file
            eco_commands: List of ECO commands (non-comment lines)
        """
        try:
            # Get clock tree instance names
            clock_tree_instances = self._parse_clock_tree_cells()
            if not clock_tree_instances:
                print(f"    {Color.YELLOW}[SKIP] ClockTree.rpt not found - cannot check clock cells on data paths{Color.RESET}")
                return
            
            # Get allowed clock cell patterns
            allowed_patterns = self._get_allowed_clock_cell_patterns()
            if not allowed_patterns:
                print(f"    {Color.YELLOW}[SKIP] No allowed clock cell patterns found{Color.RESET}")
                return
            
            print(f"    {Color.CYAN}Checking for clock cells on data paths (area optimization info)...{Color.RESET}")
            
            # Track clock cells used on data paths
            clock_cells_on_data = []
            
            for command_line in eco_commands:
                # Parse ECO command to extract instance and cell type
                # (Reuse same parsing logic as instNotAllowedOnClocks check)
                instance_name = None
                cell_type = None
                
                # Pattern 1: ecoChangeCell -inst {INSTANCE} -cell {CELLTYPE}
                match = re.search(r'ecoChangeCell\s+-inst\s+\{([^}]+)\}\s+-cell\s+\{([^}]+)\}', command_line)
                if match:
                    instance_name = match.group(1)
                    cell_type = match.group(2)
                
                # Pattern 2: ecoAddRepeater ... -name {INSTANCE} ... -cell {CELLTYPE}
                # Note: ecoAddRepeater uses -name (not -inst) for new instance name
                # Special handling: NEW buffer won't be in ClockTree.rpt yet (ECO not applied)
                # So we check the PREVIOUS cell (from -term) to see if it's on clock network
                if not instance_name and 'ecoAddRepeater' in command_line:
                    cell_match = re.search(r'-cell\s+\{([^}]+)\}', command_line)
                    name_match = re.search(r'-name\s+\{([^}]+)\}', command_line)
                    term_match = re.search(r'-term\s+\{([^}]+)\}', command_line)
                    
                    if cell_match and name_match and term_match:
                        cell_type = cell_match.group(1)
                        instance_name = name_match.group(1)
                        
                        # Extract previous instance from -term {PREV_INST/PIN}
                        term_path = term_match.group(1)
                        # Remove /PIN suffix to get instance name
                        prev_instance = term_path.rsplit('/', 1)[0] if '/' in term_path else term_path
                        
                        # Check if PREVIOUS instance is in clock tree
                        # If yes, the NEW buffer will also be on clock network
                        if prev_instance in clock_tree_instances:
                            # Mark this for checking (new buffer connects to clock network)
                            pass  # instance_name already set, will be checked below
                        elif instance_name not in clock_tree_instances:
                            # Neither new nor previous instance in clock tree - skip this ECO
                            instance_name = None
                            cell_type = None
                
                # Pattern 3: size_cell INSTANCE CELLTYPE
                if not instance_name:
                    match = re.match(r'size_cell\s+(\S+)\s+(\S+)', command_line)
                    if match:
                        instance_name = match.group(1)
                        cell_type = match.group(2)
                
                # Pattern 4: insert_buffer NET CELLTYPE INSTANCE
                if not instance_name:
                    words = command_line.split()
                    uppercase_words = [w for w in words if re.match(r'^[A-Z][A-Z0-9_]+$', w)]
                    
                    if len(uppercase_words) >= 2:
                        for i, word in enumerate(uppercase_words):
                            if re.search(r'T6|T8|MT6|LVT|HVT', word):
                                cell_type = word
                                if i + 1 < len(uppercase_words):
                                    instance_name = uppercase_words[i + 1]
                                elif i > 0:
                                    instance_name = uppercase_words[i - 1]
                                break
                
                # If we found both instance and cell type, check if it's a clock cell on data path
                if instance_name and cell_type:
                    # Check if cell type is a clock cell (matches allowed clock patterns)
                    is_clock_cell = False
                    for pattern in allowed_patterns:
                        if pattern.match(cell_type):
                            is_clock_cell = True
                            break
                    
                    if is_clock_cell:
                        # Check if instance is NOT in clock tree (i.e., it's on data path)
                        if instance_name not in clock_tree_instances:
                            clock_cells_on_data.append((instance_name, cell_type, command_line))
            
            # Report findings
            if clock_cells_on_data:
                print(f"  {Color.CYAN}[INFO] Found {len(clock_cells_on_data)} ECO commands using clock cells on data paths:{Color.RESET}")
                print(f"    {Color.CYAN}Clock cells on data signals (larger cells, consider for area optimization):{Color.RESET}")
                
                # Group by cell type
                cells_by_type = {}
                for inst, cell_type, cmd in clock_cells_on_data:
                    if cell_type not in cells_by_type:
                        cells_by_type[cell_type] = []
                    cells_by_type[cell_type].append((inst, cmd))
                
                # Show grouped by cell type (most used first)
                for cell_type, instances in sorted(cells_by_type.items(), key=lambda x: len(x[1]), reverse=True)[:10]:  # Top 10 cell types
                    print(f"      {cell_type}: {len(instances)} instance(s)")
                    for inst, cmd in instances[:2]:  # Show first 2 examples per type
                        print(f"        - {inst}")
                    if len(instances) > 2:
                        print(f"        ... and {len(instances) - 2} more")
                
                if len(cells_by_type) > 10:
                    print(f"      ... and {len(cells_by_type) - 10} more cell types")
            else:
                print(f"  {Color.GREEN}[INFO] No clock cells used on data paths{Color.RESET}")
                
        except Exception as e:
            print(f"  {Color.YELLOW}[WARN] Error checking clock cells on data: {e}{Color.RESET}")
    
    def _check_eco_for_dont_use_cells(self, eco_file: str, eco_commands: List[str]) -> None:
        """Check if ECO file contains any dont_use cells (ultra-optimized using set lookup)
        
        Args:
            eco_file: Path to ECO file
            eco_commands: List of ECO commands
        """
        try:
            # OPTIMIZATION: Try to use dont_use_cell_patterns.tcl from gl-check results (much faster)
            # This file contains actual cell names (not regex patterns) that can be used for O(1) set lookup
            dont_use_cells = set()
            patterns_file = None
            
            for gl_dir in ["signoff_flow/gl-check", "signoff_flow/gl_check", "gl_check", "gl-check"]:
                test_path = os.path.join(self.workarea, gl_dir, "results", "dont_use_cell_patterns.tcl")
                if os.path.exists(test_path):
                    patterns_file = test_path
                    break
            
            if patterns_file:
                # TIER 1: Use GL-Check results (most accurate, workarea-specific)
                print(f"    {Color.CYAN}Using dont_use check: GL-Check results ({os.path.basename(patterns_file)}){Color.RESET}")
                with open(patterns_file, 'r') as f:
                    for line in f:
                        # Format: "dont_use CELLNAME patterns = ..."
                        match = re.match(r'dont_use\s+(\S+)\s+patterns\s+=', line)
                        if match:
                            dont_use_cells.add(match.group(1))
                
                # Check using Tier 1 cells (O(1) set lookup - FAST!)
                violations = []
                violation_cells = {}
                for line in eco_commands:
                    words = re.findall(r'\b[A-Z][A-Z0-9_]+\b', line)
                    for word in words:
                        if word in dont_use_cells:
                            violations.append((line, word))
                            violation_cells[word] = violation_cells.get(word, 0) + 1
                            break
                
                if violations:
                    print(f"  {Color.RED}[WARN] Found {len(violations)} ECO commands using dont_use cells:{Color.RESET}")
                    
                    # Show all matched cells with counts
                    print(f"    {Color.CYAN}Matched cells ({len(violation_cells)}):{Color.RESET}")
                    for cell, count in sorted(violation_cells.items(), key=lambda x: x[1], reverse=True)[:15]:
                        print(f"      {cell}: {count}")
                    if len(violation_cells) > 15:
                        print(f"      ... and {len(violation_cells)-15} more cells")
                    
                    # Show example violations
                    print(f"    {Color.YELLOW}Example violations:{Color.RESET}")
                    for line, cell in violations[:3]:
                        print(f"      Cell '{cell}': {line[:70]}...")
                else:
                    print(f"  {Color.GREEN}[OK] No dont_use cells found in ECO{Color.RESET}")
                return  # Tier 1 found and checked, done
            
            else:
                # FALLBACK 1: Try project-level agur_dont_use.tcl (fast O(1) set lookup with exact cell names)
                agur_dont_use_file = None
                runset_paths = glob.glob(os.path.join(self.workarea, "pnr_flow/nv_flow/*/ipo*/runset.tcl"))
                if runset_paths:
                    try:
                        with open(runset_paths[0], 'r') as f:
                            for line in f:
                                match = re.match(r'set PROJECT\(CUSTOM_SCRIPTS_DIR\)\s+(.+)', line)
                                if match:
                                    custom_scripts_dir = match.group(1).strip()
                                    test_path = os.path.join(custom_scripts_dir, "agur_dont_use.tcl")
                                    if os.path.exists(test_path):
                                        agur_dont_use_file = test_path
                                    break
                    except:
                        pass
                
                if agur_dont_use_file:
                    # Parse agur_dont_use.tcl for exact cell names (O(1) set lookup - FAST!)
                    with open(agur_dont_use_file, 'r') as f:
                        for line in f:
                            # Format: "nvb_dont_use CELLNAME" (exact names, no regex)
                            match = re.match(r'nvb_dont_use\s+(\S+)', line)
                            if match:
                                dont_use_cells.add(match.group(1))
                
                # ALWAYS combine with Tier 3 (BeFlow rules) for comprehensive checking
                # Tier 2 provides project-specific exact names, Tier 3 provides regex patterns
                # Together they form a complete superset of banned cells
                beflow_path = self._find_beflow_path()
                nbu_dont_use_file = None
                if beflow_path:
                    test_path = os.path.join(beflow_path, "bytech/tsmc5/nvidia/tcl/nbu_dont_use.tcl")
                    if os.path.exists(test_path):
                        nbu_dont_use_file = test_path
                
                dont_use_patterns = []
                if nbu_dont_use_file:
                    # Parse nbu_dont_use.tcl for patterns (skip patterns that don't apply to ECO)
                    with open(nbu_dont_use_file, 'r') as f:
                        for line in f:
                            # Format: "nvb_dont_use PATTERN -regexp [-opt|-include_step rtl2gate]"
                            # Skip patterns with -opt flag (optimization-only, not for ECO)
                            # Skip patterns with -include_step rtl2gate (synthesis-only, not for ECO)
                            if '-opt' in line or '-include_step' in line:
                                continue
                            match = re.match(r'nvb_dont_use\s+(\S+)', line)
                            if match:
                                pattern = match.group(1)
                                # Convert to standard regex if needed (already in regex format)
                                dont_use_patterns.append(pattern)
                
                # Determine which sources were used for reporting
                sources_used = []
                if agur_dont_use_file:
                    sources_used.append(f"Project rules ({agur_dont_use_file})")
                if nbu_dont_use_file:
                    sources_used.append(f"BeFlow rules ({nbu_dont_use_file})")
                
                if sources_used:
                    print(f"    {Color.CYAN}Using dont_use check: {' + '.join(sources_used)}{Color.RESET}")
                    # Inform user about optimization opportunity
                    if len(eco_commands) > 10000:
                        print(f"    {Color.YELLOW}[TIP] For faster and more accurate results, run GL-Check to generate dont_use_cell_patterns.tcl{Color.RESET}")
                
                # If we have both exact cells and patterns, check both (COMBINED TIER 2 + 3)
                if dont_use_cells and dont_use_patterns:
                    # OPTIMIZATION: Sample ECO to find active cell types (reduces false checking)
                    sample_cells = set()
                    for line in eco_commands[:min(1000, len(eco_commands))]:
                        sample_cells.update(re.findall(r'\b[A-Z][A-Z0-9_]+\b', line))
                    
                    # STEP 1: Convert Tier 2 exact names to regex and combine with Tier 3 patterns
                    # This allows single-pass checking instead of two passes
                    compiled_patterns = []
                    
                    # Add exact cell names as simple regex patterns
                    for cell in dont_use_cells:
                        pattern_re = re.compile(r'\b' + re.escape(cell) + r'\b', re.IGNORECASE)
                        compiled_patterns.append((pattern_re, f"[EXACT] {cell}"))
                    
                    # Add regex patterns, but only if they might match based on sample
                    for pattern in dont_use_patterns:
                        regex_pattern = pattern.replace('*', '.*').replace('?', '.')
                        clean_pattern = regex_pattern.strip('^$')
                        word_boundary_pattern = r'\b' + clean_pattern + r'\b'
                        try:
                            pattern_re = re.compile(word_boundary_pattern, re.IGNORECASE)
                            # Quick filter: only include if pattern matches something in sample
                            if any(pattern_re.search(cell) for cell in list(sample_cells)[:50]):
                                compiled_patterns.append((pattern_re, pattern))
                        except re.error:
                            pass
                    
                    # STEP 2: Smart sampling for large ECOs with extrapolation
                    violations = []
                    violation_map = {}
                    
                    # For large ECOs, use sampling with extrapolation for speed
                    if len(eco_commands) > 10000:
                        # Check first 10K lines thoroughly for accurate pattern detection
                        sample_size = 10000
                        for line in eco_commands[:sample_size]:
                            for compiled_re, original_pattern in compiled_patterns:
                                if compiled_re.search(line):
                                    violations.append((line, original_pattern))
                                    violation_map[original_pattern] = violation_map.get(original_pattern, 0) + 1
                                    break
                        
                        # Extrapolate total count based on sample
                        if violations:
                            extrapolation_factor = len(eco_commands) / sample_size
                            estimated_total = int(len(violations) * extrapolation_factor)
                            violation_map['_ESTIMATED_TOTAL'] = estimated_total
                    else:
                        # For smaller ECOs, check everything
                        for line in eco_commands:
                            for compiled_re, original_pattern in compiled_patterns:
                                if compiled_re.search(line):
                                    violations.append((line, original_pattern))
                                    violation_map[original_pattern] = violation_map.get(original_pattern, 0) + 1
                                    break
                    
                    # Separate exact vs pattern violations for reporting
                    violations_exact = [(l, p) for l, p in violations if p.startswith('[EXACT]')]
                    violations_pattern = [(l, p) for l, p in violations if not p.startswith('[EXACT]')]
                    violation_cells_exact = {p.replace('[EXACT] ', ''): c for p, c in violation_map.items() if p.startswith('[EXACT]')}
                    violation_patterns = {p: c for p, c in violation_map.items() if not p.startswith('[EXACT]')}
                    
                    # Combine results
                    total_violations = len(violations_exact) + len(violations_pattern)
                    estimated_total = violation_map.get('_ESTIMATED_TOTAL')
                    
                    if total_violations > 0:
                        if estimated_total:
                            print(f"  {Color.RED}[WARN] Found ~{estimated_total} ECO commands using dont_use cells (estimated from {total_violations} in sample):{Color.RESET}")
                        else:
                            print(f"  {Color.RED}[WARN] Found {total_violations} ECO commands using dont_use cells:{Color.RESET}")
                        
                        # Show all matched patterns first
                        if violation_patterns:
                            # Remove the _ESTIMATED_TOTAL key if present
                            patterns_to_show = {k: v for k, v in violation_patterns.items() if k != '_ESTIMATED_TOTAL'}
                            if patterns_to_show:
                                print(f"    {Color.CYAN}Matched patterns ({len(patterns_to_show)}):{Color.RESET}")
                                for pattern, count in sorted(patterns_to_show.items(), key=lambda x: x[1], reverse=True):
                                    if estimated_total:
                                        # Extrapolate pattern count too
                                        extrapolation_factor = len(eco_commands) / 10000
                                        estimated_pattern_count = int(count * extrapolation_factor)
                                        print(f"      {pattern}: ~{estimated_pattern_count} (found {count} in sample)")
                                    else:
                                        print(f"      {pattern}: {count}")
                        
                        if violation_cells_exact:
                            print(f"    {Color.CYAN}Matched exact cells ({len(violation_cells_exact)}):{Color.RESET}")
                            for cell, count in sorted(violation_cells_exact.items(), key=lambda x: x[1], reverse=True)[:10]:
                                print(f"      {cell}: {count}")
                            if len(violation_cells_exact) > 10:
                                print(f"      ... and {len(violation_cells_exact)-10} more cells")
                        
                        # Show examples
                        print(f"    {Color.YELLOW}Example violations:{Color.RESET}")
                        if violations_exact:
                            for line, cell in violations_exact[:2]:
                                print(f"      Cell '{cell.replace('[EXACT] ', '')}': {line[:70]}...")
                        if violations_pattern:
                            for line, pattern in violations_pattern[:2]:
                                print(f"      Pattern '{pattern}': {line[:70]}...")
                    else:
                        print(f"  {Color.GREEN}[OK] No dont_use cells found in ECO{Color.RESET}")
                    return
                
                elif dont_use_cells:
                    # Only exact cells available (Tier 2 only - no BeFlow patterns found)
                    # Use fast O(1) set lookup
                    violations = []
                    violation_cells = {}
                    for line in eco_commands:
                        words = re.findall(r'\b[A-Z][A-Z0-9_]+\b', line)
                        for word in words:
                            if word in dont_use_cells:
                                violations.append((line, word))
                                violation_cells[word] = violation_cells.get(word, 0) + 1
                                break
                    
                    if violations:
                        print(f"  {Color.RED}[WARN] Found {len(violations)} ECO commands using dont_use cells:{Color.RESET}")
                        for line, cell in violations[:5]:
                            print(f"    {Color.YELLOW}Cell '{cell}':{Color.RESET} {line[:75]}...")
                        if len(violations) > 5:
                            print(f"    {Color.YELLOW}... and {len(violations)-5} more violations{Color.RESET}")
                        print(f"    {Color.YELLOW}Violating cells:{Color.RESET} {', '.join([f'{cell}({count})' for cell, count in sorted(violation_cells.items(), key=lambda x: x[1], reverse=True)[:10]])}")
                    else:
                        print(f"  {Color.GREEN}[OK] No dont_use cells found in ECO{Color.RESET}")
                    return
                
                else:
                    # No sources found
                    return
                
        except Exception as e:
            pass  # Silently skip if check fails
    
    def run_eco_analysis(self) -> None:
        """Run ECO analysis"""
        self.print_header(FlowStage.ECO_ANALYSIS)
        
        # PT-ECO
        eco_pattern = "signoff_flow/auto_pt/work*/pt_eco_out_*_final.tcl"
        eco_files = self.file_utils.find_files(eco_pattern, self.workarea)
        
        print(f"{Color.RED}{len(eco_files)} ECO loops were done{Color.RESET}")
        
        if eco_files:
            for eco_file in eco_files:
                self.print_file_info(eco_file, "ECO File")
                # Count ECO commands by reading file directly
                try:
                    with open(eco_file, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                    
                    # Filter out comments and empty lines
                    eco_commands = []
                    for line in lines:
                        line = line.strip()
                        if line and not line.startswith('#') and line != 'current_instance':
                            eco_commands.append(line)
                    
                    print(f"  Total ECO commands: {len(eco_commands)}")
                    
                    # Count commands by type
                    command_types = {}
                    for command_line in eco_commands:
                        # Extract command type (first word)
                        command = command_line.split()[0] if command_line.split() else ""
                        if command:
                            command_types[command] = command_types.get(command, 0) + 1
                            
                except Exception as e:
                    print(f"  Error reading ECO file: {e}")
                    eco_commands = []
                    command_types = {}
                
                # Display command breakdown
                if command_types:
                    print(f"  Command breakdown:")
                    for cmd_type, count in sorted(command_types.items(), key=lambda x: x[1], reverse=True):
                        print(f"    {cmd_type}: {count}")
                
                # Check for dont_use cells in ECO file
                self._check_eco_for_dont_use_cells(eco_file, eco_commands)
                
                # Check for instNotAllowedOnClocks violations
                self._check_eco_for_clock_violations(eco_file, eco_commands)
                
                # Check for clock cells on data paths (informational)
                self._check_eco_for_clock_cells_on_data(eco_file, eco_commands)
                        
        else:
            print("Didn't run PT ECO")
        
        # Unit Script ECO - check unit_scripts/${b}_eco.tcl
        if self.design_info and self.design_info.top_hier:
            unit_eco_file = os.path.join(self.workarea, f"unit_scripts/{self.design_info.top_hier}_eco.tcl")
            if os.path.exists(unit_eco_file):
                print(f"\n{Color.CYAN}Checking unit script ECO file:{Color.RESET}")
                self.print_file_info(unit_eco_file, "Unit Script ECO")
                
                # Parse unit script ECO file
                try:
                    with open(unit_eco_file, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                    
                    # Filter out comments and empty lines
                    eco_commands = []
                    for line in lines:
                        line = line.strip()
                        if line and not line.startswith('#') and line != 'current_instance':
                            eco_commands.append(line)
                    
                    print(f"  Total ECO commands: {len(eco_commands)}")
                    
                    # Count commands by type
                    command_types = {}
                    for command_line in eco_commands:
                        # Extract command type (first word)
                        command = command_line.split()[0] if command_line.split() else ""
                        if command:
                            command_types[command] = command_types.get(command, 0) + 1
                    
                    # Display command breakdown
                    if command_types:
                        print(f"  Command breakdown:")
                        for cmd_type, count in sorted(command_types.items(), key=lambda x: x[1], reverse=True):
                            print(f"    {cmd_type}: {count}")
                    
                    # Check for dont_use cells in unit script ECO file
                    self._check_eco_for_dont_use_cells(unit_eco_file, eco_commands)
                    
                    # Check for instNotAllowedOnClocks violations
                    self._check_eco_for_clock_violations(unit_eco_file, eco_commands)
                    
                    # Check for clock cells on data paths (informational)
                    self._check_eco_for_clock_cells_on_data(unit_eco_file, eco_commands)
                    
                except Exception as e:
                    print(f"  Error reading unit script ECO file: {e}")
        
        # NV Gate ECO - handled in run_nv_gate_eco method
        nv_eco_dir = os.path.join(self.workarea, "signoff_flow/nv_gate_eco")
        if self.file_utils.dir_exists(nv_eco_dir):
            print("NV Gate ECO directory found - see NV Gate ECO section for details")
        else:
            print("Didn't run NV Gate ECO")
        
        # Add section summary for master dashboard
        self._add_section_summary(
            section_name="ECO Analysis",
            section_id="eco",
            stage=FlowStage.ECO_ANALYSIS,
            status="PASS",
            key_metrics={
                "PT-ECO Loops": str(len(eco_files)) if eco_files else "0"
            },
            html_file="",
            priority=3,
            issues=[],
            icon="[ECO]"
        )
    
    def _extract_block_release_info(self, release_log: str, search_base_path: str = None) -> Optional[Dict[str, Any]]:
        """Extract block release information focusing on umake block_release commands
        
        Args:
            release_log: Path to release log file
            search_base_path: Base path to search for umake logs (for NBU signoff support)
            
        Returns:
            Dictionary with release information or None if not found
        """
        release_to_path = None
        try:
            # Use grep to extract only the line we need instead of reading entire file
            result = self.file_utils.run_command(f"grep 'Release to:' '{release_log}' | head -1")
            if result.strip():
                release_to_path = result.strip().split('Release to:')[1].strip()
                print(f"  {Color.CYAN}Release to:{Color.RESET} {release_to_path}")
            
            # Extract umake block_release command lines from umake logs
            # Pass through the search_base_path to search in the correct location
            release_data = self._extract_umake_block_release_commands(search_base_path)
            
            # Return both release data and release_to_path for central area check
            return release_data, release_to_path
                
        except Exception as e:
            print(f"  Error reading block release log: {e}")
            return None, release_to_path
    
    def _extract_umake_block_release_commands(self, search_base_path: str = None) -> Dict[str, Any]:
        """Extract comprehensive block release information from umake logs
        
        Args:
            search_base_path: Base path to search for umake logs (for NBU signoff support)
                             If None, defaults to workarea root
        
        Returns:
            Dictionary with release attempts, status, flags, and custom links:
            - 'attempts': List[Dict] - List of release attempt details
            - 'overall_status': str - Overall status (PASS/WARN/FAIL/NOT_RUN)
            - 'custom_links': List[str] - Custom link names
            - 'custom_links_with_dates': Dict[str, str] - Link names to dates
            - 'automatic_links': List[str] - Automatic link names
            - 'total_attempts': int - Total number of attempts
            - 'successful_attempts': int - Number of successful attempts
            - 'failed_attempts': int - Number of failed attempts
        """
        release_data = {
            'attempts': [],
            'overall_status': 'NOT_RUN',
            'custom_links': [],  # List of link names
            'custom_links_with_dates': {},  # Dict mapping link_name -> date
            'automatic_links': [],
            'total_attempts': 0,
            'successful_attempts': 0,
            'failed_attempts': 0
        }
        
        try:
            # Use provided path or default to workarea root
            # For NBU signoff, this will be the nbu_signoff base path
            if search_base_path is None:
                search_base_path = self.workarea
            
            # Search for "-s block_release" in umake logs (matches --step, --step_flags, etc.)
            umake_pattern = os.path.join(search_base_path, "umake_log/*/*.log")
            
            # Find all log files that contain block_release commands
            result = self.file_utils.run_command(
                f"grep -l 'Command line:.*-s block_release' {umake_pattern} 2>/dev/null"
            )
            
            if not result.strip():
                print(f"  {Color.YELLOW}No block release attempts found{Color.RESET}")
                return release_data
            
            log_files = result.strip().split('\n')
            
            # IMPORTANT: Filter out symlinks (like latest_dir) to avoid duplicates
            # Background: umake_log/latest_dir is a symlink pointing to the most recent log directory
            # This causes duplicate entries if not filtered (e.g., same log appears twice with different paths)
            # Solution: Use FileUtils.filter_symlinks() to resolve and deduplicate
            log_files = self.file_utils.filter_symlinks(log_files)
            release_data['total_attempts'] = len(log_files)
            
            print(f"\n  {Color.CYAN}Block Release Attempts: {len(log_files)}{Color.RESET}")
            print(f"  {'='*80}")
            
            # Known automatic links (generated automatically by umake)
            automatic_link_names = [
                'fcl_release', 'sta_release', 'pnr_release', 'dc_release',
                'fe_dct_release', 'fe_dct_golden_release', 'full_release',
                'last_sta_rel', 'last_fcl_rel', 'last_pnr_rel', 'last_dc_rel',
                'release', 'last_release'
            ]
            
            # Process each log file
            for idx, log_file in enumerate(log_files, 1):
                attempt = self._parse_release_log_file(log_file, automatic_link_names, idx)
                if attempt:
                    release_data['attempts'].append(attempt)
                    
                    # Count successful and failed attempts
                    if attempt['status'] == 'SUCCESS':
                        release_data['successful_attempts'] += 1
                    elif attempt['status'] == 'FAILED':
                        release_data['failed_attempts'] += 1
                    
                    # Collect custom links with their dates
                    for link in attempt.get('custom_links', []):
                        if link not in release_data['custom_links']:
                            release_data['custom_links'].append(link)
                        # Track first occurrence date for each custom link
                        if link not in release_data['custom_links_with_dates']:
                            release_data['custom_links_with_dates'][link] = attempt['date']
                    
                    # Collect automatic links
                    for link in attempt.get('automatic_links', []):
                        if link not in release_data['automatic_links']:
                            release_data['automatic_links'].append(link)
            
            # Determine overall status based on all attempts
            if release_data['successful_attempts'] > 0 and release_data['failed_attempts'] == 0:
                # All attempts successful
                release_data['overall_status'] = 'PASS'
            elif release_data['successful_attempts'] > 0 and release_data['failed_attempts'] > 0:
                # Mixed success and failure
                release_data['overall_status'] = 'WARN'
            elif release_data['failed_attempts'] > 0 and release_data['successful_attempts'] == 0:
                # All attempts failed
                release_data['overall_status'] = 'FAIL'
            else:
                # No clear status
                release_data['overall_status'] = 'NOT_RUN'
            
            # Print summary statistics for regression parsing
            print(f"\n  {Color.CYAN}Release Attempts from Workarea{Color.RESET}")
            print(f"  {Color.CYAN}---------------------------------------------{Color.RESET}")
            print(f"  Total Attempts: {release_data['total_attempts']}")
            print(f"  Successful: {release_data['successful_attempts']}")
            print(f"  Failed: {release_data['failed_attempts']}")
            if release_data['custom_links']:
                # Just show count - detailed links shown in "All Custom Links" section below
                print(f"  Custom Links: {len(release_data['custom_links'])} detected (see details below)")
                # Print individual custom link details with dates for regression HTML parsing
                sorted_links = sorted(
                    release_data['custom_links'],
                    key=lambda link: release_data['custom_links_with_dates'].get(link, ''),
                    reverse=False
                )
                for link in sorted_links:
                    link_date = release_data['custom_links_with_dates'].get(link, 'Unknown')
                    print(f"  Custom Link Detail: {link}|{link_date}")
            # Get latest success date
            successful_attempts = [a for a in release_data['attempts'] if a['status'] == 'SUCCESS']
            if successful_attempts:
                latest = successful_attempts[-1]
                print(f"  Latest Success: {latest['date']}")
            # Get release name from first attempt if available
            if release_data['attempts']:
                first_attempt = release_data['attempts'][0]
                if 'release_name' in first_attempt.get('flags', {}):
                    print(f"  Release Name: {first_attempt['flags']['release_name']}")
            print()
            
            # Note: Custom links display consolidated with "All Custom Links in Central Release Area" section
            # No need to display them here separately to avoid duplication
            
            return release_data
                
        except Exception as e:
            print(f"  {Color.RED}Error extracting block release commands: {e}{Color.RESET}")
            return release_data
    
    def _generate_block_release_html_report(self, release_data: dict, release_to_path: str, umake_custom_links: list = None, all_locations: list = None) -> str:
        """Generate comprehensive HTML report for block release
        
        Args:
            release_data: Dictionary with release attempts and custom links
            release_to_path: Path to central release area
            umake_custom_links: List of custom links created via umake (for manual detection)
            all_locations: List of tuples with all block_release locations (for NBU signoff)
            
        Returns:
            Path to generated HTML file
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            username = os.environ.get('USER', 'avice')
            html_filename = f"{self.design_info.top_hier}_{username}_block_release_{timestamp}.html"
            html_path = os.path.abspath(html_filename)
            
            # Read and encode logo as base64
            logo_data = ""
            script_dir = os.path.dirname(os.path.abspath(__file__))
            logo_path = os.path.join(script_dir, "assets/images/avice_logo.png")
            if os.path.exists(logo_path):
                with open(logo_path, "rb") as logo_file:
                    import base64
                    logo_data = base64.b64encode(logo_file.read()).decode('utf-8')
            
            # Get central area links (with target-based duplicate detection)
            central_links = []
            
            if release_to_path:
                base_release_dir = os.path.dirname(release_to_path)
                automatic_link_names = [
                    'fcl_release', 'sta_release', 'pnr_release', 'dc_release',
                    'fe_dct_release', 'fe_dct_golden_release', 'full_release',
                    'last_sta_rel', 'last_fcl_rel', 'last_pnr_rel', 'last_dc_rel',
                    'release', 'last_release'
                ]
                
                if os.path.exists(base_release_dir):
                    try:
                        # PHASE 1: Collect all custom symlinks with metadata
                        temp_links = []
                        entries = os.listdir(base_release_dir)
                        for entry in entries:
                            entry_path = os.path.join(base_release_dir, entry)
                            if os.path.islink(entry_path) and not entry.startswith('prev_'):
                                is_automatic = any(auto_link in entry for auto_link in automatic_link_names)
                                if not is_automatic:
                                    try:
                                        stat_info = os.lstat(entry_path)
                                        link_date = datetime.fromtimestamp(stat_info.st_mtime).strftime('%Y-%m-%d')
                                        link_time = datetime.fromtimestamp(stat_info.st_mtime).strftime('%Y-%m-%d %H:%M:%S')
                                        link_timestamp = stat_info.st_mtime  # Unix timestamp
                                        target = os.readlink(entry_path)
                                        target_basename = os.path.basename(target)
                                        user = "Unknown"
                                        try:
                                            import pwd
                                            user = pwd.getpwuid(stat_info.st_uid).pw_name
                                        except:
                                            if 'scratch.' in target:
                                                user_match = re.search(r'scratch\.([^_/]+)', target)
                                                if user_match:
                                                    user = user_match.group(1)
                                        
                                        # Detect if target is NBU signoff
                                        target_full = os.path.abspath(target)
                                        is_nbu, nbu_ipo = self._detect_nbu_release_from_target(target_full)
                                        
                                        temp_links.append((entry, link_date, link_time, link_timestamp, user, target_basename, target_full, is_nbu, nbu_ipo))
                                    except:
                                        pass
                        
                        # PHASE 2: Detect manual links by target duplication
                        target_groups = {}
                        for entry, link_date, link_time, link_timestamp, user, target_basename, target_full, is_nbu, nbu_ipo in temp_links:
                            if target_full not in target_groups:
                                target_groups[target_full] = []
                            target_groups[target_full].append((entry, link_timestamp, link_date))
                        
                        manual_links_set = set()
                        for target_path, links in target_groups.items():
                            if len(links) > 1:
                                links.sort(key=lambda x: x[1])  # Sort by timestamp
                                # First is AUTO, rest are MANUAL
                                for i, (link_name, timestamp, link_date) in enumerate(links):
                                    if i > 0:
                                        manual_links_set.add(link_name)
                        
                        # Build final central_links with manual flag
                        for entry, link_date, link_time, link_timestamp, user, target_basename, target_full, is_nbu, nbu_ipo in temp_links:
                            is_manual = entry in manual_links_set
                            central_links.append((entry, link_date, link_time, user, target_basename, target_full, is_manual, is_nbu, nbu_ipo))
                        
                        central_links.sort(key=lambda x: x[2])  # Sort by link_time
                    except:
                        pass
            
            # Generate HTML content
            html_content = self._generate_block_release_html_content(
                release_data, release_to_path, central_links, logo_data, all_locations
            )
            
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            return html_path
            
        except Exception as e:
            print(f"  {Color.RED}Error generating block release HTML report: {e}{Color.RESET}")
            return ""
    
    def _generate_block_release_html_content(self, release_data: dict, release_to_path: str, central_links: list, logo_data: str, all_locations: list = None) -> str:
        """Generate HTML content for block release report with multi-location support"""
        
        # Build multi-location section HTML (if NBU signoff detected)
        locations_html = ""
        if all_locations and len(all_locations) > 1:
            locations_table = ""
            for idx, (path, ipo, is_nbu, metadata) in enumerate(all_locations, 1):
                rel_path = os.path.relpath(path, self.workarea)
                location_type = f"NBU ({ipo})" if is_nbu else "ROOT"
                user_display = metadata['user']
                date_display = metadata['date_str']
                
                # Style based on USER match
                if metadata['user'] == self.workarea_owner:
                    row_style = "background-color: #d5f4e6; border-left: 4px solid #27ae60;"  # Green
                    status_badge = '<span style="background: #27ae60; color: white; padding: 2px 8px; border-radius: 4px; font-size: 0.85em;">ACTIVE</span>'
                else:
                    row_style = "background-color: #fff3cd; border-left: 4px solid #f39c12; opacity: 0.7;"  # Yellow
                    status_badge = '<span style="background: #f39c12; color: white; padding: 2px 8px; border-radius: 4px; font-size: 0.85em;">STALE</span>'
                
                locations_table += f"""
                <tr style="{row_style}">
                    <td style="padding: 12px;"><strong>{idx}</strong></td>
                    <td style="padding: 12px;"><strong>{location_type}</strong></td>
                    <td style="padding: 12px; font-family: monospace; font-size: 0.9em;">{rel_path}</td>
                    <td style="padding: 12px;">{user_display}</td>
                    <td style="padding: 12px;">{date_display}</td>
                    <td style="padding: 12px; text-align: center;">{status_badge}</td>
                </tr>
                """
            
            locations_html = f"""
            <div class="section-card" style="margin-top: 30px; border-left: 4px solid #9b59b6;">
                <h2 style="color: #9b59b6; margin-bottom: 20px;">
                    <span style="font-size: 1.5em;">ðŸ“</span> Block Release Locations Detected
                </h2>
                <p style="color: #7f8c8d; margin-bottom: 15px;">
                    This workarea uses <strong style="color: #9b59b6;">NBU Signoff mode</strong>. 
                    Multiple block_release locations were found. The <span style="color: #27ae60;"><strong>ACTIVE</strong></span> location 
                    is selected based on USER field matching the workarea owner.
                </p>
                <table style="width: 100%; border-collapse: collapse; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                    <thead>
                        <tr style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;">
                            <th style="padding: 12px; text-align: left;">#</th>
                            <th style="padding: 12px; text-align: left;">Type</th>
                            <th style="padding: 12px; text-align: left;">Path</th>
                            <th style="padding: 12px; text-align: left;">USER</th>
                            <th style="padding: 12px; text-align: left;">Date</th>
                            <th style="padding: 12px; text-align: center;">Status</th>
                        </tr>
                    </thead>
                    <tbody>
                        {locations_table}
                    </tbody>
                </table>
                <div style="margin-top: 15px; padding: 12px; background: #f8f9fa; border-radius: 4px; border-left: 3px solid #3498db;">
                    <strong style="color: #3498db;">Selection Logic:</strong> 
                    The ACTIVE release is selected based on:
                    <ol style="margin: 8px 0 0 20px; color: #555;">
                        <li>USER field matches workarea owner (highest priority)</li>
                        <li>Newest timestamp (tiebreaker)</li>
                        <li>Prefer NBU location if timestamps tied (newer workflow)</li>
                    </ol>
                </div>
            </div>
            """
        
        # Calculate statistics
        total_attempts = release_data.get('total_attempts', 0)
        success_count = release_data.get('successful_attempts', 0)
        failed_count = release_data.get('failed_attempts', 0)
        success_rate = (success_count / total_attempts * 100) if total_attempts > 0 else 0
        
        # Extract latest successful attempt's flags for release type badges
        latest_success_flags = {}
        successful_attempts = [a for a in release_data.get('attempts', []) if a['status'] == 'SUCCESS']
        if successful_attempts:
            latest_success_flags = successful_attempts[-1].get('flags', {})
        
        # Generate release type badges
        release_badges_html = ""
        badges = []
        if 'sta_release' in latest_success_flags:
            badges.append('<span class="release-badge" style="background: #3498db;" title="Timing Release">S</span>')
        if 'fcl_release' in latest_success_flags:
            badges.append('<span class="release-badge" style="background: #9b59b6;" title="FCL Release">F</span>')
        if 'pnr_release' in latest_success_flags:
            badges.append('<span class="release-badge" style="background: #e67e22;" title="PnR Release">P</span>')
        if 'fe_dct_release' in latest_success_flags:
            badges.append('<span class="release-badge" style="background: #27ae60;" title="FE DCT Release">D</span>')
        
        if badges:
            release_badges_html = f"""
            <div style="display: flex; align-items: center; justify-content: center; gap: 8px; margin-top: 10px;">
                <span style="color: white; opacity: 0.9; font-size: 0.9em;">Latest Release Contains:</span>
                {' '.join(badges)}
            </div>
            """
        
        # Build attempts HTML (expandable)
        attempts_list = release_data.get('attempts', [])
        total_release_attempts = len(attempts_list)
        success_attempts = [a for a in attempts_list if a['status'] == 'SUCCESS']
        fail_attempts = [a for a in attempts_list if a['status'] == 'FAILED']
        
        # Summary line
        attempts_summary = f"<strong>{total_release_attempts} total attempt(s)</strong> - "
        attempts_summary += f"<span style='color: #27ae60;'>{len(success_attempts)} SUCCESS</span>, "
        attempts_summary += f"<span style='color: #e74c3c;'>{len(fail_attempts)} FAILED</span>"
        
        # Build detailed attempts HTML
        attempts_detail_html = ""
        for attempt in attempts_list:
            status_class = "success" if attempt['status'] == 'SUCCESS' else ("failed" if attempt['status'] == 'FAILED' else "unknown")
            status_icon = "âœ“" if attempt['status'] == 'SUCCESS' else ("âœ—" if attempt['status'] == 'FAILED' else "?")
            
            custom_links_html = ""
            if attempt.get('custom_links'):
                custom_links_html = f"<div class='custom-links'><strong>Custom Links:</strong> {', '.join(attempt['custom_links'])}</div>"
            
            failure_html = ""
            if attempt.get('failure_reason'):
                failure_html = f"<div class='failure-reason'><strong>Failure:</strong> {attempt['failure_reason']}</div>"
            
            flags_list = [f"{k}={v}" if v != True else k for k, v in attempt.get('flags', {}).items()]
            flags_html = ", ".join(flags_list) if flags_list else "None"
            
            attempts_detail_html += f"""
            <div class="attempt-card {status_class}">
                <div class="attempt-header">
                    <span class="attempt-number">#{attempt['attempt_num']}</span>
                    <span class="attempt-timestamp">{attempt['timestamp']}</span>
                    <span class="attempt-user">User: {attempt['user']}</span>
                    <span class="status-badge {status_class}">{status_icon} {attempt['status']}</span>
                </div>
                <div class="attempt-details">
                    <div class="command-line"><strong>Command:</strong> <code>{attempt.get('command', 'N/A')}</code></div>
                    <div class="flags-line"><strong>Flags:</strong> <span style="color: #3498db;">{flags_html}</span></div>
                    {custom_links_html}
                    {failure_html}
                </div>
            </div>
            """
        
        # Combine with expandable button
        attempts_html = f"""
        <div class="attempts-summary">{attempts_summary}</div>
        <button class="expand-btn" onclick="toggleSection('attemptsDetail')">
            <span id="attemptsDetailToggle">â–¼</span> Show all {total_release_attempts} attempt(s)
        </button>
        <div id="attemptsDetail" class="expandable-content">
            {attempts_detail_html}
        </div>
        """
        
        # Build central links HTML (expandable with color coding and manual detection)
        central_links_html = ""
        if central_links:
            # Sort by date (most recent first)
            sorted_links = sorted(central_links, key=lambda x: x[2], reverse=True)
            total_links = len(sorted_links)
            
            # Show top 3 with color coding
            for idx, (link_name, link_date, link_time, user, target_basename, target_full_path, is_manual, is_nbu, nbu_ipo) in enumerate(sorted_links[:3]):
                # Color code: 1st=green, 2nd=orange, 3rd=gray
                if idx == 0:
                    row_color = "#d5f4e6"  # Light green
                elif idx == 1:
                    row_color = "#ffeaa7"  # Light orange
                else:
                    row_color = "#dfe6e9"  # Light gray
                
                manual_badge = '<span class="manual-badge">Manual</span>' if is_manual else ''
                
                # Determine source display
                if is_nbu and nbu_ipo:
                    source_display = f'<span style="color: #9b59b6; font-weight: bold;">NBU ({nbu_ipo})</span>'
                elif is_nbu:
                    source_display = '<span style="color: #9b59b6; font-weight: bold;">NBU</span>'
                else:
                    source_display = '<span style="color: #27ae60;">ROOT</span>'
                
                central_links_html += f"""
                <tr style="background-color: {row_color};">
                    <td><strong>{link_name}</strong> {manual_badge}</td>
                    <td>{link_date}</td>
                    <td>{user}</td>
                    <td>{source_display}</td>
                    <td><a href="file://{target_full_path}" class="path-link" title="{target_full_path}">{target_basename}</a></td>
                </tr>
                """
            
            # Build older links (expandable)
            if total_links > 3:
                older_links_html = ""
                for link_name, link_date, link_time, user, target_basename, target_full_path, is_manual, is_nbu, nbu_ipo in sorted_links[3:]:
                    manual_badge = '<span class="manual-badge">Manual</span>' if is_manual else ''
                    
                    # Determine source display
                    if is_nbu and nbu_ipo:
                        source_display = f'<span style="color: #9b59b6; font-weight: bold;">NBU ({nbu_ipo})</span>'
                    elif is_nbu:
                        source_display = '<span style="color: #9b59b6; font-weight: bold;">NBU</span>'
                    else:
                        source_display = '<span style="color: #27ae60;">ROOT</span>'
                    
                    older_links_html += f"""
                    <tr>
                        <td><strong>{link_name}</strong> {manual_badge}</td>
                        <td>{link_date}</td>
                        <td>{user}</td>
                        <td>{source_display}</td>
                        <td><a href="file://{target_full_path}" class="path-link" title="{target_full_path}">{target_basename}</a></td>
                    </tr>
                    """
                
                central_links_html += f"""
                <tr>
                    <td colspan="5" style="text-align: center; padding: 10px;">
                        <button class="expand-btn" onclick="toggleSection('olderLinks')" style="margin: 0 auto;">
                            <span id="olderLinksToggle">â–¼</span> Show {total_links - 3} more older link(s)
                        </button>
                    </td>
                </tr>
                <tbody id="olderLinks" class="expandable-content">
                    {older_links_html}
                </tbody>
                """
        else:
            central_links_html = "<tr><td colspan='5' class='no-data'>No custom links found in central release area</td></tr>"
        
        # Build timeline HTML
        timeline_html = ""
        all_events = []
        for attempt in release_data.get('attempts', []):
            all_events.append({
                'date': attempt['date'],
                'time': attempt['timestamp'],
                'type': 'attempt',
                'status': attempt['status'],
                'user': attempt['user'],
                'details': f"Release attempt #{attempt['attempt_num']}"
            })
        for link_name, link_date, link_time, user, target_basename, target_full_path, is_manual, is_nbu, nbu_ipo in central_links:
            # Determine link type with NBU source
            if is_manual:
                link_type = "Manual custom link"
            else:
                link_type = "Custom link"
            
            if is_nbu and nbu_ipo:
                link_type += f" (NBU {nbu_ipo})"
            elif is_nbu:
                link_type += " (NBU)"
            
            all_events.append({
                'date': link_date,
                'time': link_time,
                'type': 'link',
                'status': 'SUCCESS',
                'user': user,
                'details': f"{link_type} created: {link_name}"
            })
        
        all_events.sort(key=lambda x: x['time'])
        for event in all_events:
            status_class = "success" if event['status'] == 'SUCCESS' else ("failed" if event['status'] == 'FAILED' else "unknown")
            icon = "ðŸ“¦" if event['type'] == 'attempt' else "ðŸ”—"
            timeline_html += f"""
            <div class="timeline-event {status_class}">
                <div class="timeline-marker"></div>
                <div class="timeline-content">
                    <div class="timeline-time">{event['date']}</div>
                    <div class="timeline-details">{icon} {event['details']} <span class="timeline-user">by {event['user']}</span></div>
                </div>
            </div>
            """
        
        base_release_dir = os.path.dirname(release_to_path) if release_to_path else "N/A"
        
        html = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Block Release Report - {self.design_info.top_hier}</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            color: #2c3e50;
            padding: 20px;
            line-height: 1.6;
        }}
        
        .container {{
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 30px;
            flex-wrap: wrap;
        }}
        
        .logo {{
            height: 120px;
            cursor: pointer;
            transition: transform 0.3s ease;
        }}
        
        .logo:hover {{
            transform: scale(1.05);
        }}
        
        .header-text {{
            text-align: center;
        }}
        
        .header h1 {{
            font-size: 2.5em;
            margin-bottom: 10px;
        }}
        
        .header-subtitle {{
            font-size: 1.2em;
            opacity: 0.9;
        }}
        
        /* Logo Modal */
        .logo-modal {{
            display: none;
            position: fixed;
            z-index: 1000;
            left: 0;
            top: 0;
            width: 100%;
            height: 100%;
            overflow: auto;
            background-color: rgba(0,0,0,0.9);
        }}
        
        .logo-modal-content {{
            margin: auto;
            display: block;
            width: 80%;
            max-width: 700px;
            animation: zoom 0.6s;
        }}
        
        .logo-modal-close {{
            position: absolute;
            top: 15px;
            right: 35px;
            color: #f1f1f1;
            font-size: 40px;
            font-weight: bold;
            transition: 0.3s;
            cursor: pointer;
        }}
        
        .logo-modal-close:hover,
        .logo-modal-close:focus {{
            color: #bbb;
        }}
        
        @keyframes zoom {{
            from {{transform: scale(0)}}
            to {{transform: scale(1)}}
        }}
        
        .stats-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            padding: 30px;
            background: #f8f9fa;
        }}
        
        .stat-card {{
            background: white;
            padding: 20px;
            border-radius: 10px;
            text-align: center;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }}
        
        .stat-value {{
            font-size: 2.5em;
            font-weight: bold;
            color: #667eea;
        }}
        
        .stat-label {{
            font-size: 0.9em;
            color: #7f8c8d;
            margin-top: 5px;
        }}
        
        .stat-card.success .stat-value {{
            color: #27ae60;
        }}
        
        .stat-card.failed .stat-value {{
            color: #e74c3c;
        }}
        
        .content {{
            padding: 40px;
        }}
        
        .section {{
            margin-bottom: 40px;
        }}
        
        .section-title {{
            font-size: 1.8em;
            color: #2c3e50;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #667eea;
        }}
        
        .info-box {{
            background: #e8f4f8;
            border-left: 4px solid #3498db;
            padding: 15px;
            margin-bottom: 20px;
            border-radius: 5px;
        }}
        
        .attempt-card {{
            background: white;
            border: 2px solid #ecf0f1;
            border-radius: 10px;
            margin-bottom: 20px;
            overflow: hidden;
            transition: transform 0.2s ease, box-shadow 0.2s ease;
        }}
        
        .attempt-card:hover {{
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
        }}
        
        .attempt-card.success {{
            border-left: 5px solid #27ae60;
        }}
        
        .attempt-card.failed {{
            border-left: 5px solid #e74c3c;
        }}
        
        .attempt-card.unknown {{
            border-left: 5px solid #f39c12;
        }}
        
        .attempt-header {{
            background: #f8f9fa;
            padding: 15px 20px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 10px;
        }}
        
        .attempt-number {{
            font-size: 1.2em;
            font-weight: bold;
            color: #667eea;
        }}
        
        .attempt-timestamp {{
            color: #7f8c8d;
        }}
        
        .attempt-user {{
            color: #34495e;
            font-weight: 600;
        }}
        
        .status-badge {{
            padding: 5px 15px;
            border-radius: 20px;
            font-weight: bold;
            color: white;
            font-size: 0.9em;
        }}
        
        .status-badge.success {{
            background: #27ae60;
        }}
        
        .status-badge.failed {{
            background: #e74c3c;
        }}
        
        .status-badge.unknown {{
            background: #f39c12;
        }}
        
        .attempt-details {{
            padding: 20px;
        }}
        
        .command-line {{
            margin-bottom: 10px;
            padding: 10px;
            background: #f8f9fa;
            border-radius: 5px;
        }}
        
        .command-line code {{
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            word-break: break-all;
        }}
        
        .flags-line {{
            margin-bottom: 10px;
            color: #34495e;
        }}
        
        .custom-links {{
            margin-top: 10px;
            padding: 10px;
            background: #e8f8f5;
            border-radius: 5px;
            color: #27ae60;
        }}
        
        .failure-reason {{
            margin-top: 10px;
            padding: 10px;
            background: #fadbd8;
            border-radius: 5px;
            color: #e74c3c;
        }}
        
        table {{
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
        }}
        
        th {{
            background: #667eea;
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 600;
        }}
        
        td {{
            padding: 12px 15px;
            border-bottom: 1px solid #ecf0f1;
        }}
        
        tr:hover {{
            background: #f8f9fa;
        }}
        
        .no-data {{
            text-align: center;
            color: #95a5a6;
            font-style: italic;
        }}
        
        .path-link {{
            color: #3498db;
            text-decoration: none;
            word-break: break-all;
        }}
        
        .path-link:hover {{
            text-decoration: underline;
        }}
        
        .timeline {{
            position: relative;
            padding-left: 30px;
        }}
        
        .timeline::before {{
            content: '';
            position: absolute;
            left: 10px;
            top: 0;
            bottom: 0;
            width: 3px;
            background: linear-gradient(to bottom, #667eea, #764ba2);
        }}
        
        .timeline-event {{
            position: relative;
            margin-bottom: 30px;
            padding-left: 20px;
        }}
        
        .timeline-marker {{
            position: absolute;
            left: -19px;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            background: white;
            border: 3px solid #667eea;
        }}
        
        .timeline-event.success .timeline-marker {{
            border-color: #27ae60;
            background: #27ae60;
        }}
        
        .timeline-event.failed .timeline-marker {{
            border-color: #e74c3c;
            background: #e74c3c;
        }}
        
        .timeline-content {{
            background: white;
            padding: 15px;
            border-radius: 8px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }}
        
        .timeline-time {{
            font-size: 0.9em;
            color: #7f8c8d;
            margin-bottom: 5px;
        }}
        
        .timeline-details {{
            color: #2c3e50;
            font-weight: 500;
        }}
        
        .timeline-user {{
            color: #7f8c8d;
            font-size: 0.9em;
        }}
        
        /* Copyright Footer */
        .footer {{
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            text-align: center;
            padding: 20px;
            margin-top: 40px;
            border-radius: 10px;
            font-size: 14px;
        }}
        
        .footer p {{
            margin: 5px 0;
        }}
        
        .footer strong {{
            color: #00ff00;
        }}
        
        /* Release Type Badges */
        .release-badge {{
            display: inline-block;
            color: white;
            padding: 4px 12px;
            border-radius: 4px;
            font-size: 0.9em;
            font-weight: bold;
            margin: 0 4px;
            cursor: help;
        }}
        
        /* Expandable sections */
        .expand-btn {{
            background: none;
            border: none;
            color: #3498db;
            cursor: pointer;
            font-size: 0.95em;
            padding: 8px 12px;
            text-decoration: underline;
            display: flex;
            align-items: center;
            gap: 6px;
            margin: 10px 0;
        }}
        
        .expand-btn:hover {{
            color: #2980b9;
        }}
        
        .expandable-content {{
            display: none;
        }}
        
        /* Manual link annotation */
        .manual-badge {{
            background: #3498db;
            color: white;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 0.8em;
            margin-left: 8px;
        }}
    </style>
</head>
<body>
    <!-- Logo Modal -->
    <div id="logoModal" class="logo-modal" onclick="closeLogoModal()">
        <span class="logo-modal-close">&times;</span>
        <img class="logo-modal-content" src='data:image/png;base64,{logo_data}' alt='AVICE Logo'>
    </div>
    
    <div class="container">
        <div class="header">
            <img class='logo' src='data:image/png;base64,{logo_data}' alt='AVICE Logo' onclick="showLogoModal()" title="Click to enlarge">
            <div class="header-text">
                <h1>ðŸš€ Block Release Report</h1>
                <div class="header-subtitle">Design: {self.design_info.top_hier}</div>
                <div class="header-subtitle">Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</div>
                {release_badges_html}
            </div>
        </div>
        
        <!-- Multi-Location Detection (NBU Signoff) -->
        {locations_html}
        
        <div class="stats-grid">
            <div class="stat-card">
                <div class="stat-value">{total_attempts}</div>
                <div class="stat-label">Total Attempts</div>
            </div>
            <div class="stat-card success">
                <div class="stat-value">{success_count}</div>
                <div class="stat-label">Successful</div>
            </div>
            <div class="stat-card failed">
                <div class="stat-value">{failed_count}</div>
                <div class="stat-label">Failed</div>
            </div>
            <div class="stat-card">
                <div class="stat-value">{success_rate:.0f}%</div>
                <div class="stat-label">Success Rate</div>
            </div>
        </div>
        
        <div class="content">
            <div class="info-box">
                <strong>Central Release Area:</strong> {base_release_dir}
            </div>
            
            <div class="section">
                <h2 class="section-title">Release Attempts from Workarea</h2>
                {attempts_html if attempts_html else '<p class="no-data">No release attempts found in workarea logs</p>'}
            </div>
            
            <div class="section">
                <h2 class="section-title">All Custom Links (Central Release Area)</h2>
                <table>
                    <thead>
                        <tr>
                            <th>Link Name</th>
                            <th>Date Created</th>
                            <th>User</th>
                            <th>Source</th>
                            <th>Target Workarea</th>
                        </tr>
                    </thead>
                    <tbody>
                        {central_links_html}
                    </tbody>
                </table>
            </div>
            
            <div class="section">
                <h2 class="section-title">Release Timeline</h2>
                <div class="timeline">
                    {timeline_html if timeline_html else '<p class="no-data">No timeline events available</p>'}
                </div>
            </div>
        </div>
        
        <!-- Copyright Footer -->
        <div class="footer">
            <p><strong>AVICE Block Release Report</strong></p>
            <p>Copyright (c) 2025 Alon Vice (avice)</p>
            <p>Contact: avice@nvidia.com</p>
        </div>
    </div>
    
    <!-- Back to Top Button -->
    <button id="backToTopBtn" style="display: none; position: fixed; bottom: 30px; right: 30px; 
            z-index: 99; border: none; outline: none; background-color: #667eea; color: white; 
            cursor: pointer; padding: 15px 20px; border-radius: 50px; font-size: 16px; 
            font-weight: bold; box-shadow: 0 4px 6px rgba(0,0,0,0.3); transition: all 0.3s ease;"
            onmouseover="this.style.backgroundColor='#5568d3'; this.style.transform='scale(1.1)';"
            onmouseout="this.style.backgroundColor='#667eea'; this.style.transform='scale(1)';">
        â†‘ Top
    </button>
    
    <script>
        // Logo modal functionality
        function showLogoModal() {{
            document.getElementById('logoModal').style.display = 'block';
        }}
        
        function closeLogoModal() {{
            document.getElementById('logoModal').style.display = 'none';
        }}
        
        // Toggle expandable sections
        function toggleSection(sectionId) {{
            var content = document.getElementById(sectionId);
            var toggle = document.getElementById(sectionId + 'Toggle');
            
            if (content.style.display === 'none' || content.style.display === '') {{
                // Determine the appropriate display type
                var displayType = content.tagName === 'TBODY' ? 'table-row-group' : 'block';
                content.style.display = displayType;
                if (toggle) toggle.textContent = 'â–²';
            }} else {{
                content.style.display = 'none';
                if (toggle) toggle.textContent = 'â–¼';
            }}
        }}
        
        // Back to top button functionality
        const backToTopBtn = document.getElementById('backToTopBtn');
        if (backToTopBtn) {{
            window.addEventListener('scroll', function() {{
                if (window.pageYOffset > 300) {{
                    backToTopBtn.style.display = 'block';
                }} else {{
                    backToTopBtn.style.display = 'none';
                }}
            }});
            
            backToTopBtn.addEventListener('click', function() {{
                window.scrollTo({{ top: 0, behavior: 'smooth' }});
            }});
        }}
    </script>
</body>
</html>
"""
        return html
    
    def _check_central_block_release_links(self, release_to_path: str, automatic_link_names: List[str], umake_custom_links: Optional[List[str]] = None) -> List[str]:
        """Check central block release area for all custom links with metadata
        
        Uses symlink target deduplication to detect manual links:
        - Multiple symlinks pointing to the same target â†’ oldest is AUTO, rest are MANUAL
        - Single symlink per target â†’ AUTO
        
        Args:
            release_to_path: Path from 'Release to:' line (e.g., /home/agur_backend_blockRelease/block/prt/...)
            automatic_link_names: List of automatic link names
            umake_custom_links: Optional list of custom link names (kept for backward compatibility, not used)
            
        Returns:
            list: Manually created custom links (detected by target duplication)
        """
        try:
            # Extract base path (e.g., /home/agur_backend_blockRelease/block/prt/)
            if not release_to_path:
                return []
            
            # Get the parent directory (should be /home/agur_backend_blockRelease/block/<unit>/)
            base_release_dir = os.path.dirname(release_to_path)
            
            if not os.path.exists(base_release_dir):
                return []
            
            print(f"\n  {Color.CYAN}All Custom Links in Central Release Area:{Color.RESET}")
            print(f"  {Color.CYAN}Location: {base_release_dir}{Color.RESET}")
            
            # List all entries in the directory
            try:
                entries = os.listdir(base_release_dir)
            except PermissionError:
                print(f"    {Color.YELLOW}No permission to access central release area{Color.RESET}")
                return []
            
            custom_links_found = []
            manually_created_links = []  # Links detected as manual duplicates
            
            # PHASE 1: Collect all custom symlinks with their metadata
            for entry in sorted(entries):
                entry_path = os.path.join(base_release_dir, entry)
                
                # Check if it's a symlink
                if os.path.islink(entry_path):
                    # Skip prev_* links (backup/previous versions)
                    if entry.startswith('prev_'):
                        continue
                    
                    # Check if it's a custom link (not in automatic list)
                    is_automatic = any(auto_link in entry for auto_link in automatic_link_names)
                    
                    if not is_automatic:
                        # Get symlink metadata
                        try:
                            stat_info = os.lstat(entry_path)
                            # Get modification time (when symlink was created)
                            link_date = datetime.fromtimestamp(stat_info.st_mtime).strftime('%Y-%m-%d')
                            link_time = datetime.fromtimestamp(stat_info.st_mtime).strftime('%Y-%m-%d %H:%M:%S')
                            link_timestamp = stat_info.st_mtime  # Unix timestamp for sorting
                            
                            # Get target path
                            target = os.readlink(entry_path)
                            target_full = target  # Store full path for display
                            
                            # Get the owner of the symlink (the user who created it)
                            user = "Unknown"
                            try:
                                import pwd
                                user = pwd.getpwuid(stat_info.st_uid).pw_name
                            except:
                                # Fallback: try to extract from target directory path
                                if 'scratch.' in target:
                                    user_match = re.search(r'scratch\.([^_/]+)', target)
                                    if user_match:
                                        user = user_match.group(1)
                            
                            # Detect if target is NBU signoff
                            is_nbu, nbu_ipo = self._detect_nbu_release_from_target(target_full)
                            
                            # Store with timestamp for later duplicate detection
                            custom_links_found.append((entry, link_date, link_time, link_timestamp, user, target_full, is_nbu, nbu_ipo))
                            
                        except Exception as e:
                            # Store with dummy data if metadata extraction fails
                            custom_links_found.append((entry, 'Unknown', 'Unknown', 0, 'Unknown', '', False, None))
            
            # PHASE 2: Group symlinks by target and detect manual duplicates
            # Build a mapping: target_path -> list of (link_name, timestamp, link_date)
            target_groups = {}
            for entry, link_date, link_time, link_timestamp, user, target_full, is_nbu, nbu_ipo in custom_links_found:
                if target_full not in target_groups:
                    target_groups[target_full] = []
                target_groups[target_full].append((entry, link_timestamp, link_date))
            
            # Determine which links are manual based on target duplication
            manual_links_set = set()
            for target_path, links in target_groups.items():
                if len(links) > 1:
                    # Multiple symlinks point to same target
                    # Sort by timestamp (oldest first)
                    links.sort(key=lambda x: x[1])  # x[1] is timestamp
                    
                    # First (oldest) is AUTO, rest are MANUAL
                    for i, (link_name, timestamp, link_date) in enumerate(links):
                        if i > 0:  # Skip first (oldest)
                            manual_links_set.add(link_name)
                            manually_created_links.append((link_name, link_date))
                # If only 1 link per target, it's AUTO (no action needed)
            
            # Count NBU vs ROOT links
            nbu_count = sum(1 for item in custom_links_found if item[6])  # item[6] is is_nbu
            root_count = len(custom_links_found) - nbu_count
            
            if custom_links_found:
                # Sort by date (oldest first) - using timestamp for accurate sorting
                custom_links_found.sort(key=lambda x: x[3])  # x[3] is link_timestamp
                
                # Display in a table format with Source and Target columns
                print(f"    {'Link Name':<30} {'Date':<12} {'User':<15} {'Source':<16} {'Target':<60} {'Status'}")
                print(f"    {'-'*30} {'-'*12} {'-'*15} {'-'*16} {'-'*60} {'-'*20}")
                
                # Track if current link matches workarea owner for highlighting
                latest_nbu_link = None
                for entry, link_date, link_time, link_timestamp, user, target_full, is_nbu, nbu_ipo in custom_links_found:
                    # Check if this link is manual (based on target duplication)
                    is_manual = entry in manual_links_set
                    
                    # Determine source display
                    if is_nbu and nbu_ipo:
                        source_str = f"NBU ({nbu_ipo})"
                    elif is_nbu:
                        source_str = "NBU"
                    else:
                        source_str = "ROOT"
                    
                    # Check if this is old owner (not matching current workarea owner)
                    if user != self.workarea_owner and not is_nbu:
                        source_str += " [old]"
                    
                    # Extract shortened target (remove /home/agur_backend_blockRelease/block/ prefix)
                    target_short = os.path.basename(target_full)
                    if len(target_short) > 60:
                        target_short = target_short[:57] + "..."
                    
                    # Build status string
                    status_parts = []
                    if is_manual:
                        status_parts.append(f"{Color.CYAN}Manual{Color.RESET}")
                    
                    # Mark latest NBU link
                    if is_nbu and user == self.workarea_owner:
                        status_parts.append(f"{Color.GREEN}[LATEST]{Color.RESET}")
                        latest_nbu_link = (entry, nbu_ipo, link_date)
                    
                    status_str = " ".join(status_parts)
                    
                    # Color code link name based on source
                    if is_nbu:
                        link_display = f"{Color.MAGENTA}{entry:<30}{Color.RESET}"
                    else:
                        link_display = f"{Color.GREEN}{entry:<30}{Color.RESET}"
                    
                    print(f"    {link_display} {link_date:<12} {user:<15} {source_str:<16} {target_short:<60} {status_str}")
                
                # Summary line with duplicate detection statistics
                manual_count = len(manual_links_set)
                auto_count = len(custom_links_found) - manual_count
                print(f"\n    {Color.CYAN}Total: {len(custom_links_found)} custom links ({nbu_count} from NBU signoff, {root_count} from ROOT){Color.RESET}")
                print(f"    {Color.CYAN}Classification: {auto_count} auto-created, {manual_count} manual (duplicate targets){Color.RESET}")
                if latest_nbu_link:
                    print(f"    {Color.GREEN}Latest NBU release: {latest_nbu_link[0]} ({latest_nbu_link[1]}, {latest_nbu_link[2]}){Color.RESET}")
                
                # Note: Link data is available in the table above for script parsing
                # No separate output needed - run_agur_regression.sh parses the visual table
            else:
                print(f"    {Color.YELLOW}No custom links found in central release area{Color.RESET}")
            
            return manually_created_links
                
        except Exception as e:
            print(f"  {Color.YELLOW}Unable to check central block release area: {e}{Color.RESET}")
            return []
    
    def _parse_release_log_file(
        self, 
        log_file: str, 
        automatic_link_names: List[str], 
        attempt_num: int
    ) -> Optional[Dict[str, Any]]:
        """Parse a single release log file to extract all relevant information
        
        Args:
            log_file: Path to umake log file
            automatic_link_names: List of known automatic link names
            attempt_num: Sequential attempt number
            
        Returns:
            Dictionary with attempt details or None if parsing fails:
            - 'attempt_num': int - Sequential attempt number
            - 'timestamp': str - Release timestamp
            - 'date': str - Release date
            - 'command': str - Full umake command
            - 'flags': Dict[str, bool] - Release flags (Sta, Fcl, Pnr, etc.)
            - 'status': str - Release status (SUCCESS/FAILED)
            - 'custom_links': List[str] - Custom link names
            - 'automatic_links': List[str] - Automatic link names
            Returns None if log file cannot be parsed
        """
        attempt = {
            'log_file': log_file,
            'attempt_num': attempt_num,
            'timestamp': 'Unknown',
            'date': 'Unknown',
            'user': 'Unknown',
            'command': '',
            'flags': {},
            'status': 'UNKNOWN',
            'custom_links': [],
            'automatic_links': [],
            'failure_reason': None
        }
        
        try:
            # Extract timestamp and username from log filename (e.g., username.2025-10-08_15_32_33.umake.log)
            filename = os.path.basename(log_file)
            parts = filename.split('.')
            if len(parts) >= 2:
                attempt['user'] = parts[0]  # Username is the first part
                attempt['timestamp'] = parts[1].replace('_', ' ')
                attempt['date'] = parts[1].split('_')[0]  # Just the date part
            
            # Read the log file to extract command and status
            with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # Extract command line
            cmd_match = re.search(r'Command line:\s*(.+?)(?:\n|$)', content)
            if cmd_match:
                attempt['command'] = cmd_match.group(1).strip()
                attempt['flags'] = self._parse_release_flags(attempt['command'])
            
            # Check for failure
            if 'Block release FAILED. Removing partial release data' in content:
                attempt['status'] = 'FAILED'
                # Try to extract failure reason
                fail_match = re.search(r'Block release FAILED\.(.+?)(?:\n|$)', content)
                if fail_match:
                    attempt['failure_reason'] = fail_match.group(1).strip()
            elif 'Block release completed successfully' in content or 'Block release finished' in content or 'Block release step completed' in content:
                attempt['status'] = 'SUCCESS'
            # If we found linking info, it likely succeeded even without explicit success message
            elif content.count('{Linking') > 0:
                attempt['status'] = 'SUCCESS'
            
            # Extract linking information (format: "-I- {Linking link_name to: /path}")
            linking_pattern = r'\{Linking\s+(.+?)\s+to:'
            for match in re.finditer(linking_pattern, content):
                link_name = match.group(1).strip()
                
                # Determine if it's automatic or custom
                if any(auto_link in link_name for auto_link in automatic_link_names):
                    attempt['automatic_links'].append(link_name)
                else:
                    # Custom/pre-defined link
                    attempt['custom_links'].append(link_name)
            
            # Display this attempt
            status_color = Color.GREEN if attempt['status'] == 'SUCCESS' else (Color.RED if attempt['status'] == 'FAILED' else Color.YELLOW)
            print(f"\n  {Color.CYAN}Attempt #{attempt_num}:{Color.RESET} {attempt['timestamp']} | User: {attempt['user']}")
            print(f"    Status: {status_color}{attempt['status']}{Color.RESET}")
            
            # Show the actual command used
            if attempt['command']:
                print(f"    Command: {attempt['command']}")
            
            if attempt['flags']:
                print(f"    Flags: {', '.join(f'{k}={v}' if v != True else k for k, v in attempt['flags'].items())}")
            
            if attempt['failure_reason']:
                print(f"    {Color.RED}Failure: {attempt['failure_reason']}{Color.RESET}")
            
            if attempt['custom_links']:
                print(f"    {Color.GREEN}Custom Links: {', '.join(attempt['custom_links'])}{Color.RESET}")
            
            return attempt
            
        except Exception as e:
            print(f"  {Color.RED}Error parsing log {log_file}: {e}{Color.RESET}")
            return None
    
    def _parse_release_flags(self, command: str) -> Dict[str, bool]:
        """Parse umake block_release command flags
        
        Args:
            command: Full umake command line string
            
        Returns:
            Dictionary mapping flag names to boolean values:
            - 'Sta': bool - Static timing analysis release
            - 'Fcl': bool - Functional/Layout release
            - 'Pnr': bool - Place and route release
            - 'DC': bool - Design compiler release
            - 'FE_DCT': bool - Front-end DCT release
            - 'Full': bool - Full release (all components)
        """
        flags = {}
        
        # Flag definitions with their long forms
        flag_map = {
            '-b': 'block', '--block': 'block',
            '-r': 'release_name', '--rel_name': 'release_name',
            '-c': 'copy_only', '--copy_only': 'copy_only',
            '-n': 'no_clean', '--no_clean': 'no_clean',
            '-a': 'run_text', '--run_text': 'run_text',
            '-f': 'force', '--force': 'force',
            '-l': 'fcl_release', '--fcl_release': 'fcl_release',
            '-s': 'sta_release', '--sta_release': 'sta_release',
            '-p': 'pnr_release', '--pnr_release': 'pnr_release',
            '-z': 'full_release', '--full_release': 'full_release',
            '-x': 'ndm', '--ndm': 'ndm',
            '--dc_release': 'dc_release',
            '--fe_dct_release': 'fe_dct_release',
            '--preview': 'preview',
            '--hs': 'hs_copy'
        }
        
        # Flags that can take values (not boolean)
        flags_with_values = {'release_name', 'block'}
        
        # Simple flag parsing
        parts = command.split()
        i = 0
        while i < len(parts):
            part = parts[i]
            
            if part in flag_map:
                flag_name = flag_map[part]
                # Only check for value if this flag can take values
                if flag_name in flags_with_values and i + 1 < len(parts) and not parts[i + 1].startswith('-'):
                    flags[flag_name] = parts[i + 1]
                    i += 2
                else:
                    # Boolean flag - always True
                    flags[flag_name] = True
                    i += 1
            else:
                i += 1
        
        return flags
    
    def run_nv_gate_eco(self) -> None:
        """Run NV Gate ECO analysis"""
        self.print_header(FlowStage.NV_GATE_ECO)
        
        # Initialize tracking variables for dashboard
        total_eco_changes = 0
        inst_additions = 0
        inst_swaps = 0
        net_connections = 0
        cells_moved = 0
        drc_violations = None
        open_nets_status = "PASS"
        status = "NOT_RUN"
        
        # Determine the correct workarea path to use
        # If we're in an nbu_signoff subdirectory, navigate up to find the root workarea
        search_workarea = self.workarea
        if 'nbu_signoff' in self.workarea:
            # Navigate up to find signoff_flow at root level
            current = self.workarea
            for _ in range(10):  # Safety limit
                parent = os.path.dirname(current)
                if os.path.exists(os.path.join(parent, 'signoff_flow')):
                    search_workarea = parent
                    break
                current = parent
        
        # Show NV Gate ECO flow timeline
        nv_gate_eco_local_flow_dirs = [
            os.path.join(search_workarea, f"signoff_flow/nv_gate_eco/{self.design_info.top_hier}/local_flow"),
            os.path.join(search_workarea, f"signoff_flow/nv_gate_eco/local_flow")
        ]
        self._show_flow_timeline("NV Gate ECO", nv_gate_eco_local_flow_dirs)
        
        # Check if NV Gate ECO directory exists
        nv_gate_eco_dir = os.path.join(search_workarea, "signoff_flow/nv_gate_eco")
        
        if os.path.exists(nv_gate_eco_dir):
            # Look for ECO change summary
            eco_change_pattern = f"signoff_flow/nv_gate_eco/{self.design_info.top_hier}/ipo*/sum.eco_change"
            eco_change_files = self.file_utils.find_files(eco_change_pattern, search_workarea)
            
            if eco_change_files:
                # File path will be in HTML report, not terminal (to minimize console output)
                try:
                    # Read and extract the ECO change summary table
                    with open(eco_change_files[0], 'r') as f:
                        content = f.read()
                    
                    # Find the ECO summary table section
                    lines = content.split('\n')
                    in_table = False
                    table_lines = []
                    
                    for line in lines:
                        line = line.strip()
                        # Start of table
                        if 'OBJECT' in line and 'CHANGE' in line and 'COUNT' in line:
                            in_table = True
                            table_lines.append(line)
                            continue
                        # End of table
                        if in_table and line.startswith('---') and 'TOTAL' in line:
                            table_lines.append(line)
                            break
                        # Table content
                        if in_table and line and not line.startswith('---'):
                            table_lines.append(line)
                    
                    # Parse and format the ECO summary table
                    if table_lines:
                        eco_data = []
                        total_count = 0
                        
                        for line in table_lines:
                            if line.startswith('OBJECT') or line.startswith('---'):
                                continue  # Skip header and separator lines
                            
                            parts = line.split()
                            if len(parts) >= 2:
                                try:
                                    count = int(parts[-1])
                                    if count > 0:  # Only include non-zero changes
                                        if 'TOTAL' in line.upper():
                                            eco_data.append(('TOTAL', '', count))
                                        else:
                                            obj_type = parts[0]
                                            change_type = ' '.join(parts[1:-1])
                                            eco_data.append((obj_type, change_type, count))
                                except (ValueError, IndexError):
                                    pass
                        
                        if eco_data:
                            status = "PASS"
                            print(f"  {Color.CYAN}ECO Changes Summary:{Color.RESET}")
                            # Print formatted table header
                            print(f"    {'Object':<10} {'Change':<15} {'Count':>8}")
                            print(f"    {'-'*10} {'-'*15} {'-'*8}")
                            
                            # Print data rows and collect metrics for dashboard
                            for obj_type, change_type, count in eco_data:
                                if obj_type == 'TOTAL':
                                    total_eco_changes = count
                                    print(f"    {'-'*10} {'-'*15} {'-'*8}")
                                    print(f"    {'TOTAL':<10} {'':<15} {count:>8}")
                                else:
                                    print(f"    {obj_type:<10} {change_type:<15} {count:>8}")
                                    # Track specific metrics for dashboard
                                    if obj_type == 'INST' and 'ADDITION' in change_type:
                                        inst_additions = count
                                    elif obj_type == 'INST' and 'SWAP' in change_type:
                                        inst_swaps += count
                                    elif obj_type == 'NET' and 'CONNECTION' in change_type:
                                        net_connections = count
                    else:
                        print(f"  {Color.YELLOW}No ECO changes found{Color.RESET}")
                except Exception as e:
                    print(f"  Error reading ECO change summary: {e}")
            
            # Look for timing reports
            # Note: Use wildcard for IPO as directory may be ipo1000 but files use ipo1400 (actual implementation IPO)
            setup_pattern = f"signoff_flow/nv_gate_eco/{self.design_info.top_hier}/ipo*/REPs/SUMMARY/{self.design_info.top_hier}.ipo*.eco.timing.setup.rpt.gz"
            hold_pattern = f"signoff_flow/nv_gate_eco/{self.design_info.top_hier}/ipo*/REPs/SUMMARY/{self.design_info.top_hier}.ipo*.eco.timing.hold.rpt.gz"
            
            setup_files = self.file_utils.find_files(setup_pattern, search_workarea)
            hold_files = self.file_utils.find_files(hold_pattern, search_workarea)
            
            # Process setup timing report - Extract WNS/TNS/NVP with corner info (external and internal)
            setup_ext_wns = setup_ext_tns = setup_ext_nvp = None
            setup_int_wns = setup_int_tns = setup_int_nvp = None
            setup_corner = None
            setup_file_path = None
            if setup_files:
                setup_file_path = setup_files[0]
                try:
                    import re
                    # Extract both external and internal timing metrics
                    result = self.file_utils.run_command(f"zcat {setup_files[0]} | grep -E '^\s*(external|internal)' | head -2")
                    if result.strip():
                        lines = result.strip().split('\n')
                        for line in lines:
                            parts = line.split('|')
                            if len(parts) >= 4:
                                try:
                                    line_type = parts[0].strip()
                                    wns = float(parts[1].strip())
                                    tns = float(parts[2].strip())
                                    nvp = int(parts[3].strip())
                                    
                                    if line_type == 'external':
                                        setup_ext_wns = wns
                                        setup_ext_tns = tns
                                        setup_ext_nvp = nvp
                                    elif line_type == 'internal':
                                        setup_int_wns = wns
                                        setup_int_tns = tns
                                        setup_int_nvp = nvp
                                except (ValueError, IndexError):
                                    pass
                    
                    # Find full corner/scenario name (use external WNS as it's typically worse)
                    if setup_ext_wns is not None:
                        wns_str = f"{setup_ext_wns:.3f}"
                        wns_search = wns_str.replace('-', '\\-')
                        corner_result = self.file_utils.run_command(f"zcat {setup_files[0]} | grep -E 'func\\.std.*setup\\.typical' | grep '{wns_search}' | head -1")
                        if corner_result.strip():
                            # Extract full scenario name
                            corner_match = re.search(r'(func\.std_tt_\d+c_[\d.]+p[\d.]+v\.setup\.typical)', corner_result)
                            if corner_match:
                                setup_corner = corner_match.group(1)
                except Exception as e:
                    pass  # Silently continue if extraction fails
            
            # Extract sub_category breakdown for setup timing
            setup_subcats = {}
            if setup_files:
                try:
                    import re
                    # Get sub_category lines (without per-scenario breakdown)
                    result = self.file_utils.run_command(f"zcat {setup_files[0]} | grep -E 'port_to_flop|port_to_clock_gate|flop_to_port|flop_to_flop|flop_to_hard_macro|flop_to_clock_gate' | grep -v 'func\\.std'")
                    if result.strip():
                        lines = result.strip().split('\n')
                        for line in lines:
                            # Skip comment/description lines
                            if 'flop_to_ram' in line or 'port_to_ilm' in line:
                                continue
                            parts = line.split('|')
                            if len(parts) >= 5:
                                try:
                                    subcat = parts[2].strip()
                                    wns = float(parts[3].strip())
                                    tns = float(parts[4].strip())
                                    fep = int(parts[5].strip().split()[0])  # First number after | is FEP
                                    
                                    if subcat and (wns != 0 or tns != 0 or fep != 0):  # Only store non-zero entries
                                        setup_subcats[subcat] = {
                                            'wns': wns,
                                            'tns': tns,
                                            'fep': fep
                                        }
                                except (ValueError, IndexError):
                                    pass
                except Exception:
                    pass
            
            # Process hold timing report - Extract WNS/TNS/NVP with corner info (external and internal)
            hold_ext_wns = hold_ext_tns = hold_ext_nvp = None
            hold_int_wns = hold_int_tns = hold_int_nvp = None
            hold_corner = None
            hold_file_path = None
            if hold_files:
                hold_file_path = hold_files[0]
                try:
                    import re
                    # Extract both external and internal timing metrics
                    result = self.file_utils.run_command(f"zcat {hold_files[0]} | grep -E '^\s*(external|internal)' | head -2")
                    if result.strip():
                        lines = result.strip().split('\n')
                        for line in lines:
                            parts = line.split('|')
                            if len(parts) >= 4:
                                try:
                                    line_type = parts[0].strip()
                                    wns = float(parts[1].strip())
                                    tns = float(parts[2].strip())
                                    nvp = int(parts[3].strip())
                                    
                                    if line_type == 'external':
                                        hold_ext_wns = wns
                                        hold_ext_tns = tns
                                        hold_ext_nvp = nvp
                                    elif line_type == 'internal':
                                        hold_int_wns = wns
                                        hold_int_tns = tns
                                        hold_int_nvp = nvp
                                except (ValueError, IndexError):
                                    pass
                    
                    # Find full corner/scenario name
                    if hold_ext_wns is not None:
                        # If values are 0, it means all corners pass
                        if hold_ext_wns == 0.0 and hold_ext_tns == 0.0 and hold_int_wns == 0.0 and hold_int_tns == 0.0:
                            hold_corner = "ALL (PASS)"  # All corners pass
                        else:
                            wns_str = f"{hold_ext_wns:.3f}"
                            wns_search = wns_str.replace('-', '\\-')
                            corner_result = self.file_utils.run_command(f"zcat {hold_files[0]} | grep -E 'func\\.std.*hold\\.typical' | grep '{wns_search}' | head -1")
                            if corner_result.strip():
                                corner_match = re.search(r'(func\.std_tt_\d+c_[\d.]+p[\d.]+v\.hold\.typical)', corner_result)
                                if corner_match:
                                    hold_corner = corner_match.group(1)
                except Exception as e:
                    pass  # Silently continue if extraction fails
            
            # Extract sub_category breakdown for hold timing
            hold_subcats = {}
            if hold_files:
                try:
                    import re
                    # Get sub_category lines (without per-scenario breakdown)
                    result = self.file_utils.run_command(f"zcat {hold_files[0]} | grep -E 'port_to_flop|port_to_clock_gate|flop_to_port|flop_to_flop|flop_to_hard_macro|flop_to_clock_gate' | grep -v 'func\\.std'")
                    if result.strip():
                        lines = result.strip().split('\n')
                        for line in lines:
                            # Skip comment/description lines
                            if 'flop_to_ram' in line or 'port_to_ilm' in line:
                                continue
                            parts = line.split('|')
                            if len(parts) >= 5:
                                try:
                                    subcat = parts[2].strip()
                                    wns = float(parts[3].strip())
                                    tns = float(parts[4].strip())
                                    fep = int(parts[5].strip().split()[0])  # First number after | is FEP
                                    
                                    if subcat and (wns != 0 or tns != 0 or fep != 0):  # Only store non-zero entries
                                        hold_subcats[subcat] = {
                                            'wns': wns,
                                            'tns': tns,
                                            'fep': fep
                                        }
                                except (ValueError, IndexError):
                                    pass
                except Exception:
                    pass
            
            # Extract cell count and utilization from .data file
            cell_count = None
            effective_util = None
            if setup_corner:
                # Look for .data file matching the corner
                data_pattern = f"signoff_flow/nv_gate_eco/{self.design_info.top_hier}/ipo*/reports/*.{setup_corner}.data"
                data_files = self.file_utils.find_files(data_pattern, search_workarea)
                if data_files:
                    try:
                        import re
                        # Extract CellCount
                        result = self.file_utils.run_command(f"grep '^CellCount' {data_files[0]} | head -1")
                        if result.strip():
                            match = re.search(r'CellCount\s*=\s*(\d+)', result)
                            if match:
                                cell_count = int(match.group(1))
                        
                        # Extract EffictiveUtilization (note: typo in file - "Effictive" not "Effective")
                        result = self.file_utils.run_command(f"grep '^EffictiveUtilization' {data_files[0]} | head -1")
                        if result.strip():
                            match = re.search(r'EffictiveUtilization\s*=\s*([\d.]+)%?', result)
                            if match:
                                effective_util = float(match.group(1))
                    except Exception:
                        pass
            
            # Display clean timing summary with external/internal breakdown
            if setup_files or hold_files:
                print(f"  {Color.CYAN}ECO Timing Summary:{Color.RESET}")
                
                # Display corner name
                if setup_corner:
                    print(f"    Corner: {setup_corner}")
                if setup_file_path:
                    print(f"    File:   {os.path.abspath(setup_file_path)}")
                
                # Setup timing - External
                if setup_ext_wns is not None:
                    wns_color = Color.GREEN if setup_ext_wns >= 0 else Color.RED
                    tns_color = Color.GREEN if setup_ext_tns >= 0 else Color.RED
                    print(f"    Setup External:  WNS = {wns_color}{setup_ext_wns:>7.3f}ns{Color.RESET} | TNS = {tns_color}{setup_ext_tns:>10.3f}ns{Color.RESET} | NVP = {setup_ext_nvp:>6}")
                
                # Setup timing - Internal
                if setup_int_wns is not None:
                    wns_color = Color.GREEN if setup_int_wns >= 0 else Color.RED
                    tns_color = Color.GREEN if setup_int_tns >= 0 else Color.RED
                    print(f"    Setup Internal:  WNS = {wns_color}{setup_int_wns:>7.3f}ns{Color.RESET} | TNS = {tns_color}{setup_int_tns:>10.3f}ns{Color.RESET} | NVP = {setup_int_nvp:>6}")
                
                # Setup sub_category breakdown
                if setup_subcats:
                    print(f"    {Color.CYAN}Setup Sub-Categories:{Color.RESET}")
                    # Define display order
                    subcat_order = ['port_to_flop', 'port_to_clock_gate', 'flop_to_port', 'flop_to_flop', 'flop_to_hard_macro', 'flop_to_clock_gate']
                    for subcat in subcat_order:
                        if subcat in setup_subcats:
                            data = setup_subcats[subcat]
                            wns_color = Color.GREEN if data['wns'] >= 0 else Color.RED
                            tns_color = Color.GREEN if data['tns'] >= 0 else Color.RED
                            print(f"      {subcat:20s}:  WNS = {wns_color}{data['wns']:>7.3f}ns{Color.RESET} | TNS = {tns_color}{data['tns']:>10.3f}ns{Color.RESET} | FEP = {data['fep']:>6}")
                
                # Hold timing section
                if hold_files:
                    if hold_corner:
                        print(f"    Corner: {hold_corner}")
                    if hold_file_path:
                        print(f"    File:   {os.path.abspath(hold_file_path)}")
                    
                    # Hold timing - External
                    if hold_ext_wns is not None:
                        wns_color = Color.GREEN if hold_ext_wns >= 0 else Color.RED
                        tns_color = Color.GREEN if hold_ext_tns >= 0 else Color.RED
                        print(f"    Hold External:   WNS = {wns_color}{hold_ext_wns:>7.3f}ns{Color.RESET} | TNS = {tns_color}{hold_ext_tns:>10.3f}ns{Color.RESET} | NVP = {hold_ext_nvp:>6}")
                    
                    # Hold timing - Internal
                    if hold_int_wns is not None:
                        wns_color = Color.GREEN if hold_int_wns >= 0 else Color.RED
                        tns_color = Color.GREEN if hold_int_tns >= 0 else Color.RED
                        print(f"    Hold Internal:   WNS = {wns_color}{hold_int_wns:>7.3f}ns{Color.RESET} | TNS = {tns_color}{hold_int_tns:>10.3f}ns{Color.RESET} | NVP = {hold_int_nvp:>6}")
                    
                    # Hold sub_category breakdown
                    if hold_subcats:
                        print(f"    {Color.CYAN}Hold Sub-Categories:{Color.RESET}")
                        # Define display order
                        subcat_order = ['port_to_flop', 'port_to_clock_gate', 'flop_to_port', 'flop_to_flop', 'flop_to_hard_macro', 'flop_to_clock_gate']
                        for subcat in subcat_order:
                            if subcat in hold_subcats:
                                data = hold_subcats[subcat]
                                wns_color = Color.GREEN if data['wns'] >= 0 else Color.RED
                                tns_color = Color.GREEN if data['tns'] >= 0 else Color.RED
                                print(f"      {subcat:20s}:  WNS = {wns_color}{data['wns']:>7.3f}ns{Color.RESET} | TNS = {tns_color}{data['tns']:>10.3f}ns{Color.RESET} | FEP = {data['fep']:>6}")
                
                # Cell count and utilization
                if cell_count is not None:
                    print(f"    CellCount:              {cell_count:,}")
                if effective_util is not None:
                    print(f"    EffectiveUtilization:   {effective_util:.2f}%")
            
            # Look for ECO summary reports (critical validation) - Extract key metrics only
            summary_pattern = f"signoff_flow/nv_gate_eco/{self.design_info.top_hier}/ipo*/REPs/{self.design_info.top_hier}.ipo*.eco_summary_*.rpt.gz"
            summary_files = self.file_utils.find_files(summary_pattern, search_workarea)
            
            open_nets_count = None
            total_net_count = None
            
            for summary_file in sorted(summary_files):
                filename = os.path.basename(summary_file)
                
                # Critical: Open nets check (any open net = FAIL)
                if 'open_nets' in filename:
                    try:
                        result = self.file_utils.run_command(f"zcat {summary_file} | grep 'open_nets_left:' | head -1")
                        if result.strip():
                            import re
                            open_count_match = re.search(r'open_nets_left:\s*(\d+)', result)
                            if open_count_match:
                                open_nets_count = int(open_count_match.group(1))
                                if open_nets_count > 0:
                                    open_nets_status = "FAIL"
                    except Exception:
                        pass
                
                # Cell movement summary - extract only the count
                elif 'cell_movement' in filename and 'eco_in' not in filename:
                    try:
                        result = self.file_utils.run_command(f"zcat {summary_file} | grep 'number_of_cells_moved:' | head -1")
                        if result.strip():
                            import re
                            match = re.search(r'number_of_cells_moved:\s*(\d+)', result)
                            if match:
                                cells_moved = int(match.group(1))
                    except Exception:
                        pass
                
                # DRC data summary - extract violation count
                elif 'drc_data' in filename:
                    try:
                        # Look for total DRC violations or "No DRC" indicator
                        result = self.file_utils.run_command(f"zcat {summary_file} | head -20")
                        if result.strip():
                            if 'No DRC' in result or 'no drc' in result.lower():
                                drc_violations = 0
                            else:
                                import re
                                # Try to find violation count
                                match = re.search(r'total.*?(\d+)', result, re.IGNORECASE)
                                if match:
                                    drc_violations = int(match.group(1))
                    except Exception:
                        pass
                
                # Wire length change - extract only net count
                elif 'wire_length' in filename:
                    try:
                        result = self.file_utils.run_command(f"zcat {summary_file} | grep 'total_net_count:' | head -1")
                        if result.strip():
                            import re
                            match = re.search(r'total_net_count:\s*(\d+)', result)
                            if match:
                                total_net_count = int(match.group(1))
                    except Exception:
                        pass
            
            # Display clean validation summary
            print(f"  {Color.CYAN}ECO Validation Summary:{Color.RESET}")
            
            # Open nets (CRITICAL)
            if open_nets_count is not None:
                if open_nets_count == 0:
                    print(f"    Open Nets:      {Color.GREEN}0 (PASS){Color.RESET}")
                else:
                    print(f"    Open Nets:      {Color.RED}{open_nets_count} (FAIL){Color.RESET}")
            
            # Cells moved
            if cells_moved > 0:
                print(f"    Cells Moved:    {cells_moved:,}")
            
            # DRC violations
            if drc_violations is not None:
                if drc_violations == 0:
                    print(f"    DRC Violations: {Color.GREEN}0{Color.RESET}")
                else:
                    print(f"    DRC Violations: {Color.RED}{drc_violations}{Color.RESET}")
            
            # Total nets
            if total_net_count is not None:
                print(f"    Total Nets:     {total_net_count:,}")
            
            # Trace reports and worst paths - just count them
            trace_pattern = f"signoff_flow/nv_gate_eco*/{self.design_info.top_hier}/ipo*/reports/*.traces.rpt"
            worst_paths_pattern = f"signoff_flow/nv_gate_eco*/{self.design_info.top_hier}/ipo*/reports/*.worst_paths"
            
            trace_files = self.file_utils.find_files(trace_pattern, search_workarea)
            worst_paths_files = self.file_utils.find_files(worst_paths_pattern, search_workarea)
            
            if trace_files or worst_paths_files:
                print(f"    Reports:        {len(trace_files)} trace(s), {len(worst_paths_files)} worst_path(s)")
        else:
            print("  Didn't run NV Gate ECO")
        
        # Build key metrics for dashboard
        key_metrics = {"Design": self.design_info.top_hier}
        if total_eco_changes > 0:
            key_metrics["Total Changes"] = str(total_eco_changes)
            if inst_additions > 0:
                key_metrics["Inst Additions"] = str(inst_additions)
            if inst_swaps > 0:
                key_metrics["Inst Swaps"] = str(inst_swaps)
            if net_connections > 0:
                key_metrics["Net Connections"] = str(net_connections)
        
        # Add new summary report metrics
        if cells_moved > 0:
            key_metrics["Cells Moved"] = str(cells_moved)
        if drc_violations is not None:
            key_metrics["DRC Violations"] = str(drc_violations)
        if open_nets_status != "PASS":
            key_metrics["Open Nets"] = "FAIL"
            status = "FAIL"  # Override status if open nets detected
        
        # Add section summary for master dashboard
        self._add_section_summary(
            section_name="NV Gate ECO",
            section_id="nv-gate-eco",
            stage=FlowStage.NV_GATE_ECO,
            status=status,
            key_metrics=key_metrics,
            html_file="",
            priority=3,
            issues=[],
            icon="[NV-ECO]"
        )
    
    def run_block_release(self) -> None:
        """Run comprehensive block release analysis with NBU signoff multi-location support"""
        self.print_header(FlowStage.BLOCK_RELEASE)
        
        # Initialize tracking variables
        release_data = None
        release_to_path = None
        overall_status = "NOT_RUN"
        key_metrics = {"Design": self.design_info.top_hier}
        issues = []
        
        # Discover all block_release locations (root + nbu_signoff)
        all_locations = self._discover_block_release_locations()
        
        if all_locations:
            # Compact display header
            mode_indicator = " (NBU Signoff Mode)" if len(all_locations) > 1 and any(loc[2] for loc in all_locations) else ""
            print(f"\n{Color.CYAN}Block Release{mode_indicator} - {len(all_locations)} location(s) detected:{Color.RESET}")
            
            # Select the "real" block_release first (for highlighting)
            selected_path, selected_ipo, selected_is_nbu, selected_meta, selection_reason = \
                self._select_real_block_release(all_locations)
            
            # Display all locations compactly
            for idx, (path, ipo, is_nbu, metadata) in enumerate(all_locations, 1):
                location_type = f"NBU ({ipo:>7})" if is_nbu else "ROOT       "
                user_display = metadata['user']
                date_display = metadata['date_str']
                
                # Determine status badge
                is_selected = (path == selected_path)
                if metadata['user'] == self.workarea_owner:
                    status_badge = f"{Color.GREEN}[ACTIVE-owner]{Color.RESET}"
                else:
                    status_badge = f"{Color.YELLOW}[STALE-old owner]{Color.RESET}"
                
                # Add SELECTED marker
                selected_marker = f" {Color.CYAN}[SELECTED]{Color.RESET}" if is_selected else ""
                
                # Single-line compact format
                print(f"  {idx}. {location_type} | USER: {user_display:>12} {status_badge} | {date_display}{selected_marker}")
            
            # Show selection reason
            if selected_path:
                print(f"  {Color.GREEN}-> Reason:{Color.RESET} {Color.CYAN}{selection_reason}{Color.RESET}")
                
                # Determine search path: NBU base path or workarea root
                search_path = selected_is_nbu and selected_ipo in self.nbu_signoff_paths
                if search_path:
                    umake_search_base = self.nbu_signoff_paths[selected_ipo]
                else:
                    umake_search_base = self.workarea
                
                # Extract data from selected location
                release_log = os.path.join(selected_path, "log/block_release.log")
                if self.file_utils.file_exists(release_log):
                    self.print_file_info(release_log, "\nBlock Release Log")
                    # Pass the correct search base path to extract from the right location
                    release_data, release_to_path = self._extract_block_release_info(release_log, umake_search_base)
                    
                    # If no attempts found in log, try umake logs from selected location
                    if not release_data or release_data.get('total_attempts', 0) == 0:
                        umake_data = self._extract_umake_block_release_commands(umake_search_base)
                        if umake_data and umake_data.get('total_attempts', 0) > 0:
                            release_data = umake_data
                else:
                    print(f"  {Color.YELLOW}No block_release.log found in selected location{Color.RESET}")
                    # Try to extract from umake logs in selected location (umake_search_base already determined above)
                    release_data = self._extract_umake_block_release_commands(umake_search_base)
        else:
            print(f"  {Color.YELLOW}No block_release locations found{Color.RESET}")
            # Still try to extract from umake logs in workarea root
            release_data = self._extract_umake_block_release_commands(self.workarea)
        
        # Process release data
        if release_data:
            overall_status = release_data.get('overall_status', 'NOT_RUN')
            
            # Build key metrics
            key_metrics["Total Attempts"] = str(release_data['total_attempts'])
            key_metrics["Successful"] = str(release_data['successful_attempts'])
            key_metrics["Failed"] = str(release_data['failed_attempts'])
            
            if release_data['custom_links']:
                # Sort custom links by date (oldest first)
                sorted_links = sorted(
                    release_data['custom_links'],
                    key=lambda link: release_data['custom_links_with_dates'].get(link, ''),
                    reverse=False
                )
                key_metrics["Custom Links"] = ', '.join(sorted_links[:3])
                if len(sorted_links) > 3:
                    key_metrics["Custom Links"] += f" (+{len(sorted_links) - 3} more)"
            
            # Get latest successful release info
            successful_attempts = [a for a in release_data['attempts'] if a['status'] == 'SUCCESS']
            if successful_attempts:
                latest = successful_attempts[-1]
                key_metrics["Latest Success"] = latest['date']
                if latest['flags'].get('release_name'):
                    key_metrics["Release Name"] = latest['flags']['release_name']
            
            # Track issues
            if release_data['failed_attempts'] > 0:
                issues.append(f"{release_data['failed_attempts']} failed release attempt(s)")
            
            # Add failure reasons
            for attempt in release_data['attempts']:
                if attempt['status'] == 'FAILED' and attempt.get('failure_reason'):
                    issues.append(f"Failure: {attempt['failure_reason']}")
        
        # Check central block release area for all custom links (if we have the path)
        manually_created_links = []
        if release_to_path and release_data:
            automatic_link_names = [
                'fcl_release', 'sta_release', 'pnr_release', 'dc_release',
                'fe_dct_release', 'fe_dct_golden_release', 'full_release',
                'last_sta_rel', 'last_fcl_rel', 'last_pnr_rel', 'last_dc_rel',
                'release', 'last_release'
            ]
            # Pass umake custom links for comparison to detect manually created links
            umake_custom_links = release_data.get('custom_links', [])
            manually_created_links = self._check_central_block_release_links(
                release_to_path, automatic_link_names, umake_custom_links
            ) or []
            
            # Add manually created links to key metrics if present
            if manually_created_links:
                manual_link_names = [link[0] for link in manually_created_links]
                key_metrics["Manual Links"] = ', '.join(manual_link_names[:3])
                if len(manual_link_names) > 3:
                    key_metrics["Manual Links"] += f" (+{len(manual_link_names) - 3} more)"
        
        # Generate HTML report
        html_path = ""
        if release_data:
            umake_custom_links = release_data.get('custom_links', [])
            # Pass location information to HTML generator
            html_path = self._generate_block_release_html_report(
                release_data, 
                release_to_path, 
                umake_custom_links, 
                all_locations=all_locations if all_locations else None
            )
            if html_path:
                html_filename = os.path.basename(html_path)
                # Determine display path
                html_output_dir = self._get_html_output_dir()
                display_path = os.path.relpath(html_output_dir, os.getcwd())
                
                print(f"\n  {Color.CYAN}Block Release HTML Report:{Color.RESET}")
                print(f"  Open with: /home/utils/firefox-118.0.1/firefox {Color.MAGENTA}{display_path}/{html_filename}{Color.RESET} &")
        
        # Add section summary for master dashboard
        self._add_section_summary(
            section_name="Block Release",
            section_id="block-release",
            stage=FlowStage.BLOCK_RELEASE,
            status=overall_status,
            key_metrics=key_metrics,
            html_file=html_path,
            priority=1 if overall_status == "FAIL" else (3 if overall_status == "PASS" else 4),
            issues=issues,
            icon="[Release]"
        )
    
    # ============================================================================
    # IPO Comparison Mode (Standalone)
    # ============================================================================
    
    def analyze_ipo_comparison(self, email: Optional[str] = None) -> None:
        """
        Main entry point for IPO comparison mode
        
        Compares multiple IPO directories in NBU signoff structure
        Generates consolidated Excel report with metrics comparison
        
        Args:
            email: Email address(es) to send report to (comma-separated)
        """
        # Print header
        print("=" * 80)
        print(f"{Color.CYAN}{' ' * 22}IPO COMPARISON MODE{Color.RESET}")
        print("=" * 80)
        print()
        
        # Validate NBU signoff mode
        if not self.nbu_signoff_paths:
            print(f"{Color.RED}[ERROR] No NBU signoff IPOs detected{Color.RESET}")
            print("  IPO comparison requires NBU signoff structure")
            print(f"  Expected: pnr_flow/nv_flow/<design>/<ipo>/nbu_signoff/")
            return
        
        if len(self.nbu_signoff_paths) < 2:
            print(f"{Color.YELLOW}[WARN] Only 1 IPO detected - comparison requires 2+ IPOs{Color.RESET}")
            print(f"  Detected IPO: {list(self.nbu_signoff_paths.keys())[0]}")
            return
        
        # Display detected IPOs
        print(f"Workarea: {self.workarea}")
        print(f"Design: {self.design_info.top_hier}")
        print(f"Detected {len(self.nbu_signoff_paths)} IPOs with NBU signoff:")
        for ipo in sorted(self.nbu_signoff_paths.keys()):
            print(f"  - {ipo}")
        print()
        
        # Extract data
        print("Extracting data from latest completed runs...")
        try:
            ipo_data = self._extract_all_ipo_data()
            print(f"{Color.GREEN}[OK] Cell counts extracted{Color.RESET}")
            print(f"{Color.GREEN}[OK] Max clock latencies extracted{Color.RESET}")
            print(f"{Color.GREEN}[OK] PT setup timing extracted (func.std_tt_0c_0p6v.setup.typical){Color.RESET}")
            print(f"{Color.GREEN}[OK] PT hold timing extracted (func.std_tt_0c_0p55v.hold.typical){Color.RESET}")
            print(f"{Color.GREEN}[OK] PV metrics extracted{Color.RESET}")
            print(f"{Color.GREEN}[OK] External timing extracted (Innovus nv_gate_eco){Color.RESET}")
            print(f"{Color.GREEN}[OK] Power analysis extracted (func.std_tt_105c_0p67v.setup.typical){Color.RESET}")
            print(f"{Color.GREEN}[OK] Formal verification status extracted{Color.RESET}")
        except Exception as e:
            print(f"{Color.RED}[ERROR] Failed to extract IPO data: {e}{Color.RESET}")
            import traceback
            traceback.print_exc()
            return
        
        # Generate Excel
        print()
        print("-" * 80)
        print(f"{' ' * 25}IPO COMPARISON SUMMARY")
        print("-" * 80)
        print()
        
        # Print summary first (even if Excel generation fails)
        try:
            self._print_ipo_comparison_summary(ipo_data)
        except Exception as e:
            print(f"{Color.YELLOW}[WARN] Failed to print summary: {e}{Color.RESET}")
        
        # Generate Excel report
        try:
            excel_file = self._generate_ipo_comparison_excel(ipo_data)
            
            # Display Excel path
            print()
            print("-" * 80)
            print()
            print(f"{Color.GREEN}[OK] Excel report generated:{Color.RESET}")
            print(f"  File: {excel_file}")
            print()
            print(f"  {Color.CYAN}Excel contains:{Color.RESET}")
            print(f"    Sheet 1: IPO Comparison - Consolidated table with all metrics")
            print(f"    Sheet 2: Flow Run History - Complete timeline of all runs")
            print(f"    Sheet 3: Notes & Metadata - Documentation and thresholds")
            print()
            
            # Send email if requested
            if email:
                try:
                    self._send_ipo_comparison_email(excel_file, email)
                except Exception as e:
                    print(f"{Color.YELLOW}[WARN] Failed to send email: {e}{Color.RESET}")
        except Exception as e:
            print(f"\n{Color.RED}[ERROR] Failed to generate Excel report: {e}{Color.RESET}")
            if 'openpyxl' in str(e):
                print(f"{Color.YELLOW}  Install with: pip install openpyxl{Color.RESET}")
        
        print()
        print("=" * 80)
        print(f"{' ' * 22}IPO COMPARISON COMPLETE")
        print("=" * 80)
    
    def _extract_all_ipo_data(self) -> dict:
        """
        Extract all data for all IPOs
        
        Returns:
            dict: Complete data structure for all IPOs
        """
        data = {}
        for ipo_name in sorted(self.nbu_signoff_paths.keys()):
            print(f"  Processing {ipo_name}...")
            data[ipo_name] = {
                "cell_count": self._extract_ipo_cell_count(ipo_name),
                "utilization": self._extract_ipo_utilization(ipo_name),
                "clock_latencies": self._extract_max_clock_latency(ipo_name),  # Dict: {clock_name: latency_ps}
                "pt_setup": self._extract_ipo_pt_timing(ipo_name, "setup"),
                "pt_hold": self._extract_ipo_pt_timing(ipo_name, "hold"),
                "pv": self._extract_ipo_pv_metrics(ipo_name),
                "external_timing": self._extract_external_timing_innovus(ipo_name),
                "power": self._extract_ipo_power_data(ipo_name),  # Power analysis data
                "formal": self._extract_formal_status(ipo_name),
                "flow_history": self._extract_flow_history(ipo_name)
            }
        return data
    
    def _extract_ipo_cell_count(self, ipo: str) -> Optional[int]:
        """
        Extract total cell count for IPO
        
        Source: pnr_flow/nv_flow/{design}/{ipo}/reports/{design}_{design}_{ipo}_report_{design}_{ipo}_postroute.func.std_tt_0c_0p6v.setup.typical.data
        Pattern: CellCount = <value>
        
        This uses the same source as PnR Analysis section for consistency and accuracy.
        
        Args:
            ipo: IPO name
            
        Returns:
            int: Cell count or None if not found
        """
        try:
            design = self.design_info.top_hier
            
            # Build path to .data file (same as PnR section)
            # Extract base IPO name without suffix (e.g., ipo1000 from ipo1000_ref)
            ipo_base = ipo.split('_')[0]
            
            data_file = os.path.join(
                self.workarea,
                f"pnr_flow/nv_flow/{design}/{ipo}/reports/{design}_{design}_{ipo_base}_report_{design}_{ipo_base}_postroute.func.std_tt_0c_0p6v.setup.typical.data"
            )
            
            if not os.path.exists(data_file):
                return None
            
            # Parse .data file for CellCount parameter
            with open(data_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line.startswith('CellCount = '):
                        value_str = line.split(' = ', 1)[1]
                        try:
                            return int(value_str)
                        except ValueError:
                            pass
            
            return None
        except Exception as e:
            print(f"    {Color.YELLOW}[WARN] Failed to extract cell count for {ipo}: {e}{Color.RESET}")
            return None
    
    def _extract_ipo_utilization(self, ipo: str) -> Optional[float]:
        """
        Extract effective utilization for IPO
        
        Source: Same .data file as cell count
        Pattern: EffictiveUtilization = <value>
        
        Note: The parameter name has a typo in the source file - "Effictive" not "Effective"
        
        Args:
            ipo: IPO name
            
        Returns:
            float: Utilization percentage or None if not found
        """
        try:
            design = self.design_info.top_hier
            ipo_base = ipo.split('_')[0]
            
            data_file = os.path.join(
                self.workarea,
                f"pnr_flow/nv_flow/{design}/{ipo}/reports/{design}_{design}_{ipo_base}_report_{design}_{ipo_base}_postroute.func.std_tt_0c_0p6v.setup.typical.data"
            )
            
            if not os.path.exists(data_file):
                return None
            
            # Parse .data file for EffictiveUtilization parameter
            with open(data_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line.startswith('EffictiveUtilization = '):
                        value_str = line.split(' = ', 1)[1]
                        # Remove % sign if present
                        value_str = value_str.rstrip('%').strip()
                        try:
                            return float(value_str)
                        except ValueError:
                            pass
            
            return None
        except Exception as e:
            print(f"    {Color.YELLOW}[WARN] Failed to extract utilization for {ipo}: {e}{Color.RESET}")
            return None
    
    def _extract_max_clock_latency(self, ipo: str) -> dict:
        """
        Extract per-clock maximum latency from PT clock latency report
        
        Source: {nbu_signoff}/signoff_flow/auto_pt/last_work/func.std_tt_0c_0p6v.setup.typical/reports/timing_reports/{design}_func.std_tt_0c_0p6v.setup.typical.clock_latency
        
        This uses the same source as Clock Analysis section - PT reports are more accurate than PnR reports.
        
        Args:
            ipo: IPO name
            
        Returns:
            dict: Per-clock latencies in picoseconds {clock_name: max_latency_ps} or empty dict if not found
        """
        try:
            design = self.design_info.top_hier
            
            # Get NBU signoff path for this IPO
            nbu_path = self.nbu_signoff_paths.get(ipo)
            if not nbu_path:
                return {}
            
            # Build path to PT clock latency report (same as Clock Analysis section)
            pt_clock_file = os.path.join(
                nbu_path,
                f"signoff_flow/auto_pt/last_work/func.std_tt_0c_0p6v.setup.typical/reports/timing_reports/{design}_func.std_tt_0c_0p6v.setup.typical.clock_latency"
            )
            
            if not os.path.exists(pt_clock_file):
                return {}
            
            # Parse PT clock latency report using same logic as _extract_pt_clock_latency
            clock_latencies = {}  # {clock_name: [latencies...]}
            current_clock = None
            
            with open(pt_clock_file, 'r', encoding='utf-8') as f:
                for line in f:
                    # Check for clock section header
                    if line.strip().startswith("Clock:"):
                        current_clock = line.split("Clock:")[1].strip()
                        if current_clock not in clock_latencies:
                            clock_latencies[current_clock] = []
                        continue
                    
                    # Check for total clock latency line
                    if "total clock latency" in line and current_clock:
                        # Extract the latency value (last number in the line)
                        parts = line.split()
                        if parts:
                            try:
                                latency_ns = float(parts[-1])
                                clock_latencies[current_clock].append(latency_ns)
                            except (ValueError, IndexError):
                                continue
            
            # Convert to max latency per clock in picoseconds
            clock_max_latencies = {}
            for clock, latencies in clock_latencies.items():
                if latencies:
                    max_latency_ns = max(latencies)
                    clock_max_latencies[clock] = max_latency_ns * 1000.0  # Convert ns to ps
            
            return clock_max_latencies
            
        except Exception as e:
            print(f"    {Color.YELLOW}[WARN] Failed to extract clock latency for {ipo}: {e}{Color.RESET}")
            return {}
    
    def _extract_ipo_power_data(self, ipo: str) -> dict:
        """
        Extract power analysis data with dynamic test selection - Production Ready
        
        Automatically discovers available tests and selects based on Activity Factor (FF/Q):
        - Test 1: Highest Activity Factor FF/Q (most switching activity)
        - Test 2: Lowest Activity Factor FF/Q (least switching activity)
        
        Source: {nbu_signoff}/signoff_flow/auto_pt/<latest_work>/func.std_tt_105c_0p67v.setup.typical/
                reports/power_reports/func.std_tt_105c_0p67v.setup.typical/power_analysis/
                power_RTLSim_<TestName>.fsdb/{design}_rollup.rep
        
        Args:
            ipo: IPO name
            
        Returns:
            dict: Power data {test1, test2, available_tests, run_date, status}
        """
        try:
            import csv
            import glob
            
            design = self.design_info.top_hier
            
            # Get NBU signoff path for this IPO
            nbu_path = self.nbu_signoff_paths.get(ipo)
            if not nbu_path:
                return self._empty_power_data()
            
            # Find latest work directory
            auto_pt_base = os.path.join(nbu_path, "signoff_flow/auto_pt")
            if not os.path.exists(auto_pt_base):
                return self._empty_power_data()
            
            # Get latest work_* directory
            work_dirs = glob.glob(os.path.join(auto_pt_base, "work_*"))
            if not work_dirs:
                return self._empty_power_data()
            
            latest_work = sorted(work_dirs, key=os.path.getmtime, reverse=True)[0]
            
            # Build path to power analysis directory
            power_analysis_dir = os.path.join(
                latest_work,
                "func.std_tt_105c_0p67v.setup.typical/reports/power_reports",
                "func.std_tt_105c_0p67v.setup.typical/power_analysis"
            )
            
            if not os.path.exists(power_analysis_dir):
                return self._empty_power_data()
            
            # Extract run date from work directory
            work_name = os.path.basename(latest_work)
            run_date = work_name.replace("work_", "").replace("_", " ").replace(":", ":")
            
            # Discover all available power test directories
            test_dirs = glob.glob(os.path.join(power_analysis_dir, "power_*"))
            available_tests = []
            test_data = {}
            
            for test_dir in test_dirs:
                test_name = os.path.basename(test_dir).replace("power_RTLSim_", "").replace(".fsdb", "")
                rep_file = os.path.join(test_dir, f"{design}_rollup.rep")
                
                if os.path.exists(rep_file):
                    available_tests.append(test_name)
                    try:
                        test_data[test_name] = self._parse_power_rep(rep_file)
                    except Exception as e:
                        print(f"    {Color.YELLOW}[WARN] Failed to parse {test_name} for {ipo}: {e}{Color.RESET}")
                        test_data[test_name] = None
            
            # Smart test selection: Highest AF (FF/Q) vs Lowest AF (FF/Q)
            selected_tests = self._select_power_tests(available_tests, test_data)
            
            power_data = {
                "scenario": "func.std_tt_105c_0p67v.setup.typical",
                "run_date": run_date,
                "available_tests": sorted(available_tests),
                "test1": test_data.get(selected_tests[0], self._empty_test_power_data()) if selected_tests[0] else self._empty_test_power_data(),
                "test1_name": selected_tests[0],
                "test2": test_data.get(selected_tests[1], self._empty_test_power_data()) if selected_tests[1] else self._empty_test_power_data(),
                "test2_name": selected_tests[1],
                "status": "FOUND" if available_tests else "NOT_RUN"
            }
            
            return power_data
            
        except Exception as e:
            print(f"    {Color.YELLOW}[WARN] Failed to extract power data for {ipo}: {e}{Color.RESET}")
            return self._empty_power_data()
    
    def _select_power_tests(self, available_tests: list, test_data: dict) -> tuple:
        """
        Intelligently select 2 power tests to compare based on Activity Factor (FF/Q)
        
        Selection Strategy:
        1. Test with HIGHEST Activity Factor FF/Q (most switching activity)
        2. Test with LOWEST Activity Factor FF/Q (least switching activity)
        
        This provides objective comparison between most active and least active scenarios
        
        Args:
            available_tests: List of available test names
            test_data: Dictionary of parsed test data
            
        Returns:
            tuple: (highest_af_test, lowest_af_test)
        """
        if not available_tests:
            return (None, None)
        
        if len(available_tests) == 1:
            return (available_tests[0], None)
        
        # Filter tests with valid AF data
        tests_with_af = []
        for test in available_tests:
            if test in test_data and test_data[test] is not None:
                if 'avg_af' in test_data[test]:
                    tests_with_af.append((test, test_data[test]['avg_af']))
        
        # If no tests have AF data, fall back to first two tests
        if not tests_with_af:
            return (available_tests[0], available_tests[1] if len(available_tests) > 1 else None)
        
        # Sort by AF (descending)
        tests_with_af.sort(key=lambda x: x[1], reverse=True)
        
        # Select highest AF and lowest AF
        highest_af_test = tests_with_af[0][0]  # First element (highest AF)
        lowest_af_test = tests_with_af[-1][0]  # Last element (lowest AF)
        
        # If they're the same (only one test with AF), use second test if available
        if highest_af_test == lowest_af_test and len(available_tests) > 1:
            other_tests = [t for t in available_tests if t != highest_af_test]
            lowest_af_test = other_tests[0] if other_tests else None
        
        return (highest_af_test, lowest_af_test)
    
    def _parse_power_rep(self, rep_file: str) -> dict:
        """
        Parse power rollup .rep file
        
        Args:
            rep_file: Path to rollup.rep file
            
        Returns:
            dict: Parsed power metrics (all fields initialized)
        """
        try:
            with open(rep_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Initialize all fields to None (for safe access)
            power_metrics = {
                'total_power': None,
                'dynamic_power': None,
                'leakage_power': None,
                'annotation_score': None,
                'avg_af': None,
                'hvt_pct': None,
                'svt_pct': None,
                'lvt_pct': None,
                'status': 'FOUND'
            }
            
            # Extract Total Power
            total_match = re.search(r'Total\s+Power\[(\w+)\]\s+(\d+\.?\d*)', content, re.IGNORECASE)
            if total_match:
                power_metrics['total_power'] = float(total_match.group(2))
            
            # Extract Dynamic Power
            dynamic_match = re.search(r'Dynamic\s+Power\[(\w+)\]\s+(\d+\.?\d*)', content, re.IGNORECASE)
            if dynamic_match:
                power_metrics['dynamic_power'] = float(dynamic_match.group(2))
            
            # Extract Leakage Power
            leakage_match = re.search(r'Leakage\s+Power\[(\w+)\]\s+(\d+\.?\d*)', content, re.IGNORECASE)
            if leakage_match:
                power_metrics['leakage_power'] = float(leakage_match.group(2))
            
            # Extract Annotation Score
            score_match = re.search(r'Annotation\s+Score\s+(\d+\.?\d*)\s*%', content, re.IGNORECASE)
            if score_match:
                power_metrics['annotation_score'] = float(score_match.group(1))
            
            # Extract Activity Factor FF/Q (used for test selection)
            af_ffq_match = re.search(r'Average\s+AF\s+FF/Q\s+(\d+\.?\d*)\s*%', content, re.IGNORECASE)
            if af_ffq_match:
                power_metrics['avg_af'] = float(af_ffq_match.group(1))
            
            # Extract VT distribution from tabular format
            # Format: HVT  <leakage_mw>  <leakage_%>  <area_um2>  <area_%>  <count>  <count_%>
            # We want the last column (Count %)
            hvt_match = re.search(r'HVT\s+[\d.]+\s+[\d.]+\s+[\d.]+\s+[\d.]+\s+[\d.]+\s+([\d.]+)', content, re.IGNORECASE)
            if hvt_match:
                power_metrics['hvt_pct'] = float(hvt_match.group(1))
            
            svt_match = re.search(r'SVT\s+[\d.]+\s+[\d.]+\s+[\d.]+\s+[\d.]+\s+[\d.]+\s+([\d.]+)', content, re.IGNORECASE)
            if svt_match:
                power_metrics['svt_pct'] = float(svt_match.group(1))
            
            lvt_match = re.search(r'LVT\s+[\d.]+\s+[\d.]+\s+[\d.]+\s+[\d.]+\s+[\d.]+\s+([\d.]+)', content, re.IGNORECASE)
            if lvt_match:
                power_metrics['lvt_pct'] = float(lvt_match.group(1))
            
            # Only return if we found at least the basic power metrics
            if power_metrics['total_power'] is not None:
                return power_metrics
            else:
                return self._empty_test_power_data()
                
        except Exception as e:
            return self._empty_test_power_data()
    
    def _empty_power_data(self) -> dict:
        """Return empty power data structure"""
        return {
            "scenario": "func.std_tt_105c_0p67v.setup.typical",
            "available_tests": [],
            "test1": self._empty_test_power_data(),
            "test1_name": None,
            "test2": self._empty_test_power_data(),
            "test2_name": None,
            "run_date": None,
            "status": "NOT_RUN"
        }
    
    def _empty_test_power_data(self) -> dict:
        """Return empty test power data"""
        return {
            "total_power": None,
            "dynamic_power": None,
            "leakage_power": None,
            "annotation_score": None,
            "avg_af": None,
            "hvt_pct": None,
            "svt_pct": None,
            "lvt_pct": None,
            "status": "NOT_FOUND"
        }
    
    def _extract_ipo_pt_timing(self, ipo: str, corner_type: str) -> dict:
        """
        Extract PT timing for specific corner (setup or hold) - Production Ready
        
        Args:
            ipo: IPO name
            corner_type: "setup" or "hold"
            
        Returns:
            dict: Timing metrics {corner, wns, tns, nvp, run_date, status}
        """
        corner_map = {
            "setup": "func.std_tt_0c_0p6v.setup.typical",
            "hold": "func.std_tt_0c_0p55v.hold.typical"
        }
        
        corner_name = corner_map.get(corner_type, "")
        
        try:
            nbu_path = self.nbu_signoff_paths[ipo]
            pt_base = os.path.join(nbu_path, "signoff_flow/auto_pt")
            
            if not os.path.exists(pt_base):
                return {"corner": corner_name, "wns": None, "tns": None, "nvp": None, "run_date": None, "status": "NOT_RUN"}
            
            # Find latest work directory
            work_dirs = []
            for item in os.listdir(pt_base):
                if item.startswith("work_"):
                    work_path = os.path.join(pt_base, item)
                    if os.path.isdir(work_path):
                        work_dirs.append((work_path, os.path.getmtime(work_path)))
            
            if not work_dirs:
                return {"corner": corner_name, "wns": None, "tns": None, "nvp": None, "run_date": None, "status": "NOT_RUN"}
            
            # Get latest work directory
            latest_work = max(work_dirs, key=lambda x: x[1])[0]
            run_date = datetime.fromtimestamp(max(work_dirs, key=lambda x: x[1])[1]).strftime("%m/%d %H:%M")
            
            # Look for timing report
            timing_file = os.path.join(latest_work, f"{corner_name}/reports/timing_reports/{self.design_info.top_hier}_{corner_name}.timing")
            
            if not os.path.exists(timing_file):
                return {"corner": corner_name, "wns": None, "tns": None, "nvp": None, "run_date": run_date, "status": "NOT_FOUND"}
            
            # Parse timing report using same logic as main PT analysis
            try:
                with open(timing_file, 'r') as f:
                    content = f.read()
                
                # Find all Group lines - improved regex to handle asterisks properly
                # Group: **all_taps** | NVP: 12345 | WNS: -0.123 | TNS: -456.789
                group_pattern = r'Group:\s*\*\*([^*]+)\*\*\s*\|\s*NVP:\s*(\d+)\s*\|\s*WNS:\s*([-\d.]+)\s*\|\s*TNS:\s*([-\d.]+)|Group:\s*([^|]+)\s*\|\s*NVP:\s*(\d+)\s*\|\s*WNS:\s*([-\d.]+)\s*\|\s*TNS:\s*([-\d.]+)'
                matches = re.findall(group_pattern, content)
                
                worst_wns = 0.0
                total_tns = 0.0
                total_nvp = 0
                
                # Extract metrics from all groups (excluding FEEDTHROUGH/REGIN/REGOUT which are external)
                external_groups = {'FEEDTHROUGH', 'REGIN', 'REGOUT'}
                
                for match in matches:
                    if match[0]:  # Group with asterisks
                        group_name = match[0].strip().upper()
                        nvp = int(match[1])
                        wns = float(match[2])
                        tns = float(match[3])
                    else:  # Regular group
                        group_name = match[4].strip().upper()
                        nvp = int(match[5])
                        wns = float(match[6])
                        tns = float(match[7])
                    
                    # Skip external timing groups
                    if group_name not in external_groups:
                        if wns < worst_wns:
                            worst_wns = wns
                        total_tns += tns
                        total_nvp += nvp
                
                return {
                    "corner": corner_name,
                    "wns": worst_wns if worst_wns < 0 else 0.0,
                    "tns": total_tns,
                    "nvp": total_nvp,
                    "run_date": run_date,
                    "status": "CLEAN" if worst_wns >= 0 else "VIOLATIONS"
                }
                
            except Exception as parse_err:
                print(f"    {Color.YELLOW}[WARN] Failed to parse PT timing file for {ipo} ({corner_type}): {parse_err}{Color.RESET}")
                return {"corner": corner_name, "wns": None, "tns": None, "nvp": None, "run_date": run_date, "status": "PARSE_ERROR"}
            
        except Exception as e:
            print(f"    {Color.YELLOW}[WARN] Failed to extract PT timing for {ipo} ({corner_type}): {e}{Color.RESET}")
            return {"corner": corner_name, "wns": None, "tns": None, "nvp": None, "run_date": None, "status": "ERROR"}
    
    def _extract_ipo_pv_metrics(self, ipo: str) -> dict:
        """
        Extract PV metrics (LVS/DRC/Antenna) for IPO - Production Ready
        
        Args:
            ipo: IPO name
            
        Returns:
            dict: PV metrics {lvs, drc, antenna, run_date, status}
        """
        try:
            nbu_path = self.nbu_signoff_paths[ipo]
            pv_base = os.path.join(nbu_path, "pv_flow/drc_dir", self.design_info.top_hier)
            
            if not os.path.exists(pv_base):
                return {"lvs": None, "drc": None, "antenna": None, "run_date": None, "status": "NOT_RUN"}
            
            lvs_errors = None
            drc_errors = None
            antenna_errors = None
            latest_mtime = None
            
            # Extract LVS errors
            lvs_pattern = os.path.join(pv_base, "lvs_icv_ipo*", f"{self.design_info.top_hier}_ipo*_fill.LVS_ERRORS")
            lvs_files = glob.glob(lvs_pattern)
            
            if lvs_files:
                lvs_file = sorted(lvs_files, key=os.path.getmtime, reverse=True)[0]
                latest_mtime = os.path.getmtime(lvs_file)
                
                try:
                    violations = self.lvs_parser.parse_lvs_errors(lvs_file)
                    # Calculate total LVS errors
                    lvs_errors = (violations['failed_equivalence_points'] + 
                                 violations['first_priority_errors'] + 
                                 violations['second_priority_errors'])
                except Exception as e:
                    print(f"    {Color.YELLOW}[WARN] Failed to parse LVS file for {ipo}: {e}{Color.RESET}")
            
            # Extract DRC errors
            drc_pattern = os.path.join(pv_base, "drc_icv_ipo*", f"{self.design_info.top_hier}_ipo*_fill.LAYOUT_ERRORS")
            drc_files = glob.glob(drc_pattern)
            
            if drc_files:
                drc_file = sorted(drc_files, key=os.path.getmtime, reverse=True)[0]
                if latest_mtime is None or os.path.getmtime(drc_file) > latest_mtime:
                    latest_mtime = os.path.getmtime(drc_file)
                
                try:
                    with open(drc_file, 'r') as f:
                        content = f.read()
                    
                    # Check if CLEAN
                    if "LAYOUT ERRORS RESULTS: CLEAN" in content:
                        drc_errors = 0
                    else:
                        # Extract violation counts
                        violation_pattern = r'^\s*([A-Za-z0-9._]+)\s*:\s*(.*?)(\d+)\s+violations?\s+found\.'
                        matches = re.findall(violation_pattern, content, re.MULTILINE | re.DOTALL)
                        
                        total_violations = 0
                        for rule, description, count in matches:
                            count_int = int(count)
                            if count_int > 0:
                                total_violations += count_int
                        
                        drc_errors = total_violations
                except Exception as e:
                    print(f"    {Color.YELLOW}[WARN] Failed to parse DRC file for {ipo}: {e}{Color.RESET}")
            
            # Extract Antenna errors
            antenna_pattern = os.path.join(pv_base, "drc_icv_antenna_ipo*", f"{self.design_info.top_hier}_ipo*_fill.LAYOUT_ERRORS")
            antenna_files = glob.glob(antenna_pattern)
            
            if antenna_files:
                antenna_file = sorted(antenna_files, key=os.path.getmtime, reverse=True)[0]
                if latest_mtime is None or os.path.getmtime(antenna_file) > latest_mtime:
                    latest_mtime = os.path.getmtime(antenna_file)
                
                try:
                    with open(antenna_file, 'r') as f:
                        content = f.read()
                    
                    # Check if CLEAN
                    if "CLEAN" in content.upper():
                        antenna_errors = 0
                    else:
                        # Extract antenna violations
                        if "LAYOUT ERRORS RESULTS: ERRORS" in content:
                            total_violations = 0
                            numbers = re.findall(r'(\d+)\s+violations?\s+found', content, re.IGNORECASE)
                            for count in numbers:
                                total_violations += int(count)
                            antenna_errors = total_violations
                        else:
                            antenna_errors = 0
                except Exception as e:
                    print(f"    {Color.YELLOW}[WARN] Failed to parse Antenna file for {ipo}: {e}{Color.RESET}")
            
            # Determine overall status
            if lvs_errors is None and drc_errors is None and antenna_errors is None:
                status = "NOT_FOUND"
            elif lvs_errors == 0 and drc_errors == 0 and antenna_errors == 0:
                status = "CLEAN"
            else:
                status = "ERRORS"
            
            run_date = datetime.fromtimestamp(latest_mtime).strftime("%m/%d %H:%M") if latest_mtime else None
            
            return {
                "lvs": lvs_errors,
                "drc": drc_errors,
                "antenna": antenna_errors,
                "run_date": run_date,
                "status": status
            }
            
        except Exception as e:
            print(f"    {Color.YELLOW}[WARN] Failed to extract PV metrics for {ipo}: {e}{Color.RESET}")
            return {"lvs": None, "drc": None, "antenna": None, "run_date": None, "status": "ERROR"}
    
    def _extract_external_timing_innovus(self, ipo: str) -> dict:
        """
        Extract external timing from Innovus NV Gate ECO - Production Ready
        
        Args:
            ipo: IPO name
            
        Returns:
            dict: External timing {feedthrough, regin, regout, run_date}
        """
        try:
            nbu_path = self.nbu_signoff_paths[ipo]
            eco_base = os.path.join(nbu_path, "signoff_flow/nv_gate_eco", self.design_info.top_hier)
            
            if not os.path.exists(eco_base):
                return {
                    "feedthrough": {"wns": None, "tns": None, "nvp": None},
                    "regin": {"wns": None, "tns": None, "nvp": None},
                    "regout": {"wns": None, "tns": None, "nvp": None},
                    "port_cgate": {"wns": None, "tns": None, "nvp": None},
                    "run_date": None,
                    "status": "NOT_RUN"
                }
            
            # Look for Innovus timing reports in nv_gate_eco
            # Pattern: signoff_flow/nv_gate_eco/<design>/ipo*/REPs/SUMMARY/<design>.ipo*.eco.timing.setup.rpt.gz
            setup_pattern = os.path.join(eco_base, "ipo*/REPs/SUMMARY", f"{self.design_info.top_hier}.ipo*.eco.timing.setup.rpt.gz")
            setup_files = glob.glob(setup_pattern)
            
            if not setup_files:
                return {
                    "feedthrough": {"wns": None, "tns": None, "nvp": None},
                    "regin": {"wns": None, "tns": None, "nvp": None},
                    "regout": {"wns": None, "tns": None, "nvp": None},
                    "port_cgate": {"wns": None, "tns": None, "nvp": None},
                    "run_date": None,
                    "status": "NOT_FOUND"
                }
            
            # Get the most recent setup timing report
            setup_file = sorted(setup_files, key=os.path.getmtime, reverse=True)[0]
            run_date = datetime.fromtimestamp(os.path.getmtime(setup_file)).strftime("%m/%d %H:%M")
            
            # Parse the timing report for external timing groups
            try:
                import gzip
                with gzip.open(setup_file, 'rt', encoding='utf-8') as f:
                    content = f.read()
                
                # Initialize results
                external_timing = {
                    "feedthrough": {"wns": None, "tns": None, "nvp": None},
                    "regin": {"wns": None, "tns": None, "nvp": None},
                    "regout": {"wns": None, "tns": None, "nvp": None},
                    "port_cgate": {"wns": None, "tns": None, "nvp": None}  # Port2Cgate (external timing)
                }
                
                # Look for histogram table with external timing breakdown
                # Format: external|feedthrough | WNS | TNS | NVP | histogram...
                # The table shows timing breakdown by group (FEEDTHROUGH, REGIN, REGOUT)
                
                lines = content.split('\n')
                
                for i, line in enumerate(lines):
                    line_upper = line.upper()
                    
                    # Look for lines with external timing groups
                    # Multiple naming conventions:
                    # 1. external|feedthrough, external|input, external|output
                    # 2. Specific path types: port_to_flop, flop_to_port, port_to_clock_gate
                    if '|' in line and ('EXTERNAL|' in line_upper or any(g in line_upper for g in ['FEEDTHROUGH', 'REGIN', 'REGOUT', 'PORT_TO_FLOP', 'FLOP_TO_PORT', 'PORT_TO_CLOCK_GATE'])):
                        # Parse the line with pipe delimiters
                        # Format: external|feedthrough | 0.000|  0.000|  0| histogram...
                        # Format: external|input     |func.std... | -0.023| -0.163| 11|...
                        # Format: |output    |flop_to_port | -0.073|-11.837|716|...
                        # Format: |          |port_to_flop | -0.045| -5.252|465|...
                        parts = [p.strip() for p in line.split('|')]
                        
                        # Determine which group this is
                        # Collect all potential matches, then prioritize specific over general
                        target_group = None
                        group_part = None
                        fallback_group = None
                        fallback_part = None
                        
                        for part in parts:
                            part_upper = part.upper()
                            part_lower = part.lower()
                            
                            # Check for explicit FEEDTHROUGH/REGIN/REGOUT (PT format) - highest priority
                            if 'FEEDTHROUGH' in part_upper:
                                target_group = 'feedthrough'
                                group_part = part
                                break
                            elif 'REGIN' in part_upper:
                                target_group = 'regin'
                                group_part = part
                                break
                            elif 'REGOUT' in part_upper:
                                target_group = 'regout'
                                group_part = part
                                break
                            # Check for specific path types (high priority)
                            elif 'port_to_clock_gate' in part_lower:
                                # Port2Cgate: port-to-clock-gate (external timing)
                                target_group = 'port_cgate'
                                group_part = part
                                break
                            elif 'port_to_flop' in part_lower:
                                # REGIN: port-to-register (flop)
                                target_group = 'regin'
                                group_part = part
                                break
                            elif 'flop_to_port' in part_lower:
                                # REGOUT: register (flop)-to-port
                                target_group = 'regout'
                                group_part = part
                                break
                            # Check for Innovus general categories (fallback - lower priority)
                            elif part_lower == 'feedthrough' and not fallback_group:
                                fallback_group = 'feedthrough'
                                fallback_part = part
                            elif part_lower == 'input' and not fallback_group:
                                fallback_group = 'regin'
                                fallback_part = part
                            elif part_lower == 'output' and not fallback_group:
                                fallback_group = 'regout'
                                fallback_part = part
                        
                        # Use fallback if no specific match found
                        if not target_group and fallback_group:
                            target_group = fallback_group
                            group_part = fallback_part
                        
                        if target_group and group_part:
                            try:
                                # Find the index of the group part
                                group_idx = parts.index(group_part)
                                
                                # Next parts should be WNS, TNS, NVP
                                # But there might be a scenario name in between, so we need to find the numeric parts
                                # Look for the first three numeric values after the group name
                                numeric_parts = []
                                for j in range(group_idx + 1, len(parts)):
                                    part_str = parts[j].strip()
                                    # Check if this looks like a numeric value (including negative)
                                    if part_str and (part_str[0].isdigit() or part_str[0] == '-'):
                                        try:
                                            # Try to convert to float to verify it's numeric
                                            float(part_str)
                                            numeric_parts.append(part_str)
                                            if len(numeric_parts) == 3:
                                                break
                                        except ValueError:
                                            continue
                                
                                if len(numeric_parts) >= 3:
                                    wns_str = numeric_parts[0]
                                    tns_str = numeric_parts[1]
                                    nvp_str = numeric_parts[2]
                                    
                                    # Parse values
                                    wns = float(wns_str) if wns_str and wns_str != '-' else 0.0
                                    tns = float(tns_str) if tns_str and tns_str != '-' else 0.0
                                    nvp = int(float(nvp_str)) if nvp_str and nvp_str != '-' else 0
                                    
                                    # Only update if not already populated or if this is more specific
                                    # Prefer specific path types (port_to_flop, flop_to_port, port_to_clock_gate) over general categories
                                    if external_timing[target_group]['wns'] is None or 'port_to_flop' in part_lower or 'flop_to_port' in part_lower or 'port_to_clock_gate' in part_lower:
                                        external_timing[target_group] = {
                                            "wns": wns,
                                            "tns": tns,
                                            "nvp": nvp
                                        }
                            except (ValueError, IndexError) as e:
                                # Continue parsing other groups
                                pass
                
                return {
                    "feedthrough": external_timing["feedthrough"],
                    "regin": external_timing["regin"],
                    "regout": external_timing["regout"],
                    "port_cgate": external_timing["port_cgate"],
                    "run_date": run_date,
                    "status": "FOUND"
                }
                
            except Exception as parse_err:
                print(f"    {Color.YELLOW}[WARN] Failed to parse ECO timing file for {ipo}: {parse_err}{Color.RESET}")
                return {
                    "feedthrough": {"wns": None, "tns": None, "nvp": None},
                    "regin": {"wns": None, "tns": None, "nvp": None},
                    "regout": {"wns": None, "tns": None, "nvp": None},
                    "port_cgate": {"wns": None, "tns": None, "nvp": None},
                    "run_date": run_date,
                    "status": "PARSE_ERROR"
                }
            
        except Exception as e:
            print(f"    {Color.YELLOW}[WARN] Failed to extract external timing for {ipo}: {e}{Color.RESET}")
            return {
                "feedthrough": {"wns": None, "tns": None, "nvp": None},
                "regin": {"wns": None, "tns": None, "nvp": None},
                "regout": {"wns": None, "tns": None, "nvp": None},
                "port_cgate": {"wns": None, "tns": None, "nvp": None},
                "run_date": None,
                "status": "ERROR"
            }
    
    def _extract_formal_status(self, ipo: str) -> dict:
        """
        Extract formal verification status - Production Ready
        
        Checks both signoff_flow/formal and formal_flow directories
        
        Args:
            ipo: IPO name
            
        Returns:
            dict: Formal status {status, is_clean, run_date, details}
        """
        try:
            nbu_path = self.nbu_signoff_paths[ipo]
            
            # Check both possible formal directory locations
            formal_bases = [
                os.path.join(nbu_path, "formal_flow"),  # Check formal_flow first (newer convention)
                os.path.join(nbu_path, "signoff_flow/formal")  # Fallback to old location
            ]
            
            formal_base = None
            for base in formal_bases:
                if os.path.exists(base):
                    formal_base = base
                    break
            
            if not formal_base:
                return {"status": "NOT_RUN", "is_clean": None, "run_date": None, "details": "Formal directory not found"}
            
            # Look for formal log files (rtl_vs_pnr_bbox_fm.log, rtl_vs_pnr_fm.log, eco_vs_pnr_fm.log, etc.)
            # Prioritize specific formal result logs over generic .log files
            priority_patterns = [
                os.path.join(formal_base, "**/rtl_vs_pnr_bbox_fm.log"),
                os.path.join(formal_base, "**/rtl_vs_pnr_fm.log"),
                os.path.join(formal_base, "**/eco_vs_pnr_fm.log"),
            ]
            
            fallback_patterns = [
                os.path.join(formal_base, "**/formality.log"),
                os.path.join(formal_base, "**/log/*.log"),  # Only logs in log/ directories
            ]
            
            log_files = []
            # First try priority patterns (actual formal result logs)
            for pattern in priority_patterns:
                log_files.extend(glob.glob(pattern, recursive=True))
            
            # If no priority logs found, try fallback patterns
            if not log_files:
                for pattern in fallback_patterns:
                    log_files.extend(glob.glob(pattern, recursive=True))
            
            if not log_files:
                # Check for formal run directories
                formal_dirs = []
                for item in os.listdir(formal_base):
                    item_path = os.path.join(formal_base, item)
                    if os.path.isdir(item_path):
                        formal_dirs.append((item_path, os.path.getmtime(item_path)))
                
                if formal_dirs:
                    latest_dir, mtime = max(formal_dirs, key=lambda x: x[1])
                    run_date = datetime.fromtimestamp(mtime).strftime("%m/%d %H:%M")
                    return {
                        "status": "FOUND",
                        "is_clean": None,
                        "run_date": run_date,
                        "details": f"Latest: {os.path.basename(latest_dir)}"
                    }
                else:
                    return {"status": "NOT_RUN", "is_clean": None, "run_date": None, "details": "No formal runs found"}
            
            # Parse ALL formal log files and prioritize worst-case status
            # Status priority: FAILED > UNRESOLVED > SUCCEEDED
            all_results = []
            
            for log_file in log_files:
                try:
                    with open(log_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    lines = content.split('\n')
                    
                    # Extract verification status from "Verification Results" section
                    status = "UNKNOWN"
                    passing_points = 0
                    failing_points = 0
                    last_results_index = -1
                    
                    # Find the last "Verification Results" section
                    for i, line in enumerate(lines):
                        if "Verification Results" in line and "*****" in line:
                            last_results_index = i
                    
                    # Extract status from the last Verification Results section
                    if last_results_index >= 0:
                        # Look in the next 20 lines after "Verification Results" header
                        for i in range(last_results_index + 1, min(last_results_index + 21, len(lines))):
                            line = lines[i]
                            if "Verification SUCCEEDED" in line:
                                status = "SUCCEEDED"
                            elif "Verification FAILED" in line:
                                status = "FAILED"
                            elif "Verification UNRESOLVED" in line or "Verification INCONCLUSIVE" in line:
                                status = "UNRESOLVED"
                            
                            # Extract passing compare points
                            if "Passing compare points" in line:
                                match = re.search(r'(\d+)\s+Passing compare points', line)
                                if match:
                                    passing_points = int(match.group(1))
                            
                            # Extract failing compare points
                            if "Failing compare points" in line:
                                match = re.search(r'(\d+)\s+Failing compare points', line)
                                if match:
                                    failing_points = int(match.group(1))
                    
                    # Fallback: search entire log if no Verification Results section found
                    if status == "UNKNOWN":
                        for line in lines:
                            if "Verification SUCCEEDED" in line:
                                status = "SUCCEEDED"
                            elif "Verification FAILED" in line:
                                status = "FAILED"
                            elif "Verification UNRESOLVED" in line or "Verification INCONCLUSIVE" in line:
                                status = "UNRESOLVED"
                    
                    # Check for tool crash/error
                    if status == "UNKNOWN":
                        for line in lines[-100:]:
                            if "stopped at line" in line and "due to error" in line:
                                status = "CRASHED"
                                break
                            elif "Error: Unknown name:" in line and "FM-036" in line:
                                status = "CRASHED"
                                break
                    
                    # Check if running
                    if status == "UNKNOWN":
                        file_mtime = os.path.getmtime(log_file)
                        current_time = time.time()
                        time_since_update = current_time - file_mtime
                        
                        if time_since_update < 300:  # 5 minutes
                            for line in reversed(lines[-50:]):
                                if any(indicator in line for indicator in [
                                    "Status:  Building verification models",
                                    "Status:  Verifying",
                                    "Status:  Checking designs",
                                    "Matching in progress"
                                ]):
                                    status = "RUNNING"
                                    break
                    
                    # Store result for this log
                    all_results.append({
                        "log_file": log_file,
                        "status": status,
                        "passing_points": passing_points,
                        "failing_points": failing_points,
                        "mtime": os.path.getmtime(log_file)
                    })
                        
                except Exception as parse_err:
                    print(f"    {Color.YELLOW}[WARN] Failed to parse {os.path.basename(log_file)} for {ipo}: {parse_err}{Color.RESET}")
                    continue
            
            # If no results parsed, return error
            if not all_results:
                return {
                    "status": "PARSE_ERROR",
                    "is_clean": None,
                    "run_date": None,
                    "details": "Failed to parse any formal logs"
                }
            
            # Prioritize worst-case status: FAILED > CRASHED > UNRESOLVED > RUNNING > SUCCEEDED > UNKNOWN
            status_priority = {
                "FAILED": 0,
                "CRASHED": 1,
                "UNRESOLVED": 2,
                "RUNNING": 3,
                "SUCCEEDED": 4,
                "UNKNOWN": 5
            }
            
            # Sort by status priority (worst first), then by recency
            all_results.sort(key=lambda x: (status_priority.get(x["status"], 99), -x["mtime"]))
            best_result = all_results[0]
            
            # Get details from the selected result
            status = best_result["status"]
            passing_points = best_result["passing_points"]
            failing_points = best_result["failing_points"]
            selected_log = best_result["log_file"]
            run_date = datetime.fromtimestamp(best_result["mtime"]).strftime("%m/%d %H:%M")
            
            # Determine if clean (SUCCEEDED with 0 failing points)
            is_clean = (status == "SUCCEEDED" and failing_points == 0)
            
            # Build details string
            log_name = os.path.basename(selected_log)
            if len(all_results) > 1:
                # Show which flow was selected if multiple exist
                flow_name = log_name.replace(".log", "").replace("_fm", "")
                details = f"{flow_name}"
            else:
                details = f"{log_name}"
            
            if status == "SUCCEEDED":
                details += f" | Passing: {passing_points}"
            elif status == "FAILED":
                details += f" | Passing: {passing_points}, Failing: {failing_points}"
            
            # Add note if multiple flows analyzed
            if len(all_results) > 1:
                other_statuses = [r["status"] for r in all_results[1:]]
                details += f" | Other flows: {', '.join(set(other_statuses))}"
            
            return {
                "status": status,
                "is_clean": is_clean if status in ["SUCCEEDED", "FAILED"] else None,
                "run_date": run_date,
                "details": details
            }
            
        except Exception as e:
            print(f"    {Color.YELLOW}[WARN] Failed to extract formal status for {ipo}: {e}{Color.RESET}")
            return {"status": "ERROR", "is_clean": None, "run_date": None, "details": str(e)}
    
    def _extract_flow_history(self, ipo: str) -> list:
        """
        Extract complete flow run history for IPO
        
        Args:
            ipo: IPO name
            
        Returns:
            list: Flow history [{flow, run, work_dir, start, end, duration, status}, ...]
        """
        history = []
        
        try:
            nbu_path = self.nbu_signoff_paths.get(ipo)
            if not nbu_path:
                return history
            
            # 1. Extract PT (auto_pt) runs
            pt_base = os.path.join(nbu_path, "signoff_flow/auto_pt")
            if os.path.exists(pt_base):
                pt_runs = self._extract_work_dir_history(pt_base, "PT", "work_*")
                history.extend(pt_runs)
            
            # 2. Extract Star runs
            star_base = os.path.join(nbu_path, "signoff_flow/star")
            if os.path.exists(star_base):
                star_runs = self._extract_work_dir_history(star_base, "Star", "work_*")
                history.extend(star_runs)
            
            # 3. Extract PV runs (different structure - no work_* dirs)
            pv_base = os.path.join(nbu_path, "pv_flow")
            if os.path.exists(pv_base):
                for item in os.listdir(pv_base):
                    item_path = os.path.join(pv_base, item)
                    if os.path.isdir(item_path) and item.startswith(ipo.split('_')[0]):
                        mtime = os.path.getmtime(item_path)
                        history.append({
                            "flow": "PV",
                            "run": 1,
                            "work_dir": item,
                            "start": None,
                            "end": datetime.fromtimestamp(mtime).strftime("%m/%d %H:%M"),
                            "duration": None,
                            "status": "COMPLETED"
                        })
            
            # 4. Extract Formal runs (check both possible locations)
            formal_bases = [
                os.path.join(nbu_path, "formal_flow"),
                os.path.join(nbu_path, "signoff_flow/formal")
            ]
            
            for formal_base in formal_bases:
                if os.path.exists(formal_base):
                    run_num = 1
                    for item in os.listdir(formal_base):
                        item_path = os.path.join(formal_base, item)
                        # Skip symlinks like 'latest'
                        if os.path.isdir(item_path) and not os.path.islink(item_path):
                            # Check for log files to determine status
                            log_files = glob.glob(os.path.join(item_path, "log/*.log"))
                            if log_files:
                                latest_log = max(log_files, key=os.path.getmtime)
                                mtime = os.path.getmtime(latest_log)
                                
                                # Try to extract start/end times from log
                                start_time = None
                                end_time = datetime.fromtimestamp(mtime).strftime("%m/%d %H:%M")
                                duration = None
                                status = "COMPLETED"
                                start_timestamp = None
                                end_timestamp = None
                                
                                try:
                                    with open(latest_log, 'r', encoding='utf-8', errors='ignore') as f:
                                        content = f.read()
                                        lines = content.split('\n')
                                        
                                        # Try to find start timestamp in first 100 lines (formal has long copyright banner)
                                        for line in lines[:100]:
                                            # Look for patterns like: -I- [2025/10/28 11:08:15]
                                            match = re.search(r'\[(\d{4}/\d{2}/\d{2})\s+(\d{2}:\d{2})(?::\d{2})?\]', line)
                                            if match:
                                                date_str = match.group(1)
                                                time_str = match.group(2)
                                                # Extract MM/DD HH:MM
                                                start_time = f"{date_str[5:7]}/{date_str[8:10]} {time_str}"
                                                # Save full timestamp for duration calculation
                                                start_timestamp = datetime.strptime(f"{date_str} {time_str}", "%Y/%m/%d %H:%M")
                                                break
                                        
                                        # Try to find end timestamp in last 50 lines
                                        for line in reversed(lines[-50:]):
                                            match = re.search(r'\[(\d{4}/\d{2}/\d{2})\s+(\d{2}:\d{2})(?::\d{2})?\]', line)
                                            if match:
                                                date_str = match.group(1)
                                                time_str = match.group(2)
                                                end_time = f"{date_str[5:7]}/{date_str[8:10]} {time_str}"
                                                end_timestamp = datetime.strptime(f"{date_str} {time_str}", "%Y/%m/%d %H:%M")
                                                break
                                        
                                        # Calculate duration
                                        if start_timestamp and end_timestamp:
                                            duration_seconds = (end_timestamp - start_timestamp).total_seconds()
                                            if duration_seconds > 0:
                                                hours = int(duration_seconds // 3600)
                                                minutes = int((duration_seconds % 3600) // 60)
                                                seconds = int(duration_seconds % 60)
                                                duration = f"{hours:02d}:{minutes:02d}:{seconds:02d}"
                                except Exception as e:
                                    pass
                                
                                history.append({
                                    "flow": f"Formal ({item})",
                                    "run": run_num,
                                    "work_dir": item,
                                    "start": start_time,
                                    "end": end_time,
                                    "duration": duration,
                                    "status": status
                                })
                                run_num += 1
                    break  # Only process first found directory
            
            # 5. Extract NV Gate ECO runs  
            eco_base = os.path.join(nbu_path, "signoff_flow/nv_gate_eco")
            if os.path.exists(eco_base):
                eco_runs = self._extract_work_dir_history(eco_base, "NV Gate ECO", "work_*")
                history.extend(eco_runs)
            
            # Sort by end time
            history.sort(key=lambda x: x['end'] if x['end'] else '')
            
        except Exception as e:
            print(f"    {Color.YELLOW}[WARN] Failed to extract flow history for {ipo}: {e}{Color.RESET}")
        
        return history
    
    def _extract_work_dir_history(self, base_path: str, flow_name: str, pattern: str) -> list:
        """
        Extract history from work_* directories
        
        Args:
            base_path: Base directory containing work dirs
            flow_name: Flow name (PT, Star, etc.)
            pattern: Directory pattern to match
            
        Returns:
            list: Flow runs
        """
        runs = []
        try:
            work_dirs = []
            for item in os.listdir(base_path):
                if item.startswith("work_"):
                    work_path = os.path.join(base_path, item)
                    if os.path.isdir(work_path):
                        work_dirs.append((item, work_path, os.path.getmtime(work_path)))
            
            # Sort by modification time
            work_dirs.sort(key=lambda x: x[2])
            
            for run_num, (work_dir, work_path, mtime) in enumerate(work_dirs, start=1):
                # Count auto_pt_fix ECO iterations (for PT flows only)
                eco_count = 0
                if flow_name == "PT":
                    eco_files = glob.glob(os.path.join(work_path, "pt_eco_out_*_final.tcl"))
                    eco_count = len(eco_files)
                # Extract start time from work directory name (format: work_DD.MM.YY_HH:MM)
                start_time = None
                end_time = None
                duration = None
                status = "COMPLETED"
                
                # Parse timestamp from directory name
                match = re.match(r'work_(\d{2})\.(\d{2})\.(\d{2})_(\d{2}):(\d{2})', work_dir)
                if match:
                    day, month, year, hour, minute = match.groups()
                    start_time = f"{month}/{day} {hour}:{minute}"
                    
                    # Get end time from directory mtime
                    end_time = datetime.fromtimestamp(mtime).strftime("%m/%d %H:%M")
                    
                    # Calculate duration
                    try:
                        # Parse start time to timestamp
                        start_dt = datetime.strptime(f"20{year}-{month}-{day} {hour}:{minute}", "%Y-%m-%d %H:%M")
                        end_dt = datetime.fromtimestamp(mtime)
                        duration_seconds = (end_dt - start_dt).total_seconds()
                        
                        if duration_seconds > 0:
                            hours = int(duration_seconds // 3600)
                            minutes = int((duration_seconds % 3600) // 60)
                            if hours > 0:
                                duration = f"{hours:02d}:{minutes:02d}:00"
                            else:
                                duration = f"00:{minutes:02d}:{int(duration_seconds % 60):02d}"
                    except:
                        pass
                else:
                    # Fallback: use directory mtime
                    start_time = datetime.fromtimestamp(mtime).strftime("%m/%d %H:%M")
                    end_time = start_time
                    status = "UNKNOWN"
                
                runs.append({
                    "flow": flow_name,
                    "run": run_num,
                    "work_dir": work_dir,
                    "start": start_time,
                    "end": end_time,
                    "duration": duration,
                    "status": status,
                    "eco_count": eco_count
                })
        except Exception:
            pass
        
        return runs
    
    @staticmethod
    def _format_number_smart(value, decimal_places=2):
        """
        Format number intelligently: integers without decimals, floats with precision
        
        Args:
            value: Number to format (can be None)
            decimal_places: Number of decimal places for non-integers
            
        Returns:
            str: Formatted number or "-" if None
        """
        if value is None:
            return "-"
        
        # Check if value is effectively an integer (within small threshold)
        if abs(value - round(value)) < 0.0001:
            return str(int(round(value)))
        
        # Format with specified decimal places
        formatted = f"{value:.{decimal_places}f}"
        
        # Remove unnecessary trailing zeros
        if '.' in formatted:
            formatted = formatted.rstrip('0').rstrip('.')
        
        return formatted
    
    @staticmethod
    def _extract_design_name(workarea_path: str) -> str:
        """
        Extract design name from workarea path
        
        Examples:
            /home/scratch.../pmux/pmux_rbv_... â†’ pmux
            /path/to/prt_wa â†’ prt
            /home/user/fth_workarea â†’ fth
        
        Args:
            workarea_path: Path to workarea
            
        Returns:
            str: Design name (lowercase)
        """
        path = os.path.normpath(workarea_path)
        parts = path.split(os.sep)
        
        # Try to find a directory name that looks like a design name
        # Priority: directory names in path that match common patterns
        for part in reversed(parts):
            if part:
                # Remove common suffixes
                design = part.lower()
                for suffix in ['_wa', '_workarea', '_rbv', '_work']:
                    if design.endswith(suffix):
                        design = design[:design.rfind(suffix)]
                
                # If we have a name with underscores, take the first part
                # e.g., "pmux_rbv_2025..." â†’ "pmux"
                if '_' in design:
                    design = design.split('_')[0]
                
                # If it looks like a design name (3-10 chars, alphanumeric), use it
                if 2 <= len(design) <= 10 and design.isalnum():
                    return design
        
        # Fallback: use last non-empty directory name
        for part in reversed(parts):
            if part and not part.startswith('.'):
                return part.lower()
        
        return "unknown"
    
    @staticmethod
    def analyze_multi_workarea_ipo_comparison(workarea_paths: list, show_logo: bool = True, quiet: bool = False, email: str = None):
        """
        Analyze and compare IPOs across multiple workareas
        
        Args:
            workarea_paths: List of workarea paths to analyze
            show_logo: Show Avice logo
            quiet: Suppress verbose output
            email: Email address to send report to
        """
        if not quiet:
            print("="*80)
            print(f"{Color.CYAN}{'MULTI-WORKAREA IPO COMPARISON MODE':^80}{Color.RESET}")
            print("="*80)
            print()
        
        # Step 1: Extract design names and collect data from each workarea
        design_data = {}
        
        if not quiet:
            print(f"Analyzing {len(workarea_paths)} workarea(s):")
        
        for idx, wa_path in enumerate(workarea_paths, start=1):
            design_name = WorkareaReviewer._extract_design_name(wa_path)
            
            if not quiet:
                print(f"  {idx}. {design_name}: {wa_path}")
            
            try:
                # Create reviewer instance for this workarea
                reviewer = WorkareaReviewer(wa_path, None, show_logo=False, skip_validation=False, quiet=True)
                
                # Check if NBU signoff IPOs detected
                if not reviewer.nbu_signoff_paths:
                    if not quiet:
                        print(f"     {Color.YELLOW}[WARN] No NBU signoff IPOs detected{Color.RESET}")
                    design_data[design_name] = {
                        'workarea_path': wa_path,
                        'ipo_data': {},
                        'design_name': design_name,
                        'ipo_count': 0,
                        'error': 'No NBU signoff IPOs detected'
                    }
                    continue
                
                ipos = list(reviewer.nbu_signoff_paths.keys())
                
                if not quiet:
                    print(f"     --> Detected {len(ipos)} IPO(s): {', '.join(ipos)}")
                
                # Extract IPO data
                if not quiet:
                    print(f"     --> Extracting data...")
                
                ipo_data = reviewer._extract_all_ipo_data()
                
                # Store design data
                design_data[design_name] = {
                    'workarea_path': wa_path,
                    'ipo_data': ipo_data,
                    'design_name': design_name,
                    'ipo_count': len(ipos)
                }
                
                if not quiet:
                    print(f"     {Color.GREEN}[OK] {design_name}: {len(ipos)} IPOs analyzed{Color.RESET}")
                
            except Exception as e:
                print(f"     {Color.RED}[ERROR] Failed to analyze {design_name}: {e}{Color.RESET}")
                import traceback
                traceback.print_exc()
                design_data[design_name] = {
                    'workarea_path': wa_path,
                    'ipo_data': {},
                    'design_name': design_name,
                    'ipo_count': 0,
                    'error': str(e)
                }
        
        # Step 2: Generate Excel report with multiple sheets
        if not quiet:
            print()
            print("Generating multi-workarea Excel report...")
        
        excel_file = WorkareaReviewer._generate_multi_workarea_excel(design_data)
        
        # Step 3: Print summary
        if not quiet:
            WorkareaReviewer._print_multi_workarea_summary(design_data)
            print()
            print(f"{Color.GREEN}[OK] Excel report generated:{Color.RESET}")
            print(f"  File: {excel_file}")
            print()
            print(f"  {Color.CYAN}Excel contains:{Color.RESET}")
            print(f"    Sheet 1: Cross-Design Summary")
            for design_name in sorted(design_data.keys()):
                ipo_count = design_data[design_name]['ipo_count']
                print(f"    Sheet N: {design_name.upper()} - IPO Comparison ({ipo_count} IPOs)")
            print(f"    Sheet N+1: Flow Run History (All Designs)")
            print(f"    Sheet N+2: Notes & Metadata")
            print()
        
        # Step 4: Send email if requested
        if email:
            if not quiet:
                print(f"{Color.CYAN}Sending email to {email}...{Color.RESET}")
            try:
                WorkareaReviewer._send_multi_workarea_email(excel_file, email, design_data)
            except Exception as e:
                print(f"{Color.RED}[ERROR] Failed to send email: {e}{Color.RESET}")
        
        if not quiet:
            print()
            print("="*80)
            print(f"{Color.CYAN}{'MULTI-WORKAREA COMPARISON COMPLETE':^80}{Color.RESET}")
            print("="*80)
    
    @staticmethod
    def _generate_multi_workarea_excel(design_data: dict) -> str:
        """
        Generate multi-workarea Excel report with summary and per-design sheets
        
        Args:
            design_data: Dictionary of design_name -> {workarea_path, ipo_data, ...}
            
        Returns:
            str: Path to generated Excel file
        """
        from openpyxl import Workbook
        from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
        from openpyxl.utils import get_column_letter
        from datetime import datetime
        
        # Create workbook
        wb = Workbook()
        wb.remove(wb.active)  # Remove default sheet
        
        # Define styles
        header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
        header_font = Font(bold=True, color="FFFFFF", size=11)
        center_align = Alignment(horizontal="center", vertical="center", wrap_text=True)
        thin_border = Border(
            left=Side(style='thin'),
            right=Side(style='thin'),
            top=Side(style='thin'),
            bottom=Side(style='thin')
        )
        
        green_fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")
        yellow_fill = PatternFill(start_color="FFEB9C", end_color="FFEB9C", fill_type="solid")
        red_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
        gray_fill = PatternFill(start_color="D9D9D9", end_color="D9D9D9", fill_type="solid")
        
        # Sheet 1: Cross-Design Summary
        ws_summary = wb.create_sheet("Cross-Design Summary")
        WorkareaReviewer._create_summary_sheet(ws_summary, design_data, header_fill, header_font, center_align, thin_border, green_fill, yellow_fill, red_fill, gray_fill)
        
        # Sheets 2-N: Per-Design IPO Comparison (reuse existing logic)
        for design_name in sorted(design_data.keys()):
            if design_data[design_name].get('error'):
                continue  # Skip failed designs
            
            ipo_data = design_data[design_name]['ipo_data']
            if not ipo_data:
                continue
            
            # Create per-design sheet (use a temporary reviewer instance to access the method)
            sheet_name = f"{design_name.upper()} - IPO Comparison"
            ws_design = wb.create_sheet(sheet_name[:31])  # Excel sheet name limit
            
            # Reuse single-workarea comparison logic for each design
            WorkareaReviewer._populate_design_sheet(ws_design, design_name, ipo_data, header_fill, header_font, center_align, thin_border, green_fill, yellow_fill, red_fill, gray_fill)
        
        # Sheet N+1: Combined Flow Run History
        ws_history = wb.create_sheet("Flow Run History - All")
        WorkareaReviewer._create_combined_flow_history(ws_history, design_data, header_fill, header_font, center_align, thin_border, green_fill, yellow_fill, red_fill, gray_fill)
        
        # Sheet N+2: Notes & Metadata
        ws_notes = wb.create_sheet("Notes & Metadata")
        WorkareaReviewer._create_notes_sheet(ws_notes, design_data, is_multi=True)
        
        # Generate filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"avice_IPO_comparison_multi_{timestamp}.xlsx"
        filepath = os.path.join(os.getcwd(), filename)
        
        # Save workbook
        wb.save(filepath)
        
        return filepath
    
    @staticmethod
    def _print_multi_workarea_summary(design_data: dict):
        """
        Print terminal summary for multi-workarea comparison
        
        Args:
            design_data: Dictionary of design_name -> {workarea_path, ipo_data, ...}
        """
        print()
        print("-" * 80)
        print(f"{'CROSS-DESIGN SUMMARY':^80}")
        print("-" * 80)
        print()
        
        # Build summary table
        print(f"  {'Design':<12} {'Best IPO':<15} {'Cells':<12} {'PT Status':<12} {'PV Status':<12} {'Formal':<10}")
        print(f"  {'-'*12} {'-'*15} {'-'*12} {'-'*12} {'-'*12} {'-'*10}")
        
        for design_name in sorted(design_data.keys()):
            data = design_data[design_name]
            
            if data.get('error'):
                print(f"  {design_name:<12} {'ERROR':<15} {'-':<12} {'-':<12} {'-':<12} {'-':<10}")
                continue
            
            ipo_data = data['ipo_data']
            if not ipo_data:
                print(f"  {design_name:<12} {'NO DATA':<15} {'-':<12} {'-':<12} {'-':<12} {'-':<10}")
                continue
            
            # Find best/latest IPO (use last one in sorted order)
            best_ipo = sorted(ipo_data.keys())[-1]
            ipo_metrics = ipo_data[best_ipo]
            
            # Format cell count
            cell_count = f"{ipo_metrics['cell_count']:,}" if ipo_metrics['cell_count'] else "-"
            
            # PT Status
            setup_wns = ipo_metrics['pt_setup']['wns']
            hold_wns = ipo_metrics['pt_hold']['wns']
            if setup_wns is not None and hold_wns is not None:
                if setup_wns >= 0 and hold_wns >= 0:
                    pt_status = "PASS"
                elif setup_wns >= -0.050 and hold_wns >= -0.050:
                    pt_status = "WARN"
                else:
                    pt_status = "FAIL"
            else:
                pt_status = "NOT_RUN"
            
            # PV Status
            lvs = ipo_metrics['pv']['lvs']
            drc = ipo_metrics['pv']['drc']
            if lvs is not None and drc is not None:
                if lvs == 0 and drc == 0:
                    pv_status = "PASS"
                elif lvs < 10 and drc < 10:
                    pv_status = f"WARN ({lvs+drc})"
                else:
                    pv_status = f"FAIL ({lvs+drc})"
            else:
                pv_status = "NOT_RUN"
            
            # Formal Status
            formal_status = ipo_metrics['formal']['status']
            if formal_status == "SUCCEEDED":
                formal = "PASS"
            elif formal_status == "NOT_RUN":
                formal = "NOT_RUN"
            elif formal_status in ["FAILED", "CRASHED"]:
                formal = "FAIL"
            else:
                formal = "WARN"
            
            print(f"  {design_name:<12} {best_ipo:<15} {cell_count:<12} {pt_status:<12} {pv_status:<12} {formal:<10}")
        
        print()
        print(f"  [Details per design available in Excel sheets]")
    
    @staticmethod
    def _create_summary_sheet(ws, design_data: dict, header_fill, header_font, center_align, thin_border, green_fill, yellow_fill, red_fill, gray_fill):
        """Create cross-design summary sheet"""
        from openpyxl.styles import Font, PatternFill, Alignment
        
        # Title
        ws.append(["Cross-Design IPO Comparison Summary"])
        ws.merge_cells("A1:G1")
        title_cell = ws["A1"]
        title_cell.font = Font(bold=True, size=14, color="FFFFFF")
        title_cell.fill = PatternFill(start_color="1F4E78", end_color="1F4E78", fill_type="solid")
        title_cell.alignment = center_align
        ws.append([])
        
        # Headers
        headers = ["Design", "Best IPO", "Cell Count", "Utilization (%)", "PT Timing", "PV Status", "Formal"]
        ws.append(headers)
        for cell in ws[3]:
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = center_align
            cell.border = thin_border
        
        # Data rows
        for design_name in sorted(design_data.keys()):
            data = design_data[design_name]
            
            if data.get('error'):
                ws.append([design_name.upper(), "ERROR", "-", "-", "-", "-", "-"])
                continue
            
            ipo_data = data['ipo_data']
            if not ipo_data:
                ws.append([design_name.upper(), "NO DATA", "-", "-", "-", "-", "-"])
                continue
            
            # Find best/latest IPO
            best_ipo = sorted(ipo_data.keys())[-1]
            ipo_metrics = ipo_data[best_ipo]
            
            # Extract metrics
            cell_count = ipo_metrics['cell_count'] if ipo_metrics['cell_count'] else "-"
            util = ipo_metrics.get('utilization', '-')
            
            # PT Timing status
            setup_wns = ipo_metrics['pt_setup']['wns']
            setup_tns = ipo_metrics['pt_setup']['tns']
            hold_wns = ipo_metrics['pt_hold']['wns']
            
            if setup_wns is not None and hold_wns is not None:
                pt_status = f"S:{WorkareaReviewer._format_number_smart(setup_wns, 3)}/{WorkareaReviewer._format_number_smart(setup_tns, 2)} H:{WorkareaReviewer._format_number_smart(hold_wns, 3)}"
            else:
                pt_status = "NOT_RUN"
            
            # PV status
            lvs = ipo_metrics['pv']['lvs']
            drc = ipo_metrics['pv']['drc']
            ant = ipo_metrics['pv']['antenna']
            if lvs is not None:
                pv_status = f"LVS:{lvs} DRC:{drc} ANT:{ant}"
            else:
                pv_status = "NOT_RUN"
            
            # Formal
            formal = ipo_metrics['formal']['status']
            
            row_data = [design_name.upper(), best_ipo, cell_count, util, pt_status, pv_status, formal]
            ws.append(row_data)
        
        # Apply borders and alignment to data rows
        for row in ws.iter_rows(min_row=4, max_row=ws.max_row, min_col=1, max_col=7):
            for cell in row:
                cell.border = thin_border
                cell.alignment = Alignment(horizontal="center", vertical="center")
        
        # Color code status columns
        for row_idx in range(4, ws.max_row + 1):
            # PT Timing (column E)
            pt_cell = ws.cell(row=row_idx, column=5)
            pt_value = str(pt_cell.value)
            if "NOT_RUN" in pt_value or pt_value == "-":
                pt_cell.fill = gray_fill
            elif pt_value != "ERROR":
                # Parse WNS from status string
                try:
                    if "S:" in pt_value:
                        wns_str = pt_value.split("S:")[1].split("/")[0]
                        wns = float(wns_str)
                        if wns >= WorkareaReviewer.TIMING_THRESHOLDS['wns_green']:
                            pt_cell.fill = green_fill
                        elif wns >= WorkareaReviewer.TIMING_THRESHOLDS['wns_yellow']:
                            pt_cell.fill = yellow_fill
                        else:
                            pt_cell.fill = red_fill
                except:
                    pass
            
            # PV Status (column F)
            pv_cell = ws.cell(row=row_idx, column=6)
            pv_value = str(pv_cell.value)
            if "NOT_RUN" in pv_value or pv_value == "-":
                pv_cell.fill = gray_fill
            elif pv_value != "ERROR":
                try:
                    if "LVS:" in pv_value:
                        lvs = int(pv_value.split("LVS:")[1].split()[0])
                        drc = int(pv_value.split("DRC:")[1].split()[0])
                        total_errors = lvs + drc
                        if total_errors == 0:
                            pv_cell.fill = green_fill
                        elif total_errors < WorkareaReviewer.PV_THRESHOLDS['errors_yellow']:
                            pv_cell.fill = yellow_fill
                        else:
                            pv_cell.fill = red_fill
                except:
                    pass
            
            # Formal (column G)
            formal_cell = ws.cell(row=row_idx, column=7)
            formal_value = str(formal_cell.value)
            if formal_value == "SUCCEEDED":
                formal_cell.fill = green_fill
            elif formal_value == "NOT_RUN":
                formal_cell.fill = gray_fill
            elif formal_value in ["FAILED", "CRASHED"]:
                formal_cell.fill = red_fill
            else:
                formal_cell.fill = yellow_fill
        
        # Set column widths
        ws.column_dimensions['A'].width = 15
        ws.column_dimensions['B'].width = 18
        ws.column_dimensions['C'].width = 15
        ws.column_dimensions['D'].width = 15
        ws.column_dimensions['E'].width = 25
        ws.column_dimensions['F'].width = 25
        ws.column_dimensions['G'].width = 15
    
    @staticmethod
    def _populate_ipo_comparison_sheet_comprehensive(ws, ipo_data: dict, header_fill, header_font, center_align, thin_border, green_fill, yellow_fill, red_fill, gray_fill, title_row_exists: bool = False):
        """
        Populate worksheet with comprehensive IPO comparison data
        
        This method provides the full detailed format with clock latencies, PT timing, 
        PV metrics, external timing, power analysis, and formal status.
        
        Args:
            ws: Worksheet to populate
            ipo_data: Complete IPO data structure
            Various fill/font/alignment styles
            title_row_exists: If True, expects title in row 1, starts headers at row 2
        """
        from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
        from openpyxl.utils import get_column_letter
        from openpyxl.cell.cell import MergedCell
        
        # Determine starting row based on whether title exists
        header_start_row = 2 if title_row_exists else 1
        data_start_row = header_start_row + 3
        
        # If title exists, format it
        if title_row_exists:
            # Merge title across many columns (we'll adjust this after knowing total cols)
            pass  # Will merge after we know total columns
        
        # Collect all unique clock names across all IPOs
        all_clocks = set()
        for ipo_name, data in ipo_data.items():
            if data.get("clock_latencies"):
                all_clocks.update(data["clock_latencies"].keys())
        
        # Sort clocks for consistent ordering
        sorted_clocks = sorted(all_clocks)
        num_clock_cols = len(sorted_clocks)
        
        # Get power test names from first IPO with power data
        test1_name = 'Test 1'
        test2_name = 'Test 2'
        for ipo_name in sorted(ipo_data.keys()):
            power_data = ipo_data[ipo_name]['power']
            if power_data.get('status') == 'FOUND' and power_data.get('test1_name'):
                test1_name = power_data.get('test1_name', 'Test 1')
                test2_name = power_data.get('test2_name', 'Test 2')
                break
        
        # Build headers (3 rows)
        headers_row1 = ["IPO", "Cell Count", "Utilization"]
        if sorted_clocks:
            headers_row1.append("Clock Latency (ps)")
            headers_row1.extend([""] * (num_clock_cols - 1))
        headers_row1.extend([
            "PT Setup", "", "",
            "PT Hold", "", "",
            "PV", "", "",
            "External Timing (NV Gate ECO - Innovus)", "", "", "", "", "", "", "", "", "", "", "", "", "", "",
            f"Power - {test1_name}", "", "", "", "", "", "", "",
            f"Power - {test2_name}", "", "", "", "", "", "", "",
            "Formal", ""
        ])
        
        headers_row2 = ["", "", ""]
        if sorted_clocks:
            headers_row2.extend(sorted_clocks)
        headers_row2.extend([
            "WNS", "TNS", "NVP",
            "WNS", "TNS", "NVP",
            "LVS", "DRC", "ANT",
            "FT WNS", "FT TNS", "FT NVP", "RI WNS", "RI TNS", "RI NVP", "RO WNS", "RO TNS", "RO NVP", "Port2Cgate WNS", "Port2Cgate TNS", "Port2Cgate NVP", "TOTAL WNS", "TOTAL TNS", "TOTAL NVP",
            "Total", "Dynamic", "Leakage", "Ann.Score", "AF (FF/Q)", "HVT%", "SVT%", "LVT%",
            "Total", "Dynamic", "Leakage", "Ann.Score", "AF (FF/Q)", "HVT%", "SVT%", "LVT%",
            "Status", "Clean"
        ])
        
        headers_row3 = ["", "", "(%)", ]
        if sorted_clocks:
            headers_row3.extend(["(ps)"] * num_clock_cols)
        headers_row3.extend([
            "(ns)", "(ns)", "",
            "(ns)", "(ns)", "",
            "", "", "",
            "(ns)", "(ns)", "", "(ns)", "(ns)", "", "(ns)", "(ns)", "", "(ns)", "(ns)", "", "(ns)", "(ns)", "",
            "(mW)", "(mW)", "(mW)", "(%)", "(%)", "(%)", "(%)", "(%)",
            "(mW)", "(mW)", "(mW)", "(%)", "(%)", "(%)", "(%)", "(%)",
            "", ""
        ])
        
        # Write headers
        ws.append(headers_row1)
        ws.append(headers_row2)
        ws.append(headers_row3)
        
        # Calculate total columns
        total_cols = 3 + num_clock_cols + 3 + 3 + 3 + 15 + 16 + 2
        
        # Format title if it exists
        if title_row_exists:
            ws.merge_cells(start_row=1, start_column=1, end_row=1, end_column=total_cols)
            title_cell = ws.cell(row=1, column=1)
            title_cell.font = Font(bold=True, size=14, color="FFFFFF")
            title_cell.fill = PatternFill(start_color="1F4E78", end_color="1F4E78", fill_type="solid")
            title_cell.alignment = center_align
        
        # Format header rows
        for row in ws.iter_rows(min_row=header_start_row, max_row=header_start_row+2, min_col=1, max_col=total_cols):
            for cell in row:
                cell.fill = header_fill
                cell.font = header_font
                cell.alignment = center_align
                cell.border = thin_border
        
        # Merge cells for grouped headers (AFTER formatting)
        col = 1
        ws.merge_cells(start_row=header_start_row, start_column=col, end_row=header_start_row+2, end_column=col)
        col += 1
        ws.merge_cells(start_row=header_start_row, start_column=col, end_row=header_start_row+2, end_column=col)
        col += 1
        ws.merge_cells(start_row=header_start_row, start_column=col, end_row=header_start_row+2, end_column=col)
        col += 1
        
        if num_clock_cols > 0:
            ws.merge_cells(start_row=header_start_row, start_column=col, end_row=header_start_row, end_column=col + num_clock_cols - 1)
            col += num_clock_cols
        
        ws.merge_cells(start_row=header_start_row, start_column=col, end_row=header_start_row, end_column=col + 2)
        col += 3
        ws.merge_cells(start_row=header_start_row, start_column=col, end_row=header_start_row, end_column=col + 2)
        col += 3
        ws.merge_cells(start_row=header_start_row, start_column=col, end_row=header_start_row, end_column=col + 2)
        col += 3
        ws.merge_cells(start_row=header_start_row, start_column=col, end_row=header_start_row, end_column=col + 14)
        col += 15
        ws.merge_cells(start_row=header_start_row, start_column=col, end_row=header_start_row, end_column=col + 7)
        col += 8
        ws.merge_cells(start_row=header_start_row, start_column=col, end_row=header_start_row, end_column=col + 7)
        col += 8
        ws.merge_cells(start_row=header_start_row, start_column=col, end_row=header_start_row, end_column=col + 1)
        
        # Freeze panes (use string reference to avoid creating empty row)
        ws.freeze_panes = f"A{data_start_row}"
        
        # Write data rows
        for ipo_name in sorted(ipo_data.keys()):
            data = ipo_data[ipo_name]
            row_data = [
                ipo_name,
                data["cell_count"] if data["cell_count"] else "-",
                WorkareaReviewer._format_number_smart(data.get("utilization"), 2) if data.get("utilization") else "-",
            ]
            
            # Clock latencies
            clock_lats = data.get("clock_latencies", {})
            for clock_name in sorted_clocks:
                if clock_name in clock_lats:
                    row_data.append(WorkareaReviewer._format_number_smart(clock_lats[clock_name], 1))
                else:
                    row_data.append("-")
            
            # PT Setup/Hold
            row_data.extend([
                WorkareaReviewer._format_number_smart(data['pt_setup']['wns'], 3),
                WorkareaReviewer._format_number_smart(data['pt_setup']['tns'], 2),
                data['pt_setup']['nvp'] if data['pt_setup']['nvp'] is not None else "-",
                WorkareaReviewer._format_number_smart(data['pt_hold']['wns'], 3),
                WorkareaReviewer._format_number_smart(data['pt_hold']['tns'], 2),
                data['pt_hold']['nvp'] if data['pt_hold']['nvp'] is not None else "-",
            ])
            
            # PV
            row_data.extend([
                data['pv']['lvs'] if data['pv']['lvs'] is not None else "-",
                data['pv']['drc'] if data['pv']['drc'] is not None else "-",
                data['pv']['antenna'] if data['pv']['antenna'] is not None else "-",
            ])
            
            # External Timing details
            for cat in ['feedthrough', 'regin', 'regout', 'port_cgate']:
                row_data.extend([
                    WorkareaReviewer._format_number_smart(data['external_timing'][cat]['wns'], 3),
                    WorkareaReviewer._format_number_smart(data['external_timing'][cat]['tns'], 2),
                    data['external_timing'][cat]['nvp'] if data['external_timing'][cat]['nvp'] is not None else "-",
                ])
            
            # TOTAL external timing
            ext = data['external_timing']
            total_wns = None
            total_tns = 0.0
            total_nvp = 0
            has_ext_data = False
            for cat in ['feedthrough', 'regin', 'regout', 'port_cgate']:
                if ext[cat]['wns'] is not None:
                    has_ext_data = True
                    if total_wns is None or ext[cat]['wns'] < total_wns:
                        total_wns = ext[cat]['wns']
                    total_tns += ext[cat]['tns']
                    total_nvp += ext[cat]['nvp']
            
            row_data.extend([
                WorkareaReviewer._format_number_smart(total_wns, 3),
                WorkareaReviewer._format_number_smart(total_tns, 2) if has_ext_data else "-",
                total_nvp if has_ext_data else "-",
            ])
            
            # Power Analysis - Test 1
            test1 = data['power']['test1']
            row_data.extend([
                WorkareaReviewer._format_number_smart(test1['total_power'], 1),
                WorkareaReviewer._format_number_smart(test1['dynamic_power'], 1),
                WorkareaReviewer._format_number_smart(test1['leakage_power'], 1),
                WorkareaReviewer._format_number_smart(test1['annotation_score'], 2),
                WorkareaReviewer._format_number_smart(test1['avg_af'], 2),
                WorkareaReviewer._format_number_smart(test1['hvt_pct'], 1),
                WorkareaReviewer._format_number_smart(test1['svt_pct'], 1),
                WorkareaReviewer._format_number_smart(test1['lvt_pct'], 1),
            ])
            
            # Power Analysis - Test 2
            test2 = data['power']['test2']
            row_data.extend([
                WorkareaReviewer._format_number_smart(test2['total_power'], 1),
                WorkareaReviewer._format_number_smart(test2['dynamic_power'], 1),
                WorkareaReviewer._format_number_smart(test2['leakage_power'], 1),
                WorkareaReviewer._format_number_smart(test2['annotation_score'], 2),
                WorkareaReviewer._format_number_smart(test2['avg_af'], 2),
                WorkareaReviewer._format_number_smart(test2['hvt_pct'], 1),
                WorkareaReviewer._format_number_smart(test2['svt_pct'], 1),
                WorkareaReviewer._format_number_smart(test2['lvt_pct'], 1),
            ])
            
            # Formal
            formal_is_clean = data['formal'].get('is_clean')
            clean_display = "Clean" if formal_is_clean else ("Issues" if formal_is_clean is False else "-")
            row_data.extend([
                data['formal']['status'],
                clean_display
            ])
            
            ws.append(row_data)
        
        # Apply color coding and borders
        for row_idx, ipo_name in enumerate(sorted(ipo_data.keys()), start=data_start_row):
            data = ipo_data[ipo_name]
            
            # Calculate column positions (adjusting for utilization + clock columns)
            col_offset = 3 + num_clock_cols
            pt_setup_wns_col = col_offset + 1
            pt_setup_tns_col = col_offset + 2
            pt_hold_wns_col = col_offset + 4
            pv_lvs_col = col_offset + 7
            pv_drc_col = col_offset + 8
            pv_ant_col = col_offset + 9
            formal_col = col_offset + 27 + 16  # After PT(6) + PV(3) + ExtTiming(15) + Power(16)
            
            # Color code PT Setup WNS
            wns_cell = ws.cell(row=row_idx, column=pt_setup_wns_col)
            if data['pt_setup']['wns'] is not None:
                if data['pt_setup']['wns'] >= 0:
                    wns_cell.fill = green_fill
                elif data['pt_setup']['wns'] >= -0.050:
                    wns_cell.fill = yellow_fill
                else:
                    wns_cell.fill = red_fill
            
            # Color code PT Setup TNS
            tns_cell = ws.cell(row=row_idx, column=pt_setup_tns_col)
            if data['pt_setup']['tns'] is not None:
                if data['pt_setup']['tns'] >= -0.01:
                    tns_cell.fill = green_fill
                elif data['pt_setup']['tns'] >= -10.0:
                    tns_cell.fill = yellow_fill
                else:
                    tns_cell.fill = red_fill
            
            # Color code PT Hold WNS
            wns_cell = ws.cell(row=row_idx, column=pt_hold_wns_col)
            if data['pt_hold']['wns'] is not None:
                if data['pt_hold']['wns'] >= 0:
                    wns_cell.fill = green_fill
                elif data['pt_hold']['wns'] >= -0.050:
                    wns_cell.fill = yellow_fill
                else:
                    wns_cell.fill = red_fill
            
            # Color code PV errors
            for col_idx, metric in [(pv_lvs_col, 'lvs'), (pv_drc_col, 'drc'), (pv_ant_col, 'antenna')]:
                cell = ws.cell(row=row_idx, column=col_idx)
                if data['pv'][metric] is not None:
                    if data['pv'][metric] == 0:
                        cell.fill = green_fill
                    elif data['pv'][metric] < 10:
                        cell.fill = yellow_fill
                    else:
                        cell.fill = red_fill
            
            # Color code Formal status
            formal_cell = ws.cell(row=row_idx, column=formal_col)
            formal_status = data['formal']['status']
            if formal_status in ["SUCCEEDED", "PASS"]:
                formal_cell.fill = green_fill
            elif formal_status == "NOT_RUN":
                formal_cell.fill = gray_fill
            elif formal_status in ["FAILED", "FAIL", "ERROR"]:
                formal_cell.fill = red_fill
            else:
                formal_cell.fill = yellow_fill
        
        # Apply borders to all cells
        num_data_rows = len(ipo_data)
        for row in ws.iter_rows(min_row=data_start_row, max_row=data_start_row + num_data_rows - 1, min_col=1, max_col=total_cols):
            for cell in row:
                if not isinstance(cell, MergedCell):
                    cell.border = thin_border
                    if cell.column > 2:
                        cell.alignment = Alignment(horizontal="center", vertical="center")
        
        # Set column widths efficiently (fixed widths for performance)
        # Instead of iterating through all cells (slow), use fixed reasonable widths
        ws.column_dimensions['A'].width = 15  # IPO name
        ws.column_dimensions['B'].width = 12  # Cell Count
        ws.column_dimensions['C'].width = 12  # Utilization
        
        # Clock columns
        for i in range(4, 4 + num_clock_cols):
            ws.column_dimensions[get_column_letter(i)].width = 10
        
        # All other metric columns (PT, PV, External Timing, Power, Formal)
        for i in range(4 + num_clock_cols, total_cols + 1):
            ws.column_dimensions[get_column_letter(i)].width = 11
    
    @staticmethod
    def _populate_design_sheet(ws, design_name: str, ipo_data: dict, header_fill, header_font, center_align, thin_border, green_fill, yellow_fill, red_fill, gray_fill):
        """Populate per-design sheet with comprehensive IPO comparison (same format as single-workarea)"""
        # Note: This now uses the same comprehensive format as single-workarea mode
        # Just add a title row and call the comprehensive sheet population
        from openpyxl.styles import Font, PatternFill
        
        # Add title row
        ws.append([f"{design_name.upper()} - IPO Comparison"])
        
        # Populate the sheet using the comprehensive format
        WorkareaReviewer._populate_ipo_comparison_sheet_comprehensive(
            ws, ipo_data, header_fill, header_font, center_align, thin_border, 
            green_fill, yellow_fill, red_fill, gray_fill, 
            title_row_exists=True
        )
    
    @staticmethod
    def _create_combined_flow_history(ws, design_data: dict, header_fill, header_font, center_align, thin_border, green_fill, yellow_fill, red_fill, gray_fill):
        """Create combined flow run history from all designs"""
        from openpyxl.styles import Font, PatternFill, Alignment
        from openpyxl.utils import get_column_letter
        
        # Title
        ws.append(["Combined Flow Run History - All Designs"])
        ws.merge_cells("A1:J1")
        title_cell = ws["A1"]
        title_cell.font = Font(bold=True, size=14, color="FFFFFF")
        title_cell.fill = PatternFill(start_color="1F4E78", end_color="1F4E78", fill_type="solid")
        title_cell.alignment = center_align
        ws.append([])
        
        # Headers
        headers = ["Design", "IPO", "Flow Type", "Flow", "Run#", "Start Time", "End Time", "Duration", "Status", "Latest"]
        ws.append(headers)
        for cell in ws[3]:
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = center_align
            cell.border = thin_border
        
        # Collect all flow history entries
        all_entries = []
        for design_name in sorted(design_data.keys()):
            data = design_data[design_name]
            if data.get('error') or not data.get('ipo_data'):
                continue
            
            for ipo_name, metrics in data['ipo_data'].items():
                history = metrics.get('flow_history', [])
                for entry in history:
                    all_entries.append({
                        'design': design_name.upper(),
                        'ipo': ipo_name,
                        **entry
                    })
        
        # Sort by start time (most recent first)
        all_entries.sort(key=lambda x: x.get('start_time', ''), reverse=True)
        
        # Add data rows
        for entry in all_entries:
            flow_type = entry.get('flow_type', 'Other')
            row_data = [
                entry['design'],
                entry['ipo'],
                flow_type,
                entry.get('flow', '-'),
                entry.get('run_number', '-'),
                entry.get('start_time', '-'),
                entry.get('end_time', '-'),
                entry.get('duration', '-'),
                entry.get('status', 'UNKNOWN'),
                "LATEST" if entry.get('latest', False) else ""
            ]
            ws.append(row_data)
        
        # Apply formatting
        for row in ws.iter_rows(min_row=4, max_row=ws.max_row, min_col=1, max_col=10):
            for cell in row:
                cell.border = thin_border
                cell.alignment = Alignment(horizontal="center", vertical="center")
        
        # Set column widths
        widths = [12, 15, 20, 20, 8, 15, 15, 12, 12, 8]
        for idx, width in enumerate(widths, start=1):
            ws.column_dimensions[get_column_letter(idx)].width = width
    
    @staticmethod
    def _create_notes_sheet(ws, design_data_or_ipo_data: dict, is_multi: bool = False):
        """Create notes and metadata sheet"""
        from datetime import datetime
        
        ws.append(["IPO Comparison Report"])
        ws.append([])
        
        if is_multi:
            # Multi-workarea mode
            ws.append(["Multi-Workarea Comparison"])
            ws.append(["Generated:", datetime.now().strftime('%Y-%m-%d %H:%M:%S')])
            ws.append([])
            ws.append(["Designs Analyzed:"])
            for design_name in sorted(design_data_or_ipo_data.keys()):
                data = design_data_or_ipo_data[design_name]
                wa_path = data.get('workarea_path', '-')
                ipo_count = data.get('ipo_count', 0)
                ws.append([f"  {design_name.upper()}:", wa_path, f"({ipo_count} IPOs)"])
        else:
            # Single workarea mode
            ws.append(["Generated:", datetime.now().strftime('%Y-%m-%d %H:%M:%S')])
        
        ws.append([])
        ws.append(["Corner Definitions:"])
        ws.append(["  PT Setup:", "func.std_tt_0c_0p6v.setup.typical"])
        ws.append(["  PT Hold:", "func.std_tt_0c_0p55v.hold.typical"])
        ws.append(["  Power:", "func.std_tt_105c_0p67v.setup.typical"])
        ws.append([])
        ws.append(["Threshold Reference:"])
        ws.append(["  PT WNS Yellow:", f"Greater than or equal to {WorkareaReviewer.TIMING_THRESHOLDS['wns_yellow']} ns"])
        ws.append(["  PT WNS Green:", f"Greater than or equal to {WorkareaReviewer.TIMING_THRESHOLDS['wns_green']} ns"])
        ws.append(["  PT TNS Yellow:", f"Greater than or equal to {WorkareaReviewer.TIMING_THRESHOLDS['tns_yellow']} ns"])
        ws.append(["  PT TNS Green:", f"Greater than or equal to {WorkareaReviewer.TIMING_THRESHOLDS['tns_green']} ns"])
        ws.append(["  PV Errors Yellow:", f"Less than {WorkareaReviewer.PV_THRESHOLDS['errors_yellow']} errors"])
        ws.append(["  PV Errors Green:", f"Equal to {WorkareaReviewer.PV_THRESHOLDS['errors_green']} errors"])
        
        # Set column widths
        ws.column_dimensions['A'].width = 25
        ws.column_dimensions['B'].width = 60
        ws.column_dimensions['C'].width = 20
    
    def _generate_ipo_comparison_excel(self, ipo_data: dict) -> str:
        """
        Generate Excel report with consolidated comparison table
        
        Args:
            ipo_data: Complete IPO data structure
            
        Returns:
            str: Path to generated Excel file
        """
        try:
            import openpyxl
            from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
            from openpyxl.utils import get_column_letter
        except ImportError:
            print(f"{Color.RED}[ERROR] openpyxl not installed{Color.RESET}")
            print("  Install with: pip install openpyxl")
            raise
        
        # Create workbook
        wb = openpyxl.Workbook()
        
        # Sheet 1: Consolidated Comparison
        ws = wb.active
        ws.title = "IPO Comparison"
        
        # Define styles (needed for comprehensive method)
        header_fill = PatternFill(start_color="1F4E78", end_color="1F4E78", fill_type="solid")
        header_font = Font(bold=True, color="FFFFFF")
        center_align = Alignment(horizontal="center", vertical="center", wrap_text=True)
        thin_border = Border(
            left=Side(style='thin', color='000000'),
            right=Side(style='thin', color='000000'),
            top=Side(style='thin', color='000000'),
            bottom=Side(style='thin', color='000000')
        )
        green_fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")
        yellow_fill = PatternFill(start_color="FFEB9C", end_color="FFEB9C", fill_type="solid")
        red_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
        gray_fill = PatternFill(start_color="D9D9D9", end_color="D9D9D9", fill_type="solid")
        
        # Use comprehensive method to populate the sheet
        WorkareaReviewer._populate_ipo_comparison_sheet_comprehensive(
            ws, ipo_data, header_fill, header_font, center_align, thin_border,
            green_fill, yellow_fill, red_fill, gray_fill, title_row_exists=False
        )
        
        # Sheet 2: Flow Run History (Enhanced)
        ws2 = wb.create_sheet("Flow Run History")
        
        # Title
        ws2.append(["Flow Run History Summary"])
        ws2.merge_cells("A1:J1")
        title_cell = ws2["A1"]
        title_cell.font = Font(bold=True, size=14, color="FFFFFF")
        title_cell.fill = PatternFill(start_color="1F4E78", end_color="1F4E78", fill_type="solid")
        title_cell.alignment = Alignment(horizontal="center", vertical="center")
        ws2.append([])
        
        # Calculate summary statistics
        flow_stats = {}
        total_runs = 0
        for ipo_name, data in ipo_data.items():
            history = data.get("flow_history", [])
            for entry in history:
                flow = entry.get("flow", "Unknown")
                status = entry.get("status", "UNKNOWN")
                
                # Categorize flow type
                if "auto_pt" in flow or "PT" in flow:
                    flow_type = "PT (PrimeTime)"
                elif "star" in flow.lower():
                    flow_type = "Star (Parasitic)"
                elif "pv" in flow.lower() or "calibre" in flow.lower():
                    flow_type = "PV (Physical Verification)"
                elif "formal" in flow.lower():
                    flow_type = "Formal (Equivalence)"
                elif "eco" in flow.lower():
                    flow_type = "NV Gate ECO"
                else:
                    flow_type = "Other"
                
                if flow_type not in flow_stats:
                    flow_stats[flow_type] = {"total": 0, "completed": 0, "failed": 0, "running": 0}
                
                flow_stats[flow_type]["total"] += 1
                total_runs += 1
                
                if status == "COMPLETED":
                    flow_stats[flow_type]["completed"] += 1
                elif status in ["FAILED", "ERROR"]:
                    flow_stats[flow_type]["failed"] += 1
                elif status == "RUNNING":
                    flow_stats[flow_type]["running"] += 1
        
        # Add summary statistics
        ws2.append(["Flow Type", "Total Runs", "Completed", "Failed", "Running", "Success Rate"])
        summary_header_row = ws2.max_row
        for cell in ws2[summary_header_row]:
            cell.fill = PatternFill(start_color="D9E1F2", end_color="D9E1F2", fill_type="solid")
            cell.font = Font(bold=True)
            cell.alignment = center_align
            cell.border = thin_border
        
        for flow_type in sorted(flow_stats.keys()):
            stats = flow_stats[flow_type]
            success_rate = (stats["completed"] / stats["total"] * 100) if stats["total"] > 0 else 0
            ws2.append([
                flow_type,
                stats["total"],
                stats["completed"],
                stats["failed"],
                stats["running"],
                f"{success_rate:.1f}%"
            ])
            
            # Apply borders and formatting to summary data
            for cell in ws2[ws2.max_row]:
                cell.border = thin_border
                cell.alignment = Alignment(horizontal="center", vertical="center")
        
        # Add total row
        ws2.append([
            "TOTAL",
            total_runs,
            sum(s["completed"] for s in flow_stats.values()),
            sum(s["failed"] for s in flow_stats.values()),
            sum(s["running"] for s in flow_stats.values()),
            f"{(sum(s['completed'] for s in flow_stats.values()) / total_runs * 100):.1f}%" if total_runs > 0 else "0%"
        ])
        for cell in ws2[ws2.max_row]:
            cell.fill = PatternFill(start_color="E7E6E6", end_color="E7E6E6", fill_type="solid")
            cell.font = Font(bold=True)
            cell.border = thin_border
            cell.alignment = Alignment(horizontal="center", vertical="center")
        
        # Detailed history headers (no blank row before)
        detail_header_row = ws2.max_row + 1
        ws2.append(["IPO", "Flow Type", "Flow", "Run #", "Start Time", "End Time", "Duration", "Status", "Latest", "Work Dir"])
        
        # Format detail headers
        for cell in ws2[detail_header_row]:
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = center_align
            cell.border = thin_border
        
        # Collect and sort all history entries
        all_entries = []
        for ipo_name in sorted(ipo_data.keys()):
            history = ipo_data[ipo_name].get("flow_history", [])
            if history:
                for entry in history:
                    all_entries.append({
                        "ipo": ipo_name,
                        "flow": entry.get("flow", ""),
                        "run": entry.get("run", ""),
                        "work_dir": entry.get("work_dir", ""),
                        "start": entry.get("start", ""),
                        "end": entry.get("end", ""),
                        "duration": entry.get("duration", ""),
                        "status": entry.get("status", ""),
                        "eco_count": entry.get("eco_count", 0)
                    })
        
        # Mark latest runs per IPO per flow type
        latest_runs = {}
        for entry in all_entries:
            key = (entry["ipo"], entry["flow"])
            start_time = entry.get("start", "")
            if key not in latest_runs:
                latest_runs[key] = entry
            elif start_time and latest_runs[key].get("start"):
                if start_time > latest_runs[key]["start"]:
                    latest_runs[key] = entry
        
        # Sort entries: by IPO, then by start time (treat None/empty as oldest)
        all_entries.sort(key=lambda x: (x["ipo"], x.get("start") or "0000-00-00 00:00"))
        
        # Add data rows with enhanced formatting
        current_ipo = None
        row_num = detail_header_row + 1
        
        for entry in all_entries:
            # Categorize flow type
            flow = entry["flow"]
            if "auto_pt" in flow or "PT" in flow:
                flow_type = "PT"
            elif "star" in flow.lower():
                flow_type = "Star"
            elif "pv" in flow.lower() or "calibre" in flow.lower():
                flow_type = "PV"
            elif "formal" in flow.lower():
                flow_type = "Formal"
            elif "eco" in flow.lower():
                flow_type = "NV_ECO"
            else:
                flow_type = "Other"
            
            # Format duration to be more readable
            duration = entry["duration"]
            if duration and ":" in duration:
                try:
                    parts = duration.split(":")
                    if len(parts) == 3:
                        h, m, s = map(int, parts)
                        if h > 0:
                            duration = f"{h}h {m}m"
                        elif m > 0:
                            duration = f"{m}m {s}s"
                        else:
                            duration = f"{s}s"
                except:
                    pass
            
            # Check if this is the latest run
            is_latest = (entry["ipo"], entry["flow"]) in latest_runs and latest_runs[(entry["ipo"], entry["flow"])] == entry
            latest_marker = "LATEST" if is_latest else ""
            
            # Format flow name with ECO count if applicable
            flow_display = entry["flow"]
            eco_count = entry.get("eco_count", 0)
            if flow_type == "PT" and eco_count > 0:
                flow_display = f"{entry['flow']} (ECO: {eco_count})"
            
            ws2.append([
                entry["ipo"],
                flow_type,
                flow_display,
                entry["run"],
                entry["start"],
                entry["end"],
                duration,
                entry["status"],
                latest_marker,
                entry["work_dir"]
            ])
            
            # Apply formatting to data row
            status = entry["status"]
            status_colors = {
                "COMPLETED": green_fill,
                "RUNNING": yellow_fill,
                "FAILED": red_fill,
                "ERROR": red_fill,
                "UNKNOWN": gray_fill
            }
            
            # Alternate row colors for readability
            if current_ipo != entry["ipo"]:
                current_ipo = entry["ipo"]
                row_color = PatternFill(start_color="F2F2F2", end_color="F2F2F2", fill_type="solid")
            else:
                row_color = None
            
            for col_idx, cell in enumerate(ws2[row_num], start=1):
                cell.border = thin_border
                cell.alignment = Alignment(horizontal="left", vertical="center")
                
                # Apply row background color
                if row_color and col_idx not in [8, 9]:  # Don't override status and latest columns
                    cell.fill = row_color
                
                # Apply status color to Status column
                if col_idx == 8:  # Status column
                    cell.fill = status_colors.get(status, gray_fill)
                    cell.alignment = Alignment(horizontal="center", vertical="center")
                
                # Center align Latest column
                if col_idx == 9:  # Latest column
                    cell.alignment = Alignment(horizontal="center", vertical="center")
                    if latest_marker:
                        cell.font = Font(bold=True, color="008000")
            
            row_num += 1
        
        # Enable auto-filter
        ws2.auto_filter.ref = f"A{detail_header_row}:J{ws2.max_row}"
        
        # Set column widths
        ws2.column_dimensions['A'].width = 15  # IPO
        ws2.column_dimensions['B'].width = 12  # Flow Type
        ws2.column_dimensions['C'].width = 20  # Flow
        ws2.column_dimensions['D'].width = 8   # Run #
        ws2.column_dimensions['E'].width = 16  # Start Time
        ws2.column_dimensions['F'].width = 16  # End Time
        ws2.column_dimensions['G'].width = 10  # Duration
        ws2.column_dimensions['H'].width = 12  # Status
        ws2.column_dimensions['I'].width = 8   # Latest
        ws2.column_dimensions['J'].width = 40  # Work Dir
        
        # Freeze panes (freeze header + summary section)
        ws2.freeze_panes = f"A{detail_header_row + 1}"
        
        # Sheet 3: Notes & Metadata
        ws3 = wb.create_sheet("Notes & Metadata")
        ws3.append(["IPO Comparison Report"])
        ws3.append([])
        ws3.append(["Workarea:", self.workarea])
        ws3.append(["Design:", self.design_info.top_hier])
        ws3.append(["Generated:", datetime.now().strftime('%Y-%m-%d %H:%M:%S')])
        ws3.append([])
        ws3.append(["Corner Definitions:"])
        ws3.append(["  Setup:", "func.std_tt_0c_0p6v.setup.typical"])
        ws3.append(["  Hold:", "func.std_tt_0c_0p55v.hold.typical"])
        ws3.append([])
        
        # Add Available Power Tests section
        ws3.append(["Available Power Tests:"])
        ws3.append([])
        ws3.append(["IPO", "Selected Tests", "All Available Tests"])
        
        # Format header row
        header_row_num = ws3.max_row
        for cell in ws3[header_row_num]:
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = center_align
            cell.border = thin_border
        
        # Add test data for each IPO
        for ipo_name in sorted(ipo_data.keys()):
            power_data = ipo_data[ipo_name]['power']
            test1_name = power_data.get('test1_name', '-')
            test2_name = power_data.get('test2_name', '-')
            available_tests = power_data.get('available_tests', [])
            
            # Format selected tests
            if test1_name and test2_name:
                selected = f"{test1_name}, {test2_name}"
            elif test1_name:
                selected = test1_name
            else:
                selected = "No tests found"
            
            # Format available tests
            if available_tests:
                all_tests = ", ".join(available_tests)
            else:
                all_tests = "No tests found"
            
            ws3.append([ipo_name, selected, all_tests])
            
            # Apply borders to data row
            data_row_num = ws3.max_row
            for cell in ws3[data_row_num]:
                cell.border = thin_border
                cell.alignment = Alignment(horizontal="left", vertical="center", wrap_text=True)
        
        ws3.append([])
        ws3.append(["Contact:", "avice@nvidia.com"])
        
        # Adjust column widths for Sheet 3
        ws3.column_dimensions['A'].width = 20
        ws3.column_dimensions['B'].width = 40
        ws3.column_dimensions['C'].width = 60
        
        # Generate filename
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        user = os.environ.get('USER', 'avice')
        design = self.design_info.top_hier
        excel_filename = f"{user}_IPO_comparison_{design}_{timestamp}.xlsx"
        excel_path = os.path.join(os.getcwd(), excel_filename)
        
        # Save workbook
        wb.save(excel_path)
        
        return excel_path
    
    def _print_ipo_comparison_summary(self, ipo_data: dict) -> None:
        """
        Print terminal summary tables
        
        Args:
            ipo_data: Complete IPO data structure
        """
        # Collect all clock names
        all_clocks = set()
        for data in ipo_data.values():
            if data.get("clock_latencies"):
                all_clocks.update(data["clock_latencies"].keys())
        sorted_clocks = sorted(all_clocks)
        
        print("Main Metrics Comparison:")
        print(f"  {'IPO':<15} {'Cells':<10} {'Util':<8} {'PT-Setup':<15} {'PT-Hold':<15} {'PV':<14} {'Formal':<10}")
        print(f"  {'':<15} {'Count':<10} {'(%)':<8} {'WNS/TNS/NVP':<15} {'WNS/TNS/NVP':<15} {'LVS/DRC/ANT':<14} {'Status':<10}")
        print(f"  {'-'*15} {'-'*10} {'-'*8} {'-'*15} {'-'*15} {'-'*14} {'-'*10}")
        
        for ipo_name in sorted(ipo_data.keys()):
            data = ipo_data[ipo_name]
            
            # Format cell count with comma
            cell_count_str = f"{data['cell_count']:,}" if data['cell_count'] else "-"
            
            # Format utilization
            util_str = self._format_number_smart(data.get('utilization'), 2) if data.get('utilization') else "-"
            
            # Format PT setup
            setup_wns = self._format_number_smart(data['pt_setup']['wns'], 3)
            setup_tns = self._format_number_smart(data['pt_setup']['tns'], 2)
            setup_nvp = str(data['pt_setup']['nvp']) if data['pt_setup']['nvp'] is not None else "-"
            setup_str = f"{setup_wns}/{setup_tns}/{setup_nvp}"
            
            # Format PT hold
            hold_wns = self._format_number_smart(data['pt_hold']['wns'], 3)
            hold_tns = self._format_number_smart(data['pt_hold']['tns'], 2)
            hold_nvp = str(data['pt_hold']['nvp']) if data['pt_hold']['nvp'] is not None else "-"
            hold_str = f"{hold_wns}/{hold_tns}/{hold_nvp}"
            
            # Format PV
            pv_lvs = str(data['pv']['lvs']) if data['pv']['lvs'] is not None else "-"
            pv_drc = str(data['pv']['drc']) if data['pv']['drc'] is not None else "-"
            pv_ant = str(data['pv']['antenna']) if data['pv']['antenna'] is not None else "-"
            pv_str = f"{pv_lvs}/{pv_drc}/{pv_ant}"
            
            # Format formal
            formal_str = data['formal']['status']
            
            print(f"  {ipo_name:<15} {cell_count_str:<10} {util_str:<8} {setup_str:<15} {hold_str:<15} {pv_str:<14} {formal_str:<10}")
        
        # Print clock latency table if clocks exist
        if sorted_clocks:
            print()
            print("Clock Latency Comparison (ps):")
            
            # Build dynamic header
            header = f"  {'IPO':<15}"
            for clock in sorted_clocks:
                header += f" {clock:<12}"
            print(header)
            
            # Build separator
            sep = f"  {'-'*15}"
            for _ in sorted_clocks:
                sep += f" {'-'*12}"
            print(sep)
            
            # Print data rows
            for ipo_name in sorted(ipo_data.keys()):
                data = ipo_data[ipo_name]
                row = f"  {ipo_name:<15}"
                clock_lats = data.get("clock_latencies", {})
                for clock in sorted_clocks:
                    if clock in clock_lats:
                        row += f" {self._format_number_smart(clock_lats[clock], 1):>12}"
                    else:
                        row += f" {'-':>12}"
                print(row)
        
        print()
        print("External Timing (NV Gate ECO - Innovus):")
        print(f"  {'IPO':<15} {'FEEDTHROUGH':<16} {'REGIN':<16} {'REGOUT':<16} {'Port2Cgate':<16} {'TOTAL':<16}")
        print(f"  {'':<15} {'WNS/TNS/NVP':<16} {'WNS/TNS/NVP':<16} {'WNS/TNS/NVP':<16} {'WNS/TNS/NVP':<16} {'WNS/TNS/NVP':<16}")
        print(f"  {'-'*15} {'-'*16} {'-'*16} {'-'*16} {'-'*16} {'-'*16}")
        
        for ipo_name in sorted(ipo_data.keys()):
            data = ipo_data[ipo_name]
            ext = data['external_timing']
            
            # Format FEEDTHROUGH
            ft_wns = self._format_number_smart(ext['feedthrough']['wns'], 3)
            ft_tns = self._format_number_smart(ext['feedthrough']['tns'], 2)
            ft_nvp = str(ext['feedthrough']['nvp']) if ext['feedthrough']['nvp'] is not None else "-"
            ft_str = f"{ft_wns}/{ft_tns}/{ft_nvp}" if ext['feedthrough']['wns'] is not None else "NOT_RUN"
            
            # Format REGIN
            ri_wns = self._format_number_smart(ext['regin']['wns'], 3)
            ri_tns = self._format_number_smart(ext['regin']['tns'], 2)
            ri_nvp = str(ext['regin']['nvp']) if ext['regin']['nvp'] is not None else "-"
            ri_str = f"{ri_wns}/{ri_tns}/{ri_nvp}" if ext['regin']['wns'] is not None else "NOT_RUN"
            
            # Format REGOUT
            ro_wns = self._format_number_smart(ext['regout']['wns'], 3)
            ro_tns = self._format_number_smart(ext['regout']['tns'], 2)
            ro_nvp = str(ext['regout']['nvp']) if ext['regout']['nvp'] is not None else "-"
            ro_str = f"{ro_wns}/{ro_tns}/{ro_nvp}" if ext['regout']['wns'] is not None else "NOT_RUN"
            
            # Format Port2Cgate
            pc_wns = self._format_number_smart(ext['port_cgate']['wns'], 3)
            pc_tns = self._format_number_smart(ext['port_cgate']['tns'], 2)
            pc_nvp = str(ext['port_cgate']['nvp']) if ext['port_cgate']['nvp'] is not None else "-"
            pc_str = f"{pc_wns}/{pc_tns}/{pc_nvp}" if ext['port_cgate']['wns'] is not None else "NOT_RUN"
            
            # Calculate TOTAL (sum of all external timing NVP values)
            total_wns = None
            total_tns = 0.0
            total_nvp = 0
            has_data = False
            
            for cat in ['feedthrough', 'regin', 'regout', 'port_cgate']:
                if ext[cat]['wns'] is not None:
                    has_data = True
                    # Track worst (most negative) WNS
                    if total_wns is None or ext[cat]['wns'] < total_wns:
                        total_wns = ext[cat]['wns']
                    # Sum TNS and NVP
                    total_tns += ext[cat]['tns']
                    total_nvp += ext[cat]['nvp']
            
            if has_data:
                total_wns_str = self._format_number_smart(total_wns, 3)
                total_tns_str = self._format_number_smart(total_tns, 2)
                total_nvp_str = str(total_nvp)
                total_str = f"{total_wns_str}/{total_tns_str}/{total_nvp_str}"
            else:
                total_str = "NOT_RUN"
            
            print(f"  {ipo_name:<15} {ft_str:<16} {ri_str:<16} {ro_str:<16} {pc_str:<16} {total_str:<16}")
        
        # Print Power Analysis tables
        print()
        print("Power Analysis (func.std_tt_105c_0p67v.setup.typical):")
        
        # Print available tests for each IPO
        print()
        print("  Available Power Tests:")
        print(f"  {'IPO':<15} Available Tests")
        print(f"  {'-'*15} {'-'*60}")
        for ipo_name in sorted(ipo_data.keys()):
            data = ipo_data[ipo_name]
            available = data['power'].get('available_tests', [])
            if available:
                tests_str = ", ".join(available)
                print(f"  {ipo_name:<15} {tests_str}")
            else:
                print(f"  {ipo_name:<15} No power data found")
        
        # Get test names from first IPO with power data
        test1_name = 'Test 1'
        test2_name = 'Test 2'
        for ipo_name in sorted(ipo_data.keys()):
            power_data = ipo_data[ipo_name]['power']
            if power_data.get('status') == 'FOUND' and power_data.get('test1_name'):
                test1_name = power_data.get('test1_name')
                test2_name = power_data.get('test2_name') or 'Test 2'
                break
        
        # Print Test 1 comparison
        print()
        print(f"  Test: {test1_name}")
        print(f"  {'IPO':<15} {'Total':<10} {'Dynamic':<10} {'Leakage':<10} {'Ann.Score':<10} {'AF (FF/Q)':<12} {'VT Mix':<15}")
        print(f"  {'':<15} {'(mW)':<10} {'(mW)':<10} {'(mW)':<10} {'(%)':<10} {'(%)':<12} {'HVT/SVT/LVT':<15}")
        print(f"  {'-'*15} {'-'*10} {'-'*10} {'-'*10} {'-'*10} {'-'*12} {'-'*15}")
        
        for ipo_name in sorted(ipo_data.keys()):
            data = ipo_data[ipo_name]
            test1 = data['power']['test1']
            
            # Format power metrics
            total_pwr = f"{test1['total_power']:.1f}" if test1['total_power'] is not None else "-"
            dyn_pwr = f"{test1['dynamic_power']:.1f}" if test1['dynamic_power'] is not None else "-"
            leak_pwr = f"{test1['leakage_power']:.1f}" if test1['leakage_power'] is not None else "-"
            ann_score = f"{test1['annotation_score']:.2f}" if test1['annotation_score'] is not None else "-"
            avg_af = f"{test1['avg_af']:.2f}" if test1['avg_af'] is not None else "-"
            
            # Format VT mix
            if test1['hvt_pct'] is not None:
                hvt = f"{test1['hvt_pct']:.0f}"
                svt = f"{test1['svt_pct']:.0f}"
                lvt = f"{test1['lvt_pct']:.0f}"
                vt_mix = f"{hvt}/{svt}/{lvt}"
            else:
                vt_mix = "-"
            
            print(f"  {ipo_name:<15} {total_pwr:<10} {dyn_pwr:<10} {leak_pwr:<10} {ann_score:<10} {avg_af:<12} {vt_mix:<15}")
        
        # Print Test 2 comparison (if test2 exists)
        if test2_name:
            print()
            print(f"  Test: {test2_name}")
            print(f"  {'IPO':<15} {'Total':<10} {'Dynamic':<10} {'Leakage':<10} {'Ann.Score':<10} {'AF (FF/Q)':<12} {'VT Mix':<15}")
            print(f"  {'':<15} {'(mW)':<10} {'(mW)':<10} {'(mW)':<10} {'(%)':<10} {'(%)':<12} {'HVT/SVT/LVT':<15}")
            print(f"  {'-'*15} {'-'*10} {'-'*10} {'-'*10} {'-'*10} {'-'*12} {'-'*15}")
            
            for ipo_name in sorted(ipo_data.keys()):
                data = ipo_data[ipo_name]
                test2 = data['power']['test2']
                
                # Format power metrics
                total_pwr = f"{test2['total_power']:.1f}" if test2['total_power'] is not None else "-"
                dyn_pwr = f"{test2['dynamic_power']:.1f}" if test2['dynamic_power'] is not None else "-"
                leak_pwr = f"{test2['leakage_power']:.1f}" if test2['leakage_power'] is not None else "-"
                ann_score = f"{test2['annotation_score']:.2f}" if test2['annotation_score'] is not None else "-"
                avg_af = f"{test2['avg_af']:.2f}" if test2['avg_af'] is not None else "-"
                
                # Format VT mix
                if test2['hvt_pct'] is not None:
                    hvt = f"{test2['hvt_pct']:.0f}"
                    svt = f"{test2['svt_pct']:.0f}"
                    lvt = f"{test2['lvt_pct']:.0f}"
                    vt_mix = f"{hvt}/{svt}/{lvt}"
                else:
                    vt_mix = "-"
                
                print(f"  {ipo_name:<15} {total_pwr:<10} {dyn_pwr:<10} {leak_pwr:<10} {ann_score:<10} {avg_af:<12} {vt_mix:<15}")
    
    def _send_ipo_comparison_email(self, excel_file: str, recipients: str) -> None:
        """
        Send Excel report via email using SMTP
        
        Args:
            excel_file: Path to Excel file
            recipients: Email address(es), comma-separated
        """
        print(f"{Color.CYAN}Sending Excel report via email...{Color.RESET}")
        
        try:
            import smtplib
            from email.mime.multipart import MIMEMultipart
            from email.mime.base import MIMEBase
            from email.mime.text import MIMEText
            from email import encoders
            from pathlib import Path
            
            # Get sender email
            from_email = f"{os.environ.get('USER', 'avice')}@nvidia.com"
            
            # Split recipients if comma-separated
            recipient_list = [r.strip() for r in recipients.split(',')]
            to_email = recipient_list[0]
            cc_list = recipient_list[1:] if len(recipient_list) > 1 else []
            
            # Prepare email content
            subject = f"IPO Comparison Report - {self.design_info.top_hier}"
            
            file_size_kb = os.path.getsize(excel_file) / 1024
            body = f"""IPO Comparison Report

Design: {self.design_info.top_hier}
Workarea: {self.workarea}
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

Detected IPOs: {', '.join(sorted(self.nbu_signoff_paths.keys()))}

Excel File: {os.path.basename(excel_file)}
File Size: {file_size_kb:.1f} KB

See attached Excel file for detailed comparison.

---
Generated by Avice Workarea Review Tool
Contact: avice@nvidia.com
"""
            
            # Create message
            msg = MIMEMultipart()
            msg['From'] = from_email
            msg['To'] = to_email
            msg['Subject'] = subject
            
            # Add CC if provided
            if cc_list:
                msg['Cc'] = ', '.join(cc_list)
                all_recipients = [to_email] + cc_list
            else:
                all_recipients = [to_email]
            
            # Add body
            msg.attach(MIMEText(body, 'plain'))
            
            # Attach Excel file
            excel_path = Path(excel_file)
            with open(excel_path, 'rb') as f:
                part = MIMEBase('application', 'vnd.openxmlformats-officedocument.spreadsheetml.sheet')
                part.set_payload(f.read())
                encoders.encode_base64(part)
                part.add_header(
                    'Content-Disposition',
                    f'attachment; filename={excel_path.name}'
                )
                msg.attach(part)
            
            # Send email via SMTP
            smtp_server = "smtp.nvidia.com"
            smtp_port = 25
            
            with smtplib.SMTP(smtp_server, smtp_port, timeout=30) as server:
                server.sendmail(from_email, all_recipients, msg.as_string())
            
            print(f"{Color.GREEN}[OK] Email sent successfully to: {', '.join(all_recipients)}{Color.RESET}")
            
        except ImportError as e:
            print(f"{Color.RED}[ERROR] Missing required module for email: {e}{Color.RESET}")
            print(f"{Color.YELLOW}  Email functionality requires standard Python libraries (smtplib, email){Color.RESET}")
        except Exception as e:
            print(f"{Color.YELLOW}[WARN] Failed to send email: {e}{Color.RESET}")
            import traceback
            traceback.print_exc()
    
    @staticmethod
    def _send_multi_workarea_email(excel_file: str, recipients: str, design_data: dict) -> None:
        """
        Send multi-workarea Excel report via email using SMTP
        
        Args:
            excel_file: Path to Excel file
            recipients: Email address(es), comma-separated
            design_data: Dictionary of design data
        """
        try:
            import smtplib
            from email.mime.multipart import MIMEMultipart
            from email.mime.base import MIMEBase
            from email.mime.text import MIMEText
            from email import encoders
            from pathlib import Path
            from datetime import datetime
            
            # Get sender email
            from_email = f"{os.environ.get('USER', 'avice')}@nvidia.com"
            
            # Split recipients if comma-separated
            recipient_list = [r.strip() for r in recipients.split(',')]
            to_email = recipient_list[0]
            cc_list = recipient_list[1:] if len(recipient_list) > 1 else []
            
            # Prepare email content
            design_names = ', '.join(sorted(design_data.keys()))
            subject = f"Multi-Workarea IPO Comparison Report - {design_names}"
            
            file_size_kb = os.path.getsize(excel_file) / 1024
            
            # Build design summary
            design_summary = []
            for design_name in sorted(design_data.keys()):
                ipo_count = design_data[design_name].get('ipo_count', 0)
                workarea_path = design_data[design_name].get('workarea_path', 'Unknown')
                design_summary.append(f"  - {design_name}: {ipo_count} IPOs ({workarea_path})")
            
            body = f"""Multi-Workarea IPO Comparison Report

Designs Analyzed: {design_names}
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

Design Summary:
{chr(10).join(design_summary)}

Excel File: {os.path.basename(excel_file)}
File Size: {file_size_kb:.1f} KB

See attached Excel file for detailed comparison across all designs.

---
Generated by Avice Workarea Review Tool
Contact: avice@nvidia.com
"""
            
            # Create message
            msg = MIMEMultipart()
            msg['From'] = from_email
            msg['To'] = to_email
            msg['Subject'] = subject
            
            # Add CC if provided
            if cc_list:
                msg['Cc'] = ', '.join(cc_list)
                all_recipients = [to_email] + cc_list
            else:
                all_recipients = [to_email]
            
            # Add body
            msg.attach(MIMEText(body, 'plain'))
            
            # Attach Excel file
            excel_path = Path(excel_file)
            with open(excel_path, 'rb') as f:
                part = MIMEBase('application', 'vnd.openxmlformats-officedocument.spreadsheetml.sheet')
                part.set_payload(f.read())
                encoders.encode_base64(part)
                part.add_header(
                    'Content-Disposition',
                    f'attachment; filename={excel_path.name}'
                )
                msg.attach(part)
            
            # Send email via SMTP
            smtp_server = "smtp.nvidia.com"
            smtp_port = 25
            
            with smtplib.SMTP(smtp_server, smtp_port, timeout=30) as server:
                server.sendmail(from_email, all_recipients, msg.as_string())
            
            print(f"{Color.GREEN}[OK] Email sent successfully to: {', '.join(all_recipients)}{Color.RESET}")
            
        except ImportError as e:
            print(f"{Color.RED}[ERROR] Missing required module for email: {e}{Color.RESET}")
            print(f"{Color.YELLOW}  Email functionality requires standard Python libraries (smtplib, email){Color.RESET}")
        except Exception as e:
            print(f"{Color.YELLOW}[WARN] Failed to send email: {e}{Color.RESET}")
            import traceback
            traceback.print_exc()
    
    def run_complete_review(self) -> None:
        """Run complete workarea review"""
        # Display logo if enabled
        if self.show_logo:
            LogoDisplay.print_ascii_logo()
            # Try to display the actual logo image
            LogoDisplay.display_logo()
        
        # Run all analysis stages
        self.run_setup_analysis()
        self.run_runtime_analysis()
        self.run_synthesis_analysis()
        self.run_pnr_analysis()
        self.run_clock_analysis()
        self.run_formal_verification()
        self.run_parasitic_extraction()
        self.run_signoff_timing()
        self.run_physical_verification()
        self.run_gl_check()
        self.run_eco_analysis()
        self.run_nv_gate_eco()
        self.run_block_release()
        
        # Generate Master Dashboard (always print these messages)
        if self.quiet_mode:
            self.quiet_mode.print_always(f"\n{Color.CYAN}Generating Master Dashboard...{Color.RESET}")
        else:
            print(f"\n{Color.CYAN}Generating Master Dashboard...{Color.RESET}")
        
        try:
            dashboard_path = self.master_dashboard.generate_html()
            dashboard_filename = os.path.basename(dashboard_path)
            
            # Determine display path (will be moved to html/ or test_outputs/html/ by _organize_html_files)
            html_output_dir = self._get_html_output_dir()
            display_path = os.path.relpath(html_output_dir, os.getcwd())
            
            if self.quiet_mode:
                self.quiet_mode.print_always(f"Open with: /home/utils/firefox-118.0.1/firefox {Color.MAGENTA}{display_path}/{dashboard_filename}{Color.RESET} &")
            else:
                print(f"Open with: /home/utils/firefox-118.0.1/firefox {Color.MAGENTA}{display_path}/{dashboard_filename}{Color.RESET} &")
        except Exception as e:
            if self.quiet_mode:
                self.quiet_mode.print_always(f"{Color.RED}[ERROR] Failed to generate Master Dashboard: {e}{Color.RESET}")
            else:
                print(f"{Color.RED}[ERROR] Failed to generate Master Dashboard: {e}{Color.RESET}")
        
        if not self.quiet:
            print(f"\n{Color.GREEN}Review completed successfully!{Color.RESET}")

    def run_runtime_analysis(self) -> None:
        """Run runtime analysis"""
        self.print_header(FlowStage.RUNTIME)
        
        # Determine which IPOs to analyze
        # If user specified -i, filter to only that IPO (but still show data from logs if dir was deleted)
        ipos_to_analyze = self.design_info.all_ipos
        deleted_ipos = []  # Track which IPOs have deleted directories
        
        # Check which IPO directories exist vs deleted
        pnr_base_path = os.path.join(self.workarea, f"pnr_flow/nv_flow/{self.design_info.top_hier}")
        for ipo in self.design_info.all_ipos:
            ipo_dir = os.path.join(pnr_base_path, ipo)
            if not os.path.isdir(ipo_dir):
                deleted_ipos.append(ipo)
        
        if self.user_specified_ipo and self.ipo:
            # User specified an IPO - filter to only that one
            user_ipo = self.design_info.ipo  # This is the IPO from -i flag
            
            # Check if this IPO exists in the list of known IPOs (from prc.status or directories)
            if user_ipo in self.design_info.all_ipos:
                ipos_to_analyze = [user_ipo]
            else:
                # IPO not in known list - might have runtime data in logs even if dir deleted
                ipos_to_analyze = [user_ipo]
            
            # Check if IPO directory was deleted
            ipo_dir = os.path.join(pnr_base_path, user_ipo)
            if not os.path.isdir(ipo_dir):
                print(f"\n{Color.YELLOW}[INFO] IPO directory '{user_ipo}' was deleted (showing runtime from historical logs){Color.RESET}")
            else:
                print(f"\n{Color.CYAN}[INFO] Filtering runtime to specified IPO: {user_ipo}{Color.RESET}")
        else:
            # No -i flag - show all IPOs, but note which ones were deleted
            if deleted_ipos:
                print(f"\n{Color.YELLOW}[INFO] Deleted IPO directories (data from historical logs): {', '.join(sorted(deleted_ipos))}{Color.RESET}")
        
        # Collect runtime data for summary table
        runtime_data = {}
        runtime_timestamps = {}  # Store start/end timestamps for each stage
        
        # Check for fast_dc first
        fast_dc_log = os.path.join(self.workarea, "syn_flow/fast_dc/log/fast_dc.log")
        fast_dc_detected = os.path.exists(fast_dc_log)
        
        if fast_dc_detected:
            print(f"\n{Color.CYAN}Fast DC detected - analyzing both fast_dc and full dc runs{Color.RESET}")
            print(f"  Note: PnR start times adjusted to setup step (full PnR run after fast_dc)")
            
            # Fast DC runtime
            try:
                result = self.file_utils.run_command(f"grep 'Elapsed time' {fast_dc_log} | sed -e 's/.*( //' -e 's/ ).//' | tail -n1")
                full_elapsed_line = self.file_utils.run_command(f"grep 'Elapsed time' {fast_dc_log} | tail -n1")
                
                if result.strip():
                    runtime_data['Fast DC'] = result.strip()
                    
                    # Extract Fast DC timestamps
                    try:
                        end_time = os.path.getmtime(fast_dc_log)
                        duration_match = re.search(r'(\d+)\s+seconds', full_elapsed_line)
                        if duration_match:
                            duration_seconds = int(duration_match.group(1))
                            start_time = end_time - duration_seconds
                            
                            start_str = time.strftime("%m/%d %H:%M", time.localtime(start_time))
                            end_str = time.strftime("%m/%d %H:%M", time.localtime(end_time))
                            runtime_timestamps['Fast DC'] = (start_str, end_str)
                    except Exception:
                        pass
            except Exception as e:
                print(f"  Error extracting Fast DC runtime: {e}")

        # DC runtime
        dc_log = os.path.join(self.workarea, "syn_flow/dc/log/dc.log")
        if os.path.exists(dc_log):
            try:
                dc_key = "Full DC" if fast_dc_detected else "DC"
                
                # Check if DC is still running by looking for:
                # 1. Recent file modification (within last 10 minutes)
                # 2. No "Thank you" completion message in last 50 lines
                current_time = time.time()
                log_mtime = os.path.getmtime(dc_log)
                minutes_since_update = (current_time - log_mtime) / 60
                
                # Check for completion message in last 50 lines
                tail_result = self.file_utils.run_command(f"tail -50 {dc_log}")
                dc_completed = False
                if 'Thank you for using' in tail_result or 'Exiting...' in tail_result:
                    dc_completed = True
                
                # DC is running if: modified recently (< 10 min) AND no completion message
                dc_is_running = minutes_since_update < 10 and not dc_completed
                
                if dc_is_running:
                    # DC is still running - calculate elapsed time from actual start time
                    # Look for "Current time:" pattern which shows DC session start
                    # Pattern: "Current time:       Mon Jan 12 14:51:35 2026"
                    start_time_line = self.file_utils.run_command(f"grep 'Current time:' {dc_log} | head -1")
                    start_str = None
                    start_epoch = None
                    
                    if start_time_line.strip():
                        # Extract start time from "Current time:       Mon Jan 12 14:51:35 2026"
                        date_match = re.search(r'(\w+)\s+(\w+)\s+(\d+)\s+(\d+):(\d+):(\d+)\s+(\d+)', start_time_line)
                        if date_match:
                            month_map = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06',
                                       'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'}
                            month = month_map.get(date_match.group(2), '01')
                            day = date_match.group(3)
                            hour = date_match.group(4)
                            minute = date_match.group(5)
                            second = date_match.group(6)
                            year = date_match.group(7)
                            start_str = f"{month}/{int(day):02d} {hour}:{minute}"
                            
                            # Calculate start epoch for accurate elapsed time
                            try:
                                start_datetime = f"{year}-{month}-{day} {hour}:{minute}:{second}"
                                start_struct = time.strptime(start_datetime, "%Y-%m-%d %H:%M:%S")
                                start_epoch = time.mktime(start_struct)
                            except:
                                pass
                    
                    # Calculate elapsed time from start to now
                    if start_epoch:
                        elapsed_hours = (current_time - start_epoch) / 3600
                    else:
                        elapsed_hours = minutes_since_update / 60  # Fallback
                    
                    # Store as RUNNING with calculated elapsed time
                    runtime_data[dc_key] = f"{elapsed_hours:.2f} hours (RUNNING)"
                    # Store actual log mtime for stall detection (not start time!)
                    runtime_data[f'{dc_key}_log_mtime'] = time.strftime("%m/%d %H:%M", time.localtime(log_mtime))
                    end_str = "RUNNING"
                    if start_str:
                        runtime_timestamps[dc_key] = (start_str, end_str)
                    else:
                        runtime_timestamps[dc_key] = (time.strftime("%m/%d %H:%M", time.localtime(current_time - elapsed_hours * 3600)), end_str)
                else:
                    # DC completed - get the last "Elapsed time" entry
                    result = self.file_utils.run_command(f"grep 'Elapsed time' {dc_log} | sed -e 's/.*( //' -e 's/ ).//' | tail -n1")
                    full_elapsed_line = self.file_utils.run_command(f"grep 'Elapsed time' {dc_log} | tail -n1")
                    
                    if result.strip():
                        runtime_data[dc_key] = result.strip()
                        # Extract DC timestamps - calculate start time from end time and duration
                        try:
                            # Get file modification time as end time
                            end_time = log_mtime
                            
                            # Extract duration in seconds from the full elapsed time line
                            duration_match = re.search(r'(\d+)\s+seconds', full_elapsed_line)
                            if duration_match:
                                duration_seconds = int(duration_match.group(1))
                                start_time = end_time - duration_seconds
                                
                                start_str = time.strftime("%m/%d %H:%M", time.localtime(start_time))
                                end_str = time.strftime("%m/%d %H:%M", time.localtime(end_time))
                                runtime_timestamps[dc_key] = (start_str, end_str)
                            else:
                                # Fallback to file timestamps
                                start, end = self._extract_timestamps_from_log(dc_log)
                                if start and end:
                                    runtime_timestamps[dc_key] = (start, end)
                        except Exception:
                            # Fallback to file timestamps
                            start, end = self._extract_timestamps_from_log(dc_log)
                            if start and end:
                                runtime_timestamps[dc_key] = (start, end)
            except Exception as e:
                print(f"  Error extracting DC runtime: {e}")
        
        # PnR runtime for each IPO
        pnr_runtimes = {}
        # Find ALL prc.status files (some workareas have multiple: {unit}.prc.status + eco_{unit}.prc.status)
        all_prc_status_files = self._find_all_prc_status_files()
        prc_status = all_prc_status_files[0] if all_prc_status_files else None
        if not prc_status:
            # Fallback to default naming pattern
            prc_status = os.path.abspath(os.path.join(self.workarea, f"pnr_flow/nv_flow/{self.design_info.top_hier}.prc.status"))
            all_prc_status_files = [prc_status] if os.path.exists(prc_status) else []
        
        for ipo in ipos_to_analyze:
            # Search ALL prc.status files for this IPO's data
            ipo_found = False
            for prc_file in all_prc_status_files:
                if not os.path.exists(prc_file) or ipo_found:
                    continue
                try:
                    # Calculate PnR runtime (BEGIN to postroute only, exclude reporting and post-postroute stages)
                    result = self.file_utils.run_command(f"grep ' {ipo}' {prc_file} | grep -E '(BEGIN|setup|edi_plan|place|cts|route|postroute)' | grep -v report | sed 's/# //' | awk '{{print $5}}' | column -t")
                    if result.strip():
                        try:
                            total_runtime_seconds = sum(int(x) for x in result.strip().split() if x.isdigit())
                            total_runtime_hours = total_runtime_seconds / 3600
                            total_runtime_days = total_runtime_hours / 24
                            
                            if total_runtime_hours >= 24:
                                runtime_str = f"{total_runtime_hours:.2f} hours ({total_runtime_days:.2f} days)"
                            else:
                                runtime_str = f"{total_runtime_hours:.2f} hours"
                            
                            # Check if PnR flow is incomplete (has RUN or UNLAUNCHED stages)
                            has_running_stage = False
                            running_stage_name = None
                            incomplete_stage_name = None
                            
                            try:
                                all_pnr_lines = self.file_utils.run_command(f"grep ' {ipo}' {prc_file} | grep -E '(BEGIN|setup|edi_plan|place|cts|route|postroute)' | grep -v report")
                                if all_pnr_lines.strip():
                                    # Check for RUN or UNLAUNCHED stages
                                    running_stage = None
                                    last_done_stage = None
                                    
                                    for line in all_pnr_lines.strip().split('\n'):
                                        parts = line.split()
                                        if len(parts) >= 3:
                                            stage_name = parts[2]
                                            stage_status = parts[3]
                                            
                                            if stage_status == 'RUN':
                                                running_stage = stage_name
                                                has_running_stage = True
                                                running_stage_name = stage_name
                                                break
                                            elif stage_status == 'DONE':
                                                last_done_stage = stage_name
                                    
                                    # Check if flow is incomplete (stopped before postroute)
                                    if not running_stage and last_done_stage and last_done_stage not in ['postroute', 'predrc_postroute']:
                                        # Flow stopped before postroute completion
                                        incomplete_stage_name = last_done_stage
                            except Exception:
                                pass
                            
                            pnr_runtimes[ipo] = runtime_str
                            ipo_found = True  # Mark as found to skip remaining prc.status files
                            
                            # Extract PnR timestamps from log file names in PRC status
                            try:
                                pnr_lines = self.file_utils.run_command(f"grep ' {ipo}' {prc_file} | grep -E '(BEGIN|setup|edi_plan|place|cts|route|postroute)' | grep -v report")
                                if pnr_lines.strip():
                                    lines = pnr_lines.strip().split('\n')
                                    if lines:
                                        # Check if fast_dc was used - if so, use setup as start instead of BEGIN
                                        fast_dc_log = os.path.join(self.workarea, "syn_flow/fast_dc/log/fast_dc.log")
                                        use_setup_as_start = os.path.exists(fast_dc_log)
                                        
                                        if use_setup_as_start:
                                            # Find setup step as start (full PnR after fast_dc)
                                            setup_line = None
                                            for line in lines:
                                                if 'setup' in line and 'DONE' in line:
                                                    setup_line = line
                                                    break
                                            first_line = setup_line if setup_line else lines[0]
                                        else:
                                            # Use BEGIN as normal
                                            first_line = lines[0]
                                        
                                        # Find the last DONE step (not RUN or UNLAUNCHED)
                                        last_line = None
                                        for line in reversed(lines):
                                            if 'DONE' in line:
                                                last_line = line
                                                break
                                        
                                        # If no DONE step found, use the last line (might be running)
                                        if not last_line:
                                            last_line = lines[-1]
                                        
                                        # Look for timestamp patterns in log file names (format: YYYYMMDDHHMMSS)
                                        first_match = re.search(r'(\d{8})(\d{6})', first_line)
                                        last_match = re.search(r'(\d{8})(\d{6})', last_line) if last_line else None
                                        
                                        # Extract the last stage's duration to calculate actual end time
                                        last_stage_duration_seconds = 0
                                        if last_line:
                                            last_line_parts = last_line.split()
                                            if len(last_line_parts) >= 5:
                                                try:
                                                    last_stage_duration_seconds = int(last_line_parts[4])
                                                except (ValueError, IndexError):
                                                    pass
                                        
                                        if first_match and last_match:
                                            try:
                                                first_date = first_match.group(1)  # YYYYMMDD
                                                first_time = first_match.group(2)  # HHMMSS
                                                last_date = last_match.group(1)
                                                last_time = last_match.group(2)
                                                
                                                # Parse start timestamp
                                                start_datetime = f"{first_date[:4]}-{first_date[4:6]}-{first_date[6:8]} {first_time[:2]}:{first_time[2:4]}:{first_time[4:6]}"
                                                start_time = time.strptime(start_datetime, "%Y-%m-%d %H:%M:%S")
                                                start_epoch = time.mktime(start_time)
                                                start_str = time.strftime("%m/%d %H:%M", start_time)
                                                
                                                # Calculate actual end time using TOTAL runtime from start
                                                # Parse total runtime hours
                                                runtime_match = re.search(r'(\d+\.?\d*)\s*hours?', runtime_str)
                                                if runtime_match:
                                                    total_runtime_hours = float(runtime_match.group(1))
                                                    total_runtime_seconds = int(total_runtime_hours * 3600)
                                                    actual_end_epoch = start_epoch + total_runtime_seconds
                                                else:
                                                    # Fallback: Use last stage start + last stage duration
                                                    last_start_datetime = f"{last_date[:4]}-{last_date[4:6]}-{last_date[6:8]} {last_time[:2]}:{last_time[2:4]}:{last_time[4:6]}"
                                                    last_start_time = time.strptime(last_start_datetime, "%Y-%m-%d %H:%M:%S")
                                                    last_start_epoch = time.mktime(last_start_time)
                                                    actual_end_epoch = last_start_epoch + last_stage_duration_seconds
                                                
                                                # If stage is running or incomplete, show status in end column
                                                if has_running_stage and running_stage_name:
                                                    end_str = f"RUNNING ({running_stage_name})"
                                                elif incomplete_stage_name:
                                                    end_str = f"INCOMPLETE ({incomplete_stage_name})"
                                                else:
                                                    end_str = time.strftime("%m/%d %H:%M", time.localtime(actual_end_epoch))
                                                
                                                runtime_timestamps[f'PnR ({ipo})'] = (start_str, end_str)
                                            except Exception as e:
                                                # Debug: print exception to see what's failing
                                                if self.verbose:
                                                    print(f"  [DEBUG] Failed to extract PnR timestamps for {ipo}: {e}")
                            except Exception as e:
                                if self.verbose:
                                    print(f"  [DEBUG] Failed to parse PnR lines for {ipo}: {e}")
                        except ValueError:
                            print(f"  Total PnR Runtime for {ipo}: Unable to calculate (non-numeric values)")
                except Exception as e:
                    if self.verbose:
                        print(f"  Error extracting PnR runtime for {ipo} from {prc_file}: {e}")
            # If no data found in any prc.status file, it's an ECO-only IPO (skip silently)
        
        # Star extraction runtime
        star_runtime = self._extract_star_runtime()
        if star_runtime:
            runtime_data['Star'] = star_runtime
            # Extract Star timestamps from log files
            star_log_patterns = [
                f"export/nv_star/{self.design_info.top_hier}/ipo*/LOGs/PRIME/STEP__star_extraction__*.log",
                f"export/nv_star/{self.design_info.top_hier}/ipo*/LOGs/*.log",
                f"export/nv_star/ipo*/LOGs/PRIME/STEP__star_extraction__*.log"
            ]
            star_logs = []
            for pattern in star_log_patterns:
                logs = self.file_utils.find_files(pattern, self.workarea)
                star_logs.extend(logs)
            if star_logs:
                latest_log = max(star_logs, key=os.path.getmtime)
                # Try to calculate accurate timestamps from runtime duration
                end_time = os.path.getmtime(latest_log)
                calculated_times = self._calculate_start_time_from_duration(end_time, star_runtime)
                if calculated_times[0] and calculated_times[1]:
                    runtime_timestamps['Star'] = calculated_times
                else:
                    # Fallback to file timestamps
                    start, end = self._extract_timestamps_from_log(latest_log)
                    if start and end:
                        runtime_timestamps['Star'] = (start, end)
        
        # Auto PT runtime
        auto_pt_runtime = self._extract_auto_pt_runtime()
        if auto_pt_runtime:
            runtime_data['Auto PT'] = auto_pt_runtime
            # Extract Auto PT timestamps from log files
            auto_pt_log_patterns = [
                f"signoff_flow/auto_pt/log/auto_pt.log",
                f"signoff_flow/auto_pt/*/LOGs/PRIME/STEP__auto_pt__*.log",
                f"signoff_flow/auto_pt/work_*/LOGs/*.log"
            ]
            auto_pt_logs = []
            for pattern in auto_pt_log_patterns:
                logs = self.file_utils.find_files(pattern, self.workarea)
                auto_pt_logs.extend(logs)
            if auto_pt_logs:
                latest_log = max(auto_pt_logs, key=os.path.getmtime)
                # Check if running (runtime string contains "(running)")
                if auto_pt_runtime and "(running)" in auto_pt_runtime:
                    # For running PT, use start time and "RUNNING" as end
                    start, _ = self._extract_timestamps_from_log(latest_log)
                    if start:
                        runtime_timestamps['Auto PT'] = (start, "RUNNING")
                else:
                    # Try to calculate accurate timestamps from runtime duration
                    end_time = os.path.getmtime(latest_log)
                    calculated_times = self._calculate_start_time_from_duration(end_time, auto_pt_runtime)
                    if calculated_times[0] and calculated_times[1]:
                        runtime_timestamps['Auto PT'] = calculated_times
                    else:
                        # Fallback to file timestamps
                        start, end = self._extract_timestamps_from_log(latest_log)
                        if start and end:
                            runtime_timestamps['Auto PT'] = (start, end)
        
        # Formal verification runtime (both types separately)
        formal_runtimes = self._extract_formal_runtime()
        if formal_runtimes:
            for formal_type, runtime in formal_runtimes.items():
                runtime_data[f'Formal ({formal_type})'] = runtime
                # Extract Formal verification timestamps
                formal_log_patterns = [
                    f"formal_flow/{formal_type}/LOGs/PRIME/STEP__formal__*.log",
                    f"formal_flow/{formal_type}/log/*.log",
                    f"formal_flow/{formal_type}/*.log"
                ]
                formal_logs = []
                for pattern in formal_log_patterns:
                    logs = self.file_utils.find_files(pattern, self.workarea)
                    formal_logs.extend(logs)
                if formal_logs:
                    latest_log = max(formal_logs, key=os.path.getmtime)
                    
                    # Check if formal is actually running by looking at the log file
                    is_running = False
                    try:
                        file_mtime = os.path.getmtime(latest_log)
                        current_time = time.time()
                        time_since_update = current_time - file_mtime
                        
                        if time_since_update < 300:  # 5 minutes
                            # Check for completion status
                            completion_check = self.file_utils.run_command(
                                f"grep -E 'Verification SUCCEEDED|Verification FAILED|Verification UNRESOLVED' {latest_log} | tail -1"
                            )
                            # Check for running indicators
                            running_check = self.file_utils.run_command(
                                f"tail -100 {latest_log} | grep -E 'Status:.*Building verification models|Status:.*Verifying|Status:.*Checking designs|Matching in progress' | tail -1"
                            )
                            
                            if running_check.strip() and not completion_check.strip():
                                is_running = True
                    except Exception:
                        pass
                    
                    if is_running:
                        # For running formal, find start time from formal flow directory files
                        formal_dir = os.path.dirname(latest_log)
                        start_str = None
                        
                        try:
                            # Look for files created at the start of the run (err file, do_file.tcl, etc.)
                            pattern_files = ['*.err', 'fm.do_file.tcl', 'formality.lck']
                            earliest_time = None
                            
                            for pattern in pattern_files:
                                pattern_path = os.path.join(os.path.dirname(formal_dir), pattern)
                                matching_files = glob.glob(pattern_path)
                                for f in matching_files:
                                    try:
                                        # Use modification time of these initial files as start time
                                        mtime = os.path.getmtime(f)
                                        if earliest_time is None or mtime < earliest_time:
                                            earliest_time = mtime
                                    except:
                                        pass
                            
                            if earliest_time:
                                start_str = time.strftime("%m/%d %H:%M", time.localtime(earliest_time))
                        except:
                            pass
                        
                        # Fallback to extracting from log file
                        if not start_str:
                            start, _ = self._extract_timestamps_from_log(latest_log)
                            start_str = start
                        
                        if start_str:
                            runtime_timestamps[f'Formal ({formal_type})'] = (start_str, "RUNNING")
                    else:
                        # Try to calculate accurate timestamps from runtime duration
                        end_time = os.path.getmtime(latest_log)
                        calculated_times = self._calculate_start_time_from_duration(end_time, runtime)
                        if calculated_times[0] and calculated_times[1]:
                            runtime_timestamps[f'Formal ({formal_type})'] = calculated_times
                        else:
                            # Fallback to file timestamps
                            start, end = self._extract_timestamps_from_log(latest_log)
                            if start and end:
                                runtime_timestamps[f'Formal ({formal_type})'] = (start, end)
        
        # GL Check runtime
        gl_check_runtime = self._extract_gl_check_runtime()
        if gl_check_runtime:
            runtime_data['GL Check'] = gl_check_runtime
            # Extract GL Check timestamps
            gl_check_log_patterns = [
                f"signoff_flow/gl-check*/LOGs/PRIME/STEP__gl_check__*.log",
                f"signoff_flow/gl-check*/*/gl-check.log",
                f"signoff_flow/gl-check/*/gl-check.log"
            ]
            gl_check_logs = []
            for pattern in gl_check_log_patterns:
                logs = self.file_utils.find_files(pattern, self.workarea)
                gl_check_logs.extend(logs)
            if gl_check_logs:
                latest_log = max(gl_check_logs, key=os.path.getmtime)
                # Try to calculate accurate timestamps from runtime duration
                end_time = os.path.getmtime(latest_log)
                calculated_times = self._calculate_start_time_from_duration(end_time, gl_check_runtime)
                if calculated_times[0] and calculated_times[1]:
                    runtime_timestamps['GL Check'] = calculated_times
                else:
                    # Fallback to file timestamps
                    start, end = self._extract_timestamps_from_log(latest_log)
                    if start and end:
                        runtime_timestamps['GL Check'] = (start, end)
        
        # Auto PT Fix runtime
        auto_pt_fix_runtime = self._extract_auto_pt_fix_runtime()
        if auto_pt_fix_runtime:
            runtime_data['Auto PT Fix'] = auto_pt_fix_runtime
            # Extract Auto PT Fix timestamps
            # Use pt_eco_out_*_final.tcl files to identify which work areas had auto_pt_fix
            # and use auto_pt_fix.log for end timestamp
            end_time = None
            
            # Method 1: Use auto_pt_fix.log modification time as end time
            auto_pt_fix_log = os.path.join(self.workarea, "signoff_flow/auto_pt_fix/log/auto_pt_fix.log")
            if os.path.exists(auto_pt_fix_log):
                end_time = os.path.getmtime(auto_pt_fix_log)
            else:
                # Fallback: Try other auto_pt_fix log locations
                auto_pt_fix_log = os.path.join(self.workarea, "signoff_flow/auto_pt/log/auto_pt_fix.log")
                if os.path.exists(auto_pt_fix_log):
                    end_time = os.path.getmtime(auto_pt_fix_log)
            
            if end_time:
                # Try to calculate accurate timestamps from runtime duration
                calculated_times = self._calculate_start_time_from_duration(end_time, auto_pt_fix_runtime)
                if calculated_times[0] and calculated_times[1]:
                    runtime_timestamps['Auto PT Fix'] = calculated_times
                else:
                    # Fallback: Look for pt_eco_out_*_final.tcl files in work directories
                    eco_pattern = "signoff_flow/auto_pt/work_*/pt_eco_out_*_final.tcl"
                    eco_files = self.file_utils.find_files(eco_pattern, self.workarea)
                    if eco_files:
                        # Use the latest pt_eco_out file timestamp
                        latest_eco = max(eco_files, key=os.path.getmtime)
                        eco_end_time = os.path.getmtime(latest_eco)
                        calculated_times = self._calculate_start_time_from_duration(eco_end_time, auto_pt_fix_runtime)
                        if calculated_times[0] and calculated_times[1]:
                            runtime_timestamps['Auto PT Fix'] = calculated_times
        
        # Gen ECO Netlist Innovus runtime
        gen_eco_runtime = self._extract_gen_eco_runtime()
        if gen_eco_runtime:
            runtime_data['Gen ECO Netlist'] = gen_eco_runtime
            # Extract Gen ECO Netlist timestamps
            gen_eco_log_patterns = [
                f"signoff_flow/gen_eco_netlist_innovus/{self.design_info.top_hier}/ipo*/LOGs/PRIME/STEP__gen_eco_netlist__*.log",
                f"signoff_flow/gen_eco_netlist_innovus/*/LOGs/PRIME/STEP__gen_eco_netlist__*.log",
                f"signoff_flow/gen_eco_netlist_innovus/log/*.log"
            ]
            gen_eco_logs = []
            for pattern in gen_eco_log_patterns:
                logs = self.file_utils.find_files(pattern, self.workarea)
                gen_eco_logs.extend(logs)
            if gen_eco_logs:
                latest_log = max(gen_eco_logs, key=os.path.getmtime)
                # Try to calculate accurate timestamps from runtime duration
                end_time = os.path.getmtime(latest_log)
                calculated_times = self._calculate_start_time_from_duration(end_time, gen_eco_runtime)
                if calculated_times[0] and calculated_times[1]:
                    runtime_timestamps['Gen ECO Netlist'] = calculated_times
                else:
                    # Fallback to file timestamps
                    start, end = self._extract_timestamps_from_log(latest_log)
                    if start and end:
                        runtime_timestamps['Gen ECO Netlist'] = (start, end)
        
        # NV Gate ECO runtime
        eco_prc_status = os.path.join(self.workarea, f"signoff_flow/nv_gate_eco/eco_{self.design_info.top_hier}.prc.status")
        nv_gate_eco_status_exists = os.path.exists(eco_prc_status)
        if nv_gate_eco_status_exists:
            self.print_file_info(eco_prc_status, "NV Gate ECO Status")
            try:
                # Check if NV Gate ECO is currently running
                running_check = self.file_utils.run_command(f"grep -E '^{self.design_info.top_hier}.*RUN' {eco_prc_status}")
                is_running = bool(running_check.strip())
                
                # Extract NV Gate ECO runtime for completed steps
                result = self.file_utils.run_command(f"grep -E '^{self.design_info.top_hier}.*DONE' {eco_prc_status} | awk '{{sum += $5}} END {{print sum}}'")
                if result.strip() and result.strip().isdigit():
                    total_runtime_seconds = int(result.strip())
                    
                    # If running, add the elapsed time of running step
                    if is_running:
                        running_lines = running_check.strip().split('\n')
                        for line in running_lines:
                            parts = line.split()
                            if len(parts) >= 5 and parts[3] == 'RUN':
                                running_duration = int(parts[4])
                                total_runtime_seconds += running_duration
                                break
                    
                    total_runtime_hours = total_runtime_seconds / 3600
                    total_runtime_days = total_runtime_hours / 24
                    
                    if total_runtime_hours >= 24:
                        runtime_str = f"{total_runtime_hours:.2f} hours ({total_runtime_days:.2f} days)"
                    else:
                        runtime_str = f"{total_runtime_hours:.2f} hours"
                    
                    # Don't add "(running)" indicator - it will be shown in End column
                    
                    runtime_data['NV Gate ECO'] = runtime_str
                    # Extract NV Gate ECO timestamps from log file names in status file
                    # Note: All log files have the same timestamp (flow start time), so we calculate end time
                    try:
                        eco_lines = self.file_utils.run_command(f"grep '{self.design_info.top_hier}.*DONE' {eco_prc_status}")
                        if eco_lines.strip():
                            lines = eco_lines.strip().split('\n')
                            if lines:
                                # Extract start timestamp from first log file name
                                first_line = lines[0]
                                
                                # Look for timestamp patterns in log file names (format: YYYYMMDDHHMMSS)
                                first_match = re.search(r'(\d{8})(\d{6})', first_line)
                                
                                if first_match:
                                    try:
                                        first_date = first_match.group(1)  # YYYYMMDD
                                        first_time = first_match.group(2)  # HHMMSS
                                        
                                        # Parse start timestamp
                                        start_datetime_str = f"{first_date[:4]}-{first_date[4:6]}-{first_date[6:8]} {first_time[:2]}:{first_time[2:4]}:{first_time[4:6]}"
                                        start_time_struct = time.strptime(start_datetime_str, "%Y-%m-%d %H:%M:%S")
                                        start_epoch = time.mktime(start_time_struct)
                                        
                                        # Calculate end time by adding total runtime seconds
                                        # For running flows, end time is "now"
                                        if is_running:
                                            end_str = "running"
                                            start_str = time.strftime("%m/%d %H:%M", start_time_struct)
                                        else:
                                            end_epoch = start_epoch + total_runtime_seconds
                                            end_time_struct = time.localtime(end_epoch)
                                            start_str = time.strftime("%m/%d %H:%M", start_time_struct)
                                            end_str = time.strftime("%m/%d %H:%M", end_time_struct)
                                        
                                        runtime_timestamps['NV Gate ECO'] = (start_str, end_str)
                                    except Exception:
                                        pass
                    except Exception:
                        pass
                elif is_running:
                    # No completed steps yet, but flow is running
                    # Get elapsed time of running step
                    running_lines = running_check.strip().split('\n')
                    for line in running_lines:
                        parts = line.split()
                        if len(parts) >= 5 and parts[3] == 'RUN':
                            running_duration = int(parts[4])
                            runtime_hours = running_duration / 3600
                            runtime_data['NV Gate ECO'] = f"{runtime_hours:.2f} hours"
                            
                            # Try to extract start timestamp
                            timestamp_match = re.search(r'(\d{8})(\d{6})', line)
                            if timestamp_match:
                                first_date = timestamp_match.group(1)
                                first_time = timestamp_match.group(2)
                                try:
                                    start_datetime_str = f"{first_date[:4]}-{first_date[4:6]}-{first_date[6:8]} {first_time[:2]}:{first_time[2:4]}:{first_time[4:6]}"
                                    start_time_struct = time.strptime(start_datetime_str, "%Y-%m-%d %H:%M:%S")
                                    start_str = time.strftime("%m/%d %H:%M", start_time_struct)
                                    runtime_timestamps['NV Gate ECO'] = (start_str, "RUNNING")
                                except:
                                    pass
                            break
                else:
                    print("  No completed NV Gate ECO steps found")
            except Exception as e:
                print(f"  Error extracting NV Gate ECO runtime: {e}")
        
        # Scan IPO directories for signoff flows FIRST (needed to check for IPO-level ECO)
        ipo_signoff_data = self._scan_ipo_directories()
        
        # Filter ipo_signoff_data to only the specified IPO if user used -i flag
        if self.user_specified_ipo and self.ipo:
            user_ipo = self.design_info.ipo
            # Keep only the user-specified IPO and 'root' (which contains global signoff data)
            filtered_ipo_data = {}
            if user_ipo in ipo_signoff_data:
                filtered_ipo_data[user_ipo] = ipo_signoff_data[user_ipo]
            if 'root' in ipo_signoff_data:
                filtered_ipo_data['root'] = ipo_signoff_data['root']
            ipo_signoff_data = filtered_ipo_data
        
        # Check if any ECO was found (either root-level or IPO-level)
        has_any_eco = 'NV Gate ECO' in runtime_data
        if not has_any_eco:
            # Check if any IPO has ECO stage
            for ipo_name, ipo_info in ipo_signoff_data.items():
                if 'signoff' in ipo_info and 'eco' in ipo_info.get('signoff', {}):
                    has_any_eco = True
                    break
        
        # Only print "not found" message if no ECO was found anywhere
        if not has_any_eco and not nv_gate_eco_status_exists:
            print("  NV Gate ECO Status file not found")
        
        # PV runtime
        pv_runtime = self._extract_pv_runtime()
        if pv_runtime:
            runtime_data['PV'] = pv_runtime
            # Extract PV timestamps from PV flow timeline
            local_flow_dir = os.path.join(self.workarea, f"pv_flow/nv_flow/{self.design_info.top_hier}/local_flow")
            if os.path.exists(local_flow_dir):
                try:
                    begin_files = glob.glob(os.path.join(local_flow_dir, "STEP__BEGIN__*"))
                    end_files = glob.glob(os.path.join(local_flow_dir, "STEP__END__*"))
                    if begin_files:
                        begin_time = os.path.getmtime(begin_files[0])
                        start_str = time.strftime("%m/%d %H:%M", time.localtime(begin_time))
                        
                        if end_files:
                            # Flow completed
                            end_time = os.path.getmtime(end_files[0])
                            end_str = time.strftime("%m/%d %H:%M", time.localtime(end_time))
                            runtime_timestamps['PV'] = (start_str, end_str)
                        else:
                            # Flow is running
                            runtime_timestamps['PV'] = (start_str, "RUNNING")
                except Exception:
                    pass
        
        # Parse prc.status for running flows BEFORE printing timeline
        prc_running_flows = self._parse_prc_status_running_flows(prc_status)
        
        # Print unified flow timeline table (with running status from prc.status)
        self._print_unified_flow_timeline_table(runtime_data, pnr_runtimes, runtime_timestamps, ipo_signoff_data, deleted_ipos, prc_running_flows)
        
        # Display currently running flows summary from prc.status
        if prc_running_flows.get('running_flows'):
            print(f"\n{Color.CYAN}Currently Running Flows (from prc.status):{Color.RESET}")
            
            # Group running flows by step
            flows_by_step = {}
            for flow in prc_running_flows['running_flows']:
                step = flow['step']
                if step not in flows_by_step:
                    flows_by_step[step] = []
                flows_by_step[step].append(flow)
            
            # Display each running step with its IPOs
            for step, flows in sorted(flows_by_step.items()):
                ipo_list = []
                for flow in flows:
                    duration_hours = flow['duration_seconds'] / 3600 if flow['duration_seconds'] > 0 else 0
                    ipo_list.append(f"{flow['ipo']} ({duration_hours:.1f}h)")
                print(f"  {Color.YELLOW}{step}{Color.RESET}: {', '.join(ipo_list)}")
            
            # Show prc.status file being used
            if prc_running_flows.get('prc_status_file'):
                prc_filename = os.path.basename(prc_running_flows['prc_status_file'])
                print(f"  {Color.CYAN}Source: {prc_filename}{Color.RESET}")
        
        # Generate HTML runtime report
        runtime_html_path = self._generate_runtime_html_report(runtime_data, pnr_runtimes, prc_status, runtime_timestamps, ipo_signoff_data)
        
        # Calculate total runtime for summary
        total_runtime_str = "N/A"
        if runtime_data or pnr_runtimes:
            # Get PnR runtime as main metric (typically longest)
            if pnr_runtimes:
                total_runtime_str = list(pnr_runtimes.values())[0] if pnr_runtimes else "N/A"
        
        # Check for currently running flows
        status = "PASS"
        issues = []
        running_flows = []
        
        # Check runtime_timestamps for RUNNING or [RUNNING: stage] status
        for stage_name, timestamps in runtime_timestamps.items():
            if timestamps and len(timestamps) >= 2:
                end_timestamp = timestamps[1]
                if end_timestamp and ("RUNNING" in str(end_timestamp) or end_timestamp == "RUNNING"):
                    running_flows.append(stage_name)
        
        # Also check runtime_data for stages with "running" status (no timestamps yet)
        for stage_name, runtime_str in runtime_data.items():
            if runtime_str == "running" and stage_name not in running_flows:
                running_flows.append(stage_name)
        
        # Also check prc.status for running signoff flows
        if prc_running_flows.get('running_flows'):
            for flow in prc_running_flows['running_flows']:
                flow_name = f"{flow['step']} ({flow['ipo']})"
                if flow_name not in running_flows:
                    running_flows.append(flow_name)
        
        if running_flows:
            status = "WARN"
            issues.append(f"Currently running: {', '.join(running_flows)}")
        
        # Add section summary for master dashboard
        self._add_section_summary(
            section_name="Runtime Analysis",
            section_id="runtime",
            stage=FlowStage.RUNTIME,
            status=status,
            key_metrics={
                "PnR Runtime": total_runtime_str,
                "Stages": str(len(runtime_data)),
                "IPOs": str(len(pnr_runtimes))
            },
            html_file=runtime_html_path,
            priority=3,
            issues=issues,
            icon="[Runtime]"
        )

    def _extract_star_runtime(self) -> Dict[str, Any]:
        """Extract Star extraction runtime
        
        Returns:
            Dictionary with runtime data
        """
        try:
            # Look for Star extraction logs in multiple locations
            star_log_patterns = [
                f"export/nv_star/{self.design_info.top_hier}/ipo*/LOGs/PRIME/STEP__extraction__*.log",
                f"export/nv_star/{self.design_info.top_hier}/ipo*/LOGs/flow_logs/*extraction*.log"
            ]
            
            star_logs = []
            for pattern in star_log_patterns:
                logs = self.file_utils.find_files(pattern, self.workarea)
                star_logs.extend(logs)
            
            if star_logs:
                # Extract runtime from the most recent log
                latest_log = max(star_logs, key=os.path.getmtime)
                result = self.file_utils.run_command(f"grep 'Run time' {latest_log} | tail -n1 | sed 's/.*Run time.*: *//' | sed 's/ sec.*//'")
                if result.strip() and result.strip().isdigit():
                    runtime_seconds = int(result.strip())
                    runtime_hours = runtime_seconds / 3600
                    if runtime_hours >= 24:
                        runtime_days = runtime_hours / 24
                        return f"{runtime_days:.2f} days ({runtime_hours:.2f} hours)"
                    else:
                        return f"{runtime_hours:.2f} hours"
        except Exception:
            pass
        return None

    def _extract_auto_pt_runtime(self) -> Dict[str, Any]:
        """Extract Auto PT runtime including elapsed time if currently running
        
        Returns:
            Dictionary with runtime data
        """
        try:
            # Look for Auto PT log
            auto_pt_log = os.path.join(self.workarea, "signoff_flow/auto_pt/log/auto_pt.log")
            if os.path.exists(auto_pt_log):
                # Check if PT is currently running by checking for work directory without HTML
                is_running = False
                work_pattern = os.path.join(self.workarea, "signoff_flow/auto_pt/work_*")
                all_items = glob.glob(work_pattern)
                work_dirs = [item for item in all_items if os.path.isdir(item)]
                
                if work_dirs:
                    work_dirs_sorted = sorted(work_dirs, key=os.path.getmtime, reverse=True)
                    latest_work = work_dirs_sorted[0]
                    latest_work_name = os.path.basename(latest_work)
                    
                    # Check if HTML report exists for the latest work area
                    html_for_latest = os.path.join(self.workarea, f"signoff_flow/auto_pt/{latest_work_name}.html")
                    if not os.path.exists(html_for_latest):
                        # No HTML means PT is likely still running
                        # Check if files were recently modified (within last hour)
                        latest_mtime = os.path.getmtime(latest_work)
                        current_time = time.time()
                        time_since_update = (current_time - latest_mtime) / 60  # minutes
                        
                        if time_since_update < 60:  # Within last hour
                            is_running = True
                
                # Try multiple patterns for runtime extraction
                result = self.file_utils.run_command(f"grep 'Total.*cpu seconds' {auto_pt_log} | tail -n1 | sed 's/.*Total \\([0-9]*\\) cpu seconds.*/\\1/'")
                
                if not result.strip() or not result.strip().isdigit():
                    # Look for "Elapsed time for this session" pattern
                    result = self.file_utils.run_command(f"grep 'Elapsed time for this session' {auto_pt_log} | sed 's/.*Elapsed time for this session: \\([0-9]*\\) seconds.*/\\1/'")
                
                if not result.strip() or not result.strip().isdigit():
                    # Look for elapsed time in performance table
                    result = self.file_utils.run_command(f"grep -A 10 'Elapsed Time (s)' {auto_pt_log} | grep 'local process' | awk '{{print $4}}'")
                
                if not result.strip() or not result.strip().isdigit():
                    result = self.file_utils.run_command(f"grep 'Run time' {auto_pt_log} | tail -n1 | sed 's/.*Run time.*: *//' | sed 's/ sec.*//'")
                
                # If PT is running, calculate elapsed time from start
                if is_running:
                    # Try to find start time from log or work directory creation time
                    start_time = None
                    if work_dirs:
                        # Use work directory creation time as start time
                        start_time = os.path.getctime(work_dirs_sorted[0])
                    
                    if start_time:
                        current_time = time.time()
                        elapsed_seconds = int(current_time - start_time)
                        elapsed_hours = elapsed_seconds / 3600
                        
                        if elapsed_hours >= 24:
                            elapsed_days = elapsed_hours / 24
                            return f"{elapsed_hours:.2f} hours ({elapsed_days:.2f} days)"
                        else:
                            return f"{elapsed_hours:.2f} hours"
                    else:
                        # Return just the hours, status will be shown in End column
                        return "running"
                
                # PT is not running, return completed runtime
                if result.strip() and result.strip().isdigit():
                    runtime_seconds = int(result.strip())
                    runtime_hours = runtime_seconds / 3600
                    if runtime_hours >= 24:
                        runtime_days = runtime_hours / 24
                        return f"{runtime_days:.2f} days ({runtime_hours:.2f} hours)"
                    else:
                        return f"{runtime_hours:.2f} hours"
        except Exception:
            pass
        return None

    def _extract_formal_runtime(self) -> Dict[str, Any]:
        """Extract Formal verification runtime from both formal types separately
        
        Returns:
            Dictionary with runtime data
        """
        try:
            # Look for both types of Formal verification logs
            formal_logs = [
                ("rtl_vs_pnr_bbox_fm", os.path.join(self.workarea, "formal_flow/rtl_vs_pnr_bbox_fm/log/rtl_vs_pnr_bbox_fm.log")),
                ("rtl_vs_pnr_fm", os.path.join(self.workarea, "formal_flow/rtl_vs_pnr_fm/log/rtl_vs_pnr_fm.log"))
            ]
            
            formal_runtimes = {}
            
            for formal_type, formal_log in formal_logs:
                if os.path.exists(formal_log):
                    # Check if formal crashed
                    crash_check = self.file_utils.run_command(
                        f"tail -100 {formal_log} | grep -E 'stopped at line.*due to error|Error: Unknown name:.*FM-036|Error: The current design is not set.*FM-008' | head -1"
                    )
                    completion_check = self.file_utils.run_command(
                        f"grep -E 'Verification SUCCEEDED|Verification FAILED|Verification UNRESOLVED' {formal_log} | tail -1"
                    )
                    
                    if crash_check.strip() and not completion_check.strip():
                        # Formal crashed - still extract runtime but mark as crashed
                        runtime_seconds = 0
                        result = self.file_utils.run_command(f"grep 'Elapsed time:' {formal_log} | tail -n1 | sed 's/.*Elapsed time: \\([0-9]*\\) seconds.*/\\1/'")
                        if result.strip() and result.strip().isdigit():
                            runtime_seconds = int(result.strip())
                            runtime_hours = runtime_seconds / 3600
                            formal_runtimes[formal_type] = f"{runtime_hours:.2f} hours (CRASHED)"
                        else:
                            formal_runtimes[formal_type] = "CRASHED"
                        continue
                    
                    # Check if formal is currently running
                    file_mtime = os.path.getmtime(formal_log)
                    current_time = time.time()
                    time_since_update = current_time - file_mtime
                    
                    is_running = False
                    if time_since_update < 300:  # 5 minutes
                        # Check for running indicators and no completion status
                        check_result = self.file_utils.run_command(
                            f"tail -100 {formal_log} | grep -E 'Status:.*Building verification models|Status:.*Verifying|Status:.*Checking designs|Matching in progress' | tail -1"
                        )
                        
                        if check_result.strip() and not completion_check.strip():
                            is_running = True
                    
                    if is_running:
                        # Calculate elapsed time from earliest file in formal directory (actual start time)
                        try:
                            formal_dir = os.path.dirname(formal_log)
                            start_time = None
                            
                            # Look for files created at the start of the run
                            pattern_files = ['*.err', 'fm.do_file.tcl', 'formality.lck']
                            earliest_time = None
                            
                            for pattern in pattern_files:
                                pattern_path = os.path.join(os.path.dirname(formal_dir), pattern)
                                matching_files = glob.glob(pattern_path)
                                for f in matching_files:
                                    try:
                                        mtime = os.path.getmtime(f)
                                        if earliest_time is None or mtime < earliest_time:
                                            earliest_time = mtime
                                    except:
                                        pass
                            
                            # Use earliest file time, or fallback to log creation time
                            start_time = earliest_time if earliest_time else os.path.getctime(formal_log)
                            
                            elapsed_seconds = int(current_time - start_time)
                            elapsed_hours = elapsed_seconds / 3600
                            
                            if elapsed_hours >= 24:
                                elapsed_days = elapsed_hours / 24
                                formal_runtimes[formal_type] = f"{elapsed_hours:.2f} hours ({elapsed_days:.2f} days)"
                            else:
                                formal_runtimes[formal_type] = f"{elapsed_hours:.2f} hours"
                        except Exception:
                            formal_runtimes[formal_type] = "running"
                            continue
                    
                    # Try multiple patterns for runtime extraction
                    result = self.file_utils.run_command(f"grep 'Total.*cpu seconds' {formal_log} | tail -n1 | sed 's/.*Total \\([0-9]*\\) cpu seconds.*/\\1/'")
                    
                    if not result.strip() or not result.strip().isdigit():
                        # Look for "Elapsed time: X seconds" pattern
                        result = self.file_utils.run_command(f"grep 'Elapsed time:' {formal_log} | tail -n1 | sed 's/.*Elapsed time: \\([0-9]*\\) seconds.*/\\1/'")
                    
                    if not result.strip() or not result.strip().isdigit():
                        # Look for "Elapsed time for this session" pattern
                        result = self.file_utils.run_command(f"grep 'Elapsed time for this session' {formal_log} | sed 's/.*Elapsed time for this session: \\([0-9]*\\) seconds.*/\\1/'")
                    
                    if not result.strip() or not result.strip().isdigit():
                        # Look for elapsed time in performance table
                        result = self.file_utils.run_command(f"grep -A 10 'Elapsed Time (s)' {formal_log} | grep 'local process' | awk '{{print $4}}'")
                    
                    if not result.strip() or not result.strip().isdigit():
                        result = self.file_utils.run_command(f"grep 'Run time' {formal_log} | tail -n1 | sed 's/.*Run time.*: *//' | sed 's/ sec.*//'")
                    
                    if result.strip() and result.strip().isdigit():
                        runtime_seconds = int(result.strip())
                        runtime_hours = runtime_seconds / 3600
                        if runtime_hours >= 24:
                            runtime_days = runtime_hours / 24
                            formal_runtimes[formal_type] = f"{runtime_days:.2f} days ({runtime_hours:.2f} hours)"
                        else:
                            formal_runtimes[formal_type] = f"{runtime_hours:.2f} hours"
            
            return formal_runtimes
        except Exception:
            pass
        return None

    def _extract_gl_check_runtime(self) -> Dict[str, Any]:
        """Extract GL Check runtime
        
        Returns:
            Dictionary with runtime data
        """
        try:
            # Look for GL Check logs in multiple possible locations
            gl_check_log_patterns = [
                f"signoff_flow/gl-check*/LOGs/PRIME/STEP__gl_check__*.log",  # Original pattern
                f"signoff_flow/gl-check*/*/gl-check.log",  # New pattern for timestamped directories
                f"signoff_flow/gl-check/*/gl-check.log"    # Alternative pattern
            ]
            
            gl_check_logs = []
            for pattern in gl_check_log_patterns:
                logs = self.file_utils.find_files(pattern, self.workarea)
                gl_check_logs.extend(logs)
            
            if gl_check_logs:
                # Extract runtime from the most recent log
                latest_log = max(gl_check_logs, key=os.path.getmtime)
                
                # Try different runtime extraction methods
                # Method 1: Look for "Run time:" pattern
                result = self.file_utils.run_command(f"grep 'Run time:' {latest_log} | tail -n1 | sed 's/.*Run time: //' | sed 's/ sec//'")
                if result.strip() and result.strip().isdigit():
                    runtime_seconds = int(result.strip())
                    runtime_hours = runtime_seconds / 3600
                    if runtime_hours >= 24:
                        runtime_days = runtime_hours / 24
                        return f"{runtime_days:.2f} days ({runtime_hours:.2f} hours)"
                    else:
                        return f"{runtime_hours:.2f} hours"
                
                # Method 2: Calculate from file timestamps (directory start/end times)
                log_dir = os.path.dirname(latest_log)
                if os.path.exists(log_dir):
                    # Get directory creation time and latest file modification time
                    dir_files = glob.glob(os.path.join(log_dir, "*"))
                    if dir_files:
                        start_time = min(os.path.getmtime(f) for f in dir_files)
                        end_time = max(os.path.getmtime(f) for f in dir_files)
                        duration_seconds = int(end_time - start_time)
                        
                        if duration_seconds > 0:
                            runtime_hours = duration_seconds / 3600
                            if runtime_hours >= 24:
                                runtime_days = runtime_hours / 24
                                return f"{runtime_days:.2f} days ({runtime_hours:.2f} hours)"
                            else:
                                return f"{runtime_hours:.2f} hours"
                
        except Exception as e:
            print(f"  Error extracting GL Check runtime: {e}")
        return None

    def _extract_auto_pt_fix_runtime(self) -> Dict[str, Any]:
        """Extract Auto PT Fix runtime (elapsed/wall-clock time, not CPU time)
        
        Returns:
            Dictionary with runtime data
        """
        try:
            # Look for Auto PT Fix log in auto_pt directory (auto_pt_fix uses auto_pt structure)
            auto_pt_fix_log = os.path.join(self.workarea, "signoff_flow/auto_pt/log/auto_pt_fix.log")
            if os.path.exists(auto_pt_fix_log):
                # Extract "Elapsed time for this session" which shows actual wall-clock time
                # Format: "Elapsed time for this session: 3866 seconds"
                result = self.file_utils.run_command(f"grep 'Elapsed time for this session' {auto_pt_fix_log} | tail -n1")
                
                if result.strip():
                    # Extract seconds from the line
                    match = re.search(r'(\d+\.?\d*)\s*seconds?', result)
                    if match:
                        runtime_seconds = float(match.group(1))
                        runtime_hours = runtime_seconds / 3600
                        if runtime_hours >= 24:
                            runtime_days = runtime_hours / 24
                            return f"{runtime_hours:.2f} hours ({runtime_days:.2f} days)"
                        else:
                            return f"{runtime_hours:.2f} hours"
        except Exception as e:
            print(f"  Error extracting Auto PT Fix runtime: {e}")
        return None

    def _extract_gen_eco_runtime(self) -> Dict[str, Any]:
        """Extract Gen ECO Netlist Innovus runtime
        
        Returns:
            Dictionary with runtime data
        """
        try:
            # Look for Gen ECO Netlist logs
            gen_eco_log_pattern = f"signoff_flow/gen_eco_netlist_innovus/{self.design_info.top_hier}/ipo*/LOGs/PRIME/STEP__gen_eco_netlist__*.log"
            gen_eco_logs = self.file_utils.find_files(gen_eco_log_pattern, self.workarea)
            if gen_eco_logs:
                # Extract runtime from the most recent log
                latest_log = max(gen_eco_logs, key=os.path.getmtime)
                result = self.file_utils.run_command(f"grep 'Run time:' {latest_log} | tail -n1 | sed 's/.*Run time: //' | sed 's/ sec//'")
                if result.strip() and result.strip().isdigit():
                    runtime_seconds = int(result.strip())
                    runtime_hours = runtime_seconds / 3600
                    if runtime_hours >= 24:
                        runtime_days = runtime_hours / 24
                        return f"{runtime_days:.2f} days ({runtime_hours:.2f} hours)"
                    else:
                        return f"{runtime_hours:.2f} hours"
        except Exception:
            pass
        return None

    def _extract_pv_runtime(self) -> Dict[str, Any]:
        """Extract PV flow runtime including elapsed time of running steps
        
        Returns:
            Dictionary with runtime data
        """
        try:
            prc_status_pattern = f"pv_flow/nv_flow/pv_{self.design_info.top_hier}.prc.status"
            prc_status_files = self.file_utils.find_files(prc_status_pattern, self.workarea)
            
            if not prc_status_files:
                return None
            
            prc_status_file = prc_status_files[0]
            
            with open(prc_status_file, 'r') as f:
                content = f.read()
            
            # Extract completed steps and calculate total runtime
            total_runtime_seconds = 0
            completed_steps = 0
            running_steps = 0
            
            # Get BEGIN timestamp to calculate running step's elapsed time
            local_flow_dir = os.path.join(self.workarea, f"pv_flow/nv_flow/{self.design_info.top_hier}/local_flow")
            begin_time = None
            if os.path.exists(local_flow_dir):
                begin_files = glob.glob(os.path.join(local_flow_dir, "STEP__BEGIN__*"))
                if begin_files:
                    begin_time = os.path.getmtime(begin_files[0])
            
            lines = content.split('\n')
            for line in lines:
                if line.strip() and not line.startswith('#') and '    ' in line:
                    parts = line.split()
                    if len(parts) >= 6:
                        status = parts[3]
                        duration = parts[4]
                        
                        if status == 'DONE' and duration.isdigit() and int(duration) > 0:
                            # Add completed step duration
                            total_runtime_seconds += int(duration)
                            completed_steps += 1
                        elif status == 'RUN' and duration.isdigit() and int(duration) > 0:
                            # For running steps, use the duration from status file 
                            # (which represents elapsed time so far)
                            total_runtime_seconds += int(duration)
                            running_steps += 1
            
            # If there are running steps and we have begin time, calculate current elapsed time
            if running_steps > 0 and begin_time:
                current_time = time.time()
                # Calculate total elapsed time from BEGIN
                total_elapsed_seconds = int(current_time - begin_time)
                # Use the greater of: sum of step durations OR total elapsed time
                # (Sometimes the status file durations lag behind real time)
                total_runtime_seconds = max(total_runtime_seconds, total_elapsed_seconds)
            
            if total_runtime_seconds > 0:
                total_runtime_hours = total_runtime_seconds / 3600
                total_runtime_days = total_runtime_hours / 24
                
                if total_runtime_hours >= 24:
                    runtime_str = f"{total_runtime_hours:.2f} hours ({total_runtime_days:.2f} days)"
                else:
                    runtime_str = f"{total_runtime_hours:.2f} hours"
                
                # Don't add "(running)" indicator - it will be shown in End column
                
                return runtime_str
            
        except Exception as e:
            print(f"  Error extracting PV runtime: {e}")
        
        return None

    def _extract_timestamps_from_log(self, log_file: str) -> Dict[str, Optional[float]]:
        """Extract start and end timestamps from a log file
        
        Args:
            log_file: Path to log file
            
        Returns:
            Dictionary with 'start' and 'end' timestamps (float or None)
        """
        try:
            if not os.path.exists(log_file):
                return None, None
            
            # Get file creation and modification times as fallback
            stat = os.stat(log_file)
            start_time = stat.st_ctime  # Creation time
            end_time = stat.st_mtime    # Modification time
            
            # Try to extract more precise timestamps from log content
            try:
                with open(log_file, 'r') as f:
                    lines = f.readlines()
                
                # Look for timestamp patterns in the first and last few lines
                if lines:
                    # Check first few lines for start timestamp
                    for line in lines[:10]:
                        # Look for common timestamp patterns
                        timestamp_match = re.search(r'(\d{4}[/-]\d{2}[/-]\d{2}[\s_]\d{2}:\d{2}:\d{2})', line)
                        if timestamp_match:
                            try:
                                # Try to parse the timestamp
                                timestamp_str = timestamp_match.group(1).replace('_', ' ').replace('/', '-')
                                parsed_time = time.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S")
                                start_time = time.mktime(parsed_time)
                                break
                            except:
                                continue
                    
                    # Check last few lines for end timestamp
                    for line in reversed(lines[-10:]):
                        timestamp_match = re.search(r'(\d{4}[/-]\d{2}[/-]\d{2}[\s_]\d{2}:\d{2}:\d{2})', line)
                        if timestamp_match:
                            try:
                                timestamp_str = timestamp_match.group(1).replace('_', ' ').replace('/', '-')
                                parsed_time = time.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S")
                                end_time = time.mktime(parsed_time)
                                break
                            except:
                                continue
            except:
                pass  # Use file timestamps as fallback
            
            # Format timestamps
            start_str = time.strftime("%m/%d %H:%M", time.localtime(start_time))
            end_str = time.strftime("%m/%d %H:%M", time.localtime(end_time))
            
            return start_str, end_str
            
        except Exception:
            return None, None

    def _find_latest_prc_status_file(self, search_dir: str = None) -> str:
        """
        Find the most recently modified *.prc.status file in pnr_flow/nv_flow/.
        
        Users may have multiple .prc.status files with different names:
        - {unit}.prc.status (e.g., ccorea.prc.status)
        - {experiment_name}.prc.status (e.g., ww02_3_vivid_loop3.prc.status)
        
        This method returns the most recently modified one, which represents
        the current active flow.
        
        Args:
            search_dir: Optional directory to search in. If None, uses pnr_flow/nv_flow/
            
        Returns:
            str: Absolute path to the latest .prc.status file, or None if not found
        """
        if not self.workarea:
            return None
        
        if search_dir is None:
            search_dir = os.path.join(self.workarea, "pnr_flow/nv_flow")
        
        if not os.path.isdir(search_dir):
            return None
        
        # Find all .prc.status files
        prc_status_files = []
        try:
            for filename in os.listdir(search_dir):
                if filename.endswith('.prc.status'):
                    filepath = os.path.join(search_dir, filename)
                    if os.path.isfile(filepath):
                        mtime = os.path.getmtime(filepath)
                        prc_status_files.append((filepath, mtime))
        except OSError:
            return None
        
        if not prc_status_files:
            return None
        
        # Sort by modification time (newest first) and return the latest
        prc_status_files.sort(key=lambda x: x[1], reverse=True)
        return os.path.abspath(prc_status_files[0][0])
    
    def _find_all_prc_status_files(self, search_dir: str = None) -> list:
        """
        Find ALL *.prc.status files in pnr_flow/nv_flow/.
        
        Some workareas have multiple prc.status files:
        - {unit}.prc.status (original PnR flow with full stages)
        - eco_{unit}.prc.status (ECO flows with copy_db/eco stages only)
        
        To get complete data, we need to read from all files and merge.
        
        Args:
            search_dir: Optional directory to search in. If None, uses pnr_flow/nv_flow/
            
        Returns:
            list: List of absolute paths to all .prc.status files (sorted by mtime, newest first)
        """
        if not self.workarea:
            return []
        
        if search_dir is None:
            search_dir = os.path.join(self.workarea, "pnr_flow/nv_flow")
        
        if not os.path.isdir(search_dir):
            return []
        
        # Find all .prc.status files
        prc_status_files = []
        try:
            for filename in os.listdir(search_dir):
                if filename.endswith('.prc.status'):
                    filepath = os.path.join(search_dir, filename)
                    if os.path.isfile(filepath):
                        mtime = os.path.getmtime(filepath)
                        prc_status_files.append((os.path.abspath(filepath), mtime))
        except OSError:
            return []
        
        # Sort by modification time (newest first)
        prc_status_files.sort(key=lambda x: x[1], reverse=True)
        return [f[0] for f in prc_status_files]
    
    def _parse_prc_status_running_flows(self, prc_status_file: str = None) -> dict:
        """
        Parse ALL .prc.status files to extract currently running flows.
        
        This parses the prc.status file format:
        BLOCK    EXPERIMENT    STEP        STATUS    DURATION    LOGFILE
        ccorea   ipo1403       nv_star     RUN       11186       /path/to/log
        
        Args:
            prc_status_file: Path to prc.status file. If None, reads ALL prc.status files.
            
        Returns:
            dict: {
                'running_flows': [
                    {'ipo': 'ipo1403', 'step': 'nv_star', 'duration_seconds': 11186, 'status': 'RUN'},
                    ...
                ],
                'unlaunched_flows': [
                    {'ipo': 'ipo1403', 'step': 'auto_pt', 'status': 'UNLAUNCHED'},
                    ...
                ],
                'prc_status_files': ['/path/to/file1.prc.status', '/path/to/file2.prc.status'],
                'prc_status_file': '/path/to/latest.prc.status'  # For backward compatibility
            }
        """
        result = {
            'running_flows': [],
            'unlaunched_flows': [],
            'prc_status_files': [],
            'prc_status_file': None
        }
        
        # Get list of prc.status files to parse
        if prc_status_file is not None:
            prc_files = [prc_status_file] if os.path.exists(prc_status_file) else []
        else:
            prc_files = self._find_all_prc_status_files()
        
        if not prc_files:
            return result
        
        result['prc_status_files'] = prc_files
        result['prc_status_file'] = prc_files[0]  # Latest file for backward compatibility
        
        # Skip these trivial/internal steps (BEGIN/END markers, not real flows)
        skip_steps = {'BEGIN', 'END'}
        
        # Track unique (ipo, step) combinations to avoid duplicates
        seen_flows = set()
        
        for prc_file in prc_files:
            try:
                with open(prc_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if not line or line.startswith('#'):
                            continue
                        
                        # Parse: block ipo step status duration logfile
                        parts = line.split()
                        if len(parts) >= 5:
                            block, ipo, step, status, duration_str = parts[0], parts[1], parts[2], parts[3], parts[4]
                            
                            # Skip trivial steps
                            if step in skip_steps:
                                continue
                            
                            # Skip if already seen this (ipo, step) combination
                            flow_key = (ipo.lower(), step.lower())
                            if flow_key in seen_flows:
                                continue
                            seen_flows.add(flow_key)
                            
                            try:
                                duration = int(duration_str) if duration_str.lstrip('-').isdigit() else -1
                            except ValueError:
                                duration = -1
                            
                            # Get log file path (last field in prc.status line)
                            log_path = parts[-1] if len(parts) >= 6 and '/' in parts[-1] else None
                            
                            flow_info = {
                                'ipo': ipo,
                                'step': step,
                                'duration_seconds': duration,
                                'status': status,
                                'log_path': log_path
                            }
                            
                            if status == 'RUN':
                                result['running_flows'].append(flow_info)
                            elif status == 'UNLAUNCHED':
                                result['unlaunched_flows'].append(flow_info)
                            
            except Exception as e:
                pass  # Silently fail - this is supplementary info
        
        return result

    def _scan_ipo_directories(self) -> Dict[str, Dict]:
        """Scan all IPO directories for PnR and signoff flows
        
        Detects flows in:
        - Root level: workarea/signoff_flow/
        - IPO-specific: workarea/pnr_flow/nv_flow/{design}/ipo*/
        - PnR flows: workarea/pnr_flow/nv_flow/{design}/ipo*/
        - Signoff flows: workarea/pnr_flow/nv_flow/{design}/ipo*/nbu_signoff/signoff_flow/
        
        Returns:
            Dictionary with IPO-specific flow data
            {
                'root': { flow data },
                'ipo1000': { 'pnr': {...}, 'signoff': {...} },
                'ipo1001': { 'pnr': {...}, 'signoff': {...} },
                ...
            }
        """
        ipo_data = {}
        
        try:
            # Scan for IPO-specific directories
            pnr_nv_flow = os.path.join(self.workarea, f"pnr_flow/nv_flow/{self.design_info.top_hier}")
            
            if os.path.exists(pnr_nv_flow):
                # Find all ipo* directories
                ipo_pattern = os.path.join(pnr_nv_flow, "ipo*")
                ipo_dirs = glob.glob(ipo_pattern)
                
                for ipo_dir in sorted(ipo_dirs):
                    if not os.path.isdir(ipo_dir):
                        continue
                    
                    ipo_name = os.path.basename(ipo_dir)
                    ipo_data[ipo_name] = {}
                    
                    # Check for PnR status - read from ALL prc.status files and merge data
                    # Some workareas have multiple prc.status files:
                    # - {unit}.prc.status (original PnR with full stages)
                    # - eco_{unit}.prc.status (ECO flows with only copy_db/eco stages)
                    all_prc_files = self._find_all_prc_status_files()
                    if all_prc_files:
                        ipo_data[ipo_name]['pnr'] = self._extract_ipo_pnr_status_from_all_files(all_prc_files, ipo_name)
                    
                    # Check for nbu_signoff/signoff_flow
                    signoff_path = os.path.join(ipo_dir, "nbu_signoff/signoff_flow")
                    if os.path.exists(signoff_path):
                        ipo_data[ipo_name]['signoff'] = self._extract_ipo_signoff_flows(signoff_path, ipo_name)
                    
                    # Check for standalone ECO stage (for IPOs not in prc.status)
                    # Pattern: ipo*/LOGs/PRIME/eco.log
                    # ECO is part of signoff loop, not PnR (comes after Auto_Pt_Fix)
                    eco_log = os.path.join(ipo_dir, "LOGs/PRIME/eco.log")
                    if os.path.exists(eco_log):
                        eco_stage_data = self._extract_eco_stage_from_log(eco_log)
                        if eco_stage_data:
                            # Add ECO to signoff flows (it comes after Auto_Pt_Fix)
                            if 'signoff' not in ipo_data[ipo_name]:
                                ipo_data[ipo_name]['signoff'] = {}
                            ipo_data[ipo_name]['signoff']['eco'] = {
                                'status': 'COMPLETED',
                                'runtime': f"{eco_stage_data['runtime_hours']:.2f}h",
                                'start_time': eco_stage_data['start_time'],
                                'end_time': eco_stage_data['end_time']
                            }
            
            # Also check root-level signoff flows
            root_signoff = os.path.join(self.workarea, "signoff_flow")
            if os.path.exists(root_signoff):
                ipo_data['root'] = {'signoff': self._extract_ipo_signoff_flows(root_signoff, 'root')}
        
        except Exception as e:
            print(f"  Warning: Error scanning IPO directories: {e}")
        
        return ipo_data
    
    def _extract_eco_stage_from_log(self, eco_log_path: str) -> Dict:
        """Extract ECO stage data from LOGs/PRIME/eco.log
        
        Args:
            eco_log_path: Path to eco.log file
            
        Returns:
            Dictionary with ECO stage data or None
        """
        try:
            with open(eco_log_path, 'r', encoding='utf-8', errors='ignore') as f:
                eco_content = f.read()
            
            # Look for "Run time : XXXX sec" or "Run time: XXXX sec" pattern (take last occurrence)
            runtime_matches = re.findall(r'Run time\s*:\s*(\d+)\s*sec', eco_content, re.IGNORECASE)
            if runtime_matches:
                # Take the last runtime entry (most complete)
                runtime_seconds = int(runtime_matches[-1])
                runtime_hours = runtime_seconds / 3600
                
                # Get log file modification time as end time
                eco_mtime = os.path.getmtime(eco_log_path)
                end_time_str = time.strftime("%m/%d %H:%M", time.localtime(eco_mtime))
                
                # Calculate start time
                start_epoch = eco_mtime - runtime_seconds
                start_time_str = time.strftime("%m/%d %H:%M", time.localtime(start_epoch))
                
                return {
                    'runtime_hours': runtime_hours,
                    'start_time': start_time_str,
                    'end_time': end_time_str,
                    'status': 'OK'
                }
        except Exception as e:
            if self.verbose:
                print(f"  Warning: Error extracting ECO stage from {eco_log_path}: {e}")
        
        return None
    
    def _extract_ipo_pnr_status(self, prc_status_file: str, ipo_label: str) -> Dict:
        """Extract PnR status for specific IPO with per-stage runtime breakdown
        
        Args:
            prc_status_file: Path to {design}.prc.status file
            ipo_label: IPO identifier (e.g., 'ipo1000')
        
        Returns:
            Dictionary with PnR status data including per-stage runtimes
        """
        pnr_status = {
            'status': 'NOT_RUN',
            'current_stage': None,
            'completed_stages': [],
            'running_stage': None,
            'start_time': None,
            'end_time': None,
            'last_update': None,
            'stages': {}  # Per-stage breakdown: {stage_name: {runtime_hours, start_time, end_time, status}}
        }
        
        try:
            if not os.path.exists(prc_status_file):
                return pnr_status
            
            with open(prc_status_file, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # Parse prc.status file
            lines = content.split('\n')
            stage_order = ['setup', 'plan', 'place', 'cts', 'route', 'postroute']
            
            for line in lines:
                if line.strip() and not line.startswith('#') and '    ' in line and ipo_label in line:
                    parts = line.split()
                    if len(parts) >= 6:
                        stage_name = parts[2]
                        status = parts[3]
                        duration_str = parts[4]
                        
                        # Skip reporting stages
                        if 'report' in stage_name.lower():
                            continue
                        
                        # Extract timestamp from log file path (format: YYYYMMDDHHMMSS)
                        # Log path is always the last element in prc.status line
                        log_path = parts[-1] if len(parts) >= 6 else None
                        start_time_str = None
                        end_time_str = None
                        
                        if log_path and '/' in log_path:
                            timestamp_match = re.search(r'(\d{8})(\d{6})', log_path)
                            if timestamp_match:
                                date_str = timestamp_match.group(1)  # YYYYMMDD
                                time_str = timestamp_match.group(2)  # HHMMSS
                                
                                # Parse as start time
                                start_datetime = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]} {time_str[:2]}:{time_str[2:4]}:{time_str[4:6]}"
                                start_time_struct = time.strptime(start_datetime, "%Y-%m-%d %H:%M:%S")
                                start_epoch = time.mktime(start_time_struct)
                                start_time_str = time.strftime("%m/%d %H:%M", start_time_struct)
                                
                                # Calculate end time from start + duration
                                if duration_str.isdigit():
                                    duration_seconds = int(duration_str)
                                    end_epoch = start_epoch + duration_seconds
                                    end_time_str = time.strftime("%m/%d %H:%M", time.localtime(end_epoch))
                        
                        # Store per-stage data (only for main PnR stages)
                        if stage_name.lower() in stage_order or stage_name.lower() == 'edi_plan':
                            # Normalize stage name
                            normalized_name = 'plan' if stage_name.lower() == 'edi_plan' else stage_name.lower()
                            
                            if duration_str.isdigit() and int(duration_str) > 0:
                                runtime_hours = int(duration_str) / 3600
                                
                                # For RUNNING stages, get actual log file mtime for accurate stall detection
                                log_mtime_str = end_time_str  # Default to calculated end time
                                if status == 'RUN' and log_path and os.path.exists(log_path):
                                    try:
                                        actual_mtime = os.path.getmtime(log_path)
                                        log_mtime_str = time.strftime("%m/%d %H:%M", time.localtime(actual_mtime))
                                    except OSError:
                                        pass  # Keep calculated end time
                                
                                pnr_status['stages'][normalized_name] = {
                                    'runtime_hours': runtime_hours,
                                    'start_time': start_time_str,
                                    'end_time': end_time_str,
                                    'status': 'OK' if status == 'DONE' else ('RUNNING' if status == 'RUN' else status),
                                    'log_mtime': log_mtime_str  # Actual log mtime for RUNNING, calculated for DONE
                                }
                                
                                if status == 'DONE':
                                    pnr_status['completed_stages'].append(normalized_name)
                                elif status == 'RUN':
                                    pnr_status['running_stage'] = normalized_name
                                    pnr_status['status'] = 'RUNNING'
                                    pnr_status['current_stage'] = normalized_name
            
            # Get file modification time as last update
            mtime = os.path.getmtime(prc_status_file)
            pnr_status['last_update'] = time.strftime("%m/%d %H:%M", time.localtime(mtime))
            
            # If no running stage but has completed stages, mark as completed
            if not pnr_status['running_stage'] and pnr_status['completed_stages']:
                pnr_status['status'] = 'COMPLETED'
            
            # CRITICAL FIX: Recalculate sequential timestamps for PnR stages
            # PnR stages run sequentially, not in parallel
            # Each stage starts when the previous stage ends
            # For RUNNING stages: calculate start from previous stage end (sequential flow)
            #                     OR use current_time - duration if no previous stage
            if pnr_status['stages']:
                stage_order_list = ['setup', 'plan', 'place', 'cts', 'route', 'postroute']
                
                # Find the first stage with a valid timestamp to use as the starting point
                current_end_epoch = None
                for stage in stage_order_list:
                    if stage in pnr_status['stages']:
                        stage_data = pnr_status['stages'][stage]
                        
                        if current_end_epoch is None:
                            # First stage: use its original start time from log
                            if stage_data['start_time'] and stage_data['start_time'] != 'N/A':
                                try:
                                    # Parse the start time to get epoch
                                    start_str = stage_data['start_time']
                                    year = time.localtime().tm_year
                                    full_datetime = f"{year}/{start_str}"
                                    start_struct = time.strptime(full_datetime, "%Y/%m/%d %H:%M")
                                    current_end_epoch = time.mktime(start_struct)
                                    
                                    # Calculate end time
                                    duration_seconds = stage_data['runtime_hours'] * 3600
                                    current_end_epoch += duration_seconds
                                    if stage_data.get('status') != 'RUNNING':
                                        stage_data['end_time'] = time.strftime("%m/%d %H:%M", time.localtime(current_end_epoch))
                                except:
                                    # If parsing fails, use duration to calculate from start epoch
                                    current_end_epoch = None
                        else:
                            # Subsequent stages: start when previous stage ended
                            stage_data['start_time'] = time.strftime("%m/%d %H:%M", time.localtime(current_end_epoch))
                            duration_seconds = stage_data['runtime_hours'] * 3600
                            current_end_epoch += duration_seconds
                            if stage_data.get('status') != 'RUNNING':
                                stage_data['end_time'] = time.strftime("%m/%d %H:%M", time.localtime(current_end_epoch))
        
        except Exception as e:
            if self.verbose:
                print(f"  Warning: Error extracting PnR status for {ipo_label}: {e}")
        
        return pnr_status
    
    def _extract_ipo_pnr_status_from_all_files(self, prc_status_files: list, ipo_label: str) -> Dict:
        """Extract PnR status for specific IPO by reading ALL prc.status files and merging data.
        
        This handles workareas with multiple prc.status files:
        - {unit}.prc.status (original PnR with full stages: place, cts, route, postroute)
        - eco_{unit}.prc.status (ECO flows with only copy_db/eco stages)
        
        We merge data from all files, preferring actual PnR stages over ECO-only stages.
        
        Args:
            prc_status_files: List of paths to prc.status files
            ipo_label: IPO identifier (e.g., 'ipo1000')
        
        Returns:
            Dictionary with merged PnR status data including per-stage runtimes
        """
        # Initialize result structure
        merged_status = {
            'status': 'NOT_RUN',
            'current_stage': None,
            'completed_stages': [],
            'running_stage': None,
            'start_time': None,
            'end_time': None,
            'last_update': None,
            'stages': {},
            'is_real_pnr': False  # True if this IPO has actual PnR stages (not just ECO)
        }
        
        # Define what constitutes "real PnR" stages vs "ECO-only" stages
        real_pnr_stages = {'place', 'cts', 'route', 'postroute', 'plan', 'edi_plan'}
        eco_only_stages = {'copy_db', 'eco', 'export'}
        
        # Read from all prc.status files
        for prc_file in prc_status_files:
            try:
                pnr_data = self._extract_ipo_pnr_status(prc_file, ipo_label)
                
                if pnr_data and pnr_data['stages']:
                    # Check if this file has real PnR stages
                    stage_names = set(pnr_data['stages'].keys())
                    has_real_pnr = bool(stage_names & real_pnr_stages)
                    
                    if has_real_pnr:
                        # This file has real PnR stages - use this data and mark as real PnR
                        merged_status['is_real_pnr'] = True
                        
                        # Merge stages (prefer data from files with real PnR stages)
                        for stage, stage_data in pnr_data['stages'].items():
                            if stage not in merged_status['stages']:
                                merged_status['stages'][stage] = stage_data
                        
                        # Update status fields
                        if pnr_data['status'] != 'NOT_RUN':
                            merged_status['status'] = pnr_data['status']
                        if pnr_data['completed_stages']:
                            merged_status['completed_stages'].extend(pnr_data['completed_stages'])
                        if pnr_data['running_stage']:
                            merged_status['running_stage'] = pnr_data['running_stage']
                        if pnr_data['current_stage']:
                            merged_status['current_stage'] = pnr_data['current_stage']
                        if pnr_data['last_update']:
                            merged_status['last_update'] = pnr_data['last_update']
                    
                    elif not merged_status['is_real_pnr']:
                        # Only use ECO-only data if we haven't found real PnR stages yet
                        # But DON'T mark as real PnR - these are ECO IPOs
                        pass  # Skip ECO-only IPOs for PnR display
                        
            except Exception as e:
                if self.verbose:
                    print(f"  Warning: Error reading {prc_file} for {ipo_label}: {e}")
        
        # Remove duplicates from completed_stages
        merged_status['completed_stages'] = list(set(merged_status['completed_stages']))
        
        return merged_status
    
    def _extract_ipo_signoff_flows(self, signoff_path: str, ipo_label: str) -> Dict:
        """Extract signoff flow data for specific IPO or root
        
        Args:
            signoff_path: Path to signoff_flow directory (or parent for nv_star)
            ipo_label: IPO identifier (e.g., 'ipo1000' or 'root')
        
        Returns:
            Dictionary with flow data
        """
        flows = {}
        
        try:
            # Star extraction - Pattern 1: signoff_flow/star (separate signoff)
            star_path = os.path.join(signoff_path, "star")
            if os.path.exists(star_path):
                star_log = os.path.join(star_path, "log/star.log")
                if os.path.exists(star_log):
                    flows['star'] = self._detect_flow_status(star_log, None)
            
            # Pattern 2: export/nv_star (separate signoff location)
            # signoff_path could be: $WA/signoff_flow or $WA/pnr_flow/nv_flow/design/ipo*/nbu_signoff/signoff_flow
            if 'nbu_signoff' in signoff_path:
                # For IPO-specific: go up multiple levels to reach workarea root
                parent_path = signoff_path
                for _ in range(5):  # Go up: signoff_flow -> nbu_signoff -> ipo* -> design -> nv_flow -> pnr_flow
                    parent_path = os.path.dirname(parent_path)
                # Also get the IPO path for Pattern 3 detection
                ipo_path = signoff_path
                for _ in range(2):  # Go up: signoff_flow -> nbu_signoff -> ipo*
                    ipo_path = os.path.dirname(ipo_path)
            else:
                # For root-level: signoff_path is $WA/signoff_flow, so go up one level
                parent_path = os.path.dirname(signoff_path)
                ipo_path = None
            
            nv_star_path = os.path.join(parent_path, "export/nv_star")
            if os.path.exists(nv_star_path) and 'star' not in flows:
                # Look for actual Star extraction logs (not PRIME orchestrator logs!)
                # Pattern: export/nv_star/*/ipo*/LOGs/flow_logs/make.STAR*.log
                star_logs = glob.glob(os.path.join(nv_star_path, "*/ipo*/LOGs/flow_logs/make.STAR*.log"))
                
                if star_logs:
                    # Select latest Star log by modification time (handles multiple runs)
                    latest_star_log = max(star_logs, key=os.path.getmtime)
                    
                    # Extract actual runtime from log content (same as Pattern 3)
                    runtime_hours = None
                    start_time_str = None
                    end_time_str = None
                    
                    try:
                        with open(latest_star_log, 'r', encoding='utf-8', errors='ignore') as f:
                            log_content = f.read()
                        
                        import re
                        
                        # Extract "Run time : NNNN sec." from log
                        runtime_match = re.search(r'Run time\s*:\s*(\d+)\s*sec', log_content)
                        if runtime_match:
                            runtime_seconds = int(runtime_match.group(1))
                            runtime_hours = runtime_seconds / 3600
                        
                        # Extract start time: "Starting STAR job............. Mon Nov 10 20:06:21 IST 2025"
                        start_match = re.search(r'Starting STAR job.*?\s+\w+\s+(\w+)\s+(\d+)\s+(\d+):(\d+):(\d+)\s+\w+\s+(\d+)', log_content)
                        if start_match:
                            month_name = start_match.group(1)
                            day = start_match.group(2)
                            hour = start_match.group(3)
                            minute = start_match.group(4)
                            # Convert month name to number
                            month_map = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06',
                                       'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'}
                            month_num = month_map.get(month_name, '01')
                            start_time_str = f"{month_num}/{int(day):02d} {hour}:{minute}"
                        
                        # Extract end time: "Ending STAR job............. Mon Nov 10 20:34:03 IST 2025"
                        end_match = re.search(r'Ending STAR job.*?\s+\w+\s+(\w+)\s+(\d+)\s+(\d+):(\d+):(\d+)\s+\w+\s+(\d+)', log_content)
                        if end_match:
                            month_name = end_match.group(1)
                            day = end_match.group(2)
                            hour = end_match.group(3)
                            minute = end_match.group(4)
                            month_map = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06',
                                       'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'}
                            month_num = month_map.get(month_name, '01')
                            end_time_str = f"{month_num}/{int(day):02d} {hour}:{minute}"
                        
                        # If we have runtime, create the flow entry
                        if runtime_hours is not None:
                            flows['star'] = {
                                'status': 'COMPLETED',
                                'runtime': f"{runtime_hours:.2f}h",
                                'start_time': start_time_str or 'N/A',
                                'end_time': end_time_str or 'N/A',
                                'log_file': latest_star_log,
                                'type': 'separate_signoff'
                            }
                    except Exception as e:
                        # Fallback: use file mtime if we can't extract runtime
                        flows['star'] = {
                            'status': 'COMPLETED',
                            'runtime': 'N/A',
                            'start_time': 'N/A',
                            'end_time': time.strftime("%m/%d %H:%M", time.localtime(os.path.getmtime(latest_star_log))),
                            'log_file': latest_star_log,
                            'type': 'separate_signoff'
                        }
            
            # Pattern 3: Star as part of PnR flow (ipo*/LOGs/flow_logs/make.STAR*.log)
            if 'star' not in flows and ipo_path:
                star_pnr_logs = glob.glob(os.path.join(ipo_path, "LOGs/flow_logs/make.STAR*.log"))
                if star_pnr_logs:
                    latest_star_log = max(star_pnr_logs, key=os.path.getmtime)
                    
                    # Extract actual runtime from log content (NOT filename timestamps)
                    # Filename timestamps are make invocation times, not Star execution times!
                    runtime_hours = None
                    start_time_str = None
                    end_time_str = None
                    
                    try:
                        with open(latest_star_log, 'r', encoding='utf-8', errors='ignore') as f:
                            log_content = f.read()
                        
                        import re
                        
                        # Extract "Run time : NNNN sec." from log
                        runtime_match = re.search(r'Run time\s*:\s*(\d+)\s*sec', log_content)
                        if runtime_match:
                            runtime_seconds = int(runtime_match.group(1))
                            runtime_hours = runtime_seconds / 3600
                        
                        # Extract start time: "Starting STAR job............. Mon Nov 10 20:06:21 IST 2025"
                        start_match = re.search(r'Starting STAR job.*?\s+\w+\s+(\w+)\s+(\d+)\s+(\d+):(\d+):(\d+)\s+\w+\s+(\d+)', log_content)
                        if start_match:
                            month_name = start_match.group(1)
                            day = start_match.group(2)
                            hour = start_match.group(3)
                            minute = start_match.group(4)
                            # Convert month name to number
                            month_map = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06',
                                       'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'}
                            month_num = month_map.get(month_name, '01')
                            start_time_str = f"{month_num}/{int(day):02d} {hour}:{minute}"
                        
                        # Extract end time: "Ending STAR job............. Mon Nov 10 20:34:03 IST 2025"
                        end_match = re.search(r'Ending STAR job.*?\s+\w+\s+(\w+)\s+(\d+)\s+(\d+):(\d+):(\d+)\s+\w+\s+(\d+)', log_content)
                        if end_match:
                            month_name = end_match.group(1)
                            day = end_match.group(2)
                            hour = end_match.group(3)
                            minute = end_match.group(4)
                            month_map = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06',
                                       'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'}
                            month_num = month_map.get(month_name, '01')
                            end_time_str = f"{month_num}/{int(day):02d} {hour}:{minute}"
                        
                        # If we have runtime, create the flow entry
                        if runtime_hours is not None:
                            flows['star'] = {
                                'status': 'COMPLETED',
                                'runtime': f"{runtime_hours:.2f}h",
                                'start_time': start_time_str or 'N/A',
                                'end_time': end_time_str or 'N/A',
                                'log_file': latest_star_log,
                                'type': 'pnr_integrated'  # Flag to indicate this is part of PnR
                            }
                        else:
                            # Fallback: use file mtime if we can't extract runtime
                            flows['star'] = {
                                'status': 'COMPLETED',
                                'runtime': 'N/A',
                                'start_time': 'N/A',
                                'end_time': time.strftime("%m/%d %H:%M", time.localtime(os.path.getmtime(latest_star_log))),
                                'log_file': latest_star_log,
                                'type': 'pnr_integrated'
                            }
                    except Exception as e:
                        # Fallback: just use file mtime
                        flows['star'] = {
                            'status': 'COMPLETED',
                            'runtime': 'N/A',
                            'start_time': 'N/A',
                            'end_time': time.strftime("%m/%d %H:%M", time.localtime(os.path.getmtime(latest_star_log))),
                            'log_file': latest_star_log,
                            'type': 'pnr_integrated'
                        }
            
            # Pattern 4: Star under ipo*/nbu_signoff/export/nv_star/ (IPO-specific separate signoff)
            if 'star' not in flows and ipo_path:
                ipo_nbu_star_path = os.path.join(ipo_path, "nbu_signoff/export/nv_star")
                if os.path.exists(ipo_nbu_star_path):
                    # Look for Star logs: ipo*/nbu_signoff/export/nv_star/design/ipo*/LOGs/flow_logs/make.STAR*.log
                    star_logs = glob.glob(os.path.join(ipo_nbu_star_path, "*/ipo*/LOGs/flow_logs/make.STAR*.log"))
                    
                    if star_logs:
                        # Select latest Star log by modification time
                        latest_star_log = max(star_logs, key=os.path.getmtime)
                        
                        # Extract actual runtime from log content (same as Pattern 3)
                        runtime_hours = None
                        start_time_str = None
                        end_time_str = None
                        
                        try:
                            with open(latest_star_log, 'r', encoding='utf-8', errors='ignore') as f:
                                log_content = f.read()
                            
                            import re
                            
                            # Extract "Run time : NNNN sec." from log
                            runtime_match = re.search(r'Run time\s*:\s*(\d+)\s*sec', log_content)
                            if runtime_match:
                                runtime_seconds = int(runtime_match.group(1))
                                runtime_hours = runtime_seconds / 3600
                            
                            # Extract start time
                            start_match = re.search(r'Starting STAR job.*?\s+\w+\s+(\w+)\s+(\d+)\s+(\d+):(\d+):(\d+)\s+\w+\s+(\d+)', log_content)
                            if start_match:
                                month_name = start_match.group(1)
                                day = start_match.group(2)
                                hour = start_match.group(3)
                                minute = start_match.group(4)
                                month_map = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06',
                                           'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'}
                                month_num = month_map.get(month_name, '01')
                                start_time_str = f"{month_num}/{int(day):02d} {hour}:{minute}"
                            
                            # Extract end time
                            end_match = re.search(r'Ending STAR job.*?\s+\w+\s+(\w+)\s+(\d+)\s+(\d+):(\d+):(\d+)\s+\w+\s+(\d+)', log_content)
                            if end_match:
                                month_name = end_match.group(1)
                                day = end_match.group(2)
                                hour = end_match.group(3)
                                minute = end_match.group(4)
                                month_map = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06',
                                           'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'}
                                month_num = month_map.get(month_name, '01')
                                end_time_str = f"{month_num}/{int(day):02d} {hour}:{minute}"
                            
                            # If we have runtime, create the flow entry
                            if runtime_hours is not None:
                                flows['star'] = {
                                    'status': 'COMPLETED',
                                    'runtime': f"{runtime_hours:.2f}h",
                                    'start_time': start_time_str or 'N/A',
                                    'end_time': end_time_str or 'N/A',
                                    'log_file': latest_star_log,
                                    'type': 'ipo_nbu_signoff'  # Flag for IPO-specific nbu_signoff Star
                                }
                        except Exception as e:
                            # Fallback: use file mtime if we can't extract runtime
                            flows['star'] = {
                                'status': 'COMPLETED',
                                'runtime': 'N/A',
                                'start_time': 'N/A',
                                'end_time': time.strftime("%m/%d %H:%M", time.localtime(os.path.getmtime(latest_star_log))),
                                'log_file': latest_star_log,
                                'type': 'ipo_nbu_signoff'
                            }
            
            # Auto PT
            auto_pt_path = os.path.join(signoff_path, "auto_pt")
            if os.path.exists(auto_pt_path):
                flows['auto_pt'] = self._detect_flow_status(
                    os.path.join(auto_pt_path, "log/auto_pt.log"),
                    None
                )
                # Count work directories
                work_dirs = glob.glob(os.path.join(auto_pt_path, "work_*"))
                flows['auto_pt']['work_count'] = len([d for d in work_dirs if os.path.isdir(d)])
                
                # Check for Auto PT Fix (looks for pt_eco_out_*_final.tcl and eco_*.log)
                auto_pt_fix_detected = False
                eco_logs = []
                for work_dir in work_dirs:
                    if not os.path.isdir(work_dir):
                        continue
                    # Look for pt_eco_out_*_final.tcl files
                    eco_tcl_files = glob.glob(os.path.join(work_dir, "pt_eco_out_*_final.tcl"))
                    if eco_tcl_files:
                        auto_pt_fix_detected = True
                        # Look for corresponding eco_*.log file
                        eco_log_files = glob.glob(os.path.join(work_dir, "eco_*.log"))
                        eco_logs.extend(eco_log_files)
                
                if auto_pt_fix_detected and eco_logs:
                    # Get the latest eco log
                    latest_eco_log = max(eco_logs, key=os.path.getmtime)
                    flows['auto_pt_fix'] = self._detect_flow_status(latest_eco_log, None)
            
            # Auto PT Fix (standalone location)
            auto_pt_fix_path = os.path.join(signoff_path, "auto_pt_fix")
            if os.path.exists(auto_pt_fix_path):
                auto_pt_fix_log = os.path.join(auto_pt_fix_path, "log/auto_pt_fix.log")
                if os.path.exists(auto_pt_fix_log):
                    flows['auto_pt_fix'] = self._detect_flow_status(auto_pt_fix_log, None)
            
            # Formal verification (check multiple types)
            # Type 1: Generic formal_flow
            formal_path = os.path.join(signoff_path, "formal_flow")
            if os.path.exists(formal_path):
                formal_log = os.path.join(formal_path, "log/formal.log")
                if os.path.exists(formal_log):
                    flows['formal'] = self._detect_flow_status(formal_log, None)
            
            # Type 2: rtl_vs_pnr_fm
            formal_rtl_pnr = os.path.join(os.path.dirname(signoff_path) if 'signoff_flow' in signoff_path else signoff_path, "formal_flow/rtl_vs_pnr_fm")
            if os.path.exists(formal_rtl_pnr):
                # Look for log files
                formal_logs = glob.glob(os.path.join(formal_rtl_pnr, "*/log/*.log"))
                if not formal_logs:
                    formal_logs = glob.glob(os.path.join(formal_rtl_pnr, "log/*.log"))
                if formal_logs:
                    flows['formal_rtl_pnr'] = self._detect_flow_status(max(formal_logs, key=os.path.getmtime), None)
            
            # Type 3: rtl_vs_pnr_bbox_fm
            formal_bbox = os.path.join(os.path.dirname(signoff_path) if 'signoff_flow' in signoff_path else signoff_path, "formal_flow/rtl_vs_pnr_bbox_fm")
            if os.path.exists(formal_bbox):
                formal_logs = glob.glob(os.path.join(formal_bbox, "*/log/*.log"))
                if not formal_logs:
                    formal_logs = glob.glob(os.path.join(formal_bbox, "log/*.log"))
                if formal_logs:
                    flows['formal_bbox'] = self._detect_flow_status(max(formal_logs, key=os.path.getmtime), None)
            
            # GL Check
            gl_check_path = os.path.join(signoff_path, "gl-check")
            if os.path.exists(gl_check_path):
                gl_check_logs = glob.glob(os.path.join(gl_check_path, "*/log/gl-check.log"))
                if not gl_check_logs:
                    gl_check_logs = glob.glob(os.path.join(gl_check_path, "log/gl-check.log"))
                if gl_check_logs:
                    flows['gl_check'] = self._detect_flow_status(max(gl_check_logs, key=os.path.getmtime), None)
            
            # NV Gate ECO
            nv_gate_eco_path = os.path.join(signoff_path, "nv_gate_eco")
            if os.path.exists(nv_gate_eco_path):
                # Look for log files (multiple patterns)
                eco_logs = []
                # Pattern 1: */log/*.log
                eco_logs.extend(glob.glob(os.path.join(nv_gate_eco_path, "*/log/*.log")))
                # Pattern 2: log/*.log
                eco_logs.extend(glob.glob(os.path.join(nv_gate_eco_path, "log/*.log")))
                # Pattern 3: */ipo*/LOGs/PRIME/*.log (common pattern)
                eco_logs.extend(glob.glob(os.path.join(nv_gate_eco_path, "*/ipo*/LOGs/PRIME/eco.log")))
                # Pattern 4: LOGs/PRIME/*.log
                eco_logs.extend(glob.glob(os.path.join(nv_gate_eco_path, "LOGs/PRIME/eco.log")))
                
                if eco_logs:
                    eco_status = self._detect_flow_status(max(eco_logs, key=os.path.getmtime), None)
                    
                    # Try to extract runtime from prc.status if available
                    prc_status_files = glob.glob(os.path.join(nv_gate_eco_path, "*.prc.status"))
                    if prc_status_files and not eco_status.get('runtime'):
                        try:
                            prc_status_path = prc_status_files[0]
                            with open(prc_status_path, 'r', encoding='utf-8', errors='ignore') as f:
                                prc_content = f.read()
                            
                            # Extract all DONE steps and sum their durations
                            total_seconds = 0
                            for line in prc_content.split('\n'):
                                if 'DONE' in line and not line.startswith('#'):
                                    parts = line.split()
                                    if len(parts) >= 5 and parts[4].isdigit():
                                        total_seconds += int(parts[4])
                            
                            if total_seconds > 0:
                                total_hours = total_seconds / 3600
                                eco_status['runtime'] = f"{total_hours:.2f}h"
                                # Calculate start/end times from prc.status
                                end_mtime = os.path.getmtime(prc_status_path)
                                start_epoch = end_mtime - total_seconds
                                eco_status['start_time'] = time.strftime("%m/%d %H:%M", time.localtime(start_epoch))
                                eco_status['end_time'] = time.strftime("%m/%d %H:%M", time.localtime(end_mtime))
                                # Update status to COMPLETED if we have runtime data
                                if eco_status.get('status') == 'FAILED':
                                    eco_status['status'] = 'COMPLETED'
                        except Exception:
                            pass
                    
                    flows['nv_gate_eco'] = eco_status
            
            # PV Flows (LVS, DRC, Antenna) - Simple structure
            pv_path = os.path.join(signoff_path, "pv_flow")
            if os.path.exists(pv_path):
                # LVS
                lvs_log = os.path.join(pv_path, "lvs/log/lvs.log")
                if os.path.exists(lvs_log):
                    flows['lvs'] = self._detect_flow_status(lvs_log, None)
                
                # DRC
                drc_log = os.path.join(pv_path, "drc/log/drc.log")
                if os.path.exists(drc_log):
                    flows['drc'] = self._detect_flow_status(drc_log, None)
                
                # Antenna
                antenna_log = os.path.join(pv_path, "antenna/log/antenna.log")
                if os.path.exists(antenna_log):
                    flows['antenna'] = self._detect_flow_status(antenna_log, None)
            
            # NV_PV (PRIME-based PV flow with prc.status)
            # Path: nbu_signoff/pv_flow/nv_flow/pv_{design}.prc.status
            # Note: pv_flow is under nbu_signoff, NOT under signoff_flow
            nbu_signoff_path = os.path.dirname(signoff_path) if 'signoff_flow' in signoff_path else signoff_path
            nv_pv_base = os.path.join(nbu_signoff_path, "pv_flow", "nv_flow")
            if os.path.exists(nv_pv_base):
                nv_pv_prc_pattern = os.path.join(nv_pv_base, "pv_*.prc.status")
                nv_pv_prc_files = glob.glob(nv_pv_prc_pattern)
                if nv_pv_prc_files:
                    nv_pv_prc = nv_pv_prc_files[0]
                    nv_pv_data = self._extract_nv_pv_runtime(nv_pv_prc)
                    if nv_pv_data:
                        flows['nv_pv'] = nv_pv_data
        
        except Exception as e:
            print(f"  Warning: Error extracting signoff flows for {ipo_label}: {e}")
        
        return flows
    
    def _extract_nv_pv_runtime(self, prc_status_file: str) -> Optional[Dict]:
        """Extract NV_PV runtime from prc.status file.
        
        NV_PV (PRIME-based Physical Verification) has stages like:
        - temp_run_lvs (LVS verification)
        - temp_run_drc (DRC checking)
        - temp_run_ant (Antenna checking)
        
        Args:
            prc_status_file: Path to pv_{design}.prc.status file
            
        Returns:
            Dictionary with nv_pv flow data or None if not found
        """
        try:
            if not os.path.exists(prc_status_file):
                return None
            
            with open(prc_status_file, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # Parse prc.status to extract key stages
            # Key PV steps: temp_run_lvs, temp_run_drc, temp_run_ant (or drc_lvs, ant)
            pv_steps = ['temp_run_lvs', 'temp_run_drc', 'temp_run_ant', 'drc_lvs', 'ant']
            total_seconds = 0
            step_data = []
            start_time = None
            end_time = None
            key_steps_done = 0
            key_steps_running = False
            
            for line in content.split('\n'):
                if line.startswith('#') or not line.strip():
                    continue
                
                parts = line.split()
                if len(parts) >= 5:
                    step_name = parts[2]
                    status = parts[3]
                    duration_str = parts[4]
                    
                    # Sum up key PV step durations and track their status
                    if step_name in pv_steps:
                        if duration_str.isdigit():
                            duration = int(duration_str)
                            if duration > 0:
                                total_seconds += duration
                                step_data.append((step_name, duration / 3600))
                        
                        if status == 'DONE':
                            key_steps_done += 1
                        elif status == 'RUN':
                            key_steps_running = True
                    
                    # Extract timestamps from log file paths
                    if len(parts) >= 6 and 'BEGIN' in step_name:
                        log_path = parts[-1]
                        timestamp_match = re.search(r'(\d{8})(\d{6})', log_path)
                        if timestamp_match:
                            date_str = timestamp_match.group(1)
                            time_str = timestamp_match.group(2)
                            start_time = f"{date_str[4:6]}/{date_str[6:8]} {time_str[:2]}:{time_str[2:4]}"
            
            if total_seconds == 0:
                return None
            
            total_hours = total_seconds / 3600
            
            # Determine status based on KEY steps only (LVS, DRC, Antenna)
            # Ignore release_flow steps which are often UNLAUNCHED but not required for signoff
            if key_steps_running:
                status = 'RUNNING'
            elif key_steps_done >= 3:  # At least LVS, DRC, Antenna done
                status = 'COMPLETED'
            elif key_steps_done > 0:
                status = 'COMPLETED'  # Partial completion is still useful
            else:
                status = 'INCOMPLETE'
            
            # Calculate end time from file mtime
            end_mtime = os.path.getmtime(prc_status_file)
            end_time = time.strftime("%m/%d %H:%M", time.localtime(end_mtime))
            
            # Calculate start time if not found
            if not start_time:
                start_epoch = end_mtime - total_seconds
                start_time = time.strftime("%m/%d %H:%M", time.localtime(start_epoch))
            
            return {
                'status': status,
                'runtime': f"{total_hours:.2f}h",
                'start_time': start_time,
                'end_time': end_time,
                'steps': step_data,  # List of (step_name, runtime_hours)
                'prc_status_file': prc_status_file
            }
            
        except Exception as e:
            return None
    
    def _detect_flow_status(self, log_file: str, completion_marker: Optional[str] = None) -> Dict:
        """Detect flow status: RUNNING/COMPLETED/FAILED/STALE/NOT_RUN
        
        Args:
            log_file: Path to flow log file
            completion_marker: Optional path to completion marker file
        
        Returns:
            Dictionary with status, runtime, timestamps, etc.
        """
        status_data = {
            'status': 'NOT_RUN',
            'runtime': None,
            'start_time': None,
            'end_time': None,
            'elapsed_seconds': None,
            'minutes_since_update': None,
            'log_file': log_file
        }
        
        try:
            # Check if log file exists
            if not os.path.exists(log_file):
                return status_data
            
            # Get log file modification time
            mtime = os.path.getmtime(log_file)
            current_time = time.time()
            minutes_since_update = (current_time - mtime) / 60
            status_data['minutes_since_update'] = int(minutes_since_update)
            
            # Check for completion marker
            if completion_marker and os.path.exists(completion_marker):
                status_data['status'] = 'COMPLETED'
                status_data['end_time'] = time.strftime("%m/%d %H:%M", time.localtime(mtime))
                return status_data
            
            # Check log content for errors or completion
            try:
                with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                    lines = f.readlines()
                    
                    # Read first 50 lines for start time
                    first_lines = lines[:50] if len(lines) > 50 else lines
                    first_content = ''.join(first_lines)
                    
                    # Try to extract start time from first lines
                    start_time_patterns = [
                        r'Started at\s+(\d{2}/\d{2}\s+\d{2}:\d{2})',
                        r'Start time:\s+(\d{2}/\d{2}\s+\d{2}:\d{2})',
                        r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2})'  # ISO format
                    ]
                    for pattern in start_time_patterns:
                        start_match = re.search(pattern, first_content, re.IGNORECASE)
                        if start_match:
                            try:
                                start_str = start_match.group(1)
                                # Try to parse and standardize format
                                if '-' in start_str:  # ISO format
                                    start_epoch = time.mktime(time.strptime(start_str, "%Y-%m-%d %H:%M:%S"))
                                    status_data['start_time'] = time.strftime("%m/%d %H:%M", time.localtime(start_epoch))
                                else:
                                    status_data['start_time'] = start_str
                            except:
                                pass
                            break
                    
                    # Read last 100 lines for completion status
                    last_lines = lines[-100:] if len(lines) > 100 else lines
                    content = ''.join(last_lines)
                    
                    # Check for error patterns (be specific to avoid false positives)
                    error_patterns = [
                        r'ERROR[:\s]',  # ERROR: or ERROR followed by space
                        r'\bFATAL\b',
                        r'Abort',
                        r'FAILED.*Error'  # FAILED with Error nearby
                    ]
                    for pattern in error_patterns:
                        if re.search(pattern, content, re.IGNORECASE):
                            status_data['status'] = 'FAILED'
                            status_data['end_time'] = time.strftime("%m/%d %H:%M", time.localtime(mtime))
                            return status_data
                    
                    # Check for completion patterns
                    completion_patterns = [r'Successfully completed', r'Flow completed', r'Elapsed time']
                    for pattern in completion_patterns:
                        if re.search(pattern, content, re.IGNORECASE):
                            status_data['status'] = 'COMPLETED'
                            status_data['end_time'] = time.strftime("%m/%d %H:%M", time.localtime(mtime))
                            
                            # Try to extract runtime (multiple patterns)
                            runtime_extracted = False
                            
                            # Pattern 1: Standard "Elapsed time for this session: X hours"
                            runtime_match = re.search(r'Elapsed time.*?(\d+\.?\d*)\s*(hours?|minutes?|seconds?)', content, re.IGNORECASE)
                            if runtime_match:
                                value = float(runtime_match.group(1))
                                unit = runtime_match.group(2).lower()
                                if 'hour' in unit:
                                    status_data['runtime'] = f"{value:.2f}h"
                                    if not status_data['start_time']:
                                        start_epoch = mtime - (value * 3600)
                                        status_data['start_time'] = time.strftime("%m/%d %H:%M", time.localtime(start_epoch))
                                elif 'min' in unit:
                                    status_data['runtime'] = f"{value/60:.2f}h"
                                    if not status_data['start_time']:
                                        start_epoch = mtime - (value * 60)
                                        status_data['start_time'] = time.strftime("%m/%d %H:%M", time.localtime(start_epoch))
                                else:  # seconds
                                    status_data['runtime'] = f"{value/3600:.2f}h"
                                    if not status_data['start_time']:
                                        start_epoch = mtime - value
                                        status_data['start_time'] = time.strftime("%m/%d %H:%M", time.localtime(start_epoch))
                                runtime_extracted = True
                            
                            # Pattern 2: GL Check format - "Elapsed Time: Xs" (sum all occurrences)
                            if not runtime_extracted:
                                # Read entire log file for GL Check format
                                try:
                                    with open(log_file, 'r', encoding='utf-8', errors='ignore') as f2:
                                        full_content = f2.read()
                                    
                                    # Find all "Elapsed Time: Xs" entries
                                    elapsed_matches = re.findall(r'Elapsed Time:\s*(\d+)s', full_content, re.IGNORECASE)
                                    if elapsed_matches:
                                        total_seconds = sum(int(x) for x in elapsed_matches)
                                        total_hours = total_seconds / 3600
                                        if total_hours > 0.01:  # Only show if > 36 seconds
                                            status_data['runtime'] = f"{total_hours:.2f}h"
                                            if not status_data['start_time']:
                                                start_epoch = mtime - total_seconds
                                                status_data['start_time'] = time.strftime("%m/%d %H:%M", time.localtime(start_epoch))
                                            runtime_extracted = True
                                except Exception:
                                    pass
                            
                            return status_data
            except Exception:
                pass  # Continue with status detection based on timing
            
            # Determine if RUNNING or STALE based on recent activity
            if minutes_since_update < 5:  # Modified within last 5 minutes
                status_data['status'] = 'RUNNING'
                # Try to extract start time from log
                try:
                    with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                        first_lines = [next(f) for _ in range(50)]  # First 50 lines
                        for line in first_lines:
                            # Look for timestamp patterns
                            timestamp_match = re.search(r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2})', line)
                            if timestamp_match:
                                start_str = timestamp_match.group(1)
                                start_epoch = time.mktime(time.strptime(start_str, "%Y-%m-%d %H:%M:%S"))
                                elapsed = current_time - start_epoch
                                status_data['elapsed_seconds'] = int(elapsed)
                                status_data['start_time'] = time.strftime("%m/%d %H:%M", time.localtime(start_epoch))
                                break
                except Exception:
                    # If can't extract start time, use file creation time as estimate
                    try:
                        ctime = os.path.getctime(log_file)
                        elapsed = current_time - ctime
                        status_data['elapsed_seconds'] = int(elapsed)
                        status_data['start_time'] = time.strftime("%m/%d %H:%M", time.localtime(ctime))
                    except Exception:
                        pass
            else:
                # Log not modified recently and no completion marker
                status_data['status'] = 'STALE'
        
        except Exception as e:
            print(f"  Warning: Error detecting status for {log_file}: {e}")
        
        return status_data

    def _calculate_start_time_from_duration(self, end_time_epoch: float, runtime_str: str) -> Optional[float]:
        """Calculate start time from end time and runtime duration string
        
        Args:
            end_time_epoch: End time as epoch timestamp
            runtime_str: Runtime duration string (e.g., "2.5 hours", "45 minutes")
            
        Returns:
            Start time as epoch timestamp or None if calculation fails
        """
        try:
            # Extract duration in hours from runtime string
            hours_match = re.search(r'(\d+\.?\d*)\s*hours?', runtime_str)
            if hours_match:
                duration_hours = float(hours_match.group(1))
                duration_seconds = int(duration_hours * 3600)
                start_time_epoch = end_time_epoch - duration_seconds
                
                start_str = time.strftime("%m/%d %H:%M", time.localtime(start_time_epoch))
                end_str = time.strftime("%m/%d %H:%M", time.localtime(end_time_epoch))
                return start_str, end_str
        except Exception:
            pass
        return None, None
    
    def _print_unified_flow_timeline_table(self, runtime_data: Dict[str, Any], pnr_runtimes: Dict[str, Any], 
                                          runtime_timestamps: Optional[Dict[str, Any]] = None, 
                                          ipo_signoff_data: Optional[Dict[str, Dict]] = None,
                                          deleted_ipos: Optional[List[str]] = None,
                                          prc_running_flows: Optional[Dict] = None) -> None:
        """Print unified flow timeline table combining DC/fast-DC + PnR stages + Signoff flows
        
        This table provides:
        - Hierarchical view: Synthesis -> PnR -> Signoff
        - Subtotals for each phase
        - Grand total runtime
        - Bottleneck % analysis (which stage took longest)
        - Critical path identification
        - Timeline visibility (start -> end times)
        - Aggregated view across all IPOs
        - Running status from prc.status file
        
        Args:
            runtime_data: Dictionary of runtime data for each stage
            pnr_runtimes: Dictionary of PnR stage runtimes per IPO
            runtime_timestamps: Dictionary of start/end timestamps
            ipo_signoff_data: Dictionary of IPO-specific signoff flow data
            deleted_ipos: List of IPO names whose directories were deleted
            prc_running_flows: Dictionary of running flows from prc.status file
        """
        if deleted_ipos is None:
            deleted_ipos = []
        if runtime_timestamps is None:
            runtime_timestamps = {}
        if prc_running_flows is None:
            prc_running_flows = {}
        
        # Build a lookup for running flows: {(ipo, step_normalized): True}
        running_flows_lookup = set()
        for flow in prc_running_flows.get('running_flows', []):
            ipo = flow['ipo'].lower()
            step = flow['step'].lower()
            running_flows_lookup.add((ipo, step))
        
        # ========================================
        # STEP 1: COLLECT ALL FLOW DATA
        # ========================================
        all_flows = []  # List of {phase, name, runtime_hours, start, end, status, ipo}
        
        # --- SYNTHESIS PHASE ---
        synthesis_runtime_hours = 0
        synthesis_start = None
        synthesis_end = None
        
        if 'Fast DC' in runtime_data:
            start, end = runtime_timestamps.get('Fast DC', (None, None))
            runtime_str = runtime_data['Fast DC']
            runtime_hours = self._parse_runtime_to_hours(runtime_str)
            all_flows.append({
                'phase': 'Synthesis',
                'name': 'Fast DC',
                'runtime_hours': runtime_hours,
                'start': start,
                'end': end,
                'status': 'OK',
                'ipo': 'global'
            })
            synthesis_runtime_hours += runtime_hours
            if start and (not synthesis_start or start < synthesis_start):
                synthesis_start = start
            if end and (not synthesis_end or end > synthesis_end):
                synthesis_end = end
        
        dc_key = 'Full DC' if 'Full DC' in runtime_data else 'DC'
        if dc_key in runtime_data:
            start, end = runtime_timestamps.get(dc_key, (None, None))
            runtime_str = runtime_data[dc_key]
            runtime_hours = self._parse_runtime_to_hours(runtime_str)
            # Check if DC is running (runtime_str will contain "RUNNING")
            dc_is_running = 'RUNNING' in str(runtime_str).upper() or end == 'RUNNING'
            # Get actual log mtime for stall detection (stored separately from timestamps)
            dc_log_mtime = runtime_data.get(f'{dc_key}_log_mtime', end)
            all_flows.append({
                'phase': 'Synthesis',
                'name': dc_key,
                'runtime_hours': runtime_hours,
                'start': start,
                'end': end,
                'status': 'RUNNING' if dc_is_running else 'OK',
                'ipo': 'global',
                'log_mtime': dc_log_mtime  # Actual log modification time for stall detection
            })
            synthesis_runtime_hours += runtime_hours
            if start and (not synthesis_start or start < synthesis_start):
                synthesis_start = start
            if end and end != 'RUNNING' and (not synthesis_end or end > synthesis_end):
                synthesis_end = end
        
        # --- PNR PHASE (extract per-stage data) ---
        pnr_runtime_hours = 0
        pnr_start = None
        pnr_end = None
        pnr_stage_order = ['setup', 'plan', 'place', 'cts', 'route', 'postroute']
        
        # Track which (ipo, flow) combinations we've already added - initialized early to track PnR too
        added_flows = set()
        
        # Mapping from PnR stage name to prc.status step names
        pnr_stage_to_prc = {
            'setup': 'setup',
            'plan': 'edi_plan',
            'place': 'place',
            'cts': 'cts',
            'route': 'route',
            'postroute': 'postroute',
            'report_postroute': 'report_postroute',
        }
        
        # Get PnR stage data across all IPOs
        for ipo_label in sorted(pnr_runtimes.keys()):
            ipo_info = ipo_signoff_data.get(ipo_label, {}) if ipo_signoff_data else {}
            pnr_info = ipo_info.get('pnr', {})
            
            # Check if this is a "real PnR" IPO (has actual PnR stages like place/cts/route/postroute)
            # ECO-only IPOs (with only copy_db/eco stages) should not be shown under PnR phase
            is_real_pnr = pnr_info.get('is_real_pnr', False)
            
            # Check if we have per-stage breakdown (must have 'stages' key with actual stage data)
            has_stage_breakdown = 'stages' in pnr_info and pnr_info['stages'] and is_real_pnr
            
            if has_stage_breakdown:
                # Per-stage breakdown available - show stages (IPO shown in header)
                for stage in pnr_stage_order:
                    stage_data = pnr_info['stages'].get(stage, {})
                    if stage_data:
                        runtime_hours = stage_data.get('runtime_hours', 0)
                        start = stage_data.get('start_time', None)
                        end = stage_data.get('end_time', None)
                        status = stage_data.get('status', 'OK')
                        
                        # Check if this stage is running according to prc.status
                        prc_step = pnr_stage_to_prc.get(stage.lower(), stage.lower())
                        ipo_normalized = ipo_label.lower()
                        if (ipo_normalized, prc_step) in running_flows_lookup:
                            status = 'RUNNING'
                            # For RUNNING stages: recalculate start time from prc.status duration
                            # The sequential calculation assumes continuous flow, but restarts break this
                            # Get actual duration from prc_running_flows
                            for rf in prc_running_flows.get('running_flows', []):
                                if rf['ipo'].lower() == ipo_normalized and rf['step'].lower() == prc_step:
                                    duration_secs = rf.get('duration_seconds', 0)
                                    if duration_secs > 0:
                                        runtime_hours = duration_secs / 3600
                                        current_ts = time.time()
                                        start = time.strftime("%m/%d %H:%M", 
                                                    time.localtime(current_ts - duration_secs))
                                    break
                        
                        # Use actual log_mtime for stall detection (RUNNING stages have actual file mtime)
                        log_mtime = stage_data.get('log_mtime', end)
                        
                        all_flows.append({
                            'phase': 'PnR',
                            'name': stage.title(),  # No IPO suffix - shown in header
                            'runtime_hours': runtime_hours,
                            'start': start,
                            'end': end,
                            'status': status,
                            'ipo': ipo_label,
                            'log_mtime': log_mtime  # Actual log mtime for accurate stall detection
                        })
                        # Track this PnR stage as added
                        added_flows.add((ipo_label.lower(), prc_step))
                        pnr_runtime_hours += runtime_hours
                        if start and (not pnr_start or start < pnr_start):
                            pnr_start = start
                        if end and (not pnr_end or end > pnr_end):
                            pnr_end = end
            elif is_real_pnr:
                # Only total available for real PnR IPO - parse from pnr_runtimes string
                # Skip ECO-only IPOs (is_real_pnr=False) from PnR phase display
                runtime_str = pnr_runtimes.get(ipo_label, '')
                if runtime_str and runtime_str != '-':
                    runtime_hours = self._parse_runtime_to_hours(runtime_str)
                    start, end = runtime_timestamps.get(f'PnR ({ipo_label})', (None, None))
                    
                    # Treat as single "PnR Total" entry for this IPO (no suffix - shown in header)
                    all_flows.append({
                        'phase': 'PnR',
                        'name': 'PnR Total',  # No IPO suffix - shown in header
                        'runtime_hours': runtime_hours,
                        'start': start,
                        'end': end,
                        'status': 'OK',
                        'ipo': ipo_label,
                        'log_mtime': end  # Store original end time for stall detection
                    })
                    pnr_runtime_hours += runtime_hours
                    if start and (not pnr_start or start < pnr_start):
                        pnr_start = start
                    if end and (not pnr_end or end > pnr_end):
                        pnr_end = end
            # ECO-only IPOs (is_real_pnr=False with no stage breakdown) are skipped from PnR display
        
        # --- SIGNOFF PHASE (aggregate across all IPOs) ---
        signoff_runtime_hours = 0
        signoff_start = None
        signoff_end = None
        signoff_flow_totals = {}  # Aggregate same flow types across IPOs
        
        # Mapping from flow_name (ipo_signoff_data) to prc.status step names
        # ipo_signoff_data uses: star, auto_pt, formal_rtl_pnr, formal_bbox, gl_check
        # prc.status uses: nv_star, auto_pt, rtl_vs_pnr_fm, rtl_vs_pnr_bbox_fm, gl-check, nv_pv
        flow_to_prc_step = {
            'star': 'nv_star',
            'auto_pt': 'auto_pt',
            'auto_pt_fix': 'auto_pt_fix',
            'formal_rtl_pnr': 'rtl_vs_pnr_fm',
            'formal_bbox': 'rtl_vs_pnr_bbox_fm',
            'gl_check': 'gl-check',
            'gl-check': 'gl-check',
            'pv': 'nv_pv',
            'eco': 'auto_pt_fix',
        }
        
        # Also build reverse mapping to check if we need to add running flows not in ipo_signoff_data
        prc_step_to_flow = {v: k for k, v in flow_to_prc_step.items()}
        
        if ipo_signoff_data:
            for ipo_label, ipo_info in ipo_signoff_data.items():
                flows = ipo_info.get('signoff', {})
                for flow_name, flow_data in flows.items():
                    runtime_str = flow_data.get('runtime', None)
                    if runtime_str and runtime_str != 'N/A':
                        runtime_hours = self._parse_runtime_to_hours(runtime_str)
                        start = flow_data.get('start_time', None)
                        end = flow_data.get('end_time', None)
                        status = flow_data.get('status', 'NOT_RUN')
                        
                        # Check if this flow is currently running according to prc.status
                        prc_step = flow_to_prc_step.get(flow_name.lower(), flow_name.lower())
                        ipo_normalized = ipo_label.lower()
                        if (ipo_normalized, prc_step) in running_flows_lookup:
                            status = 'RUNNING'
                        
                        # For auto_pt, check alternative log file location for more accurate stall detection
                        log_mtime = end
                        if flow_name.lower() == 'auto_pt' and status == 'RUNNING':
                            log_mtime = self._get_auto_pt_log_mtime(ipo_label, end)
                        
                        # Mark as added
                        added_flows.add((ipo_normalized, prc_step))
                        
                        # Add to individual flows list
                        all_flows.append({
                            'phase': 'Signoff',
                            'name': flow_name.title(),
                            'runtime_hours': runtime_hours,
                            'start': start,
                            'end': end,
                            'status': status,
                            'ipo': ipo_label,
                            'log_mtime': log_mtime  # Store log mtime for stall detection
                        })
                        
                        # Aggregate by flow type
                        if flow_name not in signoff_flow_totals:
                            signoff_flow_totals[flow_name] = {'runtime': 0, 'count': 0, 'starts': [], 'ends': []}
                        signoff_flow_totals[flow_name]['runtime'] += runtime_hours
                        signoff_flow_totals[flow_name]['count'] += 1
                        if start:
                            signoff_flow_totals[flow_name]['starts'].append(start)
                        if end:
                            signoff_flow_totals[flow_name]['ends'].append(end)
                        
                        # Update totals
                        signoff_runtime_hours += runtime_hours
                        if start and (not signoff_start or start < signoff_start):
                            signoff_start = start
                        if end and (not signoff_end or end > signoff_end):
                            signoff_end = end
        
        # Add running flows from prc.status that weren't in ipo_signoff_data
        # These are flows that just started and don't have output yet
        import time as time_module
        current_time = time_module.time()
        
        # Define PnR stages vs Signoff stages (including all report_* PnR stages)
        pnr_stages = {'place', 'cts', 'route', 'postroute', 'plan', 'setup',
                      'edi_plan', 'edi_place', 'edi_cts', 'edi_route', 'edi_postroute',
                      'report_postroute', 'report_route', 'report_cts', 'report_place'}
        
        for running_flow in prc_running_flows.get('running_flows', []):
            ipo = running_flow['ipo'].lower()
            step = running_flow['step'].lower()
            
            # Skip if already added
            if (ipo, step) in added_flows:
                continue
            
            # Determine the correct phase based on step type
            phase = 'PnR' if step in pnr_stages else 'Signoff'
            
            # Convert prc step name to display name
            display_name = prc_step_to_flow.get(step, step).replace('_', ' ').title()
            
            # Calculate runtime from duration_seconds
            duration_seconds = running_flow.get('duration_seconds', 0)
            runtime_hours = duration_seconds / 3600 if duration_seconds > 0 else 0
            
            # Get log path from running_flow
            log_path = running_flow.get('log_path')
            
            # For RUNNING stages, calculate start time from current_time - duration
            # This gives the actual execution start time (after queue wait)
            # Note: Log filename timestamp shows SUBMISSION time, which can be misleading
            #       when multiple stages are submitted together as a batch
            start_str = None
            if duration_seconds > 0:
                start_time = current_time - duration_seconds
                start_str = time_module.strftime("%m/%d %H:%M", time_module.localtime(start_time))
            
            # For stall detection: get actual log file mtime for accurate detection
            log_mtime = start_str  # Default: use start time
            
            # Get actual log file mtime if available
            if log_path and os.path.exists(log_path):
                try:
                    actual_mtime = os.path.getmtime(log_path)
                    log_mtime = time_module.strftime("%m/%d %H:%M", time_module.localtime(actual_mtime))
                except OSError:
                    pass  # Keep default start_str
            
            # For auto_pt, also check the alternative auto_pt.log location
            if step == 'auto_pt':
                alt_mtime = self._get_auto_pt_log_mtime(ipo, log_mtime)
                if alt_mtime != log_mtime:
                    log_mtime = alt_mtime  # Use alternative log if it's more recent
            
            # Add to flows list
            all_flows.append({
                'phase': phase,
                'name': display_name,
                'runtime_hours': runtime_hours,
                'start': start_str,
                'end': 'RUNNING',
                'status': 'RUNNING',
                'ipo': ipo,
                'log_mtime': log_mtime  # Use actual log mtime or start time for stall detection
            })
            
            # Add to correct runtime total
            if phase == 'Signoff':
                signoff_runtime_hours += runtime_hours
        
        # ========================================
        # STEP 2: CALCULATE TOTALS AND METRICS
        # ========================================
        grand_total_hours = synthesis_runtime_hours + pnr_runtime_hours + signoff_runtime_hours
        
        # Calculate elapsed time (wall-clock) if we have timestamps
        elapsed_hours = None
        overall_start = None
        overall_end = None
        
        if synthesis_start:
            overall_start = synthesis_start
        elif pnr_start:
            overall_start = pnr_start
        elif signoff_start:
            overall_start = signoff_start
        
        if signoff_end:
            overall_end = signoff_end
        elif pnr_end:
            overall_end = pnr_end
        elif synthesis_end:
            overall_end = synthesis_end
        
        if overall_start and overall_end:
            try:
                # Parse timestamps and calculate elapsed time
                # Format: MM/DD HH:MM
                start_time = self._parse_timestamp_to_epoch(overall_start)
                end_time = self._parse_timestamp_to_epoch(overall_end)
                if start_time and end_time:
                    elapsed_seconds = end_time - start_time
                    elapsed_hours = elapsed_seconds / 3600
            except:
                pass
        
        # Calculate efficiency (active runtime / elapsed time)
        efficiency_pct = None
        if elapsed_hours and elapsed_hours > 0:
            efficiency_pct = (grand_total_hours / elapsed_hours) * 100
        
        # Find bottleneck (longest single stage/flow)
        bottleneck_flow = None
        bottleneck_pct = 0
        if grand_total_hours > 0:
            for flow in all_flows:
                if flow['runtime_hours'] > 0:
                    pct = (flow['runtime_hours'] / grand_total_hours) * 100
                    if pct > bottleneck_pct:
                        bottleneck_pct = pct
                        bottleneck_flow = flow
        
        # ========================================
        # STEP 3: PRINT UNIFIED TABLE (TERMINAL)
        # ========================================
        print(f"\n{Color.CYAN}{'=' * 78}{Color.RESET}")
        print(f"{Color.CYAN}              UNIFIED FLOW TIMELINE - {self.design_info.top_hier}{Color.RESET}")
        print(f"{Color.CYAN}{'=' * 78}{Color.RESET}\n")
        
        # Table header
        print(f"{'Phase':<12} {'Flow/Stage':<20} {'Runtime':<10} {'Timeline':<25} {'Status':<8}")
        print(f"{'='*12} {'='*20} {'='*10} {'='*25} {'='*8}")
        
        # --- Synthesis rows ---
        if synthesis_runtime_hours > 0:
            for flow in all_flows:
                if flow['phase'] == 'Synthesis':
                    self._print_flow_row(flow, bottleneck_flow, grand_total_hours)
            
            # Synthesis subtotal (no separator line - cleaner)
            print()
        
        # --- PnR rows (per-IPO breakdown) ---
        if pnr_runtime_hours > 0:
            # Group PnR flows by IPO
            pnr_by_ipo = {}
            for flow in all_flows:
                if flow['phase'] == 'PnR':
                    ipo = flow['ipo']
                    if ipo not in pnr_by_ipo:
                        pnr_by_ipo[ipo] = []
                    pnr_by_ipo[ipo].append(flow)
            
            # Print PnR flows per IPO with headers showing totals
            for ipo in sorted(pnr_by_ipo.keys()):
                flows = pnr_by_ipo[ipo]
                
                # Calculate IPO total
                ipo_total_hours = sum(f['runtime_hours'] for f in flows)
                
                # Calculate earliest start and latest end using datetime comparison
                ipo_start = 'N/A'
                ipo_end = 'N/A'
                earliest_epoch = None
                latest_epoch = None
                current_year = time.localtime().tm_year
                
                for f in flows:
                    if f['start'] and f['start'] != 'N/A':
                        try:
                            start_struct = time.strptime(f"{current_year}/{f['start']}", "%Y/%m/%d %H:%M")
                            start_epoch = time.mktime(start_struct)
                            if earliest_epoch is None or start_epoch < earliest_epoch:
                                earliest_epoch = start_epoch
                                ipo_start = f['start']
                        except:
                            pass
                    
                    if f['end'] and f['end'] != 'N/A':
                        try:
                            end_struct = time.strptime(f"{current_year}/{f['end']}", "%Y/%m/%d %H:%M")
                            end_epoch = time.mktime(end_struct)
                            if latest_epoch is None or end_epoch > latest_epoch:
                                latest_epoch = end_epoch
                                ipo_end = f['end']
                        except:
                            pass
                
                # Print IPO header with total (always show for consistency with Signoff)
                ipo_label = ipo.upper() if ipo != 'root' else 'ROOT'
                timeline_str = f"[{ipo_start} -> {ipo_end}]"
                # Add (deleted) marker in red if IPO directory was deleted
                is_deleted = ipo in deleted_ipos
                deleted_marker = f" {Color.RED}(deleted){Color.CYAN}" if is_deleted else ""
                flow_stage = f"[{ipo_label}]{deleted_marker}"
                runtime = f"{ipo_total_hours:.2f}h"
                print(f"{Color.CYAN}{'PnR':<12} {flow_stage:<20} {runtime:<10} {timeline_str:<25}{Color.RESET}")
                
                # Check if postroute is completed for this IPO (for report_postroute note)
                postroute_ok = any(f['name'].lower() == 'postroute' and f['status'] in ('OK', 'COMPLETED') for f in flows)
                
                # Print flows for this IPO (skip setup if < 0.1h)
                for flow in flows:
                    # Skip setup stage display (but already counted in ipo_total_hours)
                    if 'Setup' in flow['name'] and flow['runtime_hours'] < 0.1:
                        continue
                    self._print_flow_row(flow, bottleneck_flow, grand_total_hours, postroute_ok_for_ipo=postroute_ok)
                
                # Add blank line between IPOs (if multiple)
                if len(pnr_by_ipo) > 1:
                    print()  # Visual separation between IPOs
            
            # DC + PnR Total = Full DC + Slowest IPO PnR (separator line)
            print(f"             {'-' * 65}")
            
            # Find Full DC runtime
            full_dc_hours = 0
            for flow in all_flows:
                if flow['phase'] == 'Synthesis' and ('Full DC' in flow['name'] or flow['name'] == 'DC'):
                    full_dc_hours = flow['runtime_hours']
                    break
            
            # Find slowest IPO PnR
            slowest_ipo_hours = 0
            for ipo, flows in pnr_by_ipo.items():
                ipo_total = sum(f['runtime_hours'] for f in flows)
                if ipo_total > slowest_ipo_hours:
                    slowest_ipo_hours = ipo_total
            
            # DC + PnR Total = Full DC + Slowest IPO
            pnr_total_adjusted = full_dc_hours + slowest_ipo_hours
            timeline_str = f"[{pnr_start or 'N/A'} -> {pnr_end or 'N/A'}]"
            print(f"{'PnR':<12} {'DC + PnR':<20} {f'{pnr_total_adjusted:.2f}h':<10} {timeline_str:<25}")
            print()
        
        # --- Signoff rows (per-IPO breakdown with subtotals) ---
        if signoff_runtime_hours > 0:
            # Group Signoff flows by IPO
            signoff_by_ipo = {}
            for flow in all_flows:
                if flow['phase'] == 'Signoff':
                    ipo = flow['ipo']
                    if ipo not in signoff_by_ipo:
                        signoff_by_ipo[ipo] = []
                    signoff_by_ipo[ipo].append(flow)
            
            # Print Signoff flows per IPO with headers showing totals
            for ipo in sorted(signoff_by_ipo.keys()):
                flows = signoff_by_ipo[ipo]
                
                # Calculate IPO signoff total
                ipo_signoff_hours = sum(f['runtime_hours'] for f in flows)
                
                # Calculate earliest start and latest end using datetime comparison
                ipo_start = 'N/A'
                ipo_end = 'N/A'
                earliest_epoch = None
                latest_epoch = None
                current_year = time.localtime().tm_year
                
                for f in flows:
                    if f['start'] and f['start'] != 'N/A':
                        try:
                            start_struct = time.strptime(f"{current_year}/{f['start']}", "%Y/%m/%d %H:%M")
                            start_epoch = time.mktime(start_struct)
                            if earliest_epoch is None or start_epoch < earliest_epoch:
                                earliest_epoch = start_epoch
                                ipo_start = f['start']
                        except:
                            pass
                    
                    if f['end'] and f['end'] != 'N/A':
                        try:
                            end_struct = time.strptime(f"{current_year}/{f['end']}", "%Y/%m/%d %H:%M")
                            end_epoch = time.mktime(end_struct)
                            if latest_epoch is None or end_epoch > latest_epoch:
                                latest_epoch = end_epoch
                                ipo_end = f['end']
                        except:
                            pass
                
                # Print IPO header with total (always show for consistency)
                ipo_label = ipo.upper() if ipo != 'root' else 'ROOT'
                timeline_str = f"[{ipo_start} -> {ipo_end}]"
                # Add (deleted) marker in red if IPO directory was deleted
                is_deleted = ipo in deleted_ipos
                deleted_marker = f" {Color.RED}(deleted){Color.CYAN}" if is_deleted else ""
                flow_stage = f"[{ipo_label}]{deleted_marker}"
                runtime = f"{ipo_signoff_hours:.2f}h"
                print(f"{Color.CYAN}{'Signoff':<12} {flow_stage:<20} {runtime:<10} {timeline_str:<25}{Color.RESET}")
                
                # Print flows for this IPO (NO suffix - alignment maintained by header above)
                for flow in flows:
                    # Don't add IPO suffix - alignment maintained by header above
                    self._print_flow_row(flow, bottleneck_flow, grand_total_hours)
                
                # Add blank line between IPOs (if multiple)
                if len(signoff_by_ipo) > 1:
                    print()  # Visual separation between IPOs
        
        # --- Grand Total ---
        print(f"{Color.YELLOW}{'=' * 78}{Color.RESET}")
        timeline_str = f"[{overall_start or 'N/A'} -> {overall_end or 'N/A'}]"
        print(f"{Color.YELLOW}{'GRAND TOTAL':<32} {f'{grand_total_hours:.2f}h':<10} {timeline_str:<25}{Color.RESET}")
        print(f"{Color.YELLOW}{'=' * 78}{Color.RESET}")
        
        # Timeline summary
        if elapsed_hours:
            elapsed_days = elapsed_hours / 24
            timeline_summary = f"Timeline: {elapsed_days:.2f} days"
            if efficiency_pct:
                timeline_summary += f"  |  Efficiency: {efficiency_pct:.0f}% ({grand_total_hours:.1f}h active / {elapsed_hours:.1f}h elapsed)"
                if efficiency_pct > 100:
                    timeline_summary += "*"
            print(f"{timeline_summary}")
            if efficiency_pct and efficiency_pct > 100:
                print(f"* Efficiency > 100% indicates parallel execution of flows")
        
        # Bottleneck analysis
        if bottleneck_flow:
            print(f"\n{Color.CYAN}Critical Path Analysis:{Color.RESET}")
            print(f"  Bottleneck: {bottleneck_flow['name']} ({bottleneck_pct:.1f}% of total runtime)")
            if bottleneck_pct > 30:
                print(f"  {Color.YELLOW}[!] This stage dominates the flow - consider optimization{Color.RESET}")
        
        print()
    
    def _print_flow_row(self, flow: Dict, bottleneck_flow: Optional[Dict] = None, grand_total: float = 0,
                        postroute_ok_for_ipo: bool = False) -> None:
        """Print a single flow row in the unified table
        
        Args:
            flow: Flow dictionary with phase, name, runtime, start, end, status, log_mtime
            bottleneck_flow: The bottleneck flow for highlighting (optional)
            grand_total: Grand total runtime for percentage calculation (optional)
            postroute_ok_for_ipo: True if postroute is completed for this IPO (for report_postroute note)
        """
        phase = flow['phase']
        name = flow['name']
        runtime_hours = flow['runtime_hours']
        start = flow['start'] or 'N/A'
        end = flow['end'] or 'N/A'
        status = flow['status']
        log_mtime = flow.get('log_mtime')  # For stall detection
        
        # Stall detection thresholds (in hours)
        STALL_THRESHOLD_HOURS = {
            'PnR': 3.0,      # PnR stages (postroute can be long)
            'Signoff': 1.5,  # Signoff stages (auto_pt distributes to workers, needs margin)
            'Synthesis': 3.0 # Synthesis (if ever running)
        }
        
        # Check for stalled status (only for running flows)
        is_stalled = False
        stall_info = ""  # Info about when log was last modified
        if status == 'RUNNING':
            try:
                import time as time_module
                current_time = time_module.time()
                current_year = time_module.localtime().tm_year
                
                # Use log_mtime if available, otherwise fall back to start time
                check_time = log_mtime
                if not check_time or check_time in ('N/A', 'RUNNING', None):
                    check_time = start  # Fall back to start time for stall detection
                
                if check_time and check_time not in ('N/A', 'RUNNING', None):
                    # Parse check_time (format: "MM/DD HH:MM")
                    mtime_struct = time_module.strptime(f"{current_year}/{check_time}", "%Y/%m/%d %H:%M")
                    mtime_epoch = time_module.mktime(mtime_struct)
                    
                    hours_since_update = (current_time - mtime_epoch) / 3600
                    threshold = STALL_THRESHOLD_HOURS.get(phase, 1.0)
                    
                    if hours_since_update > threshold:
                        is_stalled = True
                        stall_info = f" (last log update: {check_time}, {hours_since_update:.1f}h ago)"
            except:
                pass  # If parsing fails, don't mark as stalled
        
        # Format runtime
        runtime_str = f"{runtime_hours:.2f}h"
        
        # For RUNNING status, always show "RUNNING" as end time
        if status == 'RUNNING':
            end = 'RUNNING'
        
        # Format timeline
        timeline_str = f"[{start} -> {end}]"
        
        # Format status with color
        if status == 'OK' or status == 'COMPLETED':
            status_str = f"{Color.GREEN}[OK]{Color.RESET}"
        elif status == 'RUNNING':
            if is_stalled:
                status_str = f"{Color.RED}[STALLED]{Color.RESET}"
            else:
                status_str = f"{Color.CYAN}[RUN]{Color.RESET}"
        elif status == 'FAILED':
            status_str = f"{Color.RED}[FAIL]{Color.RESET}"
        elif status == 'WARN':
            status_str = f"{Color.YELLOW}[WARN]{Color.RESET}"
        else:
            status_str = f"[{status}]"
        
        # Highlight bottleneck (check both name and IPO for accuracy)
        is_bottleneck = (bottleneck_flow and 
                        bottleneck_flow['name'] == name and 
                        bottleneck_flow.get('ipo') == flow.get('ipo'))
        is_running = status == 'RUNNING'
        prefix = "" if phase else "             "  # Empty phase column for subsequent rows
        
        # Calculate percentage of total (if grand_total provided)
        pct_str = ""
        if grand_total > 0 and runtime_hours > 0:
            pct = (runtime_hours / grand_total) * 100
            if pct >= 20:  # Show % for significant contributors
                pct_str = f" ({pct:.0f}%)"
        
        # Add inline note for report_postroute when postroute is done
        inline_note = ""
        if name.lower().replace(' ', '_') == 'report_postroute' and is_running and postroute_ok_for_ipo:
            inline_note = f" {Color.GREEN}(signoff can proceed){Color.RESET}"
        
        # Color priority: STALLED (red) > RUNNING (magenta) > Bottleneck (yellow) > Normal
        if is_stalled:
            # Stalled flows in red with info about last log update
            print(f"{Color.RED}{phase:<12} {name:<20} {runtime_str + pct_str:<10} {timeline_str:<25} {status_str} !{Color.RESET}{Color.YELLOW}{stall_info}{Color.RESET}")
        elif is_running:
            # Running flows in magenta (distinct from cyan headers)
            print(f"{Color.MAGENTA}{phase:<12} {name:<20} {runtime_str + pct_str:<10} {timeline_str:<25} {status_str} >{Color.RESET}{inline_note}")
        elif is_bottleneck:
            print(f"{Color.YELLOW}{phase:<12} {name:<20} {runtime_str + pct_str:<10} {timeline_str:<25} {status_str} *{Color.RESET}")
        else:
            print(f"{prefix if not phase else phase:<12} {name:<20} {runtime_str + pct_str:<10} {timeline_str:<25} {status_str}")
    
    def _get_auto_pt_log_mtime(self, ipo_label: str, fallback_time: str = None) -> str:
        """Get the most recent modification time for auto_pt logs.
        
        Auto PT distributes timing analysis to workers, so check:
        1. The main auto_pt.log in nbu_signoff/signoff_flow/auto_pt/log/
        2. Fall back to the provided fallback_time
        
        Args:
            ipo_label: IPO label (e.g., 'ipo1404')
            fallback_time: Original end time to use as fallback (e.g., '01/11 13:00')
            
        Returns:
            Most recent timestamp in 'MM/DD HH:MM' format, or fallback_time if not found
        """
        import time as time_module
        
        try:
            # Get design name from design_info (dataclass with top_hier attribute)
            design = None
            if hasattr(self, 'design_info') and self.design_info:
                design = getattr(self.design_info, 'top_hier', None)
            
            if not design or not self.workarea:
                return fallback_time
            
            # Alternative auto_pt.log location
            alt_log_path = os.path.join(
                self.workarea,
                'pnr_flow', 'nv_flow', design, ipo_label.lower(),
                'nbu_signoff', 'signoff_flow', 'auto_pt', 'log', 'auto_pt.log'
            )
            
            most_recent_mtime = None
            most_recent_str = fallback_time
            
            if os.path.exists(alt_log_path):
                alt_mtime = os.path.getmtime(alt_log_path)
                alt_mtime_str = time_module.strftime("%m/%d %H:%M", time_module.localtime(alt_mtime))
                
                # If fallback_time exists, compare timestamps
                if fallback_time and fallback_time not in ('N/A', 'RUNNING', None):
                    try:
                        current_year = time_module.localtime().tm_year
                        fallback_struct = time_module.strptime(f"{current_year}/{fallback_time}", "%Y/%m/%d %H:%M")
                        fallback_epoch = time_module.mktime(fallback_struct)
                        
                        # Use the more recent timestamp
                        if alt_mtime > fallback_epoch:
                            most_recent_str = alt_mtime_str
                    except:
                        # On parse error, prefer the actual file mtime
                        most_recent_str = alt_mtime_str
                else:
                    # No fallback, use alt log mtime
                    most_recent_str = alt_mtime_str
            
            return most_recent_str
            
        except Exception:
            return fallback_time
    
    def _parse_runtime_to_hours(self, runtime_str: str) -> float:
        """Parse runtime string to hours
        
        Args:
            runtime_str: Runtime string (e.g., "5.23 hours", "2.5h", "150 minutes")
        
        Returns:
            Runtime in hours (float)
        """
        if not runtime_str or runtime_str == 'N/A' or runtime_str == '-':
            return 0.0
        
        try:
            # Handle various formats
            if 'day' in runtime_str.lower():
                match = re.search(r'(\d+\.?\d*)\s*days?', runtime_str, re.IGNORECASE)
                if match:
                    return float(match.group(1)) * 24
            
            if 'hour' in runtime_str.lower() or 'h' in runtime_str.lower():
                match = re.search(r'(\d+\.?\d*)\s*(?:hours?|h)', runtime_str, re.IGNORECASE)
                if match:
                    return float(match.group(1))
            
            if 'minute' in runtime_str.lower() or 'm' in runtime_str.lower():
                match = re.search(r'(\d+\.?\d*)\s*(?:minutes?|m)', runtime_str, re.IGNORECASE)
                if match:
                    return float(match.group(1)) / 60
            
            if 'second' in runtime_str.lower() or 's' in runtime_str.lower():
                match = re.search(r'(\d+\.?\d*)\s*(?:seconds?|s)', runtime_str, re.IGNORECASE)
                if match:
                    return float(match.group(1)) / 3600
            
            # Try direct float conversion (assuming hours)
            return float(runtime_str.replace('h', '').strip())
        except:
            return 0.0
    
    def _parse_timestamp_to_epoch(self, timestamp_str: str) -> Optional[float]:
        """Parse timestamp string to epoch time
        
        Args:
            timestamp_str: Timestamp string in format "MM/DD HH:MM"
        
        Returns:
            Epoch time (float) or None if parsing fails
        """
        if not timestamp_str or timestamp_str == 'N/A':
            return None
        
        try:
            # Format: MM/DD HH:MM
            current_year = time.localtime().tm_year
            datetime_str = f"{current_year}/{timestamp_str}"
            time_struct = time.strptime(datetime_str, "%Y/%m/%d %H:%M")
            return time.mktime(time_struct)
        except:
            return None

    def _print_runtime_summary_table(self, runtime_data: Dict[str, Any], pnr_runtimes: Dict[str, Any], runtime_timestamps: Optional[Dict[str, Any]] = None, ipo_signoff_data: Optional[Dict[str, Dict]] = None) -> None:
        """Print runtime summary table with IPO-specific signoff flows (Compact Format)
        
        Args:
            runtime_data: Dictionary of runtime data for each stage
            pnr_runtimes: Dictionary of PnR stage runtimes
            runtime_timestamps: Dictionary of start/end timestamps
            ipo_signoff_data: Dictionary of IPO-specific signoff flow data
        """
        if runtime_timestamps is None:
            runtime_timestamps = {}
        
        # Print Global Synthesis Flows (compact one-liner)
        print(f"\n{Color.CYAN}Global Synthesis:{Color.RESET}")
        synthesis_parts = []
        if 'Fast DC' in runtime_data:
            start, end = runtime_timestamps.get('Fast DC', (None, None))
            synthesis_parts.append(f"Fast DC: {runtime_data['Fast DC']}")
        if 'Full DC' in runtime_data:
            start, end = runtime_timestamps.get('Full DC', (None, None))
            synthesis_parts.append(f"Full DC: {runtime_data['Full DC']}")
        elif 'DC' in runtime_data:
            start, end = runtime_timestamps.get('DC', (None, None))
            synthesis_parts.append(f"DC: {runtime_data['DC']}")
        
        if synthesis_parts:
            print(f"  {' | '.join(synthesis_parts)}  [OK]")
        else:
            print(f"  No synthesis data available")
        
        # Print IPO-specific signoff flows (compact unified table)
        if ipo_signoff_data or pnr_runtimes:
            self._print_ipo_signoff_summary(ipo_signoff_data, pnr_runtimes, runtime_timestamps)
    
    def _print_ipo_signoff_summary(self, ipo_data: Dict[str, Dict], pnr_runtimes: Dict[str, Any], runtime_timestamps: Optional[Dict[str, Any]] = None) -> None:
        """Print compact IPO signoff summary table with timestamps and detailed flow info
        
        Args:
            ipo_data: Dictionary of IPO-specific flow data (nested: pnr/signoff)
            pnr_runtimes: Dictionary of PnR runtimes (for cross-reference)
            runtime_timestamps: Dictionary of runtime timestamps
        """
        if runtime_timestamps is None:
            runtime_timestamps = {}
        
        # Get all unique IPOs from both PnR and ipo_data
        all_ipos = set(pnr_runtimes.keys())
        has_root_signoff = False
        
        if ipo_data:
            all_ipos.update([k for k in ipo_data.keys() if k != 'root'])
            # Check if root-level signoff exists
            if 'root' in ipo_data and ipo_data['root'].get('signoff'):
                has_root_signoff = True
        
        # If no IPOs but have root signoff, still show table
        if not all_ipos and not has_root_signoff:
            return
        
        print(f"\n{Color.CYAN}PnR + Signoff Summary (per IPO):{Color.RESET}")
        
        # Prepare table data
        ipo_rows = []
        active_flows = []
        detailed_flow_info = []  # Store detailed flow information
        
        # If only root-level signoff (no IPO-specific), use the single IPO from pnr_runtimes or show as "root"
        ipos_to_process = sorted(all_ipos) if all_ipos else (['root'] if has_root_signoff else [])
        
        for ipo_label in ipos_to_process:
            ipo_info = ipo_data.get(ipo_label, {}) if ipo_data else {}
            pnr_info = ipo_info.get('pnr', {})
            flows = ipo_info.get('signoff', {})
            
            # If processing root or if there's only one IPO with root-level signoff, merge root flows
            if ipo_label == 'root' or (len(all_ipos) == 1 and has_root_signoff):
                root_flows = ipo_data.get('root', {}).get('signoff', {}) if ipo_data else {}
                if root_flows:
                    # Merge root flows with IPO flows
                    flows = {**root_flows, **flows}  # IPO flows override root if both exist
            
            # Check PnR status (running stage)
            pnr_runtime = pnr_runtimes.get(ipo_label, '-')
            pnr_start, pnr_end = runtime_timestamps.get(f'PnR ({ipo_label})', (None, None))
            pnr_status_str = '[OK]'
            pnr_running_stage = None
            
            # Check if PnR is running (from runtime_timestamps)
            if pnr_end and "RUNNING" in str(pnr_end):
                pnr_status_str = '[RUN]'
                # Extract stage name from "RUNNING (stage_name)" format
                import re
                stage_match = re.search(r'RUNNING \((\w+)\)', str(pnr_end))
                if stage_match:
                    pnr_running_stage = stage_match.group(1)
                    active_flows.append({
                        'ipo': ipo_label,
                        'type': 'PnR',
                        'flow': f"stage: {pnr_running_stage}",
                        'elapsed': pnr_runtime if pnr_runtime != '-' else 'N/A'
                    })
            
            # Also check pnr_info if available
            if pnr_info:
                pnr_status = pnr_info.get('status', 'NOT_RUN')
                detected_stage = pnr_info.get('running_stage', None)
                
                if pnr_status == 'RUNNING' and detected_stage:
                    pnr_status_str = '[RUN]'
                    pnr_running_stage = detected_stage
                    if not any(f['ipo'] == ipo_label and f['type'] == 'PnR' for f in active_flows):
                        active_flows.append({
                            'ipo': ipo_label,
                            'type': 'PnR',
                            'flow': f"stage: {detected_stage}",
                            'elapsed': pnr_runtime if pnr_runtime != '-' else 'N/A'
                        })
                elif pnr_runtime == '-':
                    pnr_status_str = '[--]'
            else:
                if pnr_runtime == '-':
                    pnr_status_str = '[--]'
            
            # Calculate total signoff runtime and collect flow details
            total_signoff_hours = 0
            completed_count = 0
            running_count = 0
            failed_count = 0
            pending_count = 0
            running_flow_name = None
            running_elapsed = None
            flow_list = []  # Detailed flow list
            
            # Analyze flow statuses
            for flow_name, flow_data in flows.items():
                status = flow_data.get('status', 'NOT_RUN')
                runtime = flow_data.get('runtime', None)
                start_time = flow_data.get('start_time', None)
                end_time = flow_data.get('end_time', None)
                
                # Add to detailed flow list
                flow_entry = {
                    'name': flow_name.title(),
                    'status': status,
                    'runtime': runtime or 'N/A',
                    'start': start_time or 'N/A',
                    'end': end_time or 'N/A'
                }
                flow_list.append(flow_entry)
                
                if status == 'COMPLETED':
                    completed_count += 1
                    if runtime and 'h' in runtime:
                        try:
                            hours = float(runtime.replace('h', '').strip())
                            total_signoff_hours += hours
                        except:
                            pass
                elif status == 'RUNNING':
                    running_count += 1
                    running_flow_name = flow_name.title()
                    elapsed_sec = flow_data.get('elapsed_seconds', None)
                    if elapsed_sec:
                        hours = elapsed_sec / 3600
                        minutes = (elapsed_sec % 3600) / 60
                        running_elapsed = f"{int(hours)}h{int(minutes)}m" if hours >= 1 else f"{int(minutes)}m"
                    active_flows.append({
                        'ipo': ipo_label,
                        'type': 'Signoff',
                        'flow': running_flow_name,
                        'elapsed': running_elapsed or 'N/A'
                    })
                elif status == 'FAILED':
                    failed_count += 1
                elif status == 'STALE':
                    pending_count += 1
                else:  # NOT_RUN
                    pending_count += 1
            
            # Store detailed flow info for this IPO
            if flow_list:
                detailed_flow_info.append({
                    'ipo': ipo_label,
                    'flows': flow_list,
                    'pnr_start': pnr_start,
                    'pnr_end': pnr_end,
                    'pnr_runtime': pnr_runtime,
                    'pnr_running_stage': pnr_running_stage
                })
            
            # Determine signoff status and build summary details
            signoff_status_str = '[--]'
            if running_count > 0:
                signoff_status_str = '[RUN]'
                details = f"{running_flow_name}: {running_elapsed} elapsed" if running_flow_name and running_elapsed else f"{running_count} running"
            elif failed_count > 0:
                signoff_status_str = '[FAIL]'
                details = f"{failed_count} failed"
            elif pending_count > 0 and completed_count == 0:
                signoff_status_str = '[--]'
                details = "Not started"
            elif pending_count > 0:
                signoff_status_str = '[WARN]'
                details = f"{completed_count} done, {pending_count} pending"
            elif completed_count > 0:
                signoff_status_str = '[OK]'
                # Show brief summary
                details = f"{completed_count} flows ({total_signoff_hours:.1f}h)"
            else:
                signoff_status_str = '[--]'
                details = "No data"
            
            signoff_runtime = f"{total_signoff_hours:.1f}h {signoff_status_str}" if total_signoff_hours > 0 else f"- {signoff_status_str}"
            
            # Format PnR runtime with status
            if pnr_running_stage:
                # PnR is running - override status
                pnr_status_str = '[RUN]'
                pnr_runtime_fmt = f"{pnr_runtime} {pnr_status_str}" if pnr_runtime != '-' else f"Running {pnr_status_str}"
            else:
                pnr_runtime_fmt = f"{pnr_runtime} {pnr_status_str}" if pnr_runtime != '-' else f"- {pnr_status_str}"
            
            # Add timestamps to row
            pnr_time_str = f"{pnr_start or 'N/A'} -> {pnr_end or 'N/A'}"
            
            ipo_rows.append((ipo_label, pnr_runtime_fmt, pnr_time_str, signoff_runtime, details))
        
        # Print table (limit to 10 IPOs by default for compact output)
        if ipo_rows:
            # Calculate column widths
            ipo_width = max(len(row[0]) for row in ipo_rows)
            ipo_width = max(ipo_width, len("IPO"))
            pnr_width = max(len(row[1]) for row in ipo_rows)
            pnr_width = max(pnr_width, len("PnR"))
            time_width = max(len(row[2]) for row in ipo_rows)
            time_width = max(time_width, len("Timeline"))
            signoff_width = max(len(row[3]) for row in ipo_rows)
            signoff_width = max(signoff_width, len("Signoff"))
            details_width = 30  # Reduced width for summary details
            
            # Print header
            print(f"  {'IPO':<{ipo_width}} {'PnR':<{pnr_width}} {'Timeline':<{time_width}} {'Signoff':<{signoff_width}} {'Summary':<{details_width}}")
            print(f"  {'-' * ipo_width} {'-' * pnr_width} {'-' * time_width} {'-' * signoff_width} {'-' * details_width}")
            
            # Print rows (first 10 or all if <= 10)
            show_count = min(len(ipo_rows), 10)
            for i, (ipo, pnr, timeline, signoff, details) in enumerate(ipo_rows[:show_count]):
                # Apply color based on status
                row_color = ""
                if '[RUN]' in signoff or '[RUN]' in pnr:
                    row_color = Color.YELLOW
                elif '[FAIL]' in signoff or '[FAIL]' in pnr:
                    row_color = Color.RED
                elif '[WARN]' in signoff:
                    row_color = Color.YELLOW
                elif '[OK]' in signoff and '[OK]' in pnr:
                    row_color = Color.GREEN
                
                # Truncate columns if too long
                if len(timeline) > time_width:
                    timeline = timeline[:time_width-3] + "..."
                if len(details) > details_width:
                    details = details[:details_width-3] + "..."
                
                if row_color:
                    print(f"  {row_color}{ipo:<{ipo_width}} {pnr:<{pnr_width}} {timeline:<{time_width}} {signoff:<{signoff_width}} {details:<{details_width}}{Color.RESET}")
                else:
                    print(f"  {ipo:<{ipo_width}} {pnr:<{pnr_width}} {timeline:<{time_width}} {signoff:<{signoff_width}} {details:<{details_width}}")
            
            # Show "N more" message if truncated
            if len(ipo_rows) > show_count:
                print(f"  ... ({len(ipo_rows) - show_count} more IPOs, use -v to show all)")
        
        # Print active flows section
        if active_flows:
            print(f"\n{Color.YELLOW}Active Flows (RUNNING now):{Color.RESET}")
            for flow in active_flows[:5]:  # Show first 5
                flow_type = flow.get('type', 'Flow')
                print(f"  [*] {flow['ipo']} -> {flow_type}: {flow['flow']}  (elapsed: {flow['elapsed']})")
            if len(active_flows) > 5:
                print(f"  ... and {len(active_flows) - 5} more active flows")
        
        # Print detailed flow information (for completed IPOs)
        if detailed_flow_info:
            print(f"\n{Color.CYAN}Detailed Flow Information:{Color.RESET}")
            for ipo_detail in detailed_flow_info[:5]:  # Show first 5 IPOs
                ipo = ipo_detail['ipo']
                flows = ipo_detail['flows']
                pnr_start = ipo_detail.get('pnr_start', 'N/A')
                pnr_end = ipo_detail.get('pnr_end', 'N/A')
                pnr_runtime = ipo_detail.get('pnr_runtime', 'N/A')
                pnr_running_stage = ipo_detail.get('pnr_running_stage', None)
                
                print(f"\n  {Color.CYAN}{ipo}:{Color.RESET}")
                print(f"    PnR: {pnr_runtime}  [{pnr_start} -> {pnr_end}]" + 
                      (f"  (Running: {pnr_running_stage})" if pnr_running_stage else ""))
                
                if flows:
                    print(f"    Signoff flows:")
                    for flow in flows:
                        status_marker = {
                            'COMPLETED': '[OK]',
                            'RUNNING': '[RUN]',
                            'FAILED': '[FAIL]',
                            'STALE': '[WARN]',
                            'NOT_RUN': '[--]'
                        }.get(flow['status'], '[--]')
                        
                        print(f"      {status_marker} {flow['name']:<12} {flow['runtime']:<8} [{flow['start']} -> {flow['end']}]")
            
            if len(detailed_flow_info) > 5:
                print(f"\n  ... (showing 5/{len(detailed_flow_info)} IPOs, use -v to show all)")
    
    def _print_runtime_summary_table_advanced(self, runtime_data: Dict[str, Any], pnr_runtimes: Dict[str, Any]) -> None:
        """Alternative advanced table printing with more options
        
        Args:
            runtime_data: Dictionary of runtime data for each stage
            pnr_runtimes: Dictionary of PnR stage runtimes
        """
        print(f"\n{Color.CYAN}Runtime Summary Table (Advanced):{Color.RESET}")
        
        # Prepare table data
        table_data = []
        
        # Construction stages
        if 'DC' in runtime_data:
            table_data.append(("Construction", "DC", runtime_data['DC']))
        
        for ipo, runtime in pnr_runtimes.items():
            table_data.append(("Construction", f"PnR ({ipo})", runtime))
        
        # Signoff stages
        if 'Star' in runtime_data:
            table_data.append(("Signoff", "Star", runtime_data['Star']))
        if 'Auto PT' in runtime_data:
            table_data.append(("Signoff", "Auto PT", runtime_data['Auto PT']))
        
        # Formal verification stages
        for key, value in runtime_data.items():
            if key.startswith('Formal ('):
                formal_type = key.replace('Formal (', '').replace(')', '')
                table_data.append(("Signoff", f"Formal ({formal_type})", value))
        
        if 'GL Check' in runtime_data:
            table_data.append(("Signoff", "GL Check", runtime_data['GL Check']))
        if 'PV' in runtime_data:
            table_data.append(("Signoff", "PV", runtime_data['PV']))
        
        # ECO stages
        if 'Auto PT Fix' in runtime_data:
            table_data.append(("ECO", "Auto PT Fix", runtime_data['Auto PT Fix']))
        if 'Gen ECO Netlist' in runtime_data:
            table_data.append(("ECO", "Gen ECO Netlist", runtime_data['Gen ECO Netlist']))
        if 'NV Gate ECO' in runtime_data:
            table_data.append(("ECO", "NV Gate ECO", runtime_data['NV Gate ECO']))
        
        # Print table with borders and better formatting
        if table_data:
            # Calculate column widths
            max_category = max(len(row[0]) for row in table_data)
            max_stage = max(len(row[1]) for row in table_data)
            max_runtime = max(len(row[2]) for row in table_data)
            
            # Ensure minimum widths
            category_width = max(max_category, 8)
            stage_width = max(max_stage, 5)
            runtime_width = max(max_runtime, 6)
            
            # Print table with borders
            print(f"  +{'-' * (category_width + 2)}+{'-' * (stage_width + 2)}+{'-' * (runtime_width + 2)}+")
            print(f"  | {'Category':<{category_width}} | {'Stage':<{stage_width}} | {'Runtime':<{runtime_width}} |")
            print(f"  +{'-' * (category_width + 2)}+{'-' * (stage_width + 2)}+{'-' * (runtime_width + 2)}+")
            
            for category, stage, runtime in table_data:
                print(f"  | {category:<{category_width}} | {stage:<{stage_width}} | {runtime:<{runtime_width}} |")
            
            print(f"  +{'-' * (category_width + 2)}+{'-' * (stage_width + 2)}+{'-' * (runtime_width + 2)}+")
    
    def _generate_runtime_html_report(self, runtime_data: Dict[str, Any], pnr_runtimes: Dict[str, Any], prc_status_file: str, runtime_timestamps: Optional[Dict[str, Any]] = None, ipo_signoff_data: Optional[Dict[str, Dict]] = None) -> Optional[str]:
        """Generate comprehensive HTML runtime report with IPO-specific flows
        
        Args:
            runtime_data: Dictionary of runtime data for each stage
            pnr_runtimes: Dictionary of PnR stage runtimes
            prc_status_file: Path to PRC status file
            runtime_timestamps: Optional dictionary of runtime timestamps
            ipo_signoff_data: Optional dictionary of IPO-specific signoff flow data
            
        Returns:
            Path to generated HTML file or None if generation fails
        """
        try:
            # Get detailed PnR stage data
            pnr_stage_data = self._extract_detailed_pnr_stage_data(prc_status_file)
            
            # Check for fast_dc detection
            fast_dc_log = os.path.join(self.workarea, "syn_flow/fast_dc/log/fast_dc.log")
            fast_dc_detected = os.path.exists(fast_dc_log)
            
            # Check for RTL formal
            rtl_detected = self._check_rtl_formal_exists()
            
            # Generate HTML content
            html_content = self._create_runtime_html_content(runtime_data, pnr_runtimes, pnr_stage_data, prc_status_file, runtime_timestamps, fast_dc_detected, rtl_detected, ipo_signoff_data)
            
            # Save HTML file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            html_filename = f"{self.design_info.top_hier}_{os.environ.get('USER', 'avice')}_runtime_report_{timestamp}.html"
            html_path = os.path.join(os.getcwd(), html_filename)
            
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            # Determine display path (will be moved to html/ or test_outputs/html/ by _organize_html_files)
            html_output_dir = self._get_html_output_dir()
            display_path = os.path.relpath(html_output_dir, os.getcwd())
            
            print(f"\n  {Color.CYAN}Runtime HTML Report:{Color.RESET}")
            print(f"    Open with: /home/utils/firefox-118.0.1/firefox {Color.MAGENTA}{display_path}/{html_filename}{Color.RESET} &")
            
            return os.path.abspath(html_path)
            
        except Exception as e:
            print(f"  Error generating runtime HTML report: {e}")
            return ""
    
    def _check_rtl_formal_exists(self) -> bool:
        """Check if RTL formal verification directories exist
        
        Returns:
            True if RTL formal directory exists, False otherwise
        """
        rtl_formal_dirs = [
            os.path.join(self.workarea, "formal_flow/rtl_vs_pnr_bbox_fm"),
            os.path.join(self.workarea, "formal_flow/rtl_vs_pnr_fm"),
            os.path.join(self.workarea, "rtl_vs_pnr_bbox_fm"),
            os.path.join(self.workarea, "rtl_vs_pnr_fm")
        ]
        for rtl_dir in rtl_formal_dirs:
            if os.path.isdir(rtl_dir):
                return True
        return False
    
    def _extract_detailed_pnr_stage_data(self, prc_status_file: str) -> Dict[str, Any]:
        """Extract detailed PnR stage data for each IPO
        
        Args:
            prc_status_file: Path to prc.status file
            
        Returns:
            Dictionary with PnR stage data
        """
        pnr_stage_data = {}
        
        if not os.path.exists(prc_status_file):
            return pnr_stage_data
        
        try:
            with open(prc_status_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            for line in lines:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                
                # Parse status line: block ipo step status duration logfile
                parts = line.split()
                if len(parts) >= 6:
                    block, ipo, step, status, duration, logfile = parts[0], parts[1], parts[2], parts[3], parts[4], ' '.join(parts[5:])
                    
                    if ipo not in pnr_stage_data:
                        pnr_stage_data[ipo] = []
                    
                    # Convert duration to hours
                    try:
                        duration_seconds = int(duration)
                        duration_hours = duration_seconds / 3600
                        duration_days = duration_hours / 24
                        
                        if duration_hours >= 24:
                            duration_str = f"{duration_hours:.2f} hours ({duration_days:.2f} days)"
                        else:
                            duration_str = f"{duration_hours:.2f} hours"
                    except ValueError:
                        duration_str = f"{duration} seconds"
                    
                    # Convert logfile path to absolute for HTML links to work from any location
                    abs_logfile = os.path.abspath(os.path.join(self.workarea, logfile)) if not os.path.isabs(logfile) else logfile
                    
                    pnr_stage_data[ipo].append({
                        'step': step,
                        'status': status,
                        'duration': duration_str,
                        'duration_seconds': duration,
                        'logfile': abs_logfile
                    })
        
        except Exception as e:
            print(f"  Error extracting detailed PnR stage data: {e}")
        
        return pnr_stage_data
    
    def _generate_unified_html_table(self, runtime_data: Dict[str, Any], runtime_timestamps: Dict[str, Any], 
                                     ipo_signoff_data: Dict[str, Dict], pnr_stage_data: Dict[str, Any]) -> str:
        """Generate unified HTML table matching terminal output format
        
        Args:
            runtime_data: Dictionary of runtime data
            runtime_timestamps: Dictionary of timestamps
            ipo_signoff_data: IPO-specific signoff flow data
            pnr_stage_data: PnR stage data per IPO
            
        Returns:
            HTML string for unified flow timeline table
        """
        # Build unified flow data (same logic as terminal output)
        all_flows = []
        
        # Phase runtimes
        synthesis_runtime_hours = 0
        pnr_runtime_hours = 0
        signoff_runtime_hours = 0
        
        # Synthesis flows
        if 'Fast DC' in runtime_data:
            start, end = runtime_timestamps.get('Fast DC', (None, None))
            runtime_str = runtime_data['Fast DC']
            runtime_hours = self._parse_runtime_to_hours(runtime_str)
            all_flows.append({
                'phase': 'Synthesis',
                'name': 'Fast DC',
                'runtime_hours': runtime_hours,
                'start': start,
                'end': end,
                'status': 'COMPLETED',
                'ipo': None
            })
            synthesis_runtime_hours += runtime_hours
        
        dc_key = 'Full DC' if 'Full DC' in runtime_data else 'DC'
        if dc_key in runtime_data:
            start, end = runtime_timestamps.get(dc_key, (None, None))
            runtime_str = runtime_data[dc_key]
            runtime_hours = self._parse_runtime_to_hours(runtime_str)
            all_flows.append({
                'phase': 'Synthesis',
                'name': dc_key,
                'runtime_hours': runtime_hours,
                'start': start,
                'end': end,
                'status': 'COMPLETED',
                'ipo': None
            })
            synthesis_runtime_hours += runtime_hours
        
        # PnR flows (per-IPO with stages) - Use ipo_signoff_data for full PnR info with timestamps
        if ipo_signoff_data:
            for ipo, ipo_data in sorted(ipo_signoff_data.items()):
                if 'pnr_status' in ipo_data and 'stages' in ipo_data['pnr_status']:
                    for stage_name, stage_info in ipo_data['pnr_status']['stages'].items():
                        # Normalize stage names (no IPO suffix - shown in header)
                        display_name = stage_name.capitalize()
                        
                        runtime_hours = stage_info['runtime_hours']
                        all_flows.append({
                            'phase': 'PnR',
                            'name': display_name,  # No IPO suffix - shown in header
                            'runtime_hours': runtime_hours,
                            'start': stage_info.get('start'),
                            'end': stage_info.get('end'),
                            'status': stage_info.get('status', 'COMPLETED'),
                            'ipo': ipo
                        })
                        pnr_runtime_hours += runtime_hours
        
        # Signoff flows (IPO-specific)
        if ipo_signoff_data:
            for ipo, ipo_dict in sorted(ipo_signoff_data.items()):
                # Extract signoff flows (skip 'pnr' and 'pnr_status' keys)
                if 'signoff' in ipo_dict and isinstance(ipo_dict['signoff'], dict):
                    signoff_flows = ipo_dict['signoff']
                    
                    # Define order: Star, Auto_Pt, Auto_Pt_Fix, Eco, Formal, GL Check, NV Gate ECO, PV flows
                    flow_order = ['star', 'auto_pt', 'auto_pt_fix', 'eco', 'formal_rtl_pnr',
                                 'formal_bbox', 'gl_check', 'nv_gate_eco', 'nv_pv', 'pv']
                    flow_name_map = {
                        'star': 'Star',
                        'auto_pt': 'Auto_Pt',
                        'auto_pt_fix': 'Auto_Pt_Fix',
                        'eco': 'Eco',
                        'formal_rtl_pnr': 'Formal_Rtl_Pnr',
                        'formal_bbox': 'Formal_Bbox',
                        'gl_check': 'Gl_Check',
                        'nv_gate_eco': 'Nv_Gate_Eco',
                        'nv_pv': 'Nv_Pv',
                        'pv': 'PV'
                    }
                    
                    # Process flows in order
                    for flow_type in flow_order:
                        if flow_type in signoff_flows:
                            flow_info = signoff_flows[flow_type]
                            if flow_info and 'runtime' in flow_info:
                                runtime_hours = self._parse_runtime_to_hours(flow_info['runtime'])
                                flow_name = flow_name_map.get(flow_type, flow_type.replace('_', ' ').title())
                                
                                all_flows.append({
                                    'phase': 'Signoff',
                                    'name': flow_name,
                                    'runtime_hours': runtime_hours,
                                    'start': flow_info.get('start_time', 'N/A'),
                                    'end': flow_info.get('end_time', 'N/A'),
                                    'status': flow_info.get('status', 'COMPLETED'),
                                    'ipo': ipo
                                })
                                signoff_runtime_hours += runtime_hours
                    
                    # Process any remaining flows not in flow_order
                    for flow_type, flow_info in signoff_flows.items():
                        if flow_type not in flow_order and flow_info and 'runtime' in flow_info:
                            runtime_hours = self._parse_runtime_to_hours(flow_info['runtime'])
                            flow_name = flow_name_map.get(flow_type, flow_type.replace('_', ' ').title())
                            
                            all_flows.append({
                                'phase': 'Signoff',
                                'name': flow_name,
                                'runtime_hours': runtime_hours,
                                'start': flow_info.get('start_time', 'N/A'),
                                'end': flow_info.get('end_time', 'N/A'),
                                'status': flow_info.get('status', 'COMPLETED'),
                                'ipo': ipo
                            })
                            signoff_runtime_hours += runtime_hours
        
        # Calculate grand total and find bottleneck
        grand_total_hours = synthesis_runtime_hours + pnr_runtime_hours + signoff_runtime_hours
        bottleneck_flow = None
        bottleneck_pct = 0
        
        if grand_total_hours > 0:
            for flow in all_flows:
                if flow['runtime_hours'] > 0:
                    pct = (flow['runtime_hours'] / grand_total_hours) * 100
                    if pct > bottleneck_pct:
                        bottleneck_pct = pct
                        bottleneck_flow = flow
        
        # Generate HTML table
        html = """
        <div class="section">
            <h2>Unified Flow Timeline</h2>
            <table class="unified-table">
                <thead>
                    <tr>
                        <th>Phase</th>
                        <th>Flow/Stage</th>
                        <th>Runtime</th>
                        <th>Timeline</th>
                        <th>Status</th>
                    </tr>
                </thead>
                <tbody>"""
        
        # Group flows by phase and IPO
        pnr_by_ipo = {}
        signoff_by_ipo = {}
        
        for flow in all_flows:
            if flow['phase'] == 'Synthesis':
                # Add synthesis rows
                status_class = 'status-completed' if flow['status'] == 'COMPLETED' else 'status-running'
                timeline_str = f"[{flow['start'] or 'N/A'} -> {flow['end'] or 'N/A'}]"
                is_bottleneck = (bottleneck_flow and flow['name'] == bottleneck_flow['name'] and flow['ipo'] == bottleneck_flow['ipo'])
                bottleneck_class = ' bottleneck-row' if is_bottleneck else ''
                
                # Add percentage if â‰¥20% of total
                runtime_str = f"{flow['runtime_hours']:.2f}h"
                if grand_total_hours > 0:
                    pct = (flow['runtime_hours'] / grand_total_hours) * 100
                    if pct >= 20:
                        runtime_str += f" <span class='pct-indicator'>({pct:.0f}%)</span>"
                
                # Enhanced status badge
                status_badge_class = 'status-badge-running' if flow['status'] == 'RUNNING' else 'status-badge-ok'
                status_display = f"<span class='status-badge {status_badge_class}'>{flow['status']}</span>"
                
                html += f"""
                    <tr class="{status_class}{bottleneck_class}">
                        <td>Synthesis</td>
                        <td>{flow['name']}{' âš¡' if flow['status'] == 'RUNNING' else ''}{' *' if is_bottleneck else ''}</td>
                        <td>{runtime_str}</td>
                        <td>{timeline_str}</td>
                        <td>{status_display}</td>
                    </tr>"""
            elif flow['phase'] == 'PnR':
                ipo = flow['ipo']
                if ipo not in pnr_by_ipo:
                    pnr_by_ipo[ipo] = []
                pnr_by_ipo[ipo].append(flow)
            elif flow['phase'] == 'Signoff':
                ipo = flow['ipo']
                if ipo not in signoff_by_ipo:
                    signoff_by_ipo[ipo] = []
                signoff_by_ipo[ipo].append(flow)
        
        # Add blank line after synthesis
        if synthesis_runtime_hours > 0:
            html += '<tr class="spacer-row"><td colspan="5"></td></tr>'
        
        # PnR flows with IPO headers
        if pnr_by_ipo:
            for ipo in sorted(pnr_by_ipo.keys()):
                flows = pnr_by_ipo[ipo]
                
                # Calculate IPO total
                ipo_total_hours = sum(f['runtime_hours'] for f in flows)
                ipo_start = min([f['start'] for f in flows if f['start']], default='N/A')
                ipo_end = max([f['end'] for f in flows if f['end']], default='N/A')
                
                # IPO header (always show for consistency with terminal)
                ipo_label = ipo.upper()
                timeline_str = f"[{ipo_start} -> {ipo_end}]"
                html += f"""
                    <tr class="ipo-header-row">
                        <td>PnR</td>
                        <td colspan="4">[{ipo_label}] {ipo_total_hours:.2f}h  {timeline_str}</td>
                    </tr>"""
                
                # PnR stage rows
                for flow in flows:
                    # Skip setup if < 0.1h
                    if 'Setup' in flow['name'] and flow['runtime_hours'] < 0.1:
                        continue
                    
                    status_class = 'status-running' if flow['status'] == 'RUNNING' else 'status-completed'
                    timeline_str = f"[{flow['start'] or 'N/A'} -> {flow['end'] or 'N/A'}]"
                    is_bottleneck = (bottleneck_flow and flow['name'] == bottleneck_flow['name'] and flow['ipo'] == bottleneck_flow['ipo'])
                    bottleneck_class = ' bottleneck-row' if is_bottleneck else ''
                    
                    # Add percentage if â‰¥20% of total
                    runtime_str = f"{flow['runtime_hours']:.2f}h"
                    if grand_total_hours > 0:
                        pct = (flow['runtime_hours'] / grand_total_hours) * 100
                        if pct >= 20:
                            runtime_str += f" <span class='pct-indicator'>({pct:.0f}%)</span>"
                    
                    # Enhanced status badge
                    if flow['status'] == 'RUNNING':
                        status_badge_class = 'status-badge-running'
                    elif flow['status'] in ['OK', 'COMPLETED']:
                        status_badge_class = 'status-badge-ok'
                    elif flow['status'] == 'FAILED':
                        status_badge_class = 'status-badge-fail'
                    else:
                        status_badge_class = 'status-badge-warn'
                    status_display = f"<span class='status-badge {status_badge_class}'>{flow['status']}</span>"
                    
                    html += f"""
                    <tr class="{status_class}{bottleneck_class}">
                        <td>PnR</td>
                        <td>{flow['name'].split('(')[0].strip()}{' âš¡' if flow['status'] == 'RUNNING' else ''}{' *' if is_bottleneck else ''}</td>
                        <td>{runtime_str}</td>
                        <td>{timeline_str}</td>
                        <td>{status_display}</td>
                    </tr>"""
                
                # Blank line between IPOs (only if multiple)
                if len(pnr_by_ipo) > 1:
                    html += '<tr class="spacer-row"><td colspan="5"></td></tr>'
            
            # DC + PnR total
            full_dc_hours = 0
            for flow in all_flows:
                if flow['phase'] == 'Synthesis' and ('Full DC' in flow['name'] or flow['name'] == 'DC'):
                    full_dc_hours = flow['runtime_hours']
                    break
            
            slowest_ipo_hours = max([sum(f['runtime_hours'] for f in flows) for flows in pnr_by_ipo.values()], default=0)
            pnr_total_adjusted = full_dc_hours + slowest_ipo_hours
            
            pnr_start = min([f['start'] for f in all_flows if f['phase'] == 'PnR' and f['start']], default='N/A')
            pnr_end = max([f['end'] for f in all_flows if f['phase'] == 'PnR' and f['end']], default='N/A')
            timeline_str = f"[{pnr_start} -> {pnr_end}]"
            
            html += f"""
                    <tr class="separator-row">
                        <td colspan="5"></td>
                    </tr>
                    <tr class="total-row">
                        <td>PnR</td>
                        <td>DC + PnR</td>
                        <td>{pnr_total_adjusted:.2f}h</td>
                        <td>{timeline_str}</td>
                        <td></td>
                    </tr>
                    <tr class="spacer-row"><td colspan="5"></td></tr>"""
        
        # Signoff flows with IPO headers
        if signoff_by_ipo:
            for ipo in sorted(signoff_by_ipo.keys()):
                flows = signoff_by_ipo[ipo]
                
                # Calculate IPO signoff total
                ipo_signoff_hours = sum(f['runtime_hours'] for f in flows)
                ipo_start = min([f['start'] for f in flows if f['start'] and f['start'] != 'N/A'], default='N/A')
                ipo_end = max([f['end'] for f in flows if f['end'] and f['end'] != 'N/A'], default='N/A')
                
                # IPO header (always show for consistency with terminal)
                ipo_label = ipo.upper() if ipo != 'root' else 'ROOT'
                timeline_str = f"[{ipo_start} -> {ipo_end}]"
                html += f"""
                    <tr class="ipo-header-row">
                        <td>Signoff</td>
                        <td colspan="4">[{ipo_label}] {ipo_signoff_hours:.2f}h  {timeline_str}</td>
                    </tr>"""
                
                # Signoff flow rows
                for flow in flows:
                    status_class = 'status-running' if flow['status'] == 'RUNNING' else 'status-completed'
                    timeline_str = f"[{flow['start'] or 'N/A'} -> {flow['end'] or 'N/A'}]"
                    is_bottleneck = (bottleneck_flow and flow['name'] == bottleneck_flow['name'] and flow['ipo'] == bottleneck_flow['ipo'])
                    bottleneck_class = ' bottleneck-row' if is_bottleneck else ''
                    
                    # Add percentage if â‰¥20% of total
                    runtime_str = f"{flow['runtime_hours']:.2f}h"
                    if grand_total_hours > 0:
                        pct = (flow['runtime_hours'] / grand_total_hours) * 100
                        if pct >= 20:
                            runtime_str += f" <span class='pct-indicator'>({pct:.0f}%)</span>"
                    
                    # Enhanced status badge
                    if flow['status'] == 'RUNNING':
                        status_badge_class = 'status-badge-running'
                    elif flow['status'] in ['OK', 'COMPLETED']:
                        status_badge_class = 'status-badge-ok'
                    elif flow['status'] == 'FAILED':
                        status_badge_class = 'status-badge-fail'
                    else:
                        status_badge_class = 'status-badge-warn'
                    status_display = f"<span class='status-badge {status_badge_class}'>{flow['status']}</span>"
                    
                    html += f"""
                    <tr class="{status_class}{bottleneck_class}">
                        <td>Signoff</td>
                        <td>{flow['name']}{' âš¡' if flow['status'] == 'RUNNING' else ''}{' *' if is_bottleneck else ''}</td>
                        <td>{runtime_str}</td>
                        <td>{timeline_str}</td>
                        <td>{status_display}</td>
                    </tr>"""
                
                # Blank line between IPOs (only if multiple)
                if len(signoff_by_ipo) > 1:
                    html += '<tr class="spacer-row"><td colspan="5"></td></tr>'
        
        # Grand Total with enhanced metrics
        overall_start = min([f['start'] for f in all_flows if f['start'] and f['start'] != 'N/A'], default='N/A')
        overall_end = max([f['end'] for f in all_flows if f['end'] and f['end'] != 'N/A'], default='N/A')
        timeline_str = f"[{overall_start} -> {overall_end}]"
        
        # Calculate timeline duration and efficiency
        elapsed_hours = None
        efficiency_pct = None
        timeline_days = None
        current_year = time.localtime().tm_year
        
        try:
            if overall_start != 'N/A' and overall_end != 'N/A':
                start_struct = time.strptime(f"{current_year}/{overall_start}", "%Y/%m/%d %H:%M")
                end_struct = time.strptime(f"{current_year}/{overall_end}", "%Y/%m/%d %H:%M")
                start_epoch = time.mktime(start_struct)
                end_epoch = time.mktime(end_struct)
                elapsed_seconds = end_epoch - start_epoch
                elapsed_hours = elapsed_seconds / 3600
                timeline_days = elapsed_hours / 24
                
                if elapsed_hours > 0:
                    efficiency_pct = (grand_total_hours / elapsed_hours) * 100
        except:
            pass
        
        # Format grand total runtime
        runtime_display = f"{grand_total_hours:.2f}h"
        if grand_total_hours >= 24:
            total_days = grand_total_hours / 24
            runtime_display += f" ({total_days:.2f} days)"
        
        html += f"""
                    <tr class="separator-row">
                        <td colspan="5"></td>
                    </tr>
                    <tr class="grand-total-row">
                        <td colspan="2">GRAND TOTAL</td>
                        <td>{runtime_display}</td>
                        <td>{timeline_str}</td>
                        <td></td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <!-- Enhanced Summary Section -->
        <div class="section">
            <h2 style="color: #2c3e50; margin-bottom: 20px;">ðŸ“Š Summary Metrics</h2>
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px;">
                <div class="metric-card" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);">
                    <div class="metric-label">Total Runtime</div>
                    <div class="metric-value">{runtime_display}</div>
                </div>"""
        
        if timeline_days is not None:
            html += f"""
                <div class="metric-card" style="background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);">
                    <div class="metric-label">Timeline Duration</div>
                    <div class="metric-value">{timeline_days:.2f} days</div>
                </div>"""
        
        if efficiency_pct is not None:
            # Yellow warning if >100% (parallel execution)
            efficiency_color = "linear-gradient(135deg, #f093fb 0%, #f5576c 100%)" if efficiency_pct > 100 else "linear-gradient(135deg, #43e97b 0%, #38f9d7 100%)"
            efficiency_note = " *" if efficiency_pct > 100 else ""
            html += f"""
                <div class="metric-card" style="background: {efficiency_color};">
                    <div class="metric-label">Efficiency{efficiency_note}</div>
                    <div class="metric-value">{efficiency_pct:.0f}%</div>
                </div>"""
            
            if efficiency_pct > 100:
                html += f"""
            </div>
            <div style="margin-top: 10px; padding: 10px; background: #fff3cd; border-left: 4px solid #ffc107; border-radius: 4px;">
                <strong>* Efficiency > 100%</strong> indicates parallel execution of flows
            </div>"""
            else:
                html += "</div>"
        else:
            html += "</div>"
        
        html += """
        </div>
        
        <div class="section">
            <h2 style="color: #2c3e50; margin-bottom: 20px;">ðŸŽ¯ Critical Path Analysis</h2>"""
        
        if bottleneck_flow:
            # Warning styling if bottleneck >20%
            warning_style = ""
            warning_icon = ""
            if bottleneck_pct >= 20:
                warning_style = "border-left: 5px solid #ff9800;"
                warning_icon = "âš ï¸ "
            
            html += f"""
            <div class="metric-card" style="background: linear-gradient(135deg, #fa709a 0%, #fee140 100%); {warning_style}">
                <div class="metric-label">{warning_icon}Bottleneck Stage</div>
                <div class="metric-value">{bottleneck_flow['name']} ({bottleneck_pct:.1f}% of total)</div>
            </div>"""
            
            if bottleneck_pct >= 20:
                html += f"""
            <div style="margin-top: 15px; padding: 15px; background: #fff3cd; border-left: 4px solid #ff9800; border-radius: 4px;">
                <strong>âš ï¸ This stage dominates the flow</strong> - Consider optimization opportunities
            </div>"""
        else:
            html += """
            <div class="metric-card">
                <div class="metric-value">No bottleneck identified</div>
            </div>"""
        
        html += """
        </div>"""
        
        return html
    
    def _create_runtime_html_content(self, runtime_data: Dict[str, Any], pnr_runtimes: Dict[str, Any], pnr_stage_data: Dict[str, Any], prc_status_file: str, runtime_timestamps: Optional[Dict[str, Any]] = None, fast_dc_detected: bool = False, rtl_detected: bool = False, ipo_signoff_data: Optional[Dict[str, Dict]] = None) -> str:
        """Create HTML content for runtime report with IPO-specific sections
        
        Args:
            runtime_data: Dictionary of runtime data for each stage
            pnr_runtimes: Dictionary of PnR stage runtimes
            pnr_stage_data: Dictionary of detailed PnR stage data
            prc_status_file: Path to PRC status file
            runtime_timestamps: Optional dictionary of runtime timestamps
            fast_dc_detected: Whether fast DC mode was detected
            rtl_detected: Whether RTL formal was detected
            ipo_signoff_data: Optional dictionary of IPO-specific signoff flow data
            
        Returns:
            HTML content string
        """
        if runtime_timestamps is None:
            runtime_timestamps = {}
            
        # Get AVICE logo
        logo_path = "/home/scratch.avice_vlsi/cursor/avice_wa_review/assets/images/avice_logo.png"
        logo_data = ""
        if os.path.exists(logo_path):
            with open(logo_path, "rb") as logo_file:
                logo_data = base64.b64encode(logo_file.read()).decode('utf-8')
        
        # Generate badges HTML for detected stages
        badges_html = ""
        if fast_dc_detected:
            badges_html += '<span class="badge badge-warning">Fast DC Detected</span>'
        if rtl_detected:
            badges_html += '<span class="badge badge-info">RTL Formal Detected</span>'
        
        html = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Runtime Analysis Report - {self.design_info.top_hier}</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
            color: #333;
        }}
        .header {{
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            padding: 30px;
            display: grid;
            grid-template-columns: auto 1fr;
            gap: 20px;
            align-items: center;
            border-radius: 15px;
            margin-bottom: 20px;
        }}
        .header-text h1 {{
            font-size: 28px;
            margin: 0 0 8px 0;
        }}
        .header-text p {{
            opacity: 0.9;
            font-size: 14px;
            margin: 4px 0;
        }}
        .badge {{
            display: inline-block;
            padding: 5px 12px;
            margin: 5px 5px 5px 0;
            border-radius: 15px;
            font-size: 12px;
            font-weight: bold;
        }}
        .badge-warning {{
            background-color: #ffc107;
            color: #000;
        }}
        .badge-info {{
            background-color: #17a2b8;
            color: #fff;
        }}
        .logo {{
            width: 80px;
            height: 80px;
            border-radius: 10px;
            background: white;
            padding: 10px;
            cursor: pointer;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }}
        .logo:hover {{
            transform: scale(1.05);
            box-shadow: 0 8px 16px rgba(0,0,0,0.3);
        }}
        .logo-modal {{
            display: none;
            position: fixed;
            z-index: 9999;
            left: 0;
            top: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0,0,0,0.9);
            justify-content: center;
            align-items: center;
        }}
        .logo-modal.active {{
            display: flex;
        }}
        .logo-modal-content {{
            max-width: 90%;
            max-height: 90%;
            border-radius: 10px;
        }}
        .logo-modal-close {{
            position: absolute;
            top: 20px;
            right: 35px;
            color: #f1f1f1;
            font-size: 40px;
            font-weight: bold;
            cursor: pointer;
        }}
        .logo-modal-close:hover {{
            color: #bbb;
        }}
        .container {{
            max-width: 1400px;
            margin: 0 auto;
        }}
        
        /* Enhanced Grid Layout for Summary Cards */
        .summary-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }}
        .summary-card {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 25px;
            border-radius: 12px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.15);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            text-align: center;
        }}
        .summary-card:hover {{
            transform: translateY(-5px);
            box-shadow: 0 8px 16px rgba(0,0,0,0.25);
        }}
        .summary-card-value {{
            font-size: 2.5em;
            font-weight: bold;
            margin: 10px 0;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
        }}
        .summary-card-label {{
            font-size: 1.1em;
            opacity: 0.9;
        }}
        
        .section {{
            background: white;
            margin: 20px 0;
            padding: 25px;
            border-radius: 12px;
            box-shadow: 0 2px 12px rgba(0,0,0,0.1);
            border-left: 5px solid #667eea;
        }}
        .section h2 {{
            color: #667eea;
            border-bottom: 2px solid #667eea;
            padding-bottom: 10px;
            margin-top: 0;
        }}
        .summary-table {{
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }}
        .summary-table th, .summary-table td {{
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }}
        .summary-table th {{
            background-color: #f8f9fa;
            font-weight: bold;
            color: #495057;
        }}
        .summary-table tr:hover {{
            background-color: #f5f5f5;
        }}
        .category-construction {{ color: #28a745; font-weight: bold; }}
        .category-signoff {{ color: #007bff; font-weight: bold; }}
        .category-eco {{ color: #dc3545; font-weight: bold; }}
        
        /* Unified Flow Timeline Table */
        .unified-table {{
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
        }}
        .unified-table th, .unified-table td {{
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }}
        .unified-table th {{
            background-color: #667eea;
            color: white;
            font-weight: bold;
        }}
        .unified-table tr:hover {{
            background-color: #f5f5f5;
        }}
        .status-completed {{
            background-color: #e8f5e9;
        }}
        .status-running {{
            background-color: #e0f7fa !important;
            animation: pulse 2s infinite;
        }}
        @keyframes pulse {{
            0%, 100% {{ background-color: #e0f7fa; }}
            50% {{ background-color: #b2ebf2; }}
        }}
        .bottleneck-row {{
            background-color: #fff9c4 !important;
            font-weight: bold;
        }}
        .ipo-header-row {{
            background-color: #e3f2fd;
            font-weight: bold;
            color: #1976d2;
        }}
        .ipo-header-row td {{
            padding: 15px 12px;
        }}
        .spacer-row {{
            height: 10px;
            background: transparent;
        }}
        .spacer-row td {{
            border: none;
        }}
        .separator-row {{
            height: 2px;
            background: #ddd;
        }}
        .separator-row td {{
            border: none;
            padding: 0;
        }}
        .total-row {{
            background-color: #f5f5f5;
            font-weight: bold;
        }}
        .grand-total-row {{
            background: linear-gradient(135deg, #ffd700 0%, #ffed4e 100%);
            font-weight: bold;
            font-size: 1.1em;
        }}
        .grand-total-row td {{
            padding: 15px 12px;
        }}
        .metric-card {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 10px;
            margin: 10px 0;
        }}
        .metric-label {{
            font-size: 0.9em;
            opacity: 0.9;
            margin-bottom: 5px;
        }}
        .metric-value {{
            font-size: 1.5em;
            font-weight: bold;
        }}
        .pct-indicator {{
            color: #ff9800;
            font-weight: bold;
            font-size: 0.95em;
        }}
        /* Enhanced status badges */
        .status-badge {{
            display: inline-block;
            padding: 4px 10px;
            border-radius: 12px;
            font-size: 0.85em;
            font-weight: bold;
            text-align: center;
            min-width: 60px;
        }}
        .status-badge-ok {{
            background: linear-gradient(135deg, #4caf50 0%, #45a049 100%);
            color: white;
        }}
        .status-badge-running {{
            background: linear-gradient(135deg, #00bcd4 0%, #0097a7 100%);
            color: white;
            animation: pulse 2s infinite;
        }}
        .status-badge-fail {{
            background: linear-gradient(135deg, #f44336 0%, #d32f2f 100%);
            color: white;
        }}
        .status-badge-warn {{
            background: linear-gradient(135deg, #ff9800 0%, #f57c00 100%);
            color: white;
        }}
        /* Enhanced Grid Layout for PnR Details */
        .pnr-details {{
            margin: 20px 0;
            display: grid;
            gap: 20px;
        }}
        .ipo-section {{
            margin: 0;
            padding: 20px;
            background-color: #f8f9fa;
            border-radius: 10px;
            border-left: 5px solid #667eea;
            box-shadow: 0 2px 6px rgba(0,0,0,0.08);
            transition: transform 0.2s ease;
        }}
        .ipo-section:hover {{
            transform: translateX(5px);
            box-shadow: 0 4px 10px rgba(0,0,0,0.12);
        }}
        .ipo-title {{
            font-size: 18px;
            font-weight: bold;
            color: #667eea;
            margin-bottom: 15px;
        }}
        .stage-table {{
            width: 100%;
            border-collapse: collapse;
            margin: 10px 0;
        }}
        .stage-table th, .stage-table td {{
            padding: 8px 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }}
        .stage-table th {{
            background-color: #e9ecef;
            font-weight: bold;
        }}
        .status-done {{ color: #28a745; font-weight: bold; }}
        .status-run {{ color: #ffc107; font-weight: bold; }}
        .status-err {{ color: #dc3545; font-weight: bold; }}
        .highlight-max-runtime {{
            background-color: #fff3cd;
            border-left: 4px solid #ffc107;
            font-weight: bold;
        }}
        .highlight-max-runtime td {{
            background-color: #fff3cd;
        }}
        .prc-link {{
            display: inline-block;
            background-color: #667eea;
            color: white;
            padding: 8px 16px;
            text-decoration: none;
            border-radius: 5px;
            margin: 10px 0;
            transition: background-color 0.3s ease;
        }}
        .prc-link:hover {{
            background-color: #5a6fd8;
        }}
        .total-runtime {{
            font-size: 16px;
            font-weight: bold;
            color: #495057;
            margin: 10px 0;
            padding: 10px;
            background-color: #e9ecef;
            border-radius: 5px;
        }}
        .tablog-btn {{
            background-color: #28a745;
            color: white;
            border: none;
            padding: 5px 10px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 12px;
            margin-right: 5px;
            transition: background-color 0.3s ease;
        }}
        .tablog-btn:hover {{
            background-color: #218838;
        }}
        .tablog-btn:active {{
            background-color: #1e7e34;
        }}
        .raw-log-link {{
            display: inline-block;
            background-color: #6c757d;
            color: white;
            text-decoration: none;
            padding: 5px 10px;
            border-radius: 4px;
            font-size: 12px;
            margin-left: 5px;
            transition: background-color 0.3s ease;
        }}
        .raw-log-link:hover {{
            background-color: #5a6268;
        }}
        .toast {{
            position: fixed;
            bottom: 20px;
            right: 20px;
            background-color: #28a745;
            color: white;
            padding: 15px 20px;
            border-radius: 5px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            z-index: 1000;
            opacity: 0;
            transition: opacity 0.3s ease;
            max-width: 400px;
            white-space: pre-line;
            font-family: monospace;
            font-size: 12px;
        }}
        .toast.show {{
            opacity: 1;
        }}
        
        /* Copyright Footer */
        .footer {{
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            text-align: center;
            padding: 20px;
            margin-top: 40px;
            border-radius: 10px;
            font-size: 14px;
        }}
        
        .footer p {{
            margin: 5px 0;
        }}
        
        .footer strong {{
            color: #00ff00;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <img class='logo' src='data:image/png;base64,{logo_data}' alt='AVICE Logo' onclick="showLogoModal()" title="Click to enlarge">
            <div class="header-text">
                <h1>Runtime Analysis Report</h1>
                <p>Design: {self.design_info.top_hier} | Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                <p>Workarea: {self.workarea_abs}</p>
                <div>{badges_html}</div>
            </div>
        </div>
        
        <!-- Logo Modal -->
        <div id="logoModal" class="logo-modal" onclick="hideLogoModal()">
            <span class="logo-modal-close">&times;</span>
            <img class="logo-modal-content" src='data:image/png;base64,{logo_data}' alt='AVICE Logo'>
        </div>
        
        <!-- Generate Unified Flow Timeline Table -->
        """
        
        # Insert unified flow timeline table
        html += self._generate_unified_html_table(runtime_data, runtime_timestamps, ipo_signoff_data or {}, pnr_stage_data)
        
        html += """
        <div class="section">
            <h2>Detailed PnR Stage Analysis</h2>
            <button class="tablog-btn" onclick="openLogWithServer('{prc_status_file}', event)" title="Open in tablog: {prc_status_file}">ðŸ–¥ï¸ View PRC Status in tablog</button>""".format(prc_status_file=prc_status_file)
        
        # Add detailed PnR stage data for each IPO
        for ipo in sorted(pnr_stage_data.keys()):
            stages = pnr_stage_data[ipo]
            
            # Calculate total PnR runtime (core stages only)
            core_stages = ['BEGIN', 'setup', 'edi_plan', 'place', 'cts', 'route', 'postroute']
            total_seconds = 0
            for stage_info in stages:
                if stage_info['step'] in core_stages:
                    try:
                        total_seconds += int(stage_info['duration_seconds'])
                    except (ValueError, KeyError):
                        pass
            
            total_hours = total_seconds / 3600
            total_days = total_hours / 24
            
            if total_hours >= 24:
                total_runtime_str = f"{total_hours:.2f} hours ({total_days:.2f} days)"
            else:
                total_runtime_str = f"{total_hours:.2f} hours"
            
            # Find the stage with highest runtime for highlighting
            max_duration_seconds = 0
            max_duration_stage = None
            for stage_info in stages:
                try:
                    duration_seconds = int(stage_info['duration_seconds'])
                    if duration_seconds > max_duration_seconds:
                        max_duration_seconds = duration_seconds
                        max_duration_stage = stage_info['step']
                except (ValueError, KeyError):
                    pass
            
            html += f"""
            <div class="ipo-section">
                <div class="ipo-title">IPO {ipo}</div>
                <div class="total-runtime">Total PnR Runtime: {total_runtime_str}</div>
                <table class="stage-table">
                    <thead>
                        <tr>
                            <th>Stage</th>
                            <th>Status</th>
                            <th>Duration</th>
                            <th>Log File</th>
                        </tr>
                    </thead>
                    <tbody>"""
            
            for stage_info in stages:
                status_class = f"status-{stage_info['status'].lower()}"
                # Add highlighting class for the stage with highest runtime
                highlight_class = "highlight-max-runtime" if stage_info['step'] == max_duration_stage else ""
                html += f"""
                        <tr class="{highlight_class}">
                            <td>{stage_info['step']}</td>
                            <td class="{status_class}">{stage_info['status']}</td>
                            <td>{stage_info['duration']}</td>
                            <td>
                                <button class="tablog-btn" onclick="openLogWithServer('{stage_info['logfile']}', event)" title="Open in tablog: {stage_info['logfile']}">ðŸ–¥ï¸ tablog</button>
                            </td>
                        </tr>"""
            
            html += """
                    </tbody>
                </table>
            </div>"""
        
        html += f"""
        </div>
        
        <!-- Logo Modal -->
        <div id="logoModal" class="logo-modal" onclick="hideLogoModal()">
            <div class="logo-modal-content">
                <img src="data:image/png;base64,{logo_data}" alt="AVICE Logo Full Size">
            </div>
        </div>
    </div>
    
    <script>
        function showLogoModal() {{
            document.getElementById('logoModal').style.display = 'block';
        }}
        
        function hideLogoModal() {{
            document.getElementById('logoModal').style.display = 'none';
        }}
        
        // Close modal when clicking outside the image
        document.getElementById('logoModal').addEventListener('click', function(e) {{
            if (e.target === this) {{
                hideLogoModal();
            }}
        }});
        
        // Close modal with Escape key
        document.addEventListener('keydown', function(e) {{
            if (e.key === 'Escape') {{
                hideLogoModal();
            }}
        }});
        
        // Logo modal functions
        function showLogoModal() {{
            document.getElementById('logoModal').classList.add('active');
        }}
        
        function hideLogoModal() {{
            document.getElementById('logoModal').classList.remove('active');
        }}
        
        // Allow ESC key to close logo modal
        document.addEventListener('keydown', function(event) {{
            if (event.key === 'Escape') {{
                hideLogoModal();
            }}
        }});
        
        // Back to top button functionality - wait for DOM to load
        document.addEventListener('DOMContentLoaded', function() {{
            var backToTopBtn = document.getElementById('backToTopBtn');
            if (backToTopBtn) {{
                window.addEventListener('scroll', function() {{
                    if (window.pageYOffset > 300) {{
                        backToTopBtn.style.display = 'block';
                    }} else {{
                        backToTopBtn.style.display = 'none';
                    }}
                }});
                
                backToTopBtn.addEventListener('click', function() {{
                    window.scrollTo(0, 0);
                }});
            }}
        }});
    </script>
    
    <button id="backToTopBtn" style="display: none; position: fixed; bottom: 30px; right: 30px; 
            z-index: 99; border: none; outline: none; background-color: #667eea; color: white; 
            cursor: pointer; padding: 15px 20px; border-radius: 50px; font-size: 16px; 
            font-weight: bold; box-shadow: 0 4px 6px rgba(0,0,0,0.3); transition: all 0.3s ease;"
            onmouseover="this.style.backgroundColor='#5568d3'; this.style.transform='scale(1.1)';"
            onmouseout="this.style.backgroundColor='#667eea'; this.style.transform='scale(1)';">
        â†‘ Top
    </button>
    
    <!-- Tablog Server Integration JavaScript -->
    <script>
        // Show toast notification
        function showToast(message, type) {{
            const toast = document.createElement('div');
            toast.className = 'toast toast-' + type;
            toast.textContent = message;
            toast.style.cssText = `
                position: fixed;
                bottom: 30px;
                right: 30px;
                padding: 15px 25px;
                border-radius: 8px;
                color: white;
                font-weight: 600;
                box-shadow: 0 4px 12px rgba(0,0,0,0.3);
                z-index: 10000;
                opacity: 0;
                transform: translateY(20px);
                transition: all 0.3s ease;
            `;
            
            if (type === 'success') {{
                toast.style.background = 'linear-gradient(135deg, #27ae60 0%, #229954 100%)';
            }} else if (type === 'error') {{
                toast.style.background = 'linear-gradient(135deg, #e74c3c 0%, #c0392b 100%)';
            }} else if (type === 'warning') {{
                toast.style.background = 'linear-gradient(135deg, #f39c12 0%, #d68910 100%)';
            }}
            
            document.body.appendChild(toast);
            
            setTimeout(function() {{
                toast.style.opacity = '1';
                toast.style.transform = 'translateY(0)';
            }}, 100);
            
            setTimeout(function() {{
                toast.style.opacity = '0';
                toast.style.transform = 'translateY(20px)';
                setTimeout(function() {{ toast.remove(); }}, 300);
            }}, 3000);
        }}
        
        // Copy text to clipboard
        function copyToClipboard(text) {{
            navigator.clipboard.writeText(text).then(function() {{
                showToast('âœ“ Command copied to clipboard! Paste in terminal to execute.', 'warning');
            }}).catch(function(err) {{
                showToast('Failed to copy: ' + err, 'error');
            }});
        }}
        
        // Open log with server (with fallback to clipboard)
        function openLogWithServer(logfile, event) {{
            if (event) {{
                event.preventDefault();
            }}
            
            const serverUrl = 'http://localhost:8888/open_log?file=' + encodeURIComponent(logfile);
            
            // Try to open via server
            fetch(serverUrl, {{ method: 'GET', mode: 'cors' }})
                .then(function(response) {{
                    if (response.ok) {{
                        showToast('âœ“ Opening in tablog...', 'success');
                    }} else {{
                        throw new Error('Server returned error');
                    }}
                }})
                .catch(function(error) {{
                    // Server not running - fallback to clipboard
                    const command = '/home/scratch.avice_vlsi/tablog/tablog ' + logfile;
                    copyToClipboard(command);
                }});
            
            return false;
        }}
    </script>
    
    <!-- Copyright Footer -->
    <div class="footer">
        <p><strong>AVICE Runtime Analysis Report</strong></p>
        <p>Copyright (c) 2025 Alon Vice (avice)</p>
        <p>Contact: avice@nvidia.com</p>
    </div>
</body>
</html>"""
        
        return html

    def run_common_analysis(self) -> None:
        """Run COMMON TCL files analysis"""
        self.print_header(FlowStage.COMMON)
        
        common_dir = os.path.join(self.workarea, "pnr_flow/nv_flow/COMMON")
        if os.path.exists(common_dir):
            tcl_files = self.file_utils.find_files("pnr_flow/nv_flow/COMMON/*.tcl", self.workarea)
            if tcl_files:
                for tcl_file in tcl_files:
                    self.print_file_info(tcl_file, "Common TCL")
            else:
                print("  No .tcl files found")
        else:
            print("  No COMMON directory found")


# ============================================================================
# Workarea Comparison Feature (New Architecture)
# ============================================================================

def generate_comparison_excel(ref_data, test_data, ref_wa, test_wa, filename, sections):
    """
    Generate Excel comparison report.
    
    Args:
        ref_data: Parsed metrics from reference workarea
        test_data: Parsed metrics from test workarea
        ref_wa: Reference workarea path
        test_wa: Test workarea path
        filename: Output Excel filename
        sections: List of sections that were compared
    
    Returns:
        str: Path to generated Excel file
    """
    if not OPENPYXL_AVAILABLE:
        print(f"{Color.RED}[ERROR] openpyxl not available. Cannot generate Excel report.{Color.RESET}")
        print("Install with: pip install openpyxl")
        return None
    
    wb = openpyxl.Workbook()
    wb.remove(wb.active)  # Remove default sheet
    
    # Generate unified Setup & Summary tab (Tab 1)
    _generate_summary_dashboard(wb, ref_data, test_data, ref_wa, test_wa, sections)
    
    # Generate section-specific tabs
    if 'runtime' in sections and 'runtime' in ref_data:
        _generate_runtime_comparison_tab(wb, ref_data.get('runtime', {}), test_data.get('runtime', {}))
    
    if 'synthesis' in sections and 'synthesis' in ref_data:
        _generate_dc_comparison_tab(wb, ref_data.get('synthesis', {}), test_data.get('synthesis', {}))
    
    if 'pnr' in sections and 'pnr' in ref_data:
        _generate_pnr_comparison_tab(wb, ref_data.get('pnr', {}), test_data.get('pnr', {}))
    
    if 'clock' in sections and 'clock' in ref_data:
        _generate_clock_comparison_tab(wb, ref_data.get('clock', {}), test_data.get('clock', {}))
    
    # Save workbook
    wb.save(filename)
    print(f"{Color.GREEN}[OK]{Color.RESET} Excel report saved: {filename}")
    
    return filename


def _generate_summary_dashboard(wb, ref_data, test_data, ref_wa, test_wa, sections):
    """Generate unified Setup & Summary tab (Tab 1) - combines setup metrics with summary info"""
    ws = wb.create_sheet("Setup & Summary", 0)
    
    # Define styles
    header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
    header_font = Font(bold=True, color="FFFFFF", size=12)
    title_font = Font(bold=True, size=14)
    section_fill = PatternFill(start_color="D9E9F7", end_color="D9E9F7", fill_type="solid")  # Light blue
    section_font = Font(bold=True, size=11)
    metric_font = Font(bold=True)
    better_fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")  # Green
    worse_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")   # Red
    neutral_fill = PatternFill(start_color="FFEB9C", end_color="FFEB9C", fill_type="solid") # Yellow
    
    # Title
    ws['A1'] = "SETUP & SUMMARY - WORKAREA COMPARISON"
    ws['A1'].font = title_font
    ws.merge_cells('A1:F1')
    
    # Workarea paths section
    row = 3
    ws[f'A{row}'] = "Reference Workarea:"
    ws[f'A{row}'].font = Font(bold=True)
    ws[f'B{row}'] = ref_wa
    ws.merge_cells(f'B{row}:F{row}')
    
    row += 1
    ws[f'A{row}'] = "Test Workarea:"
    ws[f'A{row}'].font = Font(bold=True)
    ws[f'B{row}'] = test_wa
    ws.merge_cells(f'B{row}:F{row}')
    
    row += 1
    ws[f'A{row}'] = "Sections Compared:"
    ws[f'A{row}'].font = Font(bold=True)
    ws[f'B{row}'] = ", ".join(sections)
    ws.merge_cells(f'B{row}:F{row}')
    
    # Comparison headers
    row += 2
    ws[f'A{row}'] = "Metric"
    ws[f'B{row}'] = "Reference"
    ws[f'C{row}'] = "Test"
    ws[f'D{row}'] = "Delta"
    ws[f'E{row}'] = "Change (%)"
    ws[f'F{row}'] = "Status"
    
    for col in ['A', 'B', 'C', 'D', 'E', 'F']:
        cell = ws[f'{col}{row}']
        cell.fill = header_fill
        cell.font = header_font
        cell.alignment = Alignment(horizontal='center')
    
    row += 1
    
    # Helper function to add metric row with optional unit formatting
    def add_metric_row(metric_name, ref_value, test_value, is_numeric=False, unit=""):
        nonlocal row
        ws[f'A{row}'] = metric_name
        ws[f'A{row}'].font = metric_font
        
        # Format values with unit if provided
        if is_numeric and unit:
            ws[f'B{row}'] = f"{ref_value}{unit}" if ref_value is not None else "N/A"
            ws[f'C{row}'] = f"{test_value}{unit}" if test_value is not None else "N/A"
        else:
            ws[f'B{row}'] = ref_value if ref_value is not None else "N/A"
            ws[f'C{row}'] = test_value if test_value is not None else "N/A"
        
        if is_numeric and ref_value is not None and test_value is not None:
            delta = test_value - ref_value
            # Format delta with unit
            if unit:
                ws[f'D{row}'] = f"{delta:+.2f}{unit}" if isinstance(delta, float) else f"{delta:+d}{unit}"
            else:
                ws[f'D{row}'] = delta
            
            if ref_value != 0:
                pct_change = (delta / ref_value) * 100
                ws[f'E{row}'] = pct_change
                ws[f'E{row}'].number_format = '0.00"%"'
            else:
                ws[f'E{row}'] = "N/A"
            
            # Status and coloring
            if abs(delta) < 0.01:
                ws[f'F{row}'] = "UNCHANGED"
                ws[f'F{row}'].fill = neutral_fill
            elif delta > 0:
                ws[f'F{row}'] = "INCREASED"
                ws[f'F{row}'].fill = worse_fill  # Disk usage increase is usually worse
            else:
                ws[f'F{row}'] = "DECREASED"
                ws[f'F{row}'].fill = better_fill
        else:
            ws[f'D{row}'] = "N/A"
            ws[f'E{row}'] = "N/A"
            if ref_value == test_value:
                ws[f'F{row}'] = "MATCH"
                ws[f'F{row}'].fill = better_fill
            else:
                ws[f'F{row}'] = "DIFFERENT"
                ws[f'F{row}'].fill = neutral_fill
        
        row += 1
    
    # Extract and display all setup metrics
    if 'setup' in ref_data and 'setup' in test_data:
        setup_ref = ref_data['setup']
        setup_test = test_data['setup']
        
        # Basic identification metrics
        add_metric_row("Unit", setup_ref.get('unit'), setup_test.get('unit'), is_numeric=False)
        add_metric_row("TAG", setup_ref.get('tag'), setup_test.get('tag'), is_numeric=False)
        add_metric_row("IPO", setup_ref.get('ipo'), setup_test.get('ipo'), is_numeric=False)
        add_metric_row("Owner", setup_ref.get('owner'), setup_test.get('owner'), is_numeric=False)
        add_metric_row("Disk Usage", setup_ref.get('disk_usage'), setup_test.get('disk_usage'), is_numeric=True, unit="%")
        
        # Available IPOs
        ref_ipos = setup_ref.get('available_ipos', [])
        test_ipos = setup_test.get('available_ipos', [])
        add_metric_row("Available IPOs", ", ".join(ref_ipos) if ref_ipos else "N/A", 
                       ", ".join(test_ipos) if test_ipos else "N/A", is_numeric=False)
        
        # Environment and tool information
        add_metric_row("Environment Information", setup_ref.get('environment'), setup_test.get('environment'), is_numeric=False)
        add_metric_row("NBU Signoff Mode", setup_ref.get('nbu_signoff_mode'), setup_test.get('nbu_signoff_mode'), is_numeric=False)
        add_metric_row("BE_OVERRIDE_TOOLVERS", setup_ref.get('be_override_toolvers'), setup_test.get('be_override_toolvers'), is_numeric=False)
        add_metric_row("BEFLOW_CONFIG_REV", setup_ref.get('beflow_config_rev'), setup_test.get('beflow_config_rev'), is_numeric=False)
    
    # Add Alon Vice logo to the right side
    logo_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "assets/images/avice_logo.png")
    if os.path.exists(logo_path):
        try:
            from openpyxl.drawing.image import Image as OpenpyxlImage
            img = OpenpyxlImage(logo_path)
            img.width = 120
            img.height = 120
            ws.add_image(img, 'H3')
        except Exception as e:
            pass
    
    # Auto-size columns
    ws.column_dimensions['A'].width = 30
    ws.column_dimensions['B'].width = 50
    ws.column_dimensions['C'].width = 50
    ws.column_dimensions['D'].width = 15
    ws.column_dimensions['E'].width = 15
    ws.column_dimensions['F'].width = 15


def _generate_setup_comparison_tab(wb, ref_setup, test_setup):
    """Generate Setup Comparison tab (Tab 2)"""
    ws = wb.create_sheet("Setup Comparison", 1)
    
    # Define styles
    header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
    header_font = Font(bold=True, color="FFFFFF", size=11)
    metric_font = Font(bold=True)
    better_fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")  # Green
    worse_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")   # Red
    neutral_fill = PatternFill(start_color="FFEB9C", end_color="FFEB9C", fill_type="solid") # Yellow
    
    # Headers
    ws['A1'] = "Metric"
    ws['B1'] = "Reference"
    ws['C1'] = "Test"
    ws['D1'] = "Delta"
    ws['E1'] = "Change (%)"
    ws['F1'] = "Status"
    
    for col in ['A', 'B', 'C', 'D', 'E', 'F']:
        cell = ws[f'{col}1']
        cell.fill = header_fill
        cell.font = header_font
        cell.alignment = Alignment(horizontal='center')
    
    # Populate data
    row = 2
    
    # Helper function to add metric row with optional unit formatting
    def add_metric_row(metric_name, ref_value, test_value, is_numeric=False, unit=""):
        nonlocal row
        ws[f'A{row}'] = metric_name
        ws[f'A{row}'].font = metric_font
        
        # Helper to format value with unit for display
        def format_with_unit(val, unit_str):
            if val is None:
                return "N/A"
            if unit_str:
                return f"{val}{unit_str}"
            return val
        
        # Store values with unit suffix if specified
        if is_numeric and unit:
            # For numeric values with units, store as formatted strings for clarity
            ws[f'B{row}'] = format_with_unit(ref_value, unit) if ref_value is not None else "N/A"
            ws[f'C{row}'] = format_with_unit(test_value, unit) if test_value is not None else "N/A"
        else:
            ws[f'B{row}'] = ref_value if ref_value is not None else "N/A"
            ws[f'C{row}'] = test_value if test_value is not None else "N/A"
        
        if is_numeric and ref_value is not None and test_value is not None:
            delta = test_value - ref_value
            # Format delta with unit
            if unit:
                ws[f'D{row}'] = f"{delta:+.2f}{unit}" if delta != 0 else f"0{unit}"
            else:
                ws[f'D{row}'] = delta
            
            if ref_value != 0:
                pct_change = (delta / ref_value) * 100
                ws[f'E{row}'] = pct_change
                ws[f'E{row}'].number_format = '0.00"%"'
            else:
                ws[f'E{row}'] = "N/A"
            
            # Status and coloring
            if abs(delta) < 0.01:  # Essentially no change
                ws[f'F{row}'] = "UNCHANGED"
                ws[f'F{row}'].fill = neutral_fill
            elif delta > 0:
                ws[f'F{row}'] = "INCREASED"
                ws[f'F{row}'].fill = worse_fill  # Larger is usually worse for area
            else:
                ws[f'F{row}'] = "DECREASED"
                ws[f'F{row}'].fill = better_fill
        else:
            ws[f'D{row}'] = "N/A"
            ws[f'E{row}'] = "N/A"
            if ref_value == test_value:
                ws[f'F{row}'] = "SAME"
                ws[f'F{row}'].fill = neutral_fill
            else:
                ws[f'F{row}'] = "CHANGED"
                ws[f'F{row}'].fill = neutral_fill
        
        row += 1
    
    # Add metrics (Setup section - no Die dimensions, those are in DC and PnR sections)
    add_metric_row("Unit", ref_setup.get('unit'), test_setup.get('unit'), is_numeric=False)
    add_metric_row("TAG", ref_setup.get('tag'), test_setup.get('tag'), is_numeric=False)
    add_metric_row("IPO", ref_setup.get('ipo'), test_setup.get('ipo'), is_numeric=False)
    add_metric_row("Owner", ref_setup.get('owner'), test_setup.get('owner'), is_numeric=False)
    add_metric_row("Disk Usage", ref_setup.get('disk_usage'), test_setup.get('disk_usage'), is_numeric=True, unit="%")
    
    # Available IPOs
    ref_ipos = ref_setup.get('available_ipos', [])
    test_ipos = test_setup.get('available_ipos', [])
    add_metric_row("Available IPOs", ", ".join(ref_ipos) if ref_ipos else "N/A", 
                   ", ".join(test_ipos) if test_ipos else "N/A", is_numeric=False)
    
    # Environment Information
    add_metric_row("Environment Information", ref_setup.get('environment'), test_setup.get('environment'), is_numeric=False)
    
    # NBU Signoff Mode
    add_metric_row("NBU Signoff Mode", ref_setup.get('nbu_signoff_mode'), test_setup.get('nbu_signoff_mode'), is_numeric=False)
    
    # BE_OVERRIDE_TOOLVERS
    add_metric_row("BE_OVERRIDE_TOOLVERS", ref_setup.get('be_override_toolvers'), test_setup.get('be_override_toolvers'), is_numeric=False)
    
    # Auto-size columns (wider for better readability)
    ws.column_dimensions['A'].width = 30
    ws.column_dimensions['B'].width = 50
    ws.column_dimensions['C'].width = 50
    ws.column_dimensions['D'].width = 15
    ws.column_dimensions['E'].width = 15
    ws.column_dimensions['F'].width = 15


def _generate_runtime_comparison_tab(wb, ref_runtime, test_runtime):
    """Generate Runtime Comparison tab (Tab 2)"""
    ws = wb.create_sheet("Runtime Comparison", 1)
    
    # Define styles
    header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
    header_font = Font(bold=True, color="FFFFFF", size=11)
    metric_font = Font(bold=True)
    better_fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")  # Green (faster)
    worse_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")   # Red (slower)
    neutral_fill = PatternFill(start_color="FFEB9C", end_color="FFEB9C", fill_type="solid") # Yellow
    
    # Add threshold note at the top
    ws['A1'] = "NOTE: Runtime deltas < 30 minutes (0.5 hours) are considered UNCHANGED"
    ws['A1'].font = Font(italic=True, size=10, color="666666")
    ws.merge_cells('A1:F1')
    
    # Headers (row 3 to leave space for note)
    ws['A3'] = "Stage"
    ws['B3'] = "Reference"
    ws['C3'] = "Test"
    ws['D3'] = "Delta"
    ws['E3'] = "Change (%)"
    ws['F3'] = "Status"
    
    for col in ['A', 'B', 'C', 'D', 'E', 'F']:
        cell = ws[f'{col}3']
        cell.fill = header_fill
        cell.font = header_font
        cell.alignment = Alignment(horizontal='center')
    
    # Populate data
    row = 4
    
    # Helper function to add runtime row
    def add_runtime_row(stage_name, ref_key, test_key=None):
        nonlocal row
        if test_key is None:
            test_key = ref_key
        
        ref_value = ref_runtime.get(ref_key)
        test_value = test_runtime.get(test_key)
        
        ws[f'A{row}'] = stage_name
        ws[f'A{row}'].font = metric_font
        
        # Set values as numbers with "Hr" suffix
        if ref_value is not None:
            ws[f'B{row}'] = ref_value
            ws[f'B{row}'].number_format = '0.00"Hr"'
        else:
            ws[f'B{row}'] = "N/A"
        
        if test_value is not None:
            ws[f'C{row}'] = test_value
            ws[f'C{row}'].number_format = '0.00"Hr"'
        else:
            ws[f'C{row}'] = "N/A"
        
        if ref_value is not None and test_value is not None:
            delta = test_value - ref_value
            ws[f'D{row}'] = delta
            ws[f'D{row}'].number_format = '0.00"Hr"'
            
            if ref_value != 0:
                pct_change = (delta / ref_value) * 100
                ws[f'E{row}'] = pct_change
                ws[f'E{row}'].number_format = '0.00"%"'
            else:
                ws[f'E{row}'] = "N/A"
            
            # Status and coloring (for runtime, decrease is GOOD)
            if abs(delta) < 0.5:  # Less than 30 minutes difference is considered similar
                ws[f'F{row}'] = "UNCHANGED"
                ws[f'F{row}'].fill = neutral_fill
            elif delta < 0:  # Faster (negative delta is good)
                ws[f'F{row}'] = "FASTER"
                ws[f'F{row}'].fill = better_fill
            else:  # Slower (positive delta is bad)
                ws[f'F{row}'] = "SLOWER"
                ws[f'F{row}'].fill = worse_fill
        else:
            ws[f'D{row}'] = "N/A"
            ws[f'E{row}'] = "N/A"
            ws[f'F{row}'] = "N/A"
            ws[f'F{row}'].fill = neutral_fill
        
        row += 1
    
    # Construction stages
    add_runtime_row("DC (Synthesis)", 'dc_runtime')
    
    # PnR per-stage breakdown (if available) + total
    # Extract pnr_stage_data from runtime dictionaries
    ref_pnr_stages = ref_runtime.get('pnr_stage_data', {})
    test_pnr_stages = test_runtime.get('pnr_stage_data', {})
    
    # Find IPOs in both workareas
    pnr_keys_ref = [k for k in ref_runtime.keys() if k.startswith('pnr_runtime_ipo')]
    pnr_keys_test = [k for k in test_runtime.keys() if k.startswith('pnr_runtime_ipo')]
    
    # Extract IPO names
    ref_ipos = set(k.replace('pnr_runtime_', '') for k in pnr_keys_ref)
    test_ipos = set(k.replace('pnr_runtime_', '') for k in pnr_keys_test)
    
    # Find common IPOs (intersection) - these are the ones we'll compare
    common_ipos_set = ref_ipos & test_ipos
    common_ipos = sorted(common_ipos_set)
    
    # Find IPOs that exist in only one workarea
    ref_only_ipos = sorted(ref_ipos - test_ipos)
    test_only_ipos = sorted(test_ipos - common_ipos_set)
    
    # Convert back to keys format
    common_pnr_keys = [f'pnr_runtime_{ipo}' for ipo in common_ipos]
    
    # Process ONLY common IPOs
    for pnr_key in common_pnr_keys:
        ipo_name = pnr_key.replace('pnr_runtime_', '')
        
        # Show per-stage breakdown if available
        if ipo_name in ref_pnr_stages or ipo_name in test_pnr_stages:
            # Get stage lists
            ref_stages = ref_pnr_stages.get(ipo_name, {}).get('stages', [])
            test_stages = test_pnr_stages.get(ipo_name, {}).get('stages', [])
            
            # Create a unified list of all stage names
            ref_stage_names = [s['stage'] for s in ref_stages]
            test_stage_names = [s['stage'] for s in test_stages]
            all_stage_names = []
            # Preserve order: Floorplan, Placement, CTS, Route, Post-Route (Begin and Setup excluded)
            for stage_name in ['Floorplan', 'Placement', 'CTS', 'Route', 'Post-Route']:
                if stage_name in ref_stage_names or stage_name in test_stage_names:
                    all_stage_names.append(stage_name)
            
            # Display each stage
            for stage_name in all_stage_names:
                ref_stage = next((s for s in ref_stages if s['stage'] == stage_name), None)
                test_stage = next((s for s in test_stages if s['stage'] == stage_name), None)
                
                ref_value = ref_stage['runtime_hours'] if ref_stage else None
                test_value = test_stage['runtime_hours'] if test_stage else None
                
                # Add stage row (indented)
                ws[f'A{row}'] = f"  {stage_name}"  # Indent with spaces
                ws[f'A{row}'].font = Font(size=10)  # Smaller font for sub-items
                
                # Set values as numbers with "Hr" suffix
                if ref_value is not None:
                    ws[f'B{row}'] = ref_value
                    ws[f'B{row}'].number_format = '0.00"Hr"'
                else:
                    ws[f'B{row}'] = "N/A"
                
                if test_value is not None:
                    ws[f'C{row}'] = test_value
                    ws[f'C{row}'].number_format = '0.00"Hr"'
                else:
                    ws[f'C{row}'] = "N/A"
                
                if ref_value is not None and test_value is not None:
                    delta = test_value - ref_value
                    ws[f'D{row}'] = delta
                    ws[f'D{row}'].number_format = '0.00"Hr"'
                    
                    if ref_value != 0:
                        pct_change = (delta / ref_value) * 100
                        ws[f'E{row}'] = pct_change
                        ws[f'E{row}'].number_format = '0.00"%"'
                    else:
                        ws[f'E{row}'] = "N/A"
                    
                    # Color coding
                    if abs(delta) < 0.5:  # Less than 30 minutes difference is considered similar
                        ws[f'F{row}'] = "UNCHANGED"
                        ws[f'F{row}'].fill = neutral_fill
                    elif delta < 0:
                        ws[f'F{row}'] = "FASTER"
                        ws[f'F{row}'].fill = better_fill
                    else:
                        ws[f'F{row}'] = "SLOWER"
                        ws[f'F{row}'].fill = worse_fill
                else:
                    ws[f'D{row}'] = "N/A"
                    ws[f'E{row}'] = "N/A"
                    ws[f'F{row}'] = "N/A"
                
                row += 1
        
        # Add TOTAL PnR row (bold/highlighted)
        add_runtime_row(f"TOTAL PnR ({ipo_name})", pnr_key)
        # Make the total row bold
        ws[f'A{row-1}'].font = Font(bold=True, size=11)
        # Add slight background color to distinguish total
        light_blue = PatternFill(start_color="D9E9F7", end_color="D9E9F7", fill_type="solid")
        for col in ['A', 'B', 'C', 'D', 'E']:
            ws[f'{col}{row-1}'].fill = light_blue
    
    # Add info row about skipped IPOs (if any)
    if ref_only_ipos or test_only_ipos:
        ws[f'A{row}'] = "[INFO] IPO Comparison"
        ws[f'A{row}'].font = Font(bold=True, italic=True, color="0066CC")
        row += 1
        
        if common_ipos:
            ws[f'A{row}'] = f"  Compared: {', '.join(common_ipos)}"
            ws[f'A{row}'].font = Font(italic=True, size=10, color="006600")
            row += 1
        
        if ref_only_ipos:
            ws[f'A{row}'] = f"  Skipped (ref-only): {', '.join(ref_only_ipos)}"
            ws[f'A{row}'].font = Font(italic=True, size=10, color="666666")
            row += 1
        
        if test_only_ipos:
            ws[f'A{row}'] = f"  Skipped (test-only): {', '.join(test_only_ipos)}"
            ws[f'A{row}'].font = Font(italic=True, size=10, color="666666")
            row += 1
    
    # Signoff stages - Handle both global and IPO-specific flows
    # Formal flows
    formal_keys_ref = [k for k in ref_runtime.keys() if k.startswith('formal_runtime_')]
    formal_keys_test = [k for k in test_runtime.keys() if k.startswith('formal_runtime_')]
    all_formal_keys = sorted(set(formal_keys_ref + formal_keys_test))
    
    for formal_key in all_formal_keys:
        formal_flow = formal_key.replace('formal_runtime_', '')
        add_runtime_row(f"Formal ({formal_flow})", formal_key)
    
    # Try global keys first, then aggregate from IPO-specific keys
    def try_add_signoff_row(stage_name, global_key, ipo_key_prefix):
        """Add signoff row, trying global key first, then aggregating IPO-specific"""
        # Check if global key exists in either workarea
        if global_key in ref_runtime or global_key in test_runtime:
            add_runtime_row(stage_name, global_key)
        else:
            # Try to find and aggregate IPO-specific keys
            ref_ipo_keys = [k for k in ref_runtime.keys() if k.startswith(ipo_key_prefix)]
            test_ipo_keys = [k for k in test_runtime.keys() if k.startswith(ipo_key_prefix)]
            
            if ref_ipo_keys or test_ipo_keys:
                # Aggregate from IPOs (sum of all IPO-specific runtimes)
                ref_total = sum(ref_runtime.get(k, 0) for k in ref_ipo_keys) if ref_ipo_keys else None
                test_total = sum(test_runtime.get(k, 0) for k in test_ipo_keys) if test_ipo_keys else None
                
                # Create temporary aggregated keys
                if ref_total is not None and ref_total > 0:
                    ref_runtime[global_key] = ref_total
                if test_total is not None and test_total > 0:
                    test_runtime[global_key] = test_total
                
                add_runtime_row(stage_name, global_key)
    
    try_add_signoff_row("GL Check", 'gl_check_runtime', 'gl_check_runtime_ipo')
    try_add_signoff_row("Star (Parasitic Extract)", 'star_runtime', 'star_runtime_ipo')
    try_add_signoff_row("PV (Physical Verification)", 'pv_runtime', 'pv_runtime_ipo')
    try_add_signoff_row("Auto PT (Timing)", 'auto_pt_runtime', 'auto_pt_runtime_ipo')
    
    # Add separator row before Grand Total
    row += 1
    
    # Grand Total (highlighted)
    add_runtime_row("GRAND TOTAL", 'grand_total_runtime')
    # Make the GRAND TOTAL row bold with emphasis
    ws[f'A{row-1}'].font = Font(bold=True, size=12)
    highlight_fill = PatternFill(start_color="C5D9F1", end_color="C5D9F1", fill_type="solid")
    for col in ['A', 'B', 'C', 'D', 'E', 'F']:
        ws[f'{col}{row-1}'].fill = highlight_fill
    
    # Efficiency percentage (not a runtime, handle differently)
    ref_efficiency = ref_runtime.get('efficiency')
    test_efficiency = test_runtime.get('efficiency')
    
    ws[f'A{row}'] = "Efficiency"
    ws[f'A{row}'].font = metric_font
    
    ws[f'B{row}'] = f"{ref_efficiency}%" if ref_efficiency is not None else "N/A"
    ws[f'C{row}'] = f"{test_efficiency}%" if test_efficiency is not None else "N/A"
    
    if ref_efficiency is not None and test_efficiency is not None:
        delta = test_efficiency - ref_efficiency
        ws[f'D{row}'] = f"{delta:+d}%"
        
        # For efficiency, higher is better
        if abs(delta) < 1:  # Less than 1% difference is unchanged
            ws[f'F{row}'] = "UNCHANGED"
            ws[f'F{row}'].fill = neutral_fill
        elif delta > 0:  # Higher efficiency is better
            ws[f'F{row}'] = "IMPROVED"
            ws[f'F{row}'].fill = better_fill
        else:  # Lower efficiency is worse
            ws[f'F{row}'] = "DEGRADED"
            ws[f'F{row}'].fill = worse_fill
    else:
        ws[f'D{row}'] = "N/A"
        ws[f'E{row}'] = "N/A"
        ws[f'F{row}'] = "N/A"
    row += 1
    
    # Auto-size columns
    ws.column_dimensions['A'].width = 30
    ws.column_dimensions['B'].width = 20
    ws.column_dimensions['C'].width = 20
    ws.column_dimensions['D'].width = 18
    ws.column_dimensions['E'].width = 15
    ws.column_dimensions['F'].width = 15


def _generate_dc_comparison_tab(wb, ref_dc, test_dc):
    """Generate DC (Synthesis) Comparison tab (Tab 3)"""
    ws = wb.create_sheet("DC Comparison", 2)
    
    # Define styles
    header_fill = PatternFill(start_color="8B4789", end_color="8B4789", fill_type="solid")  # Purple (matching DC theme)
    header_font = Font(bold=True, color="FFFFFF", size=11)
    section_fill = PatternFill(start_color="E9D7E3", end_color="E9D7E3", fill_type="solid")  # Light purple
    section_font = Font(bold=True, size=11)
    metric_font = Font(size=10)
    better_fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")  # Green (improved)
    worse_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")   # Red (degraded)
    neutral_fill = PatternFill(start_color="FFEB9C", end_color="FFEB9C", fill_type="solid") # Yellow (unchanged)
    
    # Headers
    ws['A1'] = "Metric"
    ws['B1'] = "Reference"
    ws['C1'] = "Test"
    ws['D1'] = "Delta"
    ws['E1'] = "Change (%)"
    ws['F1'] = "Status"
    
    for col in ['A', 'B', 'C', 'D', 'E', 'F']:
        cell = ws[f'{col}1']
        cell.fill = header_fill
        cell.font = header_font
        cell.alignment = Alignment(horizontal='center')
    
    # Populate data
    row = 2
    
    # Helper function to add metric row
    def add_metric_row(metric_name, ref_key, test_key=None, num_format=None, reverse_logic=False, is_section_header=False):
        """
        Add a metric comparison row.
        
        Args:
            metric_name: Display name
            ref_key: Key in ref_dc dictionary
            test_key: Key in test_dc dictionary (defaults to ref_key)
            num_format: Excel number format string
            reverse_logic: If True, lower values are BETTER (e.g., errors, warnings, timing violations)
            is_section_header: If True, format as section header
        """
        nonlocal row
        if test_key is None:
            test_key = ref_key
        
        ref_value = ref_dc.get(ref_key)
        test_value = test_dc.get(test_key)
        
        ws[f'A{row}'] = metric_name
        
        # Section headers
        if is_section_header:
            ws[f'A{row}'].font = section_font
            ws[f'A{row}'].fill = section_fill
            for col in ['B', 'C', 'D', 'E', 'F']:
                ws[f'{col}{row}'].fill = section_fill
            row += 1
            return
        
        ws[f'A{row}'].font = metric_font
        
        # Set values
        if ref_value is not None:
            ws[f'B{row}'] = ref_value
            if num_format:
                ws[f'B{row}'].number_format = num_format
        else:
            ws[f'B{row}'] = "N/A"
        
        if test_value is not None:
            ws[f'C{row}'] = test_value
            if num_format:
                ws[f'C{row}'].number_format = num_format
        else:
            ws[f'C{row}'] = "N/A"
        
        # Calculate delta and status
        if ref_value is not None and test_value is not None and isinstance(ref_value, (int, float)) and isinstance(test_value, (int, float)):
            delta = test_value - ref_value
            ws[f'D{row}'] = delta
            if num_format:
                ws[f'D{row}'].number_format = num_format
            
            # Percent change
            if ref_value != 0:
                pct_change = (delta / ref_value) * 100
                ws[f'E{row}'] = pct_change
                ws[f'E{row}'].number_format = '0.00"%"'
            else:
                ws[f'E{row}'] = "N/A"
            
            # Status and coloring
            threshold = 0.01  # 0.01% change threshold
            if abs(pct_change if ref_value != 0 else delta) < threshold:
                ws[f'F{row}'] = "UNCHANGED"
                ws[f'F{row}'].fill = neutral_fill
            else:
                # Determine if change is good or bad
                if reverse_logic:
                    # Lower is better (errors, warnings, violations)
                    if delta < 0:
                        ws[f'F{row}'] = "IMPROVED"
                        ws[f'F{row}'].fill = better_fill
                    else:
                        ws[f'F{row}'] = "DEGRADED"
                        ws[f'F{row}'].fill = worse_fill
                else:
                    # Higher is better (or neutral)
                    if delta > 0:
                        ws[f'F{row}'] = "INCREASED"
                        ws[f'F{row}'].fill = better_fill
                    else:
                        ws[f'F{row}'] = "DECREASED"
                        ws[f'F{row}'].fill = worse_fill
        else:
            ws[f'D{row}'] = "N/A"
            ws[f'E{row}'] = "N/A"
            ws[f'F{row}'] = "N/A"
            ws[f'F{row}'].fill = neutral_fill
        
        row += 1
    
    # DC VERSION & QUALITY
    add_metric_row("DC VERSION & QUALITY", None, None, None, False, is_section_header=True)
    
    # Check if DC versions match
    ref_version = ref_dc.get('dc_version', 'N/A')
    test_version = test_dc.get('dc_version', 'N/A')
    ws[f'A{row}'] = "DC Version"
    ws[f'B{row}'] = ref_version
    ws[f'C{row}'] = test_version
    if ref_version == test_version:
        ws[f'F{row}'] = "MATCH"
        ws[f'F{row}'].fill = better_fill
    else:
        ws[f'F{row}'] = "DIFFERENT"
        ws[f'F{row}'].fill = neutral_fill
    row += 1
    
    add_metric_row("DC Errors", 'dc_errors', num_format='#,##0', reverse_logic=True)
    add_metric_row("DC Warnings", 'dc_warnings', num_format='#,##0', reverse_logic=True)
    
    # FLOORPLAN & AREA
    row += 1  # Blank line
    add_metric_row("FLOORPLAN & AREA", None, None, None, False, is_section_header=True)
    add_metric_row("Die X (um)", 'die_x', num_format='0.00')
    add_metric_row("Die Y (um)", 'die_y', num_format='0.00')
    add_metric_row("Design Area (mmÂ²)", 'design_area', num_format='0.000')
    
    # CELL COUNTS
    row += 1  # Blank line
    add_metric_row("CELL COUNTS", None, None, None, False, is_section_header=True)
    add_metric_row("Leaf Cells", 'leaf_cells', num_format='#,##0')
    add_metric_row("Combinational Cells", 'comb_cells', num_format='#,##0')
    add_metric_row("Sequential Cells", 'seq_cells', num_format='#,##0')
    add_metric_row("Buffer/Inverter Cells", 'buf_inv_cells', num_format='#,##0')
    add_metric_row("Buf/Inv Percent", 'buf_inv_percent', num_format='0.00"%"')
    add_metric_row("Macro Cells", 'macro_cells', num_format='#,##0')
    add_metric_row("Nets", 'nets', num_format='#,##0')
    
    # OPTIMIZATION
    row += 1  # Blank line
    add_metric_row("OPTIMIZATION", None, None, None, False, is_section_header=True)
    add_metric_row("Clock Gates Removed", 'clock_gates_removed', num_format='#,##0')
    add_metric_row("Registers Removed", 'removed_registers', num_format='#,##0')
    
    # CONFIGURATION
    row += 1  # Blank line
    add_metric_row("CONFIGURATION", None, None, None, False, is_section_header=True)
    
    # Configuration items (text comparison)
    for config_key, config_label in [
        ('lib_snap', 'Lib Snap'),
        ('process', 'Process'),
        ('tracks', 'Tracks'),
        ('project', 'Project'),
        ('scenario', 'Scenario'),
        ('vt', 'VT'),
        ('beflow_root', 'BEFLOW_ROOT'),
        ('beflow_config_site', 'BEFLOW_CONFIG_SITE')
    ]:
        ref_val = ref_dc.get(config_key, 'N/A')
        test_val = test_dc.get(config_key, 'N/A')
        ws[f'A{row}'] = config_label
        ws[f'A{row}'].font = metric_font
        ws[f'B{row}'] = ref_val
        ws[f'C{row}'] = test_val
        if ref_val == test_val:
            ws[f'F{row}'] = "MATCH"
            ws[f'F{row}'].fill = better_fill
        else:
            ws[f'F{row}'] = "DIFFERENT"
            ws[f'F{row}'].fill = neutral_fill
        row += 1
    
    # Arrays (show as multi-line if too long)
    ref_arrays = ref_dc.get('arrays', 'N/A')
    test_arrays = test_dc.get('arrays', 'N/A')
    ws[f'A{row}'] = "Arrays"
    ws[f'A{row}'].font = metric_font
    ws[f'B{row}'] = ref_arrays
    ws[f'C{row}'] = test_arrays
    # Top align and wrap text for arrays
    ws[f'B{row}'].alignment = Alignment(wrap_text=True, vertical='top')
    ws[f'C{row}'].alignment = Alignment(wrap_text=True, vertical='top')
    if ref_arrays == test_arrays:
        ws[f'F{row}'] = "MATCH"
        ws[f'F{row}'].fill = better_fill
    else:
        ws[f'F{row}'] = "DIFFERENT"
        ws[f'F{row}'].fill = neutral_fill
    row += 1
    
    # TIMING BY PATH GROUP section will be added as a separate table below
    row += 1
    add_metric_row("TIMING BY PATH GROUP", None, None, None, False, is_section_header=True)
    ws[f'A{row}'] = "(See detailed timing comparison table below)"
    ws[f'A{row}'].font = Font(italic=True, size=10, color="666666")
    row += 1
    
    # Auto-size columns (wider B and C for better readability)
    ws.column_dimensions['A'].width = 35
    ws.column_dimensions['B'].width = 50
    ws.column_dimensions['C'].width = 50
    ws.column_dimensions['D'].width = 18
    ws.column_dimensions['E'].width = 15
    ws.column_dimensions['F'].width = 15
    
    # Now add the detailed timing by path group table
    _add_timing_by_pathgroup_table(ws, ref_dc, test_dc, row)


def _add_timing_by_pathgroup_table(ws, ref_dc, test_dc, start_row):
    """Add detailed timing by path group comparison table with vertical layout (one metric per row)"""
    row = start_row + 2  # Add some spacing
    
    # Table header
    timing_header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
    timing_header_font = Font(bold=True, color="FFFFFF", size=10)
    path_group_fill = PatternFill(start_color="D9E2F3", end_color="D9E2F3", fill_type="solid")  # Light blue for path group headers
    
    # Define fills for timing status
    timing_improved_fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")  # Green
    timing_degraded_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")  # Red
    timing_unchanged_fill = PatternFill(start_color="FFEB9C", end_color="FFEB9C", fill_type="solid")  # Yellow
    
    # Header row
    headers = ["Path Group", "Metric", "Reference", "Test", "Delta", "Status"]
    for col_idx, header in enumerate(headers, start=1):
        col_letter = chr(ord('A') + col_idx - 1)
        ws[f'{col_letter}{row}'] = header
        ws[f'{col_letter}{row}'].fill = timing_header_fill
        ws[f'{col_letter}{row}'].font = timing_header_font
        ws[f'{col_letter}{row}'].alignment = Alignment(horizontal='center', wrap_text=True)
    row += 1
    
    # Get timing data
    ref_timing = ref_dc.get('timing_by_pathgroup', {})
    test_timing = test_dc.get('timing_by_pathgroup', {})
    
    # Combine all path groups from both workareas
    all_path_groups = sorted(set(list(ref_timing.keys()) + list(test_timing.keys())))
    
    def add_timing_metric_row(path_group, metric_name, ref_val, test_val, unit="ns", is_first_of_group=False):
        """Add a single timing metric row"""
        nonlocal row
        
        # Path group column (only show on first metric of each group)
        if is_first_of_group:
            ws[f'A{row}'] = path_group
            ws[f'A{row}'].font = Font(bold=True, size=10)
            ws[f'A{row}'].fill = path_group_fill
        else:
            ws[f'A{row}'] = ""
        
        # Metric name
        ws[f'B{row}'] = metric_name
        ws[f'B{row}'].font = Font(size=10)
        
        # Reference value
        if ref_val is not None:
            if unit == "ns":
                ws[f'C{row}'] = f"{ref_val:.3f} ns" if isinstance(ref_val, float) else f"{ref_val} ns"
            elif unit == "paths":
                ws[f'C{row}'] = f"{int(ref_val):,}" if ref_val else "0"
            else:
                ws[f'C{row}'] = ref_val
        else:
            ws[f'C{row}'] = "N/A"
        ws[f'C{row}'].alignment = Alignment(horizontal='right')
        
        # Test value
        if test_val is not None:
            if unit == "ns":
                ws[f'D{row}'] = f"{test_val:.3f} ns" if isinstance(test_val, float) else f"{test_val} ns"
            elif unit == "paths":
                ws[f'D{row}'] = f"{int(test_val):,}" if test_val else "0"
            else:
                ws[f'D{row}'] = test_val
        else:
            ws[f'D{row}'] = "N/A"
        ws[f'D{row}'].alignment = Alignment(horizontal='right')
        
        # Delta and Status
        if ref_val is not None and test_val is not None:
            delta = test_val - ref_val
            
            # Format delta
            if unit == "ns":
                ws[f'E{row}'] = f"{delta:+.3f} ns"
            elif unit == "paths":
                ws[f'E{row}'] = f"{int(delta):+,}" if delta != 0 else "0"
            else:
                ws[f'E{row}'] = f"{delta:+.3f}"
            ws[f'E{row}'].alignment = Alignment(horizontal='right')
            
            # Determine status (for timing, less negative = better, for NVP fewer = better)
            threshold = 0.001 if unit == "ns" else 1
            if abs(delta) < threshold:
                ws[f'F{row}'] = "UNCHANGED"
                ws[f'F{row}'].fill = timing_unchanged_fill
            elif unit == "paths":
                # For NVP, fewer violations is better
                if delta < 0:
                    ws[f'F{row}'] = "IMPROVED"
                    ws[f'F{row}'].fill = timing_improved_fill
                else:
                    ws[f'F{row}'] = "DEGRADED"
                    ws[f'F{row}'].fill = timing_degraded_fill
            else:
                # For WNS/TNS, higher (less negative) is better
                if delta > 0:
                    ws[f'F{row}'] = "IMPROVED"
                    ws[f'F{row}'].fill = timing_improved_fill
                else:
                    ws[f'F{row}'] = "DEGRADED"
                    ws[f'F{row}'].fill = timing_degraded_fill
        else:
            ws[f'E{row}'] = "N/A"
            ws[f'F{row}'] = "N/A"
        
        ws[f'F{row}'].font = Font(bold=True, size=9)
        ws[f'F{row}'].alignment = Alignment(horizontal='center')
        
        # Apply borders
        thin_border = Border(
            left=Side(style='thin'), right=Side(style='thin'),
            top=Side(style='thin'), bottom=Side(style='thin')
        )
        for col in ['A', 'B', 'C', 'D', 'E', 'F']:
            ws[f'{col}{row}'].border = thin_border
        
        row += 1
    
    # Add rows for each path group
    for path_group in all_path_groups:
        ref_data = ref_timing.get(path_group, {})
        test_data = test_timing.get(path_group, {})
        
        # Setup metrics
        add_timing_metric_row(
            path_group, "Setup WNS",
            ref_data.get('setup_wns'), test_data.get('setup_wns'),
            unit="ns", is_first_of_group=True
        )
        add_timing_metric_row(
            path_group, "Setup TNS",
            ref_data.get('setup_tns'), test_data.get('setup_tns'),
            unit="ns", is_first_of_group=False
        )
        add_timing_metric_row(
            path_group, "Setup NVP",
            ref_data.get('setup_nvp'), test_data.get('setup_nvp'),
            unit="paths", is_first_of_group=False
        )
        
        # Hold metrics
        add_timing_metric_row(
            path_group, "Hold WNS",
            ref_data.get('hold_wns'), test_data.get('hold_wns'),
            unit="ns", is_first_of_group=False
        )
        add_timing_metric_row(
            path_group, "Hold TNS",
            ref_data.get('hold_tns'), test_data.get('hold_tns'),
            unit="ns", is_first_of_group=False
        )
        add_timing_metric_row(
            path_group, "Hold NVP",
            ref_data.get('hold_nvp'), test_data.get('hold_nvp'),
            unit="paths", is_first_of_group=False
        )
        
        # Add separator row between path groups
        row += 1
    
    # Set column widths for the timing table
    ws.column_dimensions['A'].width = 20  # Path Group
    ws.column_dimensions['B'].width = 15  # Metric
    ws.column_dimensions['C'].width = 18  # Reference
    ws.column_dimensions['D'].width = 18  # Test
    ws.column_dimensions['E'].width = 18  # Delta
    ws.column_dimensions['F'].width = 12  # Status


def _generate_pnr_comparison_tab(wb, ref_pnr, test_pnr):
    """Generate PnR (Place & Route) Comparison tab (Tab 4)"""
    ws = wb.create_sheet("PnR Comparison", 3)
    
    # Define styles
    header_fill = PatternFill(start_color="4A90E2", end_color="4A90E2", fill_type="solid")  # Blue (PnR theme)
    header_font = Font(bold=True, color="FFFFFF", size=11)
    section_header_fill = PatternFill(start_color="D0E4F5", end_color="D0E4F5", fill_type="solid")  # Light blue
    section_header_font = Font(bold=True, size=10)
    
    # Status colors
    match_fill = PatternFill(start_color="D4EDDA", end_color="D4EDDA", fill_type="solid")  # Green
    different_fill = PatternFill(start_color="F8D7DA", end_color="F8D7DA", fill_type="solid")  # Red
    improved_fill = PatternFill(start_color="C3E6CB", end_color="C3E6CB", fill_type="solid")  # Darker green
    degraded_fill = PatternFill(start_color="F5C6CB", end_color="F5C6CB", fill_type="solid")  # Darker red
    unchanged_fill = PatternFill(start_color="FFF3CD", end_color="FFF3CD", fill_type="solid")  # Yellow
    
    row = 1
    
    # Title
    ws.merge_cells(f'A{row}:H{row}')
    ws[f'A{row}'] = "PnR (Place & Route) Comparison"
    ws[f'A{row}'].fill = header_fill
    ws[f'A{row}'].font = Font(bold=True, color="FFFFFF", size=14)
    ws[f'A{row}'].alignment = Alignment(horizontal='center', vertical='center')
    ws.row_dimensions[row].height = 30
    row += 2
    
    # Comparison headers
    ws[f'A{row}'] = "Metric"
    ws[f'B{row}'] = "Reference"
    ws[f'C{row}'] = "Test"
    ws[f'D{row}'] = "Delta"
    ws[f'E{row}'] = "% Change"
    ws[f'F{row}'] = "Status"
    
    for col in ['A', 'B', 'C', 'D', 'E', 'F']:
        ws[f'{col}{row}'].fill = header_fill
        ws[f'{col}{row}'].font = header_font
        ws[f'{col}{row}'].alignment = Alignment(horizontal='center', vertical='center')
        ws[f'{col}{row}'].border = Border(
            left=Side(style='thin'), right=Side(style='thin'),
            top=Side(style='thin'), bottom=Side(style='thin')
        )
    row += 1
    
    def add_metric_row(metric_name, ref_val, test_val, is_numeric=True, unit="", better_if_higher=True):
        nonlocal row
        
        ws[f'A{row}'] = metric_name
        
        # Get Excel custom number format based on unit
        def get_number_format(unit_str, has_decimals=True):
            """Return Excel custom number format for the given unit"""
            if not unit_str:
                return '0.00' if has_decimals else '0'
            
            # Map units to Excel number formats
            format_map = {
                'um': '0.00 "um"',
                'mmÂ²': '0.00 "mmÂ²"',
                'umÂ²': '0.00 "umÂ²"',
                'ps': '0 "ps"',  # Usually integers for timing
                'ns': '0.00 "ns"',
                '%': '0.00 "%"',
                'KB': '0 "KB"',
                'cells': '#,##0 "cells"',  # Thousands separator for large counts
                'FFs': '#,##0 "FFs"',
                'gates': '#,##0 "gates"',
                'buffers': '#,##0 "buffers"',
                'inverters': '#,##0 "inverters"',
                'paths': '#,##0 "paths"',
                'violations': '0 "violations"',
                'chains': '0 "chains"',
                'arrays': '0 "arrays"',
                'sinks': '#,##0 "sinks"',
                'taps': '#,##0 "taps"',
                'mW': '0.00 "mW"',
                'uW': '0.00 "uW"',
            }
            
            # Return mapped format or default
            return format_map.get(unit_str, f'0.00 "{unit_str}"' if has_decimals else f'0 "{unit_str}"')
        
        # Store values as numbers (not strings) with Excel number formatting
        if is_numeric and ref_val is not None and ref_val != "":
            try:
                ref_num = float(str(ref_val).replace(',', ''))
                ws[f'B{row}'] = ref_num
                # Determine if value has decimals
                has_decimals = (ref_num != int(ref_num)) or unit in ['um', 'mmÂ²', 'umÂ²', 'ns', '%', 'mW', 'uW']
                ws[f'B{row}'].number_format = get_number_format(unit, has_decimals)
            except (ValueError, TypeError):
                ws[f'B{row}'] = str(ref_val)
        else:
            ws[f'B{row}'] = "N/A" if ref_val is None or ref_val == "" else str(ref_val)
        
        if is_numeric and test_val is not None and test_val != "":
            try:
                test_num = float(str(test_val).replace(',', ''))
                ws[f'C{row}'] = test_num
                # Determine if value has decimals
                has_decimals = (test_num != int(test_num)) or unit in ['um', 'mmÂ²', 'umÂ²', 'ns', '%', 'mW', 'uW']
                ws[f'C{row}'].number_format = get_number_format(unit, has_decimals)
            except (ValueError, TypeError):
                ws[f'C{row}'] = str(test_val)
        else:
            ws[f'C{row}'] = "N/A" if test_val is None or test_val == "" else str(test_val)
        
        # Calculate delta and status
        if is_numeric and ref_val and test_val:
            try:
                ref_num = float(str(ref_val).replace(',', ''))
                test_num = float(str(test_val).replace(',', ''))
                delta = test_num - ref_num
                pct_change = (delta / ref_num * 100) if ref_num != 0 else 0
                
                # Store delta as number with format
                ws[f'D{row}'] = delta
                has_decimals = (delta != int(delta)) or unit in ['um', 'mmÂ²', 'umÂ²', 'ns', '%', 'mW', 'uW']
                delta_format = get_number_format(unit, has_decimals)
                # Add +/- sign to format
                if unit:
                    if has_decimals:
                        ws[f'D{row}'].number_format = f'+0.00 "{unit}";-0.00 "{unit}"'
                    else:
                        ws[f'D{row}'].number_format = f'+0 "{unit}";-0 "{unit}"'
                else:
                    ws[f'D{row}'].number_format = '+0.00;-0.00' if has_decimals else '+0;-0'
                
                # Percentage change - store as NUMBER (not text) with custom format
                ws[f'E{row}'] = pct_change / 100  # Store as decimal (e.g., 0.051 for 5.1%)
                ws[f'E{row}'].number_format = '+0.0%;-0.0%'  # Display with + or - sign
                
                # Determine status
                if abs(pct_change) < 0.1:
                    status = "UNCHANGED"
                    fill = unchanged_fill
                elif (better_if_higher and delta > 0) or (not better_if_higher and delta < 0):
                    status = "IMPROVED"
                    fill = improved_fill
                else:
                    status = "DEGRADED"
                    fill = degraded_fill
                
                ws[f'F{row}'] = status
                ws[f'F{row}'].fill = fill
            except (ValueError, TypeError):
                ws[f'D{row}'] = "N/A"
                ws[f'E{row}'] = "N/A"
                ws[f'F{row}'] = "N/A"
        else:
            if ref_val == test_val:
                ws[f'F{row}'] = "MATCH"
                ws[f'F{row}'].fill = match_fill
            elif ref_val and test_val:
                ws[f'F{row}'] = "DIFFERENT"
                ws[f'F{row}'].fill = different_fill
            else:
                ws[f'F{row}'] = "N/A"
        
        # Apply borders and alignment
        for col in ['A', 'B', 'C', 'D', 'E', 'F']:
            ws[f'{col}{row}'].border = Border(
                left=Side(style='thin'), right=Side(style='thin'),
                top=Side(style='thin'), bottom=Side(style='thin')
            )
            if col != 'A':
                ws[f'{col}{row}'].alignment = Alignment(horizontal='center', vertical='center')
        
        row += 1
    
    def add_section_header(title):
        nonlocal row
        ws.merge_cells(f'A{row}:F{row}')
        ws[f'A{row}'] = title
        ws[f'A{row}'].fill = section_header_fill
        ws[f'A{row}'].font = section_header_font
        ws[f'A{row}'].alignment = Alignment(horizontal='left', vertical='center')
        row += 1
    
    # Section 1: Flow Configuration (moved to top for visibility)
    add_section_header("FLOW CONFIGURATION")
    if ref_pnr.get('flow_config') or test_pnr.get('flow_config'):
        ref_config = ref_pnr.get('flow_config', {})
        test_config = test_pnr.get('flow_config', {})
        
        add_metric_row("BEFLOW_CONFIG_SITE", ref_config.get('beflow_config_site'), test_config.get('beflow_config_site'), is_numeric=False)
        add_metric_row("FLOW2_CONFIG_SITE", ref_config.get('flow2_config_site'), test_config.get('flow2_config_site'), is_numeric=False)
        add_metric_row("SCAN_INSERTION_SITE", ref_config.get('scan_insertion_site'), test_config.get('scan_insertion_site'), is_numeric=False)
        add_metric_row("GLCHECK_SITE", ref_config.get('glcheck_site'), test_config.get('glcheck_site'), is_numeric=False)
        add_metric_row("FLOW_PATH", ref_config.get('flow_path'), test_config.get('flow_path'), is_numeric=False)
        add_metric_row("CUSTOM_SCRIPTS_DIR", ref_config.get('custom_scripts_dir'), test_config.get('custom_scripts_dir'), is_numeric=False)
        add_metric_row("MULTIBIT_FLOP", ref_config.get('multibit_flop'), test_config.get('multibit_flop'), is_numeric=False)
        add_metric_row("Library Snapshot", ref_config.get('library_snapshot'), test_config.get('library_snapshot'), is_numeric=False)
        add_metric_row("NV Process", ref_config.get('nv_process'), test_config.get('nv_process'), is_numeric=False)
        add_metric_row("Tracks Number", ref_config.get('tracks_number'), test_config.get('tracks_number'), is_numeric=False)
        add_metric_row("Project", ref_config.get('project'), test_config.get('project'), is_numeric=False)
        add_metric_row("Default Scenario", ref_config.get('default_scenario'), test_config.get('default_scenario'), is_numeric=False)
        add_metric_row("VT Types", ref_config.get('vt_types'), test_config.get('vt_types'), is_numeric=False)
    row += 1
    
    # Section 2: Die & Area Metrics
    add_section_header("DIE & AREA METRICS")
    add_metric_row("Die X", ref_pnr.get('die_x'), test_pnr.get('die_x'), is_numeric=True, unit="um", better_if_higher=False)
    add_metric_row("Die Y", ref_pnr.get('die_y'), test_pnr.get('die_y'), is_numeric=True, unit="um", better_if_higher=False)
    add_metric_row("Die Area", ref_pnr.get('area_die'), test_pnr.get('area_die'), is_numeric=True, unit="mmÂ²", better_if_higher=False)
    add_metric_row("Cell Area", ref_pnr.get('area_cell'), test_pnr.get('area_cell'), is_numeric=True, unit="mmÂ²", better_if_higher=False)
    add_metric_row("Arrays Area", ref_pnr.get('area_arrays'), test_pnr.get('area_arrays'), is_numeric=True, unit="mmÂ²", better_if_higher=False)
    add_metric_row("Utilization", ref_pnr.get('utilization'), test_pnr.get('utilization'), is_numeric=True, unit="%", better_if_higher=False)
    add_metric_row("Effective Utilization", ref_pnr.get('effective_utilization'), test_pnr.get('effective_utilization'), is_numeric=True, unit="%", better_if_higher=False)
    row += 1
    
    # Section 3: Cell Counts
    add_section_header("CELL COUNTS")
    add_metric_row("Total Cells", ref_pnr.get('cell_count'), test_pnr.get('cell_count'), is_numeric=True, unit="cells", better_if_higher=False)
    add_metric_row("Combinational", ref_pnr.get('comb_count'), test_pnr.get('comb_count'), is_numeric=True, unit="cells", better_if_higher=False)
    add_metric_row("Sequential", ref_pnr.get('seq_count'), test_pnr.get('seq_count'), is_numeric=True, unit="cells", better_if_higher=False)
    add_metric_row("Flip-Flops", ref_pnr.get('ff_count'), test_pnr.get('ff_count'), is_numeric=True, unit="FFs", better_if_higher=False)
    add_metric_row("Buffers/Inverters", ref_pnr.get('buf_inv_count'), test_pnr.get('buf_inv_count'), is_numeric=True, unit="cells", better_if_higher=False)
    add_metric_row("Buf/Inv Percentage", ref_pnr.get('buf_inv_percent'), test_pnr.get('buf_inv_percent'), is_numeric=True, unit="%", better_if_higher=False)
    row += 1
    
    # Section 4: VT Distribution & Arrays
    add_section_header("VT DISTRIBUTION & ARRAYS")
    add_metric_row("HVT", ref_pnr.get('hvt_percent'), test_pnr.get('hvt_percent'), is_numeric=True, unit="%", better_if_higher=True)
    add_metric_row("SVT", ref_pnr.get('svt_percent'), test_pnr.get('svt_percent'), is_numeric=True, unit="%", better_if_higher=False)
    add_metric_row("Arrays Count", ref_pnr.get('arrays_count'), test_pnr.get('arrays_count'), is_numeric=True, unit="arrays", better_if_higher=False)
    add_metric_row("Arrays Size", ref_pnr.get('arrays_size'), test_pnr.get('arrays_size'), is_numeric=True, unit="KB", better_if_higher=False)
    row += 1
    
    # Section 5: Clock Gating
    add_section_header("CLOCK GATING EFFICIENCY")
    add_metric_row("Clock Gates Count", ref_pnr.get('clock_gates_count'), test_pnr.get('clock_gates_count'), is_numeric=True, unit="gates", better_if_higher=False)
    add_metric_row("Gated FFs", ref_pnr.get('gated_ffs_percent'), test_pnr.get('gated_ffs_percent'), is_numeric=True, unit="%", better_if_higher=True)
    add_metric_row("Multibit FFs", ref_pnr.get('multibit_ffs_percent'), test_pnr.get('multibit_ffs_percent'), is_numeric=True, unit="%", better_if_higher=True)
    row += 1
    
    # Section 6: Routing & Violations
    add_section_header("ROUTING & DESIGN VIOLATIONS")
    add_metric_row("Wire Length", ref_pnr.get('wire_length'), test_pnr.get('wire_length'), is_numeric=True, unit="um", better_if_higher=False)
    add_metric_row("Shorts", ref_pnr.get('shorts'), test_pnr.get('shorts'), is_numeric=True, unit="violations", better_if_higher=False)
    add_metric_row("Cap Violations", ref_pnr.get('violations_cap'), test_pnr.get('violations_cap'), is_numeric=True, unit="violations", better_if_higher=False)
    add_metric_row("Trans Violations", ref_pnr.get('violations_trans'), test_pnr.get('violations_trans'), is_numeric=True, unit="violations", better_if_higher=False)
    add_metric_row("Fanout Violations", ref_pnr.get('violations_fanout'), test_pnr.get('violations_fanout'), is_numeric=True, unit="violations", better_if_higher=False)
    row += 1
    
    # Section 7: DFT Metrics
    add_section_header("DFT (DESIGN FOR TEST) METRICS")
    add_metric_row("Longest Scan Chain", ref_pnr.get('longest_scan_chain'), test_pnr.get('longest_scan_chain'), is_numeric=True, unit="FFs", better_if_higher=False)
    add_metric_row("Number of Scan Chains", ref_pnr.get('num_scan_chains'), test_pnr.get('num_scan_chains'), is_numeric=True, unit="chains", better_if_higher=False)
    row += 1
    
    # Section 8: External Timing
    add_section_header("EXTERNAL TIMING (POSTROUTE)")
    if ref_pnr.get('external_timing') or test_pnr.get('external_timing'):
        for path_type in ['FEEDTHROUGH', 'REGIN', 'REGOUT']:
            ref_timing = ref_pnr.get('external_timing', {}).get(path_type, {})
            test_timing = test_pnr.get('external_timing', {}).get(path_type, {})
            
            # Use the actual unit (ps or ns) from the parsed data
            # Default to ps if not specified
            wns_unit = ref_timing.get('wns_unit', 'ps') if ref_timing.get('wns_unit') else (test_timing.get('wns_unit', 'ps') if test_timing.get('wns_unit') else 'ps')
            
            add_metric_row(f"{path_type} - WNS", ref_timing.get('wns'), test_timing.get('wns'), is_numeric=True, unit=wns_unit, better_if_higher=True)
            add_metric_row(f"{path_type} - TNS", ref_timing.get('tns'), test_timing.get('tns'), is_numeric=True, unit="ps", better_if_higher=True)
            add_metric_row(f"{path_type} - Viol Paths", ref_timing.get('viol_paths'), test_timing.get('viol_paths'), is_numeric=True, unit="paths", better_if_higher=False)
    
    # ====== PHASE 4.1 + 4.2: NEW SECTIONS ======
    
    # Section 9: Cell Count Details (Phase 4.1)
    row += 1
    add_section_header("CELL COUNT DETAILS")
    add_metric_row("DelBuf Count", ref_pnr.get('delbuf_count'), test_pnr.get('delbuf_count'), is_numeric=True, unit="buffers", better_if_higher=False)
    
    # FFs by Clock (per-clock comparison)
    if ref_pnr.get('ffs_by_clock') or test_pnr.get('ffs_by_clock'):
        all_clocks = set()
        if ref_pnr.get('ffs_by_clock'):
            all_clocks.update(ref_pnr['ffs_by_clock'].keys())
        if test_pnr.get('ffs_by_clock'):
            all_clocks.update(test_pnr['ffs_by_clock'].keys())
        
        for clock in sorted(all_clocks):
            ref_ff = ref_pnr.get('ffs_by_clock', {}).get(clock)
            test_ff = test_pnr.get('ffs_by_clock', {}).get(clock)
            add_metric_row(f"FFs in {clock}", ref_ff, test_ff, is_numeric=True, unit="FFs", better_if_higher=False)
    row += 1
    
    # Section 9: Clock Gating Details (Phase 4.1)
    add_section_header("CLOCK GATING DETAILS")
    add_metric_row("Ungated FFs", ref_pnr.get('ungated_ffs_percent'), test_pnr.get('ungated_ffs_percent'), is_numeric=True, unit="%", better_if_higher=False)
    add_metric_row("Ungated FFs Count", ref_pnr.get('ungated_ffs_count'), test_pnr.get('ungated_ffs_count'), is_numeric=True, unit="FFs", better_if_higher=False)
    add_metric_row("FFs with One ClkGate", ref_pnr.get('ffs_one_clkgate_percent'), test_pnr.get('ffs_one_clkgate_percent'), is_numeric=True, unit="%", better_if_higher=False)
    add_metric_row("FFs with >2 ClkGates", ref_pnr.get('ffs_multi_clkgate_percent'), test_pnr.get('ffs_multi_clkgate_percent'), is_numeric=True, unit="%", better_if_higher=True)
    add_metric_row("Max CG Fanout Count", ref_pnr.get('max_cg_fanout_count'), test_pnr.get('max_cg_fanout_count'), is_numeric=True, unit="FFs", better_if_higher=False)
    add_metric_row("Non-Scan Flops", ref_pnr.get('non_scan_flops'), test_pnr.get('non_scan_flops'), is_numeric=True, unit="FFs", better_if_higher=False)
    row += 1
    
    # Section 10: Max Transition Violations (Phase 4.1)
    add_section_header("MAX TRANSITION VIOLATIONS")
    add_metric_row("Max Trans WNS", ref_pnr.get('max_trans_wns'), test_pnr.get('max_trans_wns'), is_numeric=True, unit="ps", better_if_higher=True)
    add_metric_row("Max Trans TNS", ref_pnr.get('max_trans_tns'), test_pnr.get('max_trans_tns'), is_numeric=True, unit="ps", better_if_higher=True)
    add_metric_row("Max Trans Count", ref_pnr.get('max_trans_count'), test_pnr.get('max_trans_count'), is_numeric=True, unit="violations", better_if_higher=False)
    row += 1
    
    # Section 11: Internal Clock Domain Timing (Phase 4.2)
    add_section_header("INTERNAL CLOCK DOMAIN TIMING")
    if ref_pnr.get('internal_timing') or test_pnr.get('internal_timing'):
        all_internal_clocks = set()
        if ref_pnr.get('internal_timing'):
            all_internal_clocks.update(ref_pnr['internal_timing'].keys())
        if test_pnr.get('internal_timing'):
            all_internal_clocks.update(test_pnr['internal_timing'].keys())
        
        for clock in sorted(all_internal_clocks):
            ref_timing = ref_pnr.get('internal_timing', {}).get(clock, {})
            test_timing = test_pnr.get('internal_timing', {}).get(clock, {})
            
            add_metric_row(f"{clock} - WNS", ref_timing.get('wns'), test_timing.get('wns'), is_numeric=True, unit="ps", better_if_higher=True)
            add_metric_row(f"{clock} - TNS", ref_timing.get('tns'), test_timing.get('tns'), is_numeric=True, unit="ps", better_if_higher=True)
            add_metric_row(f"{clock} - Viol Paths", ref_timing.get('viol_paths'), test_timing.get('viol_paths'), is_numeric=True, unit="paths", better_if_higher=False)
    row += 1
    
    # Section 12: Clock Tree Summary (Phase 4.2)
    add_section_header("CLOCK TREE SUMMARY")
    if ref_pnr.get('clock_tree') or test_pnr.get('clock_tree'):
        all_tree_clocks = set()
        if ref_pnr.get('clock_tree'):
            all_tree_clocks.update(ref_pnr['clock_tree'].keys())
        if test_pnr.get('clock_tree'):
            all_tree_clocks.update(test_pnr['clock_tree'].keys())
        
        for clock in sorted(all_tree_clocks):
            ref_tree = ref_pnr.get('clock_tree', {}).get(clock, {})
            test_tree = test_pnr.get('clock_tree', {}).get(clock, {})
            
            add_metric_row(f"{clock} - Tree Buf", ref_tree.get('tree_buf'), test_tree.get('tree_buf'), is_numeric=True, unit="buffers", better_if_higher=False)
            add_metric_row(f"{clock} - Tree Inv", ref_tree.get('tree_inv'), test_tree.get('tree_inv'), is_numeric=True, unit="inverters", better_if_higher=False)
            add_metric_row(f"{clock} - Tree CG", ref_tree.get('tree_cg'), test_tree.get('tree_cg'), is_numeric=True, unit="gates", better_if_higher=False)
            add_metric_row(f"{clock} - Tree Total", ref_tree.get('tree_total'), test_tree.get('tree_total'), is_numeric=True, unit="cells", better_if_higher=False)
            add_metric_row(f"{clock} - Sinks FF", ref_tree.get('sinks_ff'), test_tree.get('sinks_ff'), is_numeric=True, unit="FFs", better_if_higher=False)
            add_metric_row(f"{clock} - Sinks Total", ref_tree.get('sinks_total'), test_tree.get('sinks_total'), is_numeric=True, unit="sinks", better_if_higher=False)
            if ref_tree.get('taps') or test_tree.get('taps'):
                add_metric_row(f"{clock} - Taps", ref_tree.get('taps'), test_tree.get('taps'), is_numeric=True, unit="taps", better_if_higher=False)
    row += 1
    
    # Section 13: Power Summary (Phase 4.2)
    add_section_header("POWER SUMMARY")
    if ref_pnr.get('power_summary') or test_pnr.get('power_summary'):
        for group in ['combinational', 'sequential', 'physical', 'total']:
            ref_power = ref_pnr.get('power_summary', {}).get(group, {})
            test_power = test_pnr.get('power_summary', {}).get(group, {})
            
            if ref_power or test_power:
                add_metric_row(f"{group.title()} - Area", ref_power.get('area'), test_power.get('area'), is_numeric=True, unit="umÂ²", better_if_higher=False)
                add_metric_row(f"{group.title()} - Count", ref_power.get('count'), test_power.get('count'), is_numeric=True, unit="cells", better_if_higher=False)
                add_metric_row(f"{group.title()} - Power", ref_power.get('power'), test_power.get('power'), is_numeric=True, unit="mW", better_if_higher=False)
                add_metric_row(f"{group.title()} - Leakage", ref_power.get('leakage'), test_power.get('leakage'), is_numeric=True, unit="uW", better_if_higher=False)
    
    # Set column widths
    ws.column_dimensions['A'].width = 30
    ws.column_dimensions['B'].width = 20
    ws.column_dimensions['C'].width = 20
    ws.column_dimensions['D'].width = 15
    ws.column_dimensions['E'].width = 12
    ws.column_dimensions['F'].width = 15


def _generate_clock_comparison_tab(wb, ref_clock, test_clock):
    """Generate Clock Latency Comparison tab"""
    from openpyxl.styles import PatternFill, Font, Alignment
    
    ws = wb.create_sheet("Clock Latency")
    
    # Define styles
    header_fill = PatternFill(start_color="7030A0", end_color="7030A0", fill_type="solid")  # Purple
    header_font = Font(bold=True, color="FFFFFF", size=11)
    section_fill = PatternFill(start_color="E2D4F0", end_color="E2D4F0", fill_type="solid")  # Light purple
    section_font = Font(bold=True, size=11)
    metric_font = Font(bold=True)
    better_fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")  # Green
    worse_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")   # Red
    neutral_fill = PatternFill(start_color="FFEB9C", end_color="FFEB9C", fill_type="solid") # Yellow
    
    row = 1
    
    # Title
    ws[f'A{row}'] = "CLOCK LATENCY COMPARISON"
    ws[f'A{row}'].font = Font(bold=True, size=14)
    ws.merge_cells(f'A{row}:F{row}')
    row += 2
    
    # Helper function for adding comparison rows
    def add_comparison_row(metric_name, ref_val, test_val, is_numeric=True, unit="", better_if_lower=True):
        nonlocal row
        ws[f'A{row}'] = metric_name
        ws[f'A{row}'].font = metric_font
        
        # Format values with unit
        if is_numeric and unit:
            ws[f'B{row}'] = f"{ref_val}{unit}" if ref_val is not None else "N/A"
            ws[f'C{row}'] = f"{test_val}{unit}" if test_val is not None else "N/A"
        else:
            ws[f'B{row}'] = ref_val if ref_val is not None else "N/A"
            ws[f'C{row}'] = test_val if test_val is not None else "N/A"
        
        if is_numeric and ref_val is not None and test_val is not None:
            delta = test_val - ref_val
            if unit:
                ws[f'D{row}'] = f"{delta:+.3f}{unit}"
            else:
                ws[f'D{row}'] = f"{delta:+.3f}" if isinstance(delta, float) else delta
            
            if ref_val != 0:
                pct_change = (delta / ref_val) * 100
                ws[f'E{row}'] = pct_change
                ws[f'E{row}'].number_format = '0.00"%"'
            else:
                ws[f'E{row}'] = "N/A"
            
            # Status with color based on better_if_lower
            if abs(delta) < 0.001:  # Essentially no change
                ws[f'F{row}'] = "UNCHANGED"
                ws[f'F{row}'].fill = neutral_fill
            elif (delta < 0 and better_if_lower) or (delta > 0 and not better_if_lower):
                ws[f'F{row}'] = "BETTER"
                ws[f'F{row}'].fill = better_fill
            else:
                ws[f'F{row}'] = "WORSE"
                ws[f'F{row}'].fill = worse_fill
        else:
            ws[f'D{row}'] = "N/A"
            ws[f'E{row}'] = "N/A"
            if str(ref_val) == str(test_val):
                ws[f'F{row}'] = "MATCH"
                ws[f'F{row}'].fill = neutral_fill
            else:
                ws[f'F{row}'] = "CHANGED"
                ws[f'F{row}'].fill = neutral_fill
        
        row += 1
    
    # Section 1: Per-Clock Max Latency Summary
    ws[f'A{row}'] = "PER-CLOCK MAX LATENCY COMPARISON"
    ws[f'A{row}'].fill = section_fill
    ws[f'A{row}'].font = section_font
    ws.merge_cells(f'A{row}:F{row}')
    row += 1
    
    # Get all clocks from both workareas
    ref_summary = ref_clock.get('clock_summary', {})
    test_summary = test_clock.get('clock_summary', {})
    all_clocks = sorted(set(list(ref_summary.keys()) + list(test_summary.keys())))
    
    if all_clocks:
        # Headers for per-clock max section
        clock_headers = ['Clock', 'Ref Max (ns)', 'Test Max (ns)', 'Delta (ns)', 'Change (%)', 'Status']
        for col_idx, header in enumerate(clock_headers, 1):
            cell = ws.cell(row=row, column=col_idx, value=header)
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = Alignment(horizontal='center')
        row += 1
        
        for clock in all_clocks:
            ref_lat = ref_summary.get(clock, {}).get('max_latency_ns')
            test_lat = test_summary.get(clock, {}).get('max_latency_ns')
            add_comparison_row(clock, ref_lat, test_lat, unit="", better_if_lower=True)
    else:
        ws[f'A{row}'] = "No clock latency data available"
        row += 1
    
    row += 1
    
    # Section 2: Per-Clock Median Latency Summary
    ws[f'A{row}'] = "PER-CLOCK MEDIAN LATENCY COMPARISON"
    ws[f'A{row}'].fill = section_fill
    ws[f'A{row}'].font = section_font
    ws.merge_cells(f'A{row}:F{row}')
    row += 1
    
    if all_clocks:
        # Headers for per-clock median section
        median_headers = ['Clock', 'Ref Median (ns)', 'Test Median (ns)', 'Delta (ns)', 'Change (%)', 'Status']
        for col_idx, header in enumerate(median_headers, 1):
            cell = ws.cell(row=row, column=col_idx, value=header)
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = Alignment(horizontal='center')
        row += 1
        
        for clock in all_clocks:
            ref_median = ref_summary.get(clock, {}).get('median_latency_ns')
            test_median = test_summary.get(clock, {}).get('median_latency_ns')
            add_comparison_row(clock, ref_median, test_median, unit="", better_if_lower=True)
    else:
        ws[f'A{row}'] = "No clock median latency data available"
        row += 1
    
    row += 1
    
    # Section 3: Detailed Clock Latency Table (per IPO/stage) - Vertical Layout
    ws[f'A{row}'] = "DETAILED CLOCK LATENCY BY IPO/STAGE"
    ws[f'A{row}'].fill = section_fill
    ws[f'A{row}'].font = section_font
    ws.merge_cells(f'A{row}:G{row}')
    row += 1
    
    # Get detailed tables from both workareas
    ref_table = ref_clock.get('clock_latency_table', [])
    test_table = test_clock.get('clock_latency_table', [])
    
    if ref_table or test_table:
        # Create a mapping by (ipo, clock) for comparison
        def build_lookup(table):
            lookup = {}
            for entry in table:
                key = (entry['ipo'], entry['clock'])
                lookup[key] = entry
            return lookup
        
        ref_lookup = build_lookup(ref_table)
        test_lookup = build_lookup(test_table)
        
        # Get all unique (ipo, clock) combinations
        all_keys = sorted(set(list(ref_lookup.keys()) + list(test_lookup.keys())))
        
        # Light blue for clock group headers
        clock_group_fill = PatternFill(start_color="D0E4F5", end_color="D0E4F5", fill_type="solid")
        
        # Helper function to add a metric row
        def add_detail_metric_row(metric_name, ref_val, test_val):
            nonlocal row
            ws.cell(row=row, column=1, value=metric_name)
            ws.cell(row=row, column=1).font = Font(bold=True)
            
            ws.cell(row=row, column=2, value=f"{ref_val:.3f}" if ref_val is not None else "N/A")
            ws.cell(row=row, column=3, value=f"{test_val:.3f}" if test_val is not None else "N/A")
            
            if ref_val is not None and test_val is not None:
                delta = test_val - ref_val
                ws.cell(row=row, column=4, value=f"{delta:+.3f}")
                
                if ref_val != 0:
                    pct = (delta / ref_val) * 100
                    ws.cell(row=row, column=5, value=pct)
                    ws.cell(row=row, column=5).number_format = '0.00"%"'
                else:
                    ws.cell(row=row, column=5, value="N/A")
                
                # Status with color
                if abs(delta) < 0.001:
                    ws.cell(row=row, column=6, value="UNCHANGED")
                    ws.cell(row=row, column=6).fill = neutral_fill
                elif delta < 0:
                    ws.cell(row=row, column=6, value="BETTER")
                    ws.cell(row=row, column=6).fill = better_fill
                    ws.cell(row=row, column=4).fill = better_fill
                else:
                    ws.cell(row=row, column=6, value="WORSE")
                    ws.cell(row=row, column=6).fill = worse_fill
                    ws.cell(row=row, column=4).fill = worse_fill
            else:
                ws.cell(row=row, column=4, value="N/A")
                ws.cell(row=row, column=5, value="N/A")
                ws.cell(row=row, column=6, value="N/A")
            
            row += 1
        
        for key in all_keys:
            ipo, clock = key
            ref_entry = ref_lookup.get(key, {})
            test_entry = test_lookup.get(key, {})
            
            # Clock group header row
            ref_stage = ref_entry.get('stage', 'N/A')
            test_stage = test_entry.get('stage', 'N/A')
            stage_info = f"(Ref: {ref_stage}, Test: {test_stage})" if ref_stage != test_stage else f"({ref_stage})"
            
            ws.cell(row=row, column=1, value=f"{ipo} / {clock} {stage_info}")
            ws.cell(row=row, column=1).font = Font(bold=True, size=11)
            ws.cell(row=row, column=1).fill = clock_group_fill
            ws.merge_cells(f'A{row}:F{row}')
            row += 1
            
            # Column headers for this group
            group_headers = ['Metric', 'Reference (ns)', 'Test (ns)', 'Delta (ns)', 'Change (%)', 'Status']
            for col_idx, hdr in enumerate(group_headers, 1):
                cell = ws.cell(row=row, column=col_idx, value=hdr)
                cell.fill = header_fill
                cell.font = header_font
                cell.alignment = Alignment(horizontal='center')
            row += 1
            
            # Innovus Median row
            add_detail_metric_row("Innovus Median", 
                                  ref_entry.get('innovus_median_ns'), 
                                  test_entry.get('innovus_median_ns'))
            
            # Innovus Max row
            add_detail_metric_row("Innovus Max", 
                                  ref_entry.get('innovus_max_ns'), 
                                  test_entry.get('innovus_max_ns'))
            
            # PT Max row
            add_detail_metric_row("PT Max", 
                                  ref_entry.get('pt_max_ns'), 
                                  test_entry.get('pt_max_ns'))
            
            # PT Min row (if available)
            ref_pt_min = ref_entry.get('pt_min_ns')
            test_pt_min = test_entry.get('pt_min_ns')
            if ref_pt_min is not None or test_pt_min is not None:
                add_detail_metric_row("PT Min", ref_pt_min, test_pt_min)
            
            # Empty row for separation
            row += 1
    else:
        ws[f'A{row}'] = "No detailed clock latency data available"
        row += 1
    
    # Set column widths
    ws.column_dimensions['A'].width = 35  # Metric name / IPO+Clock header
    ws.column_dimensions['B'].width = 16  # Reference value
    ws.column_dimensions['C'].width = 16  # Test value
    ws.column_dimensions['D'].width = 14  # Delta
    ws.column_dimensions['E'].width = 12  # Change %
    ws.column_dimensions['F'].width = 12  # Status


def _strip_ansi_codes(text: str) -> str:
    """
    Remove ANSI escape sequences (color codes) from text.
    
    Args:
        text: String containing ANSI escape sequences
        
    Returns:
        str: Text with all ANSI codes removed
    """
    # Pattern matches ANSI escape sequences: ESC[...m
    ansi_pattern = re.compile(r'\x1b\[[0-9;]*m')
    return ansi_pattern.sub('', text)


class OutputParser:
    """
    Parse avice_wa_review.py terminal output into structured metrics.
    
    This class parses the terminal output section-by-section, extracting
    metrics without re-searching files or duplicating extraction logic.
    """
    
    def __init__(self, output: str, workarea: str = None):
        """
        Initialize parser with terminal output.
        
        Args:
            output: Complete terminal output from avice_wa_review.py run
            workarea: Path to workarea (optional, for extracting per-stage PnR data)
        """
        # Strip ANSI color codes before parsing
        self.output = _strip_ansi_codes(output)
        self.workarea = workarea
        self.sections = self._split_into_sections()
    
    def _split_into_sections(self):
        """
        Split terminal output into individual sections.
        
        Sections are identified by headers like:
        ----------------------------------- [1] Setup -----------------------------------
        
        Returns:
            dict: {section_name: section_content}
        """
        sections = {}
        
        # Pattern matches section headers: --- [N] Section Name ---
        # Example: "----------------------------------- [1] Setup -----------------------------------"
        pattern = r'-+\s*\[(\d+)\]\s*([^\-]+?)\s*-+'
        
        # Find all section headers
        matches = list(re.finditer(pattern, self.output))
        
        for i, match in enumerate(matches):
            section_num = match.group(1)
            section_name_raw = match.group(2).strip()
            section_start = match.end()
            
            # Find end of section (start of next section or end of output)
            if i + 1 < len(matches):
                section_end = matches[i + 1].start()
            else:
                section_end = len(self.output)
            
            # Extract section content
            content = self.output[section_start:section_end].strip()
            
            # Normalize section name for consistent access
            normalized_name = self._normalize_section_name(section_name_raw)
            
            sections[normalized_name] = {
                'number': section_num,
                'raw_name': section_name_raw,
                'content': content
            }
        
        return sections
    
    def _normalize_section_name(self, name: str) -> str:
        """
        Normalize section name to match CLI argument format.
        
        Examples:
            "Setup" -> "setup"
            "Runtime Analysis" -> "runtime"
            "Synthesis (DC)" -> "synthesis"
            "Place & Route (PnR)" -> "pnr"
        
        Args:
            name: Raw section name from output
            
        Returns:
            str: Normalized section name matching CLI -s argument
        """
        # Mapping from output section names to CLI argument names
        mapping = {
            'setup': 'setup',
            'runtime analysis': 'runtime',
            'synthesis (dc)': 'synthesis',
            'place & route (pnr)': 'pnr',
            'clock analysis': 'clock',
            'clock tree analysis': 'clock',
            'formal verification': 'formal',
            'parasitic extraction (star)': 'star',
            'signoff timing (pt)': 'pt',
            'physical verification (pv)': 'pv',
            'gl check': 'gl-check',
            'eco analysis': 'eco',
            'nv gate eco': 'nv-gate-eco',
            'block release': 'block-release'
        }
        
        name_lower = name.lower().strip()
        
        # Try exact match first
        if name_lower in mapping:
            return mapping[name_lower]
        
        # Try partial matches
        for key, value in mapping.items():
            if key in name_lower or name_lower in key:
                return value
        
        # Fallback: convert to lowercase and replace spaces with dashes
        return name_lower.replace(' ', '-')
    
    def parse_setup(self) -> dict:
        """
        Parse [1] Setup section.
        
        Extracts:
        - Unit name
        - TAG/RTL tag
        - IPO
        - Available IPOs
        - Workarea owner
        - Disk usage
        
        Note: Die dimensions (X, Y) and Design area are NOT in Setup section.
              Those are extracted in DC section (from $WA/flp/) and PnR section (from stage DBs).
        
        Returns:
            dict: Extracted metrics
        """
        data = {}
        
        if 'setup' not in self.sections:
            return data
        
        content = self.sections['setup']['content']
        
        # Extract UNIT
        unit_match = re.search(r'^UNIT:\s*(\S+)', content, re.MULTILINE)
        if unit_match:
            data['unit'] = unit_match.group(1)
        
        # Extract TAG
        tag_match = re.search(r'^TAG:\s*(.+)$', content, re.MULTILINE)
        if tag_match:
            data['tag'] = tag_match.group(1).strip()
        
        # Extract IPO
        ipo_match = re.search(r'^IPO:\s*(\S+)', content, re.MULTILINE)
        if ipo_match:
            data['ipo'] = ipo_match.group(1)
        
        # Extract Available IPOs (handles both "Available IPOs:" and "Available IPO directories:")
        ipo_avail_match = re.search(r'Available IPO(?:s| directories):\s*(.+?)(?:\)|$)', content, re.MULTILINE)
        if ipo_avail_match:
            ipos_str = ipo_avail_match.group(1).strip()
            # Filter out empty strings and clean up
            data['available_ipos'] = [ipo.strip() for ipo in ipos_str.split(',') if ipo.strip()]
        
        # Extract Workarea owner
        owner_match = re.search(r'Workarea Owner:\s*(\S+)', content)
        if owner_match:
            data['owner'] = owner_match.group(1)
        
        # Extract Disk usage percentage
        disk_match = re.search(r'Usage:\s*(\d+)%', content)
        if disk_match:
            data['disk_usage'] = int(disk_match.group(1))
        
        # Extract Environment Information
        env_match = re.search(r'Environment Information:\s*(.+?)(?:\n|$)', content)
        if env_match:
            data['environment'] = env_match.group(1).strip()
        
        # Extract NBU Signoff Mode
        nbu_mode_match = re.search(r'NBU Signoff Mode:\s*(.+?)(?:\n|$)', content)
        if nbu_mode_match:
            data['nbu_signoff_mode'] = nbu_mode_match.group(1).strip()
        
        # Extract BE_OVERRIDE_TOOLVERS
        # Note: Format is "BE_OVERRIDE_TOOLVERS=/path/to/file" (uses = not :)
        be_override_match = re.search(r'BE_OVERRIDE_TOOLVERS=(.+?)(?:\n|$)', content)
        if be_override_match:
            data['be_override_toolvers'] = be_override_match.group(1).strip()
        
        # Extract BEFLOW_CONFIG_REV
        # Note: Format is "BEFLOW_CONFIG_REV=2025_ww36_08" (uses = not :)
        beflow_config_rev_match = re.search(r'BEFLOW_CONFIG_REV=(.+?)(?:\n|$)', content)
        if beflow_config_rev_match:
            data['beflow_config_rev'] = beflow_config_rev_match.group(1).strip()
        
        return data
    
    def parse_runtime(self) -> dict:
        """
        Parse [2] Runtime Analysis section.
        
        Extracts:
        - Total DC runtime
        - PnR runtime per IPO
        - Formal verification runtimes
        - GL Check runtime
        - Star (parasitic extraction) runtime
        - PV (physical verification) runtime
        - Auto PT runtime
        
        Returns:
            dict: Extracted metrics
        """
        data = {}
        
        if 'runtime' not in self.sections:
            return data
        
        content = self.sections['runtime']['content']
        lines = content.split('\n')
        
        # Parse unified timeline format
        # New format: "Phase        Flow/Stage           Runtime    Timeline                  Status"
        # Examples:
        #   "Synthesis    Fast DC              0.33h      [12/09 10:56 -> 12/09 11:15] [OK]"
        #   "Synthesis    Full DC              8.75h      [12/09 19:20 -> 12/10 04:05] [OK]"
        #   "PnR          [IPO1000]            25.30h     [12/10 08:34 -> 12/11 09:51]"
        #   "Signoff      Star                 0.47h      [12/13 18:06 -> 12/13 18:32] [OK]"
        #   "Signoff      Auto_Pt              9.11h      [12/13 18:38 -> 12/14 03:45] [OK]"
        #   "Signoff      Formal_Rtl_Pnr       0.81h      [12/14 10:05 -> 12/14 10:53] [OK]"
        #   "Signoff      Gl_Check             0.16h      [12/14 10:07 -> 12/14 10:17] [OK]"
        
        current_ipo = None
        
        for line in lines:
            # Skip headers and separator lines
            if '====' in line or '----' in line or 'Phase' in line or 'Flow/Stage' in line:
                continue
            
            # Parse DC runtime (both Fast DC and Full DC)
            # "Synthesis    Fast DC              0.33h"
            # "Synthesis    Full DC              8.75h"
            if 'Synthesis' in line and 'DC' in line:
                # Look for Fast DC or Full DC followed by runtime
                if 'Fast DC' in line:
                    runtime_match = re.search(r'Fast DC\s+(\d+\.?\d*)\s*h', line)
                    if runtime_match:
                        data['fast_dc_runtime'] = float(runtime_match.group(1))
                elif 'Full DC' in line:
                    runtime_match = re.search(r'Full DC\s+(\d+\.?\d*)\s*h', line)
                    if runtime_match:
                        data['dc_runtime'] = float(runtime_match.group(1))
                # Fallback for simple "DC" pattern (old format)
                elif 'hours' in line:
                    runtime_match = re.search(r'DC\s+(\d+\.?\d*)\s+hours', line)
                    if runtime_match:
                        data['dc_runtime'] = float(runtime_match.group(1))
            
            # Parse PnR runtime per IPO
            # "PnR          [IPO1000]            25.30h"
            if 'PnR' in line and '[IPO' in line.upper():
                ipo_match = re.search(r'\[IPO(\d+)\]', line, re.IGNORECASE)
                runtime_match = re.search(r'(\d+\.?\d*)\s*h\s', line)
                if ipo_match and runtime_match:
                    ipo_name = f'ipo{ipo_match.group(1)}'
                    runtime = float(runtime_match.group(1))
                    data[f'pnr_runtime_{ipo_name}'] = runtime
                    current_ipo = ipo_name
            
            # Parse Signoff flows
            # Determine which IPO these signoff flows belong to (use most recent PnR IPO)
            if 'Signoff' in line:
                # Extract runtime (format: "0.47h")
                runtime_match = re.search(r'(\d+\.?\d*)\s*h\s', line)
                
                if 'Star' in line and runtime_match:
                    runtime = float(runtime_match.group(1))
                    if current_ipo:
                        # IPO-specific star runtime
                        data[f'star_runtime_{current_ipo}'] = runtime
                    else:
                        # Global star runtime
                        data['star_runtime'] = runtime
                
                elif 'Auto_Pt' in line and runtime_match:
                    runtime = float(runtime_match.group(1))
                    if current_ipo:
                        # IPO-specific PT runtime
                        data[f'auto_pt_runtime_{current_ipo}'] = runtime
                    else:
                        # Global PT runtime
                        data['auto_pt_runtime'] = runtime
                
                elif 'Formal' in line and runtime_match:
                    runtime = float(runtime_match.group(1))
                    # Extract formal flow name (e.g., "Formal_Rtl_Pnr", "Formal_Bbox")
                    if 'Rtl_Pnr' in line or 'RTL_PNR' in line.upper():
                        if current_ipo:
                            data[f'formal_runtime_rtl_pnr_{current_ipo}'] = runtime
                        else:
                            data['formal_runtime_rtl_pnr'] = runtime
                    elif 'Bbox' in line or 'BBOX' in line.upper():
                        if current_ipo:
                            data[f'formal_runtime_bbox_{current_ipo}'] = runtime
                        else:
                            data['formal_runtime_bbox'] = runtime
                    else:
                        # Generic formal runtime
                        if current_ipo:
                            data[f'formal_runtime_{current_ipo}'] = runtime
                        else:
                            data['formal_runtime'] = runtime
                
                elif 'Gl_Check' in line and runtime_match:
                    runtime = float(runtime_match.group(1))
                    if current_ipo:
                        data[f'gl_check_runtime_{current_ipo}'] = runtime
                    else:
                        data['gl_check_runtime'] = runtime
                
                elif 'PV' in line and runtime_match:
                    runtime = float(runtime_match.group(1))
                    if current_ipo:
                        data[f'pv_runtime_{current_ipo}'] = runtime
                    else:
                        data['pv_runtime'] = runtime
        
        # Extract Grand Total runtime
        # Format: "GRAND TOTAL                      111.58h    [12/23 15:29 -> 12/30 13:06]"
        grand_total_match = re.search(r'GRAND TOTAL\s+(\d+\.?\d*)\s*h', content)
        if grand_total_match:
            data['grand_total_runtime'] = float(grand_total_match.group(1))
        
        # Extract Efficiency
        # Format: "Efficiency: 67% (111.6h active / 165.6h elapsed)"
        efficiency_match = re.search(r'Efficiency:\s*(\d+)%', content)
        if efficiency_match:
            data['efficiency'] = int(efficiency_match.group(1))
        
        # Extract per-stage PnR data if workarea path is provided
        if self.workarea:
            pnr_stage_data = self._extract_pnr_stage_data()
            if pnr_stage_data:
                data['pnr_stage_data'] = pnr_stage_data
        
        return data
    
    def _find_latest_prc_status_file(self) -> str:
        """
        Find the most recently modified *.prc.status file in pnr_flow/nv_flow/.
        
        Returns:
            str: Path to the latest prc.status file, or None if not found
        """
        if not self.workarea:
            return None
        
        nv_flow_dir = os.path.join(self.workarea, 'pnr_flow', 'nv_flow')
        if not os.path.isdir(nv_flow_dir):
            return None
        
        # Find all *.prc.status files
        prc_status_files = []
        try:
            for f in os.listdir(nv_flow_dir):
                if f.endswith('.prc.status'):
                    full_path = os.path.join(nv_flow_dir, f)
                    if os.path.isfile(full_path):
                        mtime = os.path.getmtime(full_path)
                        prc_status_files.append((full_path, mtime))
        except OSError:
            return None
        
        if not prc_status_files:
            return None
        
        # Return the most recently modified file
        prc_status_files.sort(key=lambda x: x[1], reverse=True)
        return prc_status_files[0][0]
    
    def _extract_pnr_stage_data(self) -> dict:
        """
        Extract per-stage PnR runtime data from prc.status file.
        
        Returns:
            dict: {ipo_name: {'stages': [{stage, runtime_hours}, ...], 'total': total_hours}}
        """
        if not self.workarea:
            return {}
        
        # Find the latest prc.status file
        prc_status_file = self._find_latest_prc_status_file()
        
        if not prc_status_file:
            return {}
        
        # Parse prc.status file
        pnr_data = {}
        
        try:
            with open(prc_status_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            for line in lines:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                
                # Parse status line: block ipo step status duration logfile
                parts = line.split()
                if len(parts) >= 5:
                    block, ipo, step, status, duration = parts[0], parts[1], parts[2], parts[3], parts[4]
                    
                    # Only process PnR construction stages
                    pnr_stages = ['BEGIN', 'setup', 'edi_plan', 'place', 'cts', 'route', 'postroute']
                    if step not in pnr_stages or status != 'DONE':
                        continue
                    
                    if ipo not in pnr_data:
                        pnr_data[ipo] = {'stages': [], 'total': 0.0}
                    
                    # Convert duration to hours
                    try:
                        duration_seconds = int(duration)
                        duration_hours = duration_seconds / 3600
                        
                        # Map step names to display names
                        step_display = {
                            'BEGIN': 'Begin',
                            'setup': 'Setup',
                            'edi_plan': 'Floorplan',
                            'place': 'Placement',
                            'cts': 'CTS',
                            'route': 'Route',
                            'postroute': 'Post-Route'
                        }.get(step, step)
                        
                        pnr_data[ipo]['stages'].append({
                            'stage': step_display,
                            'runtime_hours': duration_hours
                        })
                        pnr_data[ipo]['total'] += duration_hours
                        
                    except (ValueError, ZeroDivisionError):
                        pass
        
        except Exception as e:
            # Silent failure - this is optional enhancement data
            pass
        
        return pnr_data
    
    def parse_synthesis(self) -> dict:
        """
        Parse [3] Synthesis (DC) section.
        
        Extracts:
        - DC Version
        - DC Errors/Warnings
        - Die dimensions (X, Y)
        - Design Area (mmÂ²)
        - Cell counts (Leaf, Comb, Seq, Buf/Inv, Macro, Nets)
        - Timing by path group (per group: Setup/Hold WNS, TNS, NVP)
        - Clock gates removed
        - Removed registers
        - Configuration variables (Lib Snap, Process, Tracks, etc.)
        
        Returns:
            dict: Extracted metrics
        """
        data = {}
        
        if 'synthesis' not in self.sections:
            return data
        
        content = self.sections['synthesis']['content']
        
        # Extract DC Version
        # Pattern: "DC Version: V-2023.12-SP5-4"
        dc_version_match = re.search(r'DC Version:\s*(\S+)', content)
        if dc_version_match:
            data['dc_version'] = dc_version_match.group(1)
        
        # Extract DC Errors
        # Pattern: "[OK] DC Errors: 0" or "DC Errors: 5"
        dc_errors_match = re.search(r'DC Errors:\s*(\d+)', content)
        if dc_errors_match:
            data['dc_errors'] = int(dc_errors_match.group(1))
        
        # Extract DC Warnings
        # Pattern: "[INFO] DC Warnings: 773,339"
        dc_warnings_match = re.search(r'DC Warnings:\s*([\d,]+)', content)
        if dc_warnings_match:
            warnings_str = dc_warnings_match.group(1).replace(',', '')
            data['dc_warnings'] = int(warnings_str)
        
        # Extract Die Dimensions
        # Pattern: "Die: 600.78 x 574.56 um (from: flp)"
        die_dims_match = re.search(r'Die:\s*([\d.]+)\s*x\s*([\d.]+)\s*um', content)
        if die_dims_match:
            data['die_x'] = float(die_dims_match.group(1))
            data['die_y'] = float(die_dims_match.group(2))
        
        # Extract Design Area
        # Pattern: "Design Area: 0.345 mm2" or "Design Area: 0.345 mmÂ²"
        design_area_match = re.search(r'Design Area:\s*([\d.]+)\s*mm', content)
        if design_area_match:
            data['design_area'] = float(design_area_match.group(1))
        
        # Extract Cell Counts
        # Pattern: "Leaf: 986,347 | Comb: 875,336 | Seq: 111,011 | Buf/Inv: 125,830 (12.8%) | Macro: 56 | Nets: 997,772"
        leaf_match = re.search(r'Leaf:\s*([\d,]+)', content)
        if leaf_match:
            data['leaf_cells'] = int(leaf_match.group(1).replace(',', ''))
        
        comb_match = re.search(r'Comb:\s*([\d,]+)', content)
        if comb_match:
            data['comb_cells'] = int(comb_match.group(1).replace(',', ''))
        
        seq_match = re.search(r'Seq:\s*([\d,]+)', content)
        if seq_match:
            data['seq_cells'] = int(seq_match.group(1).replace(',', ''))
        
        bufinv_match = re.search(r'Buf/Inv:\s*([\d,]+)\s*\(([\d.]+)%\)', content)
        if bufinv_match:
            data['buf_inv_cells'] = int(bufinv_match.group(1).replace(',', ''))
            data['buf_inv_percent'] = float(bufinv_match.group(2))
        
        macro_match = re.search(r'Macro:\s*([\d,]+)', content)
        if macro_match:
            data['macro_cells'] = int(macro_match.group(1).replace(',', ''))
        
        nets_match = re.search(r'Nets:\s*([\d,]+)', content)
        if nets_match:
            data['nets'] = int(nets_match.group(1).replace(',', ''))
        
        # Extract Clock Gates Removed
        # Pattern: "Clock gates removed: 0"
        clock_gates_match = re.search(r'Clock gates removed:\s*([\d,]+)', content)
        if clock_gates_match:
            data['clock_gates_removed'] = int(clock_gates_match.group(1).replace(',', ''))
        
        # Extract Removed Registers
        # Pattern: "Removed registers: 3,708"
        removed_regs_match = re.search(r'Removed registers:\s*([\d,]+)', content)
        if removed_regs_match:
            data['removed_registers'] = int(removed_regs_match.group(1).replace(',', ''))
        
        # Extract Configuration Variables
        # Pattern: "Lib Snap: 20250819 | Process: tsmc5_t6 | Tracks: t6 | Project: agur"
        lib_snap_match = re.search(r'Lib Snap:\s*(\S+)', content)
        if lib_snap_match:
            data['lib_snap'] = lib_snap_match.group(1)
        
        process_match = re.search(r'Process:\s*(\S+)', content)
        if process_match:
            data['process'] = process_match.group(1)
        
        tracks_match = re.search(r'Tracks:\s*(\S+)', content)
        if tracks_match:
            data['tracks'] = tracks_match.group(1)
        
        project_match = re.search(r'Project:\s*(\S+)', content)
        if project_match:
            data['project'] = project_match.group(1)
        
        scenario_match = re.search(r'Scenario:\s*([^\|]+)', content)
        if scenario_match:
            data['scenario'] = scenario_match.group(1).strip()
        
        vt_match = re.search(r'VT:\s*([^\n]+)', content)
        if vt_match:
            data['vt'] = vt_match.group(1).strip()
        
        # Extract Arrays (full list without truncation)
        arrays_match = re.search(r'Arrays:\s*([^\n]+)', content)
        if arrays_match:
            data['arrays'] = arrays_match.group(1).strip()
        
        # Extract BEFLOW_ROOT
        beflow_root_match = re.search(r'BEFLOW_ROOT:\s*([^\|]+)', content)
        if beflow_root_match:
            data['beflow_root'] = beflow_root_match.group(1).strip()
        
        # Extract BEFLOW_CONFIG_SITE
        beflow_config_site_match = re.search(r'BEFLOW_CONFIG_SITE:\s*([^\n]+)', content)
        if beflow_config_site_match:
            data['beflow_config_site'] = beflow_config_site_match.group(1).strip()
        
        # Extract Timing by Path Group
        # This is complex - we need to parse the timing table
        data['timing_by_pathgroup'] = self._parse_timing_by_pathgroup(content)
        
        return data
    
    def parse_pnr(self) -> dict:
        """
        Parse [4] PnR Analysis section.
        
        Extracts:
        - Die dimensions (X, Y)
        - Area metrics (Die, Cell, Arrays)
        - Utilization (%, Effective %)
        - Cell counts (Cell, Comb, Seq, FF, Buf/Inv)
        - Clock gating metrics
        - Timing (WNS, TNS, ViolPaths for external/internal)
        - Clock latency
        - Routing metrics
        - Design violations
        - DFT metrics
        
        Returns:
            dict: PnR metrics
        """
        # Section name is normalized to lowercase with hyphens
        if 'pnr-analysis' not in self.sections:
            return {}
        
        content = self.sections['pnr-analysis']['content']
        data = {}
        
        # Extract Die Dimensions
        die_match = re.search(r'Die Dimensions \(X x Y\)\s+(\d+\.?\d*)\s*x\s*(\d+\.?\d*)\s*um', content)
        if die_match:
            data['die_x'] = die_match.group(1)
            data['die_y'] = die_match.group(2)
        
        # Extract Area metrics (mmÂ²)
        area_match = re.search(r'Area \(mm2\)\s+Die:\s*(\d+\.?\d*)\s*\|\s*Cell:\s*(\d+\.?\d*)\s*\|\s*Arrays:\s*(\d+\.?\d*)', content)
        if area_match:
            data['area_die'] = area_match.group(1)
            data['area_cell'] = area_match.group(2)
            data['area_arrays'] = area_match.group(3)
        
        # Extract Utilization
        util_match = re.search(r'Utilization\s+Util:\s*(\d+\.?\d*)%\s*\|\s*Effective:\s*(\d+\.?\d*)%', content)
        if util_match:
            data['utilization'] = util_match.group(1)
            data['effective_utilization'] = util_match.group(2)
        
        # Extract Cell Counts
        cell_match = re.search(r'Cell Counts\s+Cell:\s*([\d,]+)\s*\|\s*Comb:\s*([\d,]+)\s*\|\s*Seq:\s*([\d,]+)\s*\|\s*FF:\s*([\d,]+)\s*\|\s*Buf/Inv:\s*([\d,]+)\s*\(([\d.]+)%\)', content)
        if cell_match:
            data['cell_count'] = cell_match.group(1).replace(',', '')
            data['comb_count'] = cell_match.group(2).replace(',', '')
            data['seq_count'] = cell_match.group(3).replace(',', '')
            data['ff_count'] = cell_match.group(4).replace(',', '')
            data['buf_inv_count'] = cell_match.group(5).replace(',', '')
            data['buf_inv_percent'] = cell_match.group(6)
        
        # Extract Arrays
        arrays_match = re.search(r'Arrays\s+Count:\s*(\d+)\s*\|\s*Size:\s*([\d.]+)KB', content)
        if arrays_match:
            data['arrays_count'] = arrays_match.group(1)
            data['arrays_size'] = arrays_match.group(2)
        
        # Extract VT Distribution
        vt_match = re.search(r'VT Distribution\s+HVT:\s*([\d.]+)%\s*\|\s*SVT:\s*([\d.]+)%', content)
        if vt_match:
            data['hvt_percent'] = vt_match.group(1)
            data['svt_percent'] = vt_match.group(2)
        
        # Extract Clock Gating metrics
        cg_count_match = re.search(r'Clock Gates Count\s+(\d+)', content)
        if cg_count_match:
            data['clock_gates_count'] = cg_count_match.group(1)
        
        gated_ff_match = re.search(r'Gated FFs\s+([\d.]+)%', content)
        if gated_ff_match:
            data['gated_ffs_percent'] = gated_ff_match.group(1)
        
        multibit_ff_match = re.search(r'Multibit FFs\s+([\d.]+)%', content)
        if multibit_ff_match:
            data['multibit_ffs_percent'] = multibit_ff_match.group(1)
        
        # Extract Clock Latency (focus on max values)
        clock_latencies = {}
        for clk_lat_match in re.finditer(r'(\w+_clk) Latency\s+Max:\s*([\d.]+)\s*ps\s*\|\s*Avg:\s*([\d.]+)\s*ps', content):
            clock_name = clk_lat_match.group(1)
            max_latency = clk_lat_match.group(2)
            avg_latency = clk_lat_match.group(3)
            clock_latencies[clock_name] = {
                'max': max_latency,
                'avg': avg_latency
            }
        if clock_latencies:
            data['clock_latencies'] = clock_latencies
        
        # Extract Routing metrics
        wire_match = re.search(r'Total Wire Length\s+([\d,]+)\s*um', content)
        if wire_match:
            data['wire_length'] = wire_match.group(1).replace(',', '')
        
        shorts_match = re.search(r'Shorts\s+(\d+)', content)
        if shorts_match:
            data['shorts'] = shorts_match.group(1)
        
        # Extract Design Violations
        violations_match = re.search(r'Max Violations\s+Cap:\s*(\d+)\s*\|\s*Trans:\s*(\d+)\s*\|\s*Fanout:\s*(\d+)', content)
        if violations_match:
            data['violations_cap'] = violations_match.group(1)
            data['violations_trans'] = violations_match.group(2)
            data['violations_fanout'] = violations_match.group(3)
        
        # Extract DFT Metrics
        scan_chain_match = re.search(r'Longest Scan Chain\s+(\d+)', content)
        if scan_chain_match:
            data['longest_scan_chain'] = scan_chain_match.group(1)
        
        num_scan_chains_match = re.search(r'Number of Scan Chains\s+(\d+)', content)
        if num_scan_chains_match:
            data['num_scan_chains'] = num_scan_chains_match.group(1)
        
        # Extract External Timing (FEEDTHROUGH, REGIN, REGOUT)
        # Pattern handles both ps and ns for WNS, and floats like "9.27ns"
        external_timing = {}
        # FEEDTHROUGH
        feedthrough_match = re.search(r'FEEDTHROUGH.*?\s+([-\d.]+)(ns|ps)\s*/\s*([-\d.]+)(?:ns|ps)\s*/\s*(\d+)', content)
        if feedthrough_match:
            wns_num = float(feedthrough_match.group(1))
            wns_unit = feedthrough_match.group(2)
            
            # Convert to ps ONLY if value < 1.0 ns (for readability)
            if wns_unit == 'ns' and wns_num < 1.0:
                wns_val = str(int(wns_num * 1000))  # e.g., 0.5ns â†’ 500ps
            elif wns_unit == 'ns':
                wns_val = feedthrough_match.group(1)  # Keep as ns value (e.g., 9.13)
            else:
                wns_val = str(int(wns_num))  # ps values stay as integers
            
            external_timing['FEEDTHROUGH'] = {
                'wns': wns_val,
                'wns_unit': 'ps' if (wns_unit == 'ns' and wns_num < 1.0) else wns_unit,
                'tns': feedthrough_match.group(3),
                'viol_paths': feedthrough_match.group(4)
            }
        
        # REGIN
        regin_match = re.search(r'REGIN.*?\s+([-\d.]+)(ns|ps)\s*/\s*([-\d.]+)(?:ns|ps)\s*/\s*(\d+)', content)
        if regin_match:
            wns_num = float(regin_match.group(1))
            wns_unit = regin_match.group(2)
            
            # Convert to ps ONLY if value < 1.0 ns
            if wns_unit == 'ns' and abs(wns_num) < 1.0:
                wns_val = str(int(wns_num * 1000))
            elif wns_unit == 'ns':
                wns_val = regin_match.group(1)
            else:
                wns_val = str(int(wns_num))
            
            external_timing['REGIN'] = {
                'wns': wns_val,
                'wns_unit': 'ps' if (wns_unit == 'ns' and abs(wns_num) < 1.0) else wns_unit,
                'tns': regin_match.group(3),
                'viol_paths': regin_match.group(4)
            }
        
        # REGOUT
        regout_match = re.search(r'REGOUT.*?\s+([-\d.]+)(ns|ps)\s*/\s*([-\d.]+)(?:ns|ps)\s*/\s*(\d+)', content)
        if regout_match:
            wns_num = float(regout_match.group(1))
            wns_unit = regout_match.group(2)
            
            # Convert to ps ONLY if value < 1.0 ns
            if wns_unit == 'ns' and abs(wns_num) < 1.0:
                wns_val = str(int(wns_num * 1000))
            elif wns_unit == 'ns':
                wns_val = regout_match.group(1)
            else:
                wns_val = str(int(wns_num))
            
            external_timing['REGOUT'] = {
                'wns': wns_val,
                'wns_unit': 'ps' if (wns_unit == 'ns' and abs(wns_num) < 1.0) else wns_unit,
                'tns': regout_match.group(3),
                'viol_paths': regout_match.group(4)
            }
        
        if external_timing:
            data['external_timing'] = external_timing
        
        # Extract Internal Timing (Clock domains - just count them)
        # Pattern matches clock names like: m1_clk, m1_clk_reg2cgate, i2_clk0, i2_clk0_reg2cgate, etc.
        internal_timing_section = re.search(r'INTERNAL TIMING \(Core Logic\)(.*?)(?:={50,}|$)', content, re.DOTALL)
        if internal_timing_section:
            # Match any clock name (word chars including digits) followed by timing values
            internal_clocks = re.findall(r'(\w+_clk\w*)\s+[-\d.]+(?:ns|ps)', internal_timing_section.group(1))
            data['internal_clock_count'] = len(internal_clocks)
        
        # ====== PHASE 4.1: Quick Wins Extraction ======
        
        # Extract DelBuf count (from Cell Counts line)
        delbuf_match = re.search(r'DelBuf:\s*([\d,]+)', content)
        if delbuf_match:
            data['delbuf_count'] = delbuf_match.group(1).replace(',', '')
        
        # Extract FFs by Clock breakdown
        ffs_by_clock = {}
        for ff_clock_match in re.finditer(r'(\w+_clk):\s*([\d,]+)', content):
            clock_name = ff_clock_match.group(1)
            ff_count = ff_clock_match.group(2).replace(',', '')
            # Only capture if it's in the "FFs by Clock" context
            if 'FFs by Clock' in content[:ff_clock_match.start()]:
                ffs_by_clock[clock_name] = ff_count
        if ffs_by_clock:
            data['ffs_by_clock'] = ffs_by_clock
        
        # Extract Ungated FFs
        ungated_ff_match = re.search(r'Ungated FFs\s+([\d.]+)%\s*\(([\d,]+)', content)
        if ungated_ff_match:
            data['ungated_ffs_percent'] = ungated_ff_match.group(1)
            data['ungated_ffs_count'] = ungated_ff_match.group(2).replace(',', '')
        
        # Extract FFs with One ClkGate
        one_clkgate_match = re.search(r'FFs with One ClkGate\s+([\d.]+)%', content)
        if one_clkgate_match:
            data['ffs_one_clkgate_percent'] = one_clkgate_match.group(1)
        
        # Extract FFs with >2 ClkGates
        multi_clkgate_match = re.search(r'FFs with >2 ClkGates\s+([\d.]+)%', content)
        if multi_clkgate_match:
            data['ffs_multi_clkgate_percent'] = multi_clkgate_match.group(1)
        
        # Extract Max CG Fanout Instance
        max_cg_instance_match = re.search(r'Max CG Fanout Instance\s+(.+?)(?:\n|$)', content)
        if max_cg_instance_match:
            data['max_cg_fanout_instance'] = max_cg_instance_match.group(1).strip()
        
        # Extract Max CG Fanout Count
        max_cg_count_match = re.search(r'Max CG Fanout Count\s+(\d+)', content)
        if max_cg_count_match:
            data['max_cg_fanout_count'] = max_cg_count_match.group(1)
        
        # Extract Non-Scan Flops
        non_scan_match = re.search(r'Non-Scan Flops\s+(\d+)', content)
        if non_scan_match:
            data['non_scan_flops'] = non_scan_match.group(1)
        
        # Extract Max Transition Violations
        max_trans_section = re.search(r'MAX TRANSITION VIOLATIONS.*?TOTAL\s+([-\d]+)ps\s+([-\d]+)ps\s+(\d+)', content, re.DOTALL)
        if max_trans_section:
            data['max_trans_wns'] = max_trans_section.group(1)
            data['max_trans_tns'] = max_trans_section.group(2)
            data['max_trans_count'] = max_trans_section.group(3)
        
        # ====== PHASE 4.2: High Value Extraction ======
        
        # Extract Internal Clock Domain Timing (detailed per-clock)
        # Pattern matches clock names like: m1_clk, m1_clk_reg2cgate, i2_clk0, i2_clk0_reg2cgate, etc.
        # And handles both ps and ns units for WNS and TNS values
        internal_timing = {}
        if internal_timing_section:
            # Parse internal clock timing from INTERNAL TIMING section
            # Format: clock_name   WNS(ps or ns) / TNS(ps or ns) / NVP
            # Example: m1_clk                         105ps /          0ps /      0
            # Example: i2_clk0                        -102ps /     -86.54ns /   3097
            # Example: m1_clk_reg2cgate               4.78ns /          0ps /      0
            for clk_timing_match in re.finditer(r'(\w+_clk\w*)\s+([-\d.]+)(ns|ps)\s*/\s*([-\d.]+)(ns|ps)\s*/\s*(\d+)', internal_timing_section.group(1)):
                clock_name = clk_timing_match.group(1)
                wns_val = clk_timing_match.group(2)
                wns_unit = clk_timing_match.group(3)
                tns_val = clk_timing_match.group(4)
                tns_unit = clk_timing_match.group(5)
                viol_paths = clk_timing_match.group(6)
                
                # Store values with unit info for proper comparison
                internal_timing[clock_name] = {
                    'wns': wns_val,
                    'wns_unit': wns_unit,
                    'tns': tns_val,
                    'tns_unit': tns_unit,
                    'viol_paths': viol_paths
                }
        if internal_timing:
            data['internal_timing'] = internal_timing
        
        # Extract Clock Tree Summary (per-clock breakdown)
        clock_tree = {}
        clock_tree_section = re.search(r'CLOCK TREE SUMMARY.*?--+\n(.*?)(?:={50,}|$)', content, re.DOTALL)
        if clock_tree_section:
            lines = clock_tree_section.group(1).split('\n')
            for line in lines:
                # Skip headers and separators
                if 'Clock' in line and 'Tree Cells' in line:
                    continue
                if '---' in line or not line.strip():
                    continue
                
                # Parse clock tree line
                # Format: "i1_clk     | 22 / 1,879 / 2,945 / 4,863          | 38,619 / 39,291      | 16"
                parts = [p.strip() for p in line.split('|') if p.strip()]
                if len(parts) >= 3:
                    clock_name = parts[0].strip()
                    
                    # Parse tree cells (Buf / Inv / CG / Total)
                    tree_cells_parts = [p.strip() for p in parts[1].split('/') if p.strip()]
                    if len(tree_cells_parts) >= 4:
                        clock_tree[clock_name] = {
                            'tree_buf': tree_cells_parts[0].replace(',', ''),
                            'tree_inv': tree_cells_parts[1].replace(',', ''),
                            'tree_cg': tree_cells_parts[2].replace(',', ''),
                            'tree_total': tree_cells_parts[3].replace(',', '')
                        }
                    
                    # Parse sinks (FF / Total)
                    if len(parts) >= 3:
                        sinks_parts = [p.strip() for p in parts[2].split('/') if p.strip()]
                        if len(sinks_parts) >= 2:
                            clock_tree[clock_name]['sinks_ff'] = sinks_parts[0].replace(',', '')
                            clock_tree[clock_name]['sinks_total'] = sinks_parts[1].replace(',', '')
                    
                    # Parse taps (optional)
                    if len(parts) >= 4:
                        taps = parts[3].strip()
                        if taps and taps != 'N/A':
                            clock_tree[clock_name]['taps'] = taps.replace(',', '')
        
        if clock_tree:
            data['clock_tree'] = clock_tree
        
        # Extract Power Summary (per-group breakdown)
        power_groups = {}
        power_section = re.search(r'Power Summary Table:.*?\n(.*?)(?:={50,}|$)', content, re.DOTALL)
        if power_section:
            lines = power_section.group(1).split('\n')
            for line in lines:
                # Skip headers and separators
                if 'group' in line or '===' in line or '---' in line or not line.strip():
                    continue
                
                # Parse power line
                # Format: "combinational |  66747.912 |  994064 | 10883.144 :   0.163 :    0.011 |    0.000 :     0.000 : 0.000 | 10883.144"
                parts = [p.strip() for p in line.split('|') if p.strip()]
                if len(parts) >= 3:
                    group_name = parts[0].strip()
                    
                    # Parse area and count
                    area = parts[1].strip()
                    count = parts[2].strip()
                    
                    # Parse power (first value before colon)
                    power_parts = parts[3].split(':') if len(parts) >= 4 else []
                    power = power_parts[0].strip() if power_parts else '0'
                    
                    # Parse leakage (last column)
                    leakage = parts[-1].strip() if len(parts) >= 5 else '0'
                    
                    power_groups[group_name] = {
                        'area': area,
                        'count': count,
                        'power': power,
                        'leakage': leakage
                    }
        
        if power_groups:
            data['power_summary'] = power_groups
        
        # Extract Flow Configuration Parameters
        flow_config = {}
        flow_config_section = re.search(r'FLOW CONFIGURATION.*?-{50,}\n(.*?)={50,}', content, re.DOTALL)
        if flow_config_section:
            config_content = flow_config_section.group(1)
            
            # Extract BEFLOW_CONFIG_SITE
            beflow_config_match = re.search(r'BEFLOW_CONFIG_SITE\s+\S+\s+(.+?)(?:\n|$)', config_content)
            if beflow_config_match:
                flow_config['beflow_config_site'] = beflow_config_match.group(1).strip()
            
            # Extract FLOW2_CONFIG_SITE
            flow2_config_match = re.search(r'FLOW2_CONFIG_SITE\s+\S+\s+(.+?)(?:\n|$)', config_content)
            if flow2_config_match:
                flow_config['flow2_config_site'] = flow2_config_match.group(1).strip()
            
            # Extract SCAN_INSERTION_SITE
            scan_insertion_match = re.search(r'SCAN_INSERTION_SITE\s+\S+\s+(.+?)(?:\n|$)', config_content)
            if scan_insertion_match:
                flow_config['scan_insertion_site'] = scan_insertion_match.group(1).strip()
            
            # Extract GLCHECK_SITE
            glcheck_match = re.search(r'GLCHECK_SITE\s+\S+\s+(.+?)(?:\n|$)', config_content)
            if glcheck_match:
                flow_config['glcheck_site'] = glcheck_match.group(1).strip()
            
            # Extract FLOW_PATH
            flow_path_match = re.search(r'FLOW_PATH\s+\S+\s+(.+?)(?:\n|$)', config_content)
            if flow_path_match:
                flow_config['flow_path'] = flow_path_match.group(1).strip()
            
            # Extract CUSTOM_SCRIPTS_DIR
            scripts_match = re.search(r'CUSTOM_SCRIPTS_DIR\s+\S+\s+(.+?)(?:\n|$)', config_content)
            if scripts_match:
                flow_config['custom_scripts_dir'] = scripts_match.group(1).strip()
            
            # Extract MULTIBIT_FLOP
            multibit_match = re.search(r'MULTIBIT_FLOP\s+\S+\s+(.+?)(?:\n|$)', config_content)
            if multibit_match:
                flow_config['multibit_flop'] = multibit_match.group(1).strip()
            
            # Extract Library Snapshot
            lib_snap_match = re.search(r'Library Snapshot\s+\S+\s+(.+?)(?:\n|$)', config_content)
            if lib_snap_match:
                flow_config['library_snapshot'] = lib_snap_match.group(1).strip()
            
            # Extract NV Process
            process_match = re.search(r'NV Process\s+\S+\s+(.+?)(?:\n|$)', config_content)
            if process_match:
                flow_config['nv_process'] = process_match.group(1).strip()
            
            # Extract Tracks Number
            tracks_match = re.search(r'Tracks Number\s+\S+\s+(.+?)(?:\n|$)', config_content)
            if tracks_match:
                flow_config['tracks_number'] = tracks_match.group(1).strip()
            
            # Extract Project
            project_match = re.search(r'Project\s+\S+\s+(.+?)(?:\n|$)', config_content)
            if project_match:
                flow_config['project'] = project_match.group(1).strip()
            
            # Extract Default Scenario
            scenario_match = re.search(r'Default Scenario\s+\S+\s+(.+?)(?:\n|$)', config_content)
            if scenario_match:
                flow_config['default_scenario'] = scenario_match.group(1).strip()
            
            # Extract VT Types
            vt_types_match = re.search(r'VT Types\s+\S+\s+(.+?)(?:\n|$)', config_content)
            if vt_types_match:
                flow_config['vt_types'] = vt_types_match.group(1).strip()
        
        if flow_config:
            data['flow_config'] = flow_config
        
        return data
    
    def parse_clock(self) -> dict:
        """
        Parse [5] Clock Analysis section.
        
        Extracts:
        - Best IPO (name, max latency, stage)
        - Per-clock latency metrics (IPO, stage, clock, cycle_time, innovus_median, innovus_max, pt_max, pt_min)
        
        Returns:
            dict: Extracted clock metrics
        """
        data = {}
        
        if 'clock' not in self.sections:
            return data
        
        content = self.sections['clock']['content']
        
        # Extract Best IPO line
        # Example: "Best IPO: ipo1401 (Max Latency: 632.0ps, place)"
        best_ipo_match = re.search(r'Best IPO:\s*(\S+)\s*\(Max Latency:\s*([\d.]+)([a-z]+),\s*(\w+)\)', content)
        if best_ipo_match:
            data['best_ipo'] = best_ipo_match.group(1)
            latency_val = float(best_ipo_match.group(2))
            latency_unit = best_ipo_match.group(3)
            # Convert to ns
            if latency_unit == 'ps':
                latency_val = latency_val / 1000.0
            data['best_ipo_max_latency_ns'] = latency_val
            data['best_ipo_stage'] = best_ipo_match.group(4)
        
        # Parse the detailed clock latency table
        # Columns: IPO, Stage, Clock, Cycle Time, Innovus Median, Innovus Max, PT Max, PT Min
        clock_data = []
        
        # Find the table lines (after header line with "IPO" and "Stage")
        lines = content.split('\n')
        in_table = False
        for line in lines:
            # Skip the header line but detect when we're in the table
            if 'IPO' in line and 'Stage' in line and 'Clock' in line:
                in_table = True
                continue
            
            # Skip separator line
            if '----' in line:
                continue
            
            # Parse data rows
            if in_table and line.strip():
                # End of table if we hit Source Files or Note sections
                if 'Source Files:' in line or 'Note:' in line:
                    in_table = False
                    continue
                
                # Split by whitespace
                parts = line.split()
                
                # We expect at least 4 parts and IPO format like ipo1234 (4+ digits)
                if len(parts) >= 4 and re.match(r'^ipo\d{4,}$', parts[0]):
                    # Validate stage is a known PnR stage
                    valid_stages = ['place', 'route', 'postroute', 'PT-only', 'pre_route', 'pre_place']
                    if parts[1] not in valid_stages:
                        continue
                    
                    # Reconstruct the row more carefully
                    # Handle case where some values are N/A
                    ipo = parts[0]
                    stage = parts[1]
                    clock_name = parts[2]
                    
                    # Helper to parse timing value or N/A
                    def parse_timing_or_na(val):
                        if val == 'N/A':
                            return None
                        # Remove ANSI codes if any remain
                        val = re.sub(r'\x1b\[[0-9;]*m', '', val)
                        # Parse value with ns unit
                        if 'ns' in val:
                            return float(val.replace('ns', '').strip())
                        elif 'ps' in val:
                            return float(val.replace('ps', '').strip()) / 1000.0
                        try:
                            return float(val)
                        except ValueError:
                            return None
                    
                    # Parse remaining fields (may have varying positions due to N/A)
                    entry = {
                        'ipo': ipo,
                        'stage': stage,
                        'clock': clock_name,
                        'cycle_time_ns': None,
                        'innovus_median_ns': None,
                        'innovus_max_ns': None,
                        'pt_max_ns': None,
                        'pt_min_ns': None
                    }
                    
                    # The remaining parts are: cycle_time, innovus_median, innovus_max, pt_max, pt_min
                    remaining = parts[3:]
                    if len(remaining) >= 1:
                        entry['cycle_time_ns'] = parse_timing_or_na(remaining[0])
                    if len(remaining) >= 2:
                        entry['innovus_median_ns'] = parse_timing_or_na(remaining[1])
                    if len(remaining) >= 3:
                        entry['innovus_max_ns'] = parse_timing_or_na(remaining[2])
                    if len(remaining) >= 4:
                        entry['pt_max_ns'] = parse_timing_or_na(remaining[3])
                    if len(remaining) >= 5:
                        entry['pt_min_ns'] = parse_timing_or_na(remaining[4])
                    
                    clock_data.append(entry)
        
        if clock_data:
            data['clock_latency_table'] = clock_data
            
            # Also extract unique clocks and their best/worst latencies
            clock_summary = {}
            for entry in clock_data:
                clock = entry['clock']
                if clock not in clock_summary:
                    clock_summary[clock] = {
                        'max_latency_ns': None,
                        'median_latency_ns': None,
                        'min_latency_ns': None
                    }
                
                # Update max latency (prefer PT Max, fallback to Innovus Max)
                latency = entry['pt_max_ns'] or entry['innovus_max_ns']
                if latency is not None:
                    if clock_summary[clock]['max_latency_ns'] is None or latency > clock_summary[clock]['max_latency_ns']:
                        clock_summary[clock]['max_latency_ns'] = latency
                
                # Update median latency (from Innovus median)
                median_lat = entry['innovus_median_ns']
                if median_lat is not None:
                    if clock_summary[clock]['median_latency_ns'] is None or median_lat > clock_summary[clock]['median_latency_ns']:
                        clock_summary[clock]['median_latency_ns'] = median_lat
                
                # Update min latency (prefer PT min, fallback to Innovus median)
                min_lat = entry['pt_min_ns'] or entry['innovus_median_ns']
                if min_lat is not None:
                    if clock_summary[clock]['min_latency_ns'] is None or min_lat < clock_summary[clock]['min_latency_ns']:
                        clock_summary[clock]['min_latency_ns'] = min_lat
            
            data['clock_summary'] = clock_summary
        
        return data
    
    def _parse_timing_by_pathgroup(self, content: str) -> dict:
        """
        Parse "Timing by Path Group" table from DC section.
        
        Table format:
          Path Group                         Setup WNS     Setup TNS   Setup NVP      Hold WNS      Hold TNS    Hold NVP
          ------------------------------ ------------- ------------- ----------- ------------- ------------- -----------
          i1_clk                              -7.3ps       -480.5ps         433       -34.6ps       -5.649ns         681
          ...
        
        Returns:
            dict: {path_group_name: {'setup_wns': float, 'setup_tns': float, 'setup_nvp': int,
                                     'hold_wns': float, 'hold_tns': float, 'hold_nvp': int}}
        """
        timing_data = {}
        
        # Find the "Timing by Path Group" section
        timing_section_match = re.search(r'Timing by Path Group:.*?\n(.*?)(?=\n\s*\n|\Z)', content, re.DOTALL)
        if not timing_section_match:
            return timing_data
        
        timing_section = timing_section_match.group(1)
        
        # Parse each data line
        lines = timing_section.split('\n')
        for line in lines:
            # Skip headers, separators, and empty lines
            if not line.strip() or '---' in line or 'Path Group' in line or 'Setup WNS' in line:
                continue
            
            # Parse path group line
            # Format: "i1_clk                              -7.3ps       -480.5ps         433       -34.6ps       -5.649ns         681"
            # We need to extract: path_group, setup_wns, setup_tns, setup_nvp, hold_wns, hold_tns, hold_nvp
            
            # Split by whitespace and filter empty strings
            parts = [p.strip() for p in line.split() if p.strip()]
            
            if len(parts) >= 7:
                path_group = parts[0]
                setup_wns = self._parse_timing_value(parts[1])
                setup_tns = self._parse_timing_value(parts[2])
                setup_nvp = self._parse_int_value(parts[3])
                hold_wns = self._parse_timing_value(parts[4])
                hold_tns = self._parse_timing_value(parts[5])
                hold_nvp = self._parse_int_value(parts[6])
                
                timing_data[path_group] = {
                    'setup_wns': setup_wns,
                    'setup_tns': setup_tns,
                    'setup_nvp': setup_nvp,
                    'hold_wns': hold_wns,
                    'hold_tns': hold_tns,
                    'hold_nvp': hold_nvp
                }
        
        return timing_data
    
    def _parse_timing_value(self, value_str: str) -> float:
        """
        Parse timing value and convert to nanoseconds.
        
        Handles:
        - "0ns" or "0" -> 0.0
        - "-7.3ps" -> -0.0073
        - "348.2ps" -> 0.3482
        - "-5.649ns" -> -5.649
        - "16.186ns" -> 16.186
        
        Args:
            value_str: Timing value string with unit
            
        Returns:
            float: Value in nanoseconds
        """
        if not value_str or value_str == '0':
            return 0.0
        
        # Remove commas
        value_str = value_str.replace(',', '')
        
        # Check for ps (picoseconds) - convert to ns
        if 'ps' in value_str:
            value = float(value_str.replace('ps', '').strip())
            return value / 1000.0  # ps to ns
        
        # Check for ns (nanoseconds)
        if 'ns' in value_str:
            value = float(value_str.replace('ns', '').strip())
            return value
        
        # Default: assume ns if no unit
        try:
            return float(value_str)
        except ValueError:
            return 0.0
    
    def _parse_int_value(self, value_str: str) -> int:
        """
        Parse integer value (handles commas).
        
        Args:
            value_str: Integer string (e.g., "1,234" or "0")
            
        Returns:
            int: Parsed integer value
        """
        if not value_str or value_str == '0':
            return 0
        
        # Remove commas and parse
        value_str = value_str.replace(',', '')
        try:
            return int(value_str)
        except ValueError:
            return 0
    
    def parse_all(self, section_filter: list = None) -> dict:
        """
        Parse all sections (or filtered sections).
        
        Args:
            section_filter: List of section names to parse (e.g., ['setup', 'pnr'])
                          If None, parses all available sections.
        
        Returns:
            dict: {section_name: parsed_metrics}
        """
        data = {}
        
        # Determine which sections to parse
        sections_to_parse = section_filter if section_filter else list(self.sections.keys())
        
        # Parse each section using its dedicated parser
        parser_map = {
            'setup': self.parse_setup,
            'runtime': self.parse_runtime,
            'synthesis': self.parse_synthesis,
            'pnr': self.parse_pnr,
            'clock': self.parse_clock,
            # Future parsers will be added here:
            # etc.
        }
        
        for section_name in sections_to_parse:
            if section_name in parser_map:
                data[section_name] = parser_map[section_name]()
            else:
                # Section parser not yet implemented
                data[section_name] = {'status': 'parser_not_implemented'}
        
        return data


def main():
    """Main function"""
    
    # Color codes for help text
    CYAN = '\033[36m'
    GREEN = '\033[32m'
    YELLOW = '\033[33m'
    BOLD = '\033[1m'
    RESET = '\033[0m'
    
    parser = argparse.ArgumentParser(
        description="Avice Workarea Review Tool - Comprehensive ASIC/SoC design flow analysis",
        epilog=f"""
{CYAN}===============================================================================
                       AVICE WORKAREA REVIEW TOOL
==============================================================================={RESET}

{GREEN}{BOLD}SINGLE WORKAREA ANALYSIS:{RESET}
  # Full analysis (all sections)
  wa1 /path/to/workarea

  # Analyze specific IPO
  wa1 /path/to/workarea -i ipo1000

  # AGUR unit lookup (auto finds workarea)
  wa1 -u prt
  wa1 -u pmux -s runtime pt

  # Quick timing check
  wa1 /path/to/workarea -s runtime pt

  # Pre-release checks
  wa1 /path/to/workarea -s formal pt pv gl-check

{GREEN}{BOLD}WORKAREA COMPARISON:{RESET}
  # Compare two workareas (Excel report emailed)
  wa1 -r /path/to/ref_wa -t /path/to/test_wa -e

  # Compare specific IPOs
  wa1 -r /path/to/ref_wa -t /path/to/test_wa -i ipo1093,ipo1200 -e

  # Compare specific sections
  wa1 -r /path/to/ref_wa -t /path/to/test_wa -s pnr clock -e

{GREEN}{BOLD}OTHER:{RESET}
  wa1 --help-docs          # Detailed documentation
  wa1 --open-docs          # Open docs in browser
  wa1 /path/to/wa -q       # Quiet mode (batch processing)

{CYAN}-------------------------------------------------------------------------------
AVAILABLE SECTIONS (use with -s flag):
-------------------------------------------------------------------------------{RESET}
  setup          Environment, BeFlow config, PRC configuration
  runtime        Flow runtimes (DC, PnR, Star, PT, Formal, PV, GL Check)
  synthesis      QoR reports, floorplan, timing groups (alias: syn, dc)
  pnr            Place & route analysis, timing histograms
  clock          Clock tree analysis, DSR latency
  formal         Formal verification status, timestamp tracking
  star           Parasitic extraction (SPEF) status (alias: parasitic)
  pt             Signoff timing, WNS/TNS/NVP, DSR skew (alias: timing)
  pv             Physical verification (LVS/DRC/Antenna)
  gl-check       Gate-level check error analysis
  eco            PT-ECO analysis and dont_use cell checks
  nv-gate-eco    NVIDIA Gate ECO command analysis
  block-release  Block release information and umake commands (alias: release)

{CYAN}-------------------------------------------------------------------------------
KEY FEATURES:
-------------------------------------------------------------------------------{RESET}
  - Multi-IPO support with auto-detection (-i/--ipo for manual selection)
  - Dual-scenario timing analysis (setup/hold) with DSR skew tracking
  - Formal verification timestamp tracking vs design changes
  - Interactive HTML reports with portable absolute paths
  - Timeline visualizations for all flow stages
  - Workarea comparison with Excel reports

{GREEN}Contact: avice@nvidia.com{RESET}
{CYAN}==============================================================================={RESET}
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument("workarea", nargs="*", 
                       help="Path to workarea directory(ies) to analyze (multiple paths supported for --compare-ipos, or omit when using --unit)")
    
    # Documentation generation arguments (user-facing)
    parser.add_argument("--help-docs", action="store_true",
                       help="Display detailed documentation in terminal")
    parser.add_argument("--open-docs", action="store_true",
                       help="Open HTML documentation in browser")
    
    # Hidden/Advanced flags (use argparse.SUPPRESS to hide from help)
    parser.add_argument("--generate-pdf", action="store_true",
                       help=argparse.SUPPRESS)  # Hidden: rarely used
    parser.add_argument("--docs-section", choices=["usage", "examples", "troubleshooting", "organization", "all"],
                       default="all", help=argparse.SUPPRESS)  # Hidden: rarely used
    parser.add_argument("--send-disk-alert", action="store_true",
                       help=argparse.SUPPRESS)  # Hidden: internal use
    parser.add_argument("--verbose", "-v", action="store_true", 
                       help=argparse.SUPPRESS)  # Hidden: rarely used
    parser.add_argument("--version", action="version", version="%(prog)s 2.0.0")  # Hidden from help but still works
    parser.add_argument("--no-logo", action="store_true",
                       help=argparse.SUPPRESS)  # Hidden: for automation
    parser.add_argument("--skip-validation", action="store_true",
                       help=argparse.SUPPRESS)  # Hidden: dangerous
    
    # User-facing output control
    parser.add_argument("--quiet", "-q", action="store_true",
                       help="Suppress terminal output (for batch processing)")
    parser.add_argument("--unit", "-u", type=str,
                       help="Unit name from agur release table (e.g., prt, pmux). Automatically looks up released workarea path from AGUR_UNITS_TABLE.txt")
    parser.add_argument("--ipo", "-i", type=str, dest="ipo_select",
                       help="Select IPO to analyze (e.g., -i ipo1000). "
                            "For comparison: -i ref_ipo,test_ipo (e.g., -i ipo1093,ipo1200)")
    parser.add_argument("--sections", "-s", nargs="+", 
                       type=str.lower,
                       metavar="SECTION",
                       choices=["setup", "runtime", "synthesis", "syn", "dc", "pnr", 
                               "clock", "formal", "star", "pt", "pv", 
                               "gl-check", "eco", "nv-gate-eco", 
                               "block-release", "release"],
                       help="Run only specific analysis sections (case-insensitive). Choices: setup, runtime, synthesis (syn/dc), pnr, clock, formal, star, pt, pv, gl-check, eco, nv-gate-eco, block-release (release). Aliases: syn/dc=synthesis, release=block-release")
    # Hidden advanced flags
    parser.add_argument("--output", "-o", help=argparse.SUPPRESS)  # Hidden: rarely used
    parser.add_argument("--format", choices=["text", "json"], default="text",
                       help=argparse.SUPPRESS)  # Hidden: rarely used
    parser.add_argument("--compare-ipos", action="store_true",
                       help=argparse.SUPPRESS)  # Hidden: legacy mode
    
    # Workarea Comparison Arguments
    parser.add_argument("--email", "-e", type=str, nargs='?', const='auto',
                       help="Email Excel report (default: $USER@nvidia.com)")
    parser.add_argument("--ref", "-r", type=str,
                       help="Reference workarea (baseline for comparison)")
    parser.add_argument("--test", "-t", type=str,
                       help="Test workarea (compare against reference)")
    
    args = parser.parse_args()
    
    # Comparison mode: auto-detect when both --ref and --test are provided
    args.compare = bool(args.ref and args.test)
    
    # Handle documentation generation (doesn't require workarea)
    if args.help_docs or args.open_docs or args.generate_pdf:
        sys.path.append(os.path.dirname(os.path.abspath(__file__)))
        from docs_generator import DocumentationGenerator
        doc_gen = DocumentationGenerator()
        
        if args.help_docs:
            doc_gen.display_terminal_docs(args.docs_section)
        elif args.open_docs:
            doc_gen.generate_and_open_html_docs()
        elif args.generate_pdf:
            doc_gen.generate_pdf_docs()
        return
    
    # Handle --unit flag: look up workarea from release table
    if args.unit:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        release_tracking_dir = os.path.join(script_dir, "agur_release_tracking")
        table_file = os.path.join(release_tracking_dir, "AGUR_UNITS_TABLE.txt")
        update_checker = os.path.join(release_tracking_dir, "check_and_update_agur_table.sh")
        
        # Auto-check and update table if needed
        if os.path.exists(update_checker) and os.access(update_checker, os.X_OK):
            try:
                # Run the checker quietly to auto-update if needed
                result = subprocess.run([update_checker, "--quiet"], 
                                      stdout=subprocess.PIPE,
                                      stderr=subprocess.PIPE,
                                      timeout=60)
                # Return code 2 means first-time setup completed
                if result.returncode == 2:
                    print(f"{Color.CYAN}[INFO] First-time setup: Generated AGUR units table{Color.RESET}")
                elif result.returncode == 1:
                    print(f"{Color.YELLOW}[WARN] Failed to auto-update release table{Color.RESET}")
                # Return code 0 means table is up-to-date or successfully updated
            except subprocess.TimeoutExpired:
                print(f"{Color.YELLOW}[WARN] Table update check timed out{Color.RESET}")
            except Exception as e:
                print(f"{Color.YELLOW}[WARN] Could not check for table updates: {e}{Color.RESET}")
        
        if not os.path.exists(table_file):
            print(f"{Color.RED}[ERROR] AGUR_UNITS_TABLE.txt not found at: {table_file}{Color.RESET}")
            print(f"{Color.YELLOW}[HINT] Run: cd agur_release_tracking && ./extract_agur_releases.sh{Color.RESET}")
            sys.exit(1)
        
        # Look up unit in table
        workarea_found = False
        try:
            with open(table_file, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        parts = [p.strip() for p in line.split('|')]
                        if len(parts) >= 3 and parts[0].lower() == args.unit.lower():
                            args.workarea = [parts[2]]  # Third column is the workarea path (wrap in list for nargs="+")
                            workarea_found = True
                            
                            # Display unit information
                            print(f"{Color.CYAN}[INFO] Unit '{args.unit}' found in release table{Color.RESET}")
                            
                            # Display chiplet if available
                            if len(parts) >= 2 and parts[1]:
                                print(f"{Color.CYAN}[INFO] Chiplet: {parts[1]}{Color.RESET}")
                            
                            # Display RTL tag if available
                            if len(parts) >= 4 and parts[3]:
                                print(f"{Color.CYAN}[INFO] RTL Tag: {parts[3]}{Color.RESET}")
                            
                            # Display release date if available
                            if len(parts) >= 6 and parts[5]:
                                print(f"{Color.CYAN}[INFO] Release Date: {parts[5]}{Color.RESET}")
                            
                            # Display workarea path (user workarea where work was done)
                            print(f"{Color.CYAN}[INFO] User Workarea: {args.workarea[0]}{Color.RESET}")
                            
                            # Display central release path if available (8th field)
                            if len(parts) >= 8 and parts[7]:
                                print(f"{Color.CYAN}[INFO] Central Release: {parts[7]}{Color.RESET}")
                            break
            
            if not workarea_found:
                print(f"{Color.RED}[ERROR] Unit '{args.unit}' not found in AGUR_UNITS_TABLE.txt{Color.RESET}")
                print(f"{Color.YELLOW}[HINT] Available units:{Color.RESET}")
                with open(table_file, 'r') as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#'):
                            parts = [p.strip() for p in line.split('|')]
                            if len(parts) >= 1:
                                print(f"  - {parts[0]}")
                sys.exit(1)
        except Exception as e:
            print(f"{Color.RED}[ERROR] Failed to read release table: {e}{Color.RESET}")
            sys.exit(1)
    
    # Check if workarea is provided (skip for --compare mode with --ref/--test)
    if not args.workarea and not args.compare:
        parser.error("workarea is required (or use --unit to specify a unit name)")
    
    # Validate all workarea paths (if provided)
    if args.workarea:
        for wa_path in args.workarea:
            if not os.path.isdir(wa_path):
                print(f"{Color.RED}Error: Workarea directory '{wa_path}' does not exist{Color.RESET}")
                sys.exit(1)
    
    # IPO comparison mode (standalone)
    if args.compare_ipos:
        try:
            # Determine email recipient
            email = None
            if args.email:
                if args.email == 'auto':
                    email = f"{os.environ.get('USER', 'avice')}@nvidia.com"
                else:
                    email = args.email
            
            # Check if single or multiple workareas
            if len(args.workarea) == 1:
                # Single workarea comparison (existing logic)
                reviewer = WorkareaReviewer(args.workarea[0], args.ipo_select, show_logo=not args.no_logo, skip_validation=args.skip_validation, quiet=args.quiet)
                reviewer.analyze_ipo_comparison(email=email)
            else:
                # Multi-workarea comparison (new logic)
                WorkareaReviewer.analyze_multi_workarea_ipo_comparison(
                    workarea_paths=args.workarea,
                    show_logo=not args.no_logo,
                    quiet=args.quiet,
                    email=email
                )
            sys.exit(0)
        except KeyboardInterrupt:
            print(f"\n{Color.YELLOW}IPO comparison interrupted by user{Color.RESET}")
            sys.exit(1)
        except Exception as e:
            print(f"{Color.RED}Error during IPO comparison: {e}{Color.RESET}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
    
    # Handle workarea comparison mode (New Architecture)
    if args.compare:
        try:
            # Display logo
            if not args.no_logo:
                LogoDisplay.print_ascii_logo()
                LogoDisplay.display_logo()
            
            print(f"\n{Color.CYAN}{'=' * 80}{Color.RESET}")
            print(f"{Color.CYAN}WORKAREA COMPARISON MODE{Color.RESET}")
            print(f"{Color.CYAN}{'=' * 80}{Color.RESET}\n")
            
            # Validate that both --ref and --test are provided
            if not args.ref or not args.test:
                print(f"{Color.RED}[ERROR] Both --ref and --test arguments are required for comparison{Color.RESET}")
                print("\nUsage:")
                print(f"  {sys.argv[0]} -r /path/to/reference -t /path/to/test")
                print(f"  {sys.argv[0]} --ref /path/to/ref --test /path/to/test -s setup synthesis")
                print(f"  {sys.argv[0]} -c -r /ref -t /test --email")
                print(f"\nNote: --compare (-c) is optional when both --ref (-r) and --test (-t) are provided")
                sys.exit(1)
            
            ref_wa = args.ref
            test_wa = args.test
            
            # Validate workareas exist
            if not os.path.exists(ref_wa):
                print(f"{Color.RED}[ERROR] Reference workarea not found: {ref_wa}{Color.RESET}")
                sys.exit(1)
            if not os.path.exists(test_wa):
                print(f"{Color.RED}[ERROR] Test workarea not found: {test_wa}{Color.RESET}")
                sys.exit(1)
            
            # Parse IPO selection for comparison mode
            ref_ipo = None
            test_ipo = None
            if args.ipo_select:
                if ',' in args.ipo_select:
                    # Comparison mode: ref_ipo,test_ipo
                    ipo_parts = args.ipo_select.split(',')
                    if len(ipo_parts) != 2:
                        print(f"{Color.RED}[ERROR] Comparison mode requires two IPOs: -i ref_ipo,test_ipo{Color.RESET}")
                        sys.exit(1)
                    ref_ipo, test_ipo = ipo_parts[0].strip(), ipo_parts[1].strip()
                else:
                    # Single IPO specified - use for both
                    print(f"{Color.YELLOW}[WARN] Single IPO specified for comparison. Use -i ref_ipo,test_ipo format{Color.RESET}")
                    ref_ipo = test_ipo = args.ipo_select.strip()
            
            # Display comparison header with IPO info
            print(f"{Color.CYAN}Reference:{Color.RESET} {ref_wa}")
            if ref_ipo:
                print(f"  {Color.YELLOW}IPO: {ref_ipo}{Color.RESET}")
            print(f"{Color.CYAN}Test:{Color.RESET} {test_wa}")
            if test_ipo:
                print(f"  {Color.YELLOW}IPO: {test_ipo}{Color.RESET}")
            print()
            
            # Determine which sections to compare
            sections_to_compare = args.sections if args.sections else ['setup', 'runtime', 'synthesis', 'pnr', 'clock']  # Default: Setup + Runtime + DC + PnR + Clock
            
            # Normalize section names (handle aliases)
            normalized_sections = []
            for section in sections_to_compare:
                if section in ['syn', 'dc']:
                    normalized_sections.append('synthesis')
                elif section == 'release':
                    normalized_sections.append('block-release')
                else:
                    normalized_sections.append(section)
            
            print(f"{Color.CYAN}Comparing sections:{Color.RESET} {', '.join(normalized_sections)}\n")
            
            # Phase 1-4: Support 'setup', 'runtime', 'synthesis', and 'pnr' sections
            supported = ['setup', 'runtime', 'synthesis', 'pnr', 'clock']
            unsupported = [s for s in normalized_sections if s not in supported]
            if unsupported:
                print(f"{Color.YELLOW}[WARN] Phase 1-4 supports: {', '.join(supported)}{Color.RESET}")
                print(f"{Color.YELLOW}[WARN] Unsupported sections will be skipped: {', '.join(unsupported)}{Color.RESET}\n")
                normalized_sections = [s for s in normalized_sections if s in supported]
            
            if not normalized_sections:
                print(f"{Color.RED}[ERROR] No supported sections specified{Color.RESET}")
                sys.exit(1)
            
            # ================================================================
            # STEP 1: Run avice_wa_review.py on REFERENCE workarea
            # ================================================================
            print(f"{Color.CYAN}[1/4] Analyzing reference workarea...{Color.RESET}")
            
            python_bin = sys.executable
            script_path = os.path.abspath(__file__)
            
            ref_cmd = [
                python_bin, script_path,
                ref_wa,
                '--no-logo',
                '-s'
            ] + normalized_sections
            
            # Add IPO selection if specified
            if ref_ipo:
                ref_cmd.extend(['-i', ref_ipo])
            
            print(f"  Running: {' '.join(ref_cmd)}")
            
            ref_result = subprocess.run(
                ref_cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                universal_newlines=True,
                timeout=600  # 10 minute timeout
            )
            
            if ref_result.returncode != 0:
                print(f"{Color.RED}[ERROR] Reference analysis failed{Color.RESET}")
                print(ref_result.stderr)
                sys.exit(1)
            
            ref_output = ref_result.stdout
            print(f"{Color.GREEN}[OK] Reference analysis complete{Color.RESET}\n")
            
            # ================================================================
            # STEP 2: Run avice_wa_review.py on TEST workarea
            # ================================================================
            print(f"{Color.CYAN}[2/4] Analyzing test workarea...{Color.RESET}")
            
            test_cmd = [
                python_bin, script_path,
                test_wa,
                '--no-logo',
                '-s'
            ] + normalized_sections
            
            # Add IPO selection if specified
            if test_ipo:
                test_cmd.extend(['-i', test_ipo])
            
            print(f"  Running: {' '.join(test_cmd)}")
            
            test_result = subprocess.run(
                test_cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                universal_newlines=True,
                timeout=600  # 10 minute timeout
            )
            
            if test_result.returncode != 0:
                print(f"{Color.RED}[ERROR] Test analysis failed{Color.RESET}")
                print(test_result.stderr)
                sys.exit(1)
            
            test_output = test_result.stdout
            print(f"{Color.GREEN}[OK] Test analysis complete{Color.RESET}\n")
            
            # ================================================================
            # STEP 3: Parse outputs
            # ================================================================
            print(f"{Color.CYAN}[3/4] Parsing terminal outputs...{Color.RESET}")
            
            # Pass workarea paths for per-stage PnR data extraction
            ref_parser = OutputParser(ref_output, workarea=ref_wa)
            test_parser = OutputParser(test_output, workarea=test_wa)
            
            ref_data = ref_parser.parse_all(normalized_sections)
            test_data = test_parser.parse_all(normalized_sections)
            
            # Fallback: Extract owner from workarea path if not found in output
            if 'setup' in ref_data and 'owner' not in ref_data['setup']:
                owner_match = re.search(r'/scratch\.([^/_]+)', ref_wa)
                if owner_match:
                    ref_data['setup']['owner'] = owner_match.group(1)
            
            if 'setup' in test_data and 'owner' not in test_data['setup']:
                owner_match = re.search(r'/scratch\.([^/_]+)', test_wa)
                if owner_match:
                    test_data['setup']['owner'] = owner_match.group(1)
            
            print(f"  Reference metrics: {sum(len(v) for v in ref_data.values() if isinstance(v, dict))}")
            print(f"  Test metrics: {sum(len(v) for v in test_data.values() if isinstance(v, dict))}")
            print(f"{Color.GREEN}[OK] Parsing complete{Color.RESET}\n")
            
            # ================================================================
            # STEP 4: Generate Excel report
            # ================================================================
            print(f"{Color.CYAN}[4/4] Generating Excel report...{Color.RESET}")
            
            # Generate filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            user = os.environ.get('USER', 'avice')
            
            # Extract unit name from reference data
            unit_name = ref_data.get('setup', {}).get('unit', 'workarea')
            
            excel_filename = f"{user}_comparison_{unit_name}_ref_vs_test_{timestamp}.xlsx"
            
            # Generate Excel report
            excel_path = generate_comparison_excel(ref_data, test_data, ref_wa, test_wa, excel_filename, normalized_sections)
            
            if not excel_path:
                print(f"{Color.RED}[ERROR] Excel generation failed{Color.RESET}")
                sys.exit(1)
            
            print(f"\n{Color.GREEN}[OK] Comparison complete!{Color.RESET}\n")
            print(f"Excel Report: {excel_path}\n")
            
            # Email the Excel report
            if args.email:
                email_addr = args.email if args.email != 'auto' else f"{os.environ.get('USER', 'avice')}@nvidia.com"
                print(f"{Color.CYAN}Sending Excel report to {email_addr}...{Color.RESET}")
                
                try:
                    import smtplib
                    from email.mime.multipart import MIMEMultipart
                    from email.mime.text import MIMEText
                    from email.mime.base import MIMEBase
                    from email import encoders
                    
                    # Create message
                    msg = MIMEMultipart()
                    msg['From'] = f"{os.environ.get('USER', 'avice')}@nvidia.com"
                    msg['To'] = email_addr
                    msg['Subject'] = f"Workarea Comparison Report: {unit_name} (Reference vs Test)"
                    
                    # Email body
                    body = f"""
Workarea Comparison Report

Reference: {ref_wa}
Test: {test_wa}

Sections Compared: {', '.join(normalized_sections)}

Metrics Summary:
- Reference metrics extracted: {sum(len(v) for v in ref_data.values() if isinstance(v, dict))}
- Test metrics extracted: {sum(len(v) for v in test_data.values() if isinstance(v, dict))}

Please find the detailed comparison report attached.

---
Generated by avice_wa_review.py (Workarea Comparison Mode)
Contact: {os.environ.get('USER', 'avice')}@nvidia.com
                    """
                    
                    msg.attach(MIMEText(body, 'plain'))
                    
                    # Attach Excel file
                    with open(excel_path, 'rb') as f:
                        part = MIMEBase('application', 'octet-stream')
                        part.set_payload(f.read())
                        encoders.encode_base64(part)
                        part.add_header('Content-Disposition', f'attachment; filename={os.path.basename(excel_path)}')
                        msg.attach(part)
                    
                    # Send email via SMTP
                    smtp_server = "smtp.nvidia.com"
                    smtp_port = 25
                    
                    with smtplib.SMTP(smtp_server, smtp_port, timeout=30) as server:
                        server.sendmail(msg['From'], [email_addr], msg.as_string())
                    
                    print(f"{Color.GREEN}[OK]{Color.RESET} Email sent successfully to {email_addr}")
                    
                except ImportError:
                    print(f"{Color.RED}[ERROR]{Color.RESET} Email libraries not available (smtplib/email)")
                except Exception as e:
                    print(f"{Color.RED}[ERROR]{Color.RESET} Failed to send email: {e}")
                    import traceback
                    traceback.print_exc()
            
            sys.exit(0)
            
        except KeyboardInterrupt:
            print(f"\n{Color.YELLOW}Workarea comparison interrupted by user{Color.RESET}")
            sys.exit(1)
        except subprocess.TimeoutExpired:
            print(f"{Color.RED}[ERROR] Analysis timed out (>10 minutes){Color.RESET}")
            sys.exit(1)
        except Exception as e:
            print(f"{Color.RED}[ERROR] Comparison failed: {e}{Color.RESET}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
    
    # Regular workarea analysis mode
    try:
        # For regular analysis mode, only support single workarea
        if len(args.workarea) > 1:
            print(f"{Color.RED}Error: Multiple workareas only supported with --compare-ipos flag{Color.RESET}")
            sys.exit(1)
        
        # Create QuietMode context manager
        quiet_mode = QuietMode(enabled=args.quiet)
        
        reviewer = WorkareaReviewer(args.workarea[0], args.ipo_select, show_logo=not args.no_logo, skip_validation=args.skip_validation, quiet=args.quiet, quiet_mode=quiet_mode)
        
        # Cleanup old HTML files from previous runs to avoid confusion
        reviewer._cleanup_old_html_files()
        
        # Execute analysis within QuietMode context
        with quiet_mode:
            # Handle selective section running
            if args.sections:
                # Run only specified sections
                section_mapping = {
                    "setup": reviewer.run_setup_analysis,
                    "runtime": reviewer.run_runtime_analysis,
                    "synthesis": reviewer.run_synthesis_analysis,
                    "syn": reviewer.run_synthesis_analysis,  # Alias for synthesis
                    "dc": reviewer.run_synthesis_analysis,   # Alias for synthesis (Design Compiler)
                    "pnr": reviewer.run_pnr_analysis,
                    "clock": reviewer.run_clock_analysis,
                    "formal": reviewer.run_formal_verification,
                    "star": reviewer.run_parasitic_extraction,
                    "pt": reviewer.run_signoff_timing,
                    "pv": reviewer.run_physical_verification,
                    "gl-check": reviewer.run_gl_check,
                    "eco": reviewer.run_eco_analysis,
                    "nv-gate-eco": reviewer.run_nv_gate_eco,
                    "block-release": reviewer.run_block_release,
                    "release": reviewer.run_block_release  # Alias for block-release
                }
                
                print(f"Running selected sections: {', '.join(args.sections)}")
                for section in args.sections:
                    # Convert to lowercase for case-insensitive matching
                    section_lower = section.lower()
                    if section_lower in section_mapping:
                        try:
                            section_mapping[section_lower]()
                        except Exception as e:
                            print(f"Error running {section} section: {e}")
                    else:
                        print(f"Unknown section: {section}")
                
                # Generate Master Dashboard only if multiple sections were run
                # (Single section doesn't need a dashboard)
                if len(args.sections) > 1:
                    quiet_mode.print_always(f"\n{Color.CYAN}Generating Master Dashboard...{Color.RESET}")
                    try:
                        dashboard_path = reviewer.master_dashboard.generate_html()
                        dashboard_filename = os.path.basename(dashboard_path)
                        
                        # Determine display path (will be moved to html/ or test_outputs/html/ by _organize_html_files)
                        html_output_dir = reviewer._get_html_output_dir()
                        display_path = os.path.relpath(html_output_dir, os.getcwd())
                        
                        quiet_mode.print_always(f"Open with: /home/utils/firefox-118.0.1/firefox {Color.MAGENTA}{display_path}/{dashboard_filename}{Color.RESET} &")
                    except Exception as e:
                        quiet_mode.print_always(f"{Color.RED}[ERROR] Failed to generate Master Dashboard: {e}{Color.RESET}")
            else:
                # Run complete review
                reviewer.run_complete_review()
            
            # Organize HTML files into html/ folder
            reviewer._organize_html_files()
            
        # Handle output file
        if args.output:
            print(f"\nResults saved to: {args.output}")
            # Note: In a full implementation, you would capture the output and write to file
            
    except KeyboardInterrupt:
        print(f"\n{Color.YELLOW}Review interrupted by user{Color.RESET}")
        sys.exit(1)
    except Exception as e:
        print(f"{Color.RED}Error during review: {e}{Color.RESET}")
        sys.exit(1)


if __name__ == "__main__":
    main()

